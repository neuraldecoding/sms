TY  - CONF
ID  - Battula2025Latentneuronet
AU  - Battula, Shreyas
AU  - Kirithivasan, Shyam Krishna
AU  - Soori, Aditi
AU  - Ramesh, Richa
AU  - Srinath, Ramamoorthy
TI  - LatentNeuroNet: A Text-Conditioned Stable Diffusion Framework for Reconstructing Visual Stimuli from fMRI
PY  - 2025
JO  - Communications in Computer and Information Science
T2  - Communications in Computer and Information Science
VL  - 2194 CCIS
SP  - 225 - 235
DO  - 10.1007/978-3-031-70906-7_19
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208060558&doi=10.1007%2F978-3-031-70906-7_19&partnerID=40&md5=ea6463f1ed2c398fb520530f9bc270f5
AB  - The human brain, among the most complex and mysterious aspects of the body, harbours vast potential for extensive exploration. Unravelling these enigmas, especially within neural perception and cognition, delves into the realm of neural decoding. Harnessing advancements in generative AI, particularly in the Image Processing domain, seeks to elucidate how the brain comprehends visual stimuli perceived by humans. The paper endeavours to reconstruct human-perceived visual stimuli using Functional Magnetic Resonance Imaging (fMRI). This fMRI data is then processed through pre-trained deep-learning models to recreate the stimuli. Introducing a new architecture named LatentNeuroNet, the aim is to achieve the utmost semantic fidelity in stimuli reconstruction. The approach employs a Latent Diffusion Model (LDM), emphasizing semantic accuracy and generating superior-quality outputs. Text conditioning within the LDM’s denoising process is handled by extracting text from the brain’s ventral visual cortex region. This extracted text undergoes processing through a Bootstrapping Language-Image Pre-training (BLIP) encoder before it is injected into the denoising process. In conclusion, a successful architecture is developed that reconstructs the visual stimuli perceived and finally, this research provides us with enough evidence to identify the most substantial regions of the brain responsible for perception.
KW  - Brain
KW  - Deep learning
KW  - Functional neuroimaging
KW  - Image coding
KW  - Image denoising
KW  - Magnetic resonance imaging
KW  - Neurons
KW  - Semantic Segmentation
KW  - Visual languages
KW  - Bootstrapping language-image pre-training
KW  - De-noising
KW  - Diffusion model
KW  - Extensive explorations
KW  - Functional magnetic resonance imaging
KW  - Human brain
KW  - Latent diffusion model
KW  - Neural perception
KW  - Pre-training
KW  - Visual stimulus
KW  - Semantics
ER  - 

TY  - JOUR
ID  - Chen2025Mindgpt
AU  - Chen, Jiaxuan
AU  - Qi, Yu
AU  - Wang, Yueming
AU  - Pan, Gang
TI  - MindGPT: Interpreting What You See With Non-Invasive Brain Recordings
PY  - 2025
JO  - IEEE Transactions on Image Processing
T2  - IEEE Transactions on Image Processing
VL  - 34
SP  - 3281-3293
DO  - 10.1109/TIP.2025.3572784
AB  - Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed MindGPT, which interprets perceived visual stimuli into natural languages from functional Magnetic Resonance Imaging (fMRI) signals in an end-to-end manner. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism. By the collaborative use of data augmentation techniques, this architecture permits us to guide latent neural representations towards a desired language semantic direction in a self-supervised fashion. Through doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. The source code for the MindGPT model is publicly available at https://github.com/JxuanC/MindGPT.
KW  - Visualization
KW  - Semantics
KW  - Functional magnetic resonance imaging
KW  - Image reconstruction
KW  - Decoding
KW  - Brain modeling
KW  - Recording
KW  - Training
KW  - Data augmentation
KW  - Predictive models
KW  - Neural decoding
KW  - functional magnetic resonance imaging
KW  - text reconstruction
KW  - self-supervised learning
KW  - multimodal representation learning
SN  - 1941-0042
ER  - 

TY  - CONF
ID  - Choi2025International
AU  - Choi, Yebin
AU  - Kim, Jun-Mo
AU  - Choi, WooHyeok
AU  - Ji, Chang-Hoon
AU  - Oh, Ji-Hye
AU  - Kam, Tae-Eui
TI  - Visual Decoding Using a Learnable Wavelet-Based Spatial-Spectral-Temporal EEG Embedding
PY  - 2025
BT  - 2025 13th International Conference on Brain-Computer Interface (BCI)
T2  - 2025 13th International Conference on Brain-Computer Interface (BCI)
SP  - 1-5
DO  - 10.1109/BCI65088.2025.10931751
AB  - Visual decoding seeks to identify or reconstruct visual stimuli perceived by individuals based on neural activity. Although functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have achieved remarkable success in visual decoding, their high costs, limited portability, and real-time processing challenges necessitate alternative approaches. Electroencephalography (EEG) provides a promising solution due to its cost-effectiveness, high temporal resolution, and suitability for real-time applications. However, conventional EEG-based encoders often rely on simplistic architectures, which limits their ability to fully capture the spatial, spectral, and temporal (SST) features of EEG signals, leading to suboptimal performance. In this study, we propose a novel brain decoding framework utilizing a state-of-the-art EEG encoder specifically designed to capture the SST characteristics of EEG signals. The proposed framework was evaluated on the THINGS-EEG dataset. It achieved a mean top-l accuracy of 19.7% and a top-5 accuracy of 50.7% in zero-shot retrieval tasks, outperforming conventional EEG encoders. These results demonstrate the potential of our method in advancing EEG-based visual decoding task.
KW  - Visualization
KW  - Accuracy
KW  - Functional magnetic resonance imaging
KW  - Feature extraction
KW  - Electroencephalography
KW  - Real-time systems
KW  - Decoding
KW  - Spatial resolution
KW  - Image reconstruction
KW  - Visual perception
KW  - Brain Decoding
KW  - Visual Decoding
KW  - Elec-troencephalogram
KW  - Learnable Wavelet Kernel
KW  - Spectral-spatial-temporal Representation
SN  - 2572-7672
DA  - Feb
ER  - 

TY  - CONF
ID  - Deng2025Image
AU  - Deng, Xin
AU  - Bao, Feiyang
AU  - Liu, Bin
AU  - Li, Yijia
AU  - Zhang, Lianhua
TI  - A Study on Image Reconstruction Based on Decoding fMRI Through Extracting Image Depth Features
PY  - 2025
JO  - Communications in Computer and Information Science
T2  - Communications in Computer and Information Science
VL  - 2181 CCIS
SP  - 449 - 462
DO  - 10.1007/978-981-97-7001-4_32
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205387927&doi=10.1007%2F978-981-97-7001-4_32&partnerID=40&md5=adf7dbff53ef2b9f5d7e095c4252a434
AB  - Visualizing perceived content through functional magnetic resonance imaging (fMRI) analysis is a captivating research area in brain decoding. Previous studies have primarily focused on restoring either high-level semantic features or low-level semantic features from fMRI data, but rarely achieved effective restoration of both. This study proposes a novel approach for decoding the visual cortex activity measured by fMRI into the layered visual features that share the hierarchical information from the corresponding images. By iteratively optimizing the relationship between the layered visual features and the image’s depth features extracted by a visual transformer, the method in this research significantly improves the reconstruction of the image’s deep features. Furthermore, by incorporating the prior natural image information through a deep generator network, this work enhances the reconstruction process, resulting in richer semantic details. Experimental results verify the effectiveness of our methodology in restoring both high-level and low-level semantic features of the perceived images, ultimately enhancing the overall visual fidelity of the reconstructed image. Importantly, our model demonstrates successful generalization to reconstruct artificial shapes, indicating that the performance of our model is not simply achieved by relying on extensive sample datasets. These findings prove the efficacy of the method in effectively reconstructing the perceived content based on the hierarchical neural representations, providing a new method to study the brain’s underlying mechanisms.
KW  - Brain mapping
KW  - Deep neural networks
KW  - Image denoising
KW  - Image enhancement
KW  - Magnetic resonance imaging
KW  - Restoration
KW  - Depth features
KW  - Features extraction
KW  - Functional magnetic resonance imaging
KW  - Image depth
KW  - Images reconstruction
KW  - Imaging analysis
KW  - Neural decoding
KW  - Semantic features
KW  - Visual feature
KW  - Visual transformer
KW  - Image reconstruction
ER  - 

TY  - JOUR
ID  - Dong2025High
AU  - Dong, Zhen
AU  - Xiang, Yingjie
AU  - Wang, Songwei
TI  - High - quality decoding of RGB images from the neuronal signals of the pigeon optic tectum
PY  - 2025
JO  - Journal of Neuroscience Methods
T2  - Journal of Neuroscience Methods
VL  - 424
DO  - 10.1016/j.jneumeth.2025.110595
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017292462&doi=10.1016%2Fj.jneumeth.2025.110595&partnerID=40&md5=7a3f32ae1426799a38dde09a1d2d602d
AB  - Background: Decoding neural activity to reverse-engineer sensory inputs advances understanding of neural encoding and boosts brain-computer interface and visual prosthesis tech. A major challenge is high-quality RGB image reconstruction from natural scenes, which this study tackles using pigeon optic tectum neurons. New method: We built a neural response dataset via microelectrode arrays capturing tectal neurons' ON-OFF responses to RGB images. A modular decoding algorithm, integrating a convolutional encoding network, linear decoder, and image enhancement network, enabled inverse RGB image reconstruction from neural signals. Results: Experimental results confirmed high-quality RGB image reconstruction by the proposed algorithm. For all test set reconstructions, average metrics were: correlation coefficient (R) of 0.853, structural similarity index (SSIM) of 0.618, peak signal-to-noise ratio (PSNR) of 19.94 dB, and feature similarity index (FSIMc) of 0.801. These results confirm accurate recapitulation of both color and contour details of the original images. Comparison with existing methods: In terms of key quantitative metrics, the proposed algorithm achieves a significant improvement over traditional linear reconstruction methods, with the correlation coefficient (R) increased by 12.65 %, the structural similarity index (SSIM) increased by 38.92 %, the peak signal-to-noise ratio (PSNR) increased by 12.65 %, and the feature similarity index (FSIMc) increased by 9.28 %. Conclusions: This research provides a novel technical pathway for high-quality visual neural decoding, with robust experimental metrics validating its effectiveness. It also offers experimental evidence to support investigations into the information processing mechanisms of the avian visual pathway.
KW  - Neural decoding
KW  - Neural spikes
KW  - Optic tectum neurons
KW  - Pigeon
KW  - RGB image reconstruction
ER  - 

TY  - JOUR
ID  - Gao2025Mindd
AU  - Gao, Jianxiong
AU  - Fu, Yanwei
AU  - Fu, Yuqian
AU  - Wang, Yun
AU  - Qian, Xuelin
AU  - Feng, Jianfeng
TI  - MinD-3D++: Advancing fMRI-Based 3D Reconstruction With High-Quality Textured Mesh Generation and a Comprehensive Dataset
PY  - 2025
JO  - IEEE Transactions on Pattern Analysis and Machine Intelligence
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
SP  - 1-15
DO  - 10.1109/TPAMI.2025.3599860
AB  - Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4,768 3D objects. The dataset consists of two components: fMRI-Shape, previously introduced and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape , and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the core set in fMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Moreover, we propose MinD-3D++, a novel framework for decoding textured 3D visual information from fMRI signals. The framework evaluates the feasibility of not only reconstructing 3D objects from the human mind but also generating, for the first time, 3D textured meshes with detailed textures from fMRI data. We establish new benchmarks by designing metrics at the semantic, structural, and textured levels to evaluate model performance. Furthermore, we assess the model's effectiveness in out-of-distribution settings and analyze the attribution of the proposed 3D pari fMRI dataset in visual regions of interest (ROIs) in fMRI signals. Our experiments demonstrate that MinD-3D++ not only reconstructs 3D objects with high semantic and spatial accuracy but also provides deeper insights into how the human brain processes 3D visual information. Project page: https://jianxgao.github.io/MinD-3D.
KW  - Three-dimensional displays
KW  - Functional magnetic resonance imaging
KW  - Feature extraction
KW  - Visualization
KW  - Image reconstruction
KW  - Diffusion models
KW  - Decoding
KW  - Solid modeling
KW  - Semantics
KW  - Brain modeling
KW  - 3D vision
KW  - dataset
KW  - diffusion model
KW  - FMRI decoding
SN  - 1939-3539
ER  - 

TY  - JOUR
ID  - Gao2025Reduced
AU  - Gao, Zhiyao
AU  - Duberg, Katherine
AU  - Warren, Stacie L.
AU  - Zheng, Li
AU  - Hinshaw, Stephen P.
AU  - Menon, Vinod
AU  - Cai, Weidong
TI  - Reduced temporal and spatial stability of neural activity patterns predict cognitive control deficits in children with ADHD
PY  - 2025
JO  - Nature Communications
T2  - Nature Communications
VL  - 16
IS  - 1
DO  - 10.1038/s41467-025-57685-x
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000716747&doi=10.1038%2Fs41467-025-57685-x&partnerID=40&md5=159b263002eee810dab32981eac96e11
AB  - This study investigates the neural underpinnings of cognitive control deficits in attention-deficit/hyperactivity disorder (ADHD), focusing on trial-level variability of neural coding. Using fMRI, we apply a computational approach to single-trial neural decoding on a cued stop-signal task, probing proactive and reactive control within the dual control model. Reactive control involves suppressing an automatic response when interference is detected, and proactive control involves implementing preparatory strategies based on prior information. In contrast to typically developing children (TD), children with ADHD show disrupted neural coding during both proactive and reactive control, characterized by increased temporal variability and diminished spatial stability in neural responses in salience and frontal-parietal network regions. This variability correlates with fluctuating task performance and ADHD symptoms. Additionally, children with ADHD exhibit more heterogeneous neural response patterns across individuals compared to TD children. Our findings underscore the significance of modeling trial-wise neural variability in understanding cognitive control deficits in ADHD.
KW  - activity pattern
KW  - brain
KW  - child health
KW  - disease control
KW  - mental disorder
KW  - anterior insula
KW  - Article
KW  - attention deficit hyperactivity disorder
KW  - child
KW  - child development
KW  - cognitive defect
KW  - controlled study
KW  - diffusion kurtosis imaging
KW  - evoked response
KW  - executive function
KW  - female
KW  - frontoparietal network
KW  - functional magnetic resonance imaging
KW  - head movement
KW  - human
KW  - inferior frontal gyrus
KW  - inhibitory control
KW  - intelligence quotient
KW  - lateral prefrontal cortex
KW  - major clinical study
KW  - male
KW  - medial temporal lobe
KW  - middle frontal gyrus
KW  - motor cortex
KW  - nerve potential
KW  - neuroimaging
KW  - parietal cortex
KW  - phenotype
KW  - posterior cingulate
KW  - posterior parietal cortex
KW  - primary motor cortex
KW  - superior frontal gyrus
KW  - task performance
KW  - thalamus
KW  - visual cortex
KW  - adolescent
KW  - brain mapping
KW  - cognition
KW  - diagnostic imaging
KW  - nuclear magnetic resonance imaging
KW  - pathophysiology
KW  - physiology
KW  - procedures
KW  - reaction time
KW  - Adolescent
KW  - Attention Deficit Disorder with Hyperactivity
KW  - Brain
KW  - Brain Mapping
KW  - Child
KW  - Cognition
KW  - Female
KW  - Humans
KW  - Magnetic Resonance Imaging
KW  - Male
KW  - Reaction Time
ER  - 

TY  - CONF
ID  - Huo2025Neuropictor
AU  - Huo, Jingyang
AU  - Wang, Yikai
AU  - Wang, Yun
AU  - Qian, Xuelin
AU  - Li, Chong
AU  - Fu, Yanwei
AU  - Feng, Jianfeng
TI  - NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation
PY  - 2025
JO  - Lecture Notes in Computer Science
T2  - Lecture Notes in Computer Science
VL  - 15109 LNCS
SP  - 56 - 73
DO  - 10.1007/978-3-031-72983-6_4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209888834&doi=10.1007%2F978-3-031-72983-6_4&partnerID=40&md5=ddd07c13b4cac354e218a7141045dfb4
AB  - Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent multi-subject training; ii) fMRI-to-image multi-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with about 67,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Our code and model are available at https://jingyanghuo.github.io/neuropictor/.
KW  - Benchmarking
KW  - Direct process refining
KW  - Image coding
KW  - Image compression
KW  - Image denoising
KW  - Neuroimaging
KW  - Stereo image processing
KW  - Complex information
KW  - Condition
KW  - Diffusion model
KW  - FMRI-to-image
KW  - High quality images
KW  - Image captures
KW  - Images reconstruction
KW  - Multilevels
KW  - Neural decoding
KW  - Pre-training
KW  - Image reconstruction
ER  - 

TY  - JOUR
ID  - Kamitani2025Visual
AU  - Kamitani, Yukiyasu
AU  - Tanaka, Misato
AU  - Shirakawa, Ken
TI  - Visual Image Reconstruction from Brain Activity via Latent Representation
PY  - 2025
JO  - Annual Review of Vision Science
T2  - Annual Review of Vision Science
VL  - 11
IS  - 1
SP  - 611 - 634
DO  - 10.1146/annurev-vision-110423-023616
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016350828&doi=10.1146%2Fannurev-vision-110423-023616&partnerID=40&md5=6adf018e19b322acae793ccaa10af9ce
AB  - Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field’s evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain–machine interfaces.
KW  - artificial neural network
KW  - brain
KW  - brain computer interface
KW  - human
KW  - image processing
KW  - physiology
KW  - procedures
KW  - vision
KW  - Brain
KW  - Brain-Computer Interfaces
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Neural Networks, Computer
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - Letafati2025Deep
AU  - Letafati, Mehdi
AU  - Amirhossein Ameli Kalkhoran, Seyyed
AU  - Erdemir, Ecenaz
AU  - Hossein Khalaj, Babak
AU  - Behroozi, Hamid
AU  - Gündüz, Deniz
TI  - Deep Joint Source Channel Coding for Privacy-Aware End-to-End Image Transmission
PY  - 2025
JO  - IEEE Transactions on Machine Learning in Communications and Networking
T2  - IEEE Transactions on Machine Learning in Communications and Networking
VL  - 3
SP  - 568-584
DO  - 10.1109/TMLCN.2025.3564907
AB  - Deep neural network (DNN)-based joint source and channel coding is proposed for privacy-aware end-to-end image transmission against multiple eavesdroppers. Both scenarios of colluding and non-colluding eavesdroppers are considered. Unlike prior works that assume perfectly known and independent identically distributed (i.i.d.) source and channel statistics, the proposed scheme operates under unknown and non-i.i.d. conditions, making it more applicable to real-world scenarios. The goal is to transmit images with minimum distortion, while simultaneously preventing eavesdroppers from inferring certain private attributes of images. Simultaneously generalizing the ideas of privacy funnel and wiretap coding, a multi-objective optimization framework is expressed that characterizes the trade-off between image reconstruction quality and information leakage to eavesdroppers, taking into account the structural similarity index (SSIM) for improving the perceptual quality of image reconstruction. Extensive experiments on the CIFAR-10 and CelebA, along with ablation studies, demonstrate significant performance improvements in terms of SSIM, adversarial accuracy, and the mutual information leakage compared to benchmarks. Experiments show that the proposed scheme restrains the adversarially-trained eavesdroppers from intercepting privatized data for both cases of eavesdropping a common secret, as well as the case in which eavesdroppers are interested in different secrets. Furthermore, useful insights on the privacy-utility trade-off are also provided.
KW  - Communication system security
KW  - Security
KW  - Image communication
KW  - Wireless sensor networks
KW  - Image reconstruction
KW  - Eavesdropping
KW  - Channel coding
KW  - Autoencoders
KW  - Wireless networks
KW  - Training
KW  - DeepJSCC
KW  - secure image transmission
KW  - end-to-end learning
KW  - privacy-utility trade-off
KW  - adversarial neural networks
KW  - deep learning
SN  - 2831-316X
ER  - 

TY  - JOUR
ID  - Li2025Deep
AU  - Li, Wei
AU  - Zhao, Penglu
AU  - Xu, Cheng
AU  - Hou, Yingting
AU  - Jiang, Wenhao
AU  - Song, Aiguo
TI  - Deep Learning for EEG-Based Visual Classification and Reconstruction: Panorama, Trends, Challenges and Opportunities
PY  - 2025
JO  - IEEE Transactions on Biomedical Engineering
T2  - IEEE Transactions on Biomedical Engineering
SP  - 1-17
DO  - 10.1109/TBME.2025.3568282
AB  - Deep learning has significantly enhanced the research on the emerging issue of Electroencephalogram (EEG)-based visual classification and reconstruction, which has gained a growth of attention and concern recently. To promote the research progress, at this critical moment, a review work on the deep learning methodology for the issue becomes necessary and important. However, such a work seems absent in the literature. This paper provides the first review on EEG-based visual classification and reconstruction, whose contents can be categorized into the following four main parts: 1) comprehensively summarizing and systematically analyzing the representative deep learning methods from both feature encoding and decoding perspectives; 2) introducing the available benchmark datasets, describing the experimental paradigms, and displaying the method performances; 3) proposing the methodological essences and neuroscientific insights as well as the dynamic closed-loop interaction and promotion between them, which are potentially beneficial for technological innovations and academic progress; 4) discussing the potential challenges of current research and the prospective opportunities in future trends. We expect that this work can shed light on the technological directions and also enlighten the academic breakthroughs for the issue in the not-so-far future.
KW  - Visualization
KW  - Electroencephalography
KW  - Reviews
KW  - Image reconstruction
KW  - Deep learning
KW  - Decoding
KW  - Functional magnetic resonance imaging
KW  - Convolutional neural networks
KW  - Feature extraction
KW  - Transformers
KW  - Electroencephalogram
KW  - Deep Learning
KW  - Visual Classification
KW  - Visual Reconstruction
SN  - 1558-2531
ER  - 

TY  - CONF
ID  - Lotey2025Eegbased
AU  - Lotey, Taveena
AU  - Verma, Aman
AU  - Roy, Partha Pratim
TI  - EEG-Based Mental Imagery Task Adaptation via Ensemble of Weight-Decomposed Low-Rank Adapters
PY  - 2025
JO  - Lecture Notes in Computer Science
T2  - Lecture Notes in Computer Science
VL  - 15311 LNCS
SP  - 309 - 324
DO  - 10.1007/978-3-031-78195-7_21
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211963733&doi=10.1007%2F978-3-031-78195-7_21&partnerID=40&md5=7bd691071a1a2856dea639b3b0043761
AB  - Electroencephalography (EEG) is widely researched for neural decoding in Brain Computer Interfaces (BCIs) as it is non-invasive, portable, and economical. However, EEG signals suffer from inter- and intra-subject variability, leading to poor performance. Recent technological advancements have led to deep learning (DL) models that have achieved high performance in various fields. However, such large models are compute- and resource-intensive and are a bottleneck for real-time neural decoding. Data distribution shift can be handled with the help of domain adaptation techniques of transfer learning (fine-tuning) and adversarial training that requires model parameter updates according to the target domain. One such recent technique is Parameter-efficient fine-tuning (PEFT), which requires only a small fraction of the total trainable parameters compared to fine-tuning the whole model. Therefore, we explored PEFT methods for adapting EEG-based mental imagery tasks. We considered two mental imagery tasks: speech imagery and motor imagery, as both of these tasks are instrumental in post-stroke neuro-rehabilitation. We proposed a novel ensemble of weight-decomposed low-rank adaptation methods, EDoRA, for parameter-efficient mental imagery task adaptation through EEG signal classification. The performance of the proposed PEFT method is validated on two publicly available datasets, one speech imagery, and the other motor imagery dataset. In extensive experiments and analysis, the proposed method has performed better than full fine-tune and state-of-the-art PEFT methods for mental imagery EEG classification.
KW  - Adversarial machine learning
KW  - Brain mapping
KW  - Deep learning
KW  - Image analysis
KW  - Image reconstruction
KW  - Learning to rank
KW  - Neurons
KW  - Transfer learning
KW  - Fine tuning
KW  - Fine-tuning methods
KW  - Imagery task
KW  - Low-rank adaptation.
KW  - Mental imagery
KW  - Neural decoding
KW  - Performance
KW  - Task adaptation
KW  - Brain computer interface
ER  - 

TY  - JOUR
ID  - Ma2025Brainclip
AU  - Ma, Yongqiang
AU  - Liu, Yulong
AU  - Chen, Liangjun
AU  - Zhu, Guibo
AU  - Chen, Badong
AU  - Zheng, Nanning
TI  - BrainCLIP: Brain Representation via CLIP for Generic Natural Visual Stimulus Decoding
PY  - 2025
JO  - IEEE Transactions on Medical Imaging
T2  - IEEE Transactions on Medical Imaging
SP  - 1-1
DO  - 10.1109/TMI.2025.3537287
AB  - Functional Magnetic Resonance Imaging (fMRI) presents challenges due to limited paired samples and low signal-to-noise ratios, particularly in tasks involving reconstructing natural images or decoding their semantic content. To address these challenges, we introduce BrainCLIP, an innovative fMRI-based brain decoding model. BrainCLIP leverages Contrastive Language-Image Pre-training’s (CLIP) cross-modal generalization abilities to bridge brain activity, images, and text for the first time. Our experiments demonstrate CLIP’s effectiveness in diverse brain decoding tasks, including zero-shot visual category decoding, fMRI-image/text alignment, and fMRI-to-image generation. The core objective of BrainCLIP is to train a mapping network that translates fMRI patterns into a unified CLIP embedding space, achieved through visual and textual supervision integration. Our experiments highlight that this approach significantly enhances performance in tasks such as fMRI-text alignment and fMRI-based image generation. Notably, BrainCLIP surpasses BraVL, a recent multi-modal method, in zero-shot visual category decoding. Moreover, BrainCLIP demonstrates strong capability in reconstructing visual stimuli with high semantic fidelity, competing favorably with state-of-the-art methods in capturing high-level semantic features during fMRI-based natural image reconstruction.
KW  - Visualization
KW  - Decoding
KW  - Functional magnetic resonance imaging
KW  - Brain modeling
KW  - Semantics
KW  - Image reconstruction
KW  - Training
KW  - Brain
KW  - Feature extraction
KW  - Contrastive learning
KW  - Brain decoding
KW  - CLIP
KW  - Visual-Linguistic representation
KW  - Cross-modal
SN  - 1558-254X
ER  - 

TY  - JOUR
ID  - Mai2025Brainconditional
AU  - Mai, Weijian
AU  - Zhang, Jian
AU  - Fang, Pengfei
AU  - Zhang, Zhijun
TI  - Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy
PY  - 2025
JO  - IEEE Transactions on Artificial Intelligence
T2  - IEEE Transactions on Artificial Intelligence
VL  - 6
IS  - 5
SP  - 1080-1099
DO  - 10.1109/TAI.2024.3516698
AB  - In the era of artificial intelligence generated content (AIGC), conditional multimodal synthesis technologies (e.g., text-to-image) are dynamically reshaping the natural content. Brain signals, serving as potential reflections of how the brain interprets external information, exhibit a distinctive one-to-many correspondence with various external modalities. This correspondence makes brain signals emerge as a promising guiding condition for multimodal synthesis (e.g., image, text, and audio), which is crucial for developing practical brain–computer interface systems and unraveling complex mechanisms underlying human perception. This survey comprehensively examines the emerging field of brain-conditional multimodal synthesis, termed AIGC-brain, to delineate the current landscape and future directions. To begin, related neuroimaging datasets and generative models are introduced as the foundation of AIGC-brain decoding and analysis. Next, we present a comprehensive taxonomy according to AIGC-brain methodologies, followed by task-specific representative work and implementation details to facilitate in-depth comparison and analysis. Quality assessments are then introduced for both qualitative and quantitative evaluation. Finally, this survey explores insights gained, outlining current challenges and prospects of AIGC-brain. As a pioneering survey, this article paves the way for future advances in AIGC-brain research.
KW  - Functional magnetic resonance imaging
KW  - Decoding
KW  - Surveys
KW  - Neuroimaging
KW  - Electroencephalography
KW  - Brain modeling
KW  - Artificial intelligence
KW  - Taxonomy
KW  - Music
KW  - Faces
KW  - Artificial intelligence generated content
KW  - brain–computer interface
KW  - brain decoding
KW  - multimodal synthesis
SN  - 2691-4581
DA  - May
ER  - 

TY  - JOUR
ID  - McKilliam2025Aphantasia
AU  - McKilliam, Andy
AU  - Kirberg, Manuela
TI  - Aphantasia and the unconscious imagery hypothesis
PY  - 2025
JO  - Consciousness and Cognition
T2  - Consciousness and Cognition
VL  - 135
DO  - 10.1016/j.concog.2025.103924
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014611288&doi=10.1016%2Fj.concog.2025.103924&partnerID=40&md5=62e678a14dbb01adcb81f69816e5736d
AB  - Until recently, mental imagery has largely been regarded as an exclusively conscious phenomenon. However, recent empirical results suggest that mental imagery can also occur unconsciously. People who report having no experiences of mental imagery often perform similar to controls on behavioural tasks thought to require imagery. A surprising number of them also display significant levels of imagery-based priming, and recent neural decoding studies have shown that imagery-related information is being processed in their visual cortex. However, investigating unconscious imagery empirically is not straightforward. One challenge is to establish that imagery is genuinely unconscious as opposed to merely going unreported due to response biases. Another is to clarify how imagistic and indirect perceptual processing needs to be to qualify as imagery. In this paper, we take a closer look at the evidence for unconscious imagery, argue that it is not as compelling as it initially appears, and outline a strategy for advancing research on this question.
KW  - aphantasia
KW  - Article
KW  - guided imagery
KW  - human
KW  - imagery
KW  - mental disease
KW  - mental performance
KW  - mental task
KW  - unconscious (psychology)
KW  - unconscious imagery
KW  - visual cortex
ER  - 

TY  - CONF
ID  - Nigam2025Eegdepth
AU  - Nigam, Jyoti
AU  - Prakash, Aditya
AU  - Uthamkumar, M.
AU  - Salgotra, Samvaidan
AU  - Bhavsar, Arnav V.
TI  - EEG-Depth: Learning Structural Information from Visual Brain Decoding via Depth Estimation
PY  - 2025
JO  - Lecture Notes in Computer Science
T2  - Lecture Notes in Computer Science
VL  - 15614 LNCS
SP  - 253 - 264
DO  - 10.1007/978-3-031-87657-8_18
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005571568&doi=10.1007%2F978-3-031-87657-8_18&partnerID=40&md5=3095c110122497f20029f3dd27e9e5e6
AB  - Electroencephalography (EEG) is a crucial tool for recording the brain’s electrical activity, providing insights into neural processes. However, extracting meaningful information of a complex visual stimulus (e.g. a scene image) from EEG data is challenging due to a large domain shift and the signal’s complexity. Recent research efforts focus on decoding visual information from EEG using advanced models and techniques. In our work, we address structure and locality estimation from EEG signals with a particular focus on depth perception, which is a fundamental aspect of the visual processing pathway and could serve as a key intermediate ground for visual decoding models. Thus, we focus on the task of reconstruction of depth maps from EEG data, corresponding to the images shown to subjects. Our work involves a contrastive learning framework in an attempt to align GNN based EEG embeddings to that of and depth map embeddings. We also perform experiments to draw some insights about the importance of EEG channels in such a EEG to Depth map reconstruction.
KW  - Brain decoding
KW  - Depth Estimation
KW  - Depthmap
KW  - Electrical activities
KW  - Electroencephalography-imagenet dataset
KW  - Embeddings
KW  - Images reconstruction
KW  - Neural process
KW  - Structural information
KW  - Visual brain decoding
KW  - Image analysis
ER  - 

TY  - JOUR
ID  - Olza2025Domain
AU  - Olza, Alexander
AU  - Soto, David
AU  - Santana, Roberto
TI  - Domain Adaptation-enhanced searchlight: enabling classification of brain states from visual perception to mental imagery
PY  - 2025
JO  - Brain Informatics
T2  - Brain Informatics
VL  - 12
IS  - 1
DO  - 10.1186/s40708-025-00263-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009369560&doi=10.1186%2Fs40708-025-00263-0&partnerID=40&md5=50f9c4637a30abcde8bf4dd9eb4dab48
AB  - In cognitive neuroscience and brain-computer interface research, accurately predicting imagined stimuli is crucial. This study investigates the effectiveness of Domain Adaptation (DA) in enhancing imagery prediction using primarily visual data from fMRI scans of 18 subjects. Initially, we train a baseline model on visual stimuli to predict imagined stimuli, utilizing data from 14 brain regions. We then develop several models to improve imagery prediction, comparing different DA methods. Our results demonstrate that DA significantly enhances imagery prediction in binary classification on our dataset, as well as in multiclass classification on a publicly available dataset. We then conduct a DA-enhanced searchlight analysis, followed by permutation-based statistical tests to identify brain regions where imagery decoding is consistently above chance across subjects. Our DA-enhanced searchlight predicts imagery contents in a highly distributed set of brain regions, including the visual cortex and the frontoparietal cortex, thereby outperforming standard cross-domain classification methods. The complete code and data for this paper have been made openly available for the use of the scientific community.
KW  - Brain
KW  - Classification (of information)
KW  - Forecasting
KW  - Interfaces (computer)
KW  - Tunneling (excavation)
KW  - Baseline models
KW  - Brain decoding
KW  - Brain regions
KW  - Brain state
KW  - Cognitive neurosciences
KW  - Domain adaptation
KW  - FMRI
KW  - Mental imagery
KW  - Visual data
KW  - Visual perception
KW  - Brain computer interface
KW  - adaptation
KW  - Article
KW  - binary classification
KW  - brain
KW  - brain region
KW  - cognitive neuroscience
KW  - deep learning
KW  - Domain Adaptation
KW  - electroencephalogram
KW  - frontoparietal cortex
KW  - functional magnetic resonance imaging
KW  - fusiform gyrus
KW  - hemodynamics
KW  - human
KW  - image segmentation
KW  - imagery
KW  - logistic regression analysis
KW  - machine learning
KW  - mental imagery
KW  - neuroimaging
KW  - support vector machine
KW  - vision
KW  - visual cortex
ER  - 

TY  - JOUR
ID  - Peng2025Decoding
AU  - Peng, Jing
AU  - Jia, Shanshan
AU  - Zhang, Jiyuan
AU  - Wang, Yongxing
AU  - Yu, Zhaofei
AU  - Liu, Jian K.
TI  - Decoding natural visual scenes via learnable representations of neural spiking sequences
PY  - 2025
JO  - Neural Networks
T2  - Neural Networks
VL  - 192
DO  - 10.1016/j.neunet.2025.107863
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011203775&doi=10.1016%2Fj.neunet.2025.107863&partnerID=40&md5=c4f0bf1845f8cf856f76834d9b94c665
AB  - Visual input underpins cognitive function by providing the brain with essential environmental information. Neural decoding of visual scenes seeks to reconstruct pixel-level images from neural activity, a vital capability for vision restoration via brain-computer interfaces. However, extracting visual content from time-resolved spiking activity remains a significant challenge. Here, we introduce the Wavelet-Informed Spike Augmentation (WISA) model, which applies multilevel wavelet transforms to spike trains to learn compact representations that can be directly fed into deep reconstruction networks. When tested on recorded retinal spike data responding to natural video stimuli, WISA substantially improves reconstruction accuracy, especially in recovering fine-grained details. These results emphasize the value of temporal spike patterns for high-fidelity visual decoding and demonstrate WISA as a promising model for visual decoding.
KW  - Brain
KW  - Brain computer interface
KW  - Decoding
KW  - Deep neural networks
KW  - Image reconstruction
KW  - Neurons
KW  - Wavelet transforms
KW  - Cognitive functions
KW  - Deep learning
KW  - Environmental information
KW  - Neural decoding
KW  - Neural spike
KW  - Neural-networks
KW  - Pixel level
KW  - Video
KW  - Visual scene
KW  - Wavelet
KW  - Vision
KW  - Article
KW  - cognition
KW  - decoding
KW  - deep learning
KW  - nerve cell
KW  - nerve cell network
KW  - retina
KW  - spike
KW  - videorecording
KW  - wavelet transform
ER  - 

TY  - CONF
ID  - PossoMurillo2025Ieee
AU  - Posso-Murillo, Santiago
AU  - Sanchez-Giraldo, Luis G.
AU  - Bae, Jihye
TI  - Semantic Reconstruction from Fnirs Using Recurrent Neural Networks
PY  - 2025
BT  - 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)
T2  - 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)
SP  - 1-5
DO  - 10.1109/ISBI60581.2025.10981123
AB  - Semantic reconstruction of language aims to decode the meaning of words or sentences from neural activity. Previous studies have demonstrated that functional near-infrared spectroscopy (fNIRS) contains information suitable for language decoding. However, most of the existing work on fNIRS-based neural decoding relies on traditional machine learning algorithms such as linear models and support vector machines, and it has been limited to classification of limited set of words. To address these shortcomings, we examine 4 recurrent neural networks (RNNs) that learn features to decode semantic representations from fNIRS: the Elman recurrent neural network (ERNN), long short-term memory (LSTM), and bidirectional version of them (BiERNN and BiLSTM). Using a publicly available fNIRS dataset, we performed within-category, between-category, and leave-two-out tests. The decoding performance was measured by computing the matching score, a pairwise metric that assesses the model's ability to distinguish between two concepts. The results show that ERNN and BiLSTM models consistently outperform linear decoder models. Specifically, ERNN shows better performance for 4 out of 7 subjects in the between-category test, and BiLSTM performs better for 6 out of 7 subjects in the within-category test and 4 out of 7 subjects in the leave-two-out test. Notably, in between-category experiment, the BiLSTM scored 61 % matching score for subject 3, representing a 9% improvement, and ERNN achieved an 80% matching score for subject 2, marking a significant 33% improvement. These promising results encourage the use of advanced machine learning models for semantic reconstruction from fNIRS. Code is available at https://github.com/sposso/Semantic-Reconstruction-using-fNIRS-signal.
KW  - Support vector machines
KW  - Neuroimaging
KW  - Machine learning algorithms
KW  - Computational modeling
KW  - Semantics
KW  - Neural activity
KW  - Bidirectional long short term memory
KW  - Decoding
KW  - Functional near-infrared spectroscopy
KW  - Image reconstruction
KW  - fNIRS
KW  - RNN
KW  - Semantic Reconstruction
SN  - 1945-8452
DA  - April
ER  - 

TY  - CONF
ID  - Qu2025Uncovering
AU  - Qu, Youzhi
AU  - Xia, Junfeng
AU  - Jian, Xinyao
AU  - Li, Wendu
AU  - Peng, Kaining
AU  - Liang, Zhichao
AU  - Wu, Haiyan
AU  - Liu, Quanying
TI  - Uncovering Cognitive Taskonomy Through Transfer Learning in Masked Autoencoder-Based fMRI Reconstruction
PY  - 2025
JO  - Communications in Computer and Information Science
T2  - Communications in Computer and Information Science
VL  - 2438 CCIS
SP  - 35 - 50
DO  - 10.1007/978-981-96-4001-0_3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003635208&doi=10.1007%2F978-981-96-4001-0_3&partnerID=40&md5=0edd25d5ce8f0957c6d5ce215c2efd54
AB  - Data reconstruction is a widely used pre-training task to learn the generalized features for many downstream tasks. Although reconstruction tasks have been applied to neural signal completion and denoising, neural signal reconstruction is less studied. Here, we employ the masked autoencoder (MAE) model to reconstruct functional magnetic resonance imaging (fMRI) data, and utilize a transfer learning framework to obtain the cognitive taskonomy, a matrix to quantify the similarity between cognitive tasks. Our experimental results demonstrate that the MAE model effectively captures the temporal dynamics patterns and interactions within the brain regions, enabling robust cross-subject fMRI signal reconstruction. The cognitive taskonomy derived from the transfer learning framework reveals the relationships among cognitive tasks, highlighting subtask correlations within motor tasks and similarities between emotion, social, and gambling tasks. Our study suggests that the fMRI reconstruction with MAE model can uncover the latent representation and the obtained taskonomy offers guidance for selecting source tasks in neural decoding tasks for improving the decoding performance on target tasks.
KW  - Amplitude modulation
KW  - Frequency modulation
KW  - Image coding
KW  - Image compression
KW  - Image denoising
KW  - Image segmentation
KW  - Light modulation
KW  - Signal modulation
KW  - Transfer learning
KW  - Auto encoders
KW  - Cognitive task
KW  - FMRI reconstruction
KW  - Functional magnetic resonance imaging
KW  - Imaging reconstruction
KW  - Masked autoencoder
KW  - Neural signals
KW  - Signals reconstruction
KW  - Taskonomy
KW  - Image reconstruction
ER  - 

TY  - JOUR
ID  - RodrguezDeliz2025Neural
AU  - Rodríguez Deliz, Charlie L.
AU  - Lee, Gerick M.
AU  - Bushnell, Brittany N.
AU  - Majaj, Najib J.
AU  - Anthony Movshon, J.
AU  - Kiorpes, Lynne
TI  - Neural Sensitivity to Radial Frequency Patterns in the Visual Cortex of Developing Macaques
PY  - 2025
JO  - Journal of Neuroscience
T2  - Journal of Neuroscience
VL  - 45
IS  - 34
DO  - 10.1523/JNEUROSCI.0179-25.2025
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013674585&doi=10.1523%2FJNEUROSCI.0179-25.2025&partnerID=40&md5=604be766464fbcf8ac8be2a46d136012
AB  - Visual resolution, contrast sensitivity, and form perception improve gradually with age. In nonhuman primates, the sensitivity and resolution of cells in the retina, lateral geniculate nucleus, and primary visual cortex (V1) also improve, but not enough to account for the perceptual changes. So, what aspects of visual system development limit visual sensitivity in infants? Improvements in behavioral sensitivity might arise from maturation of regions downstream of V1 such as V2, V4, and IT, which are thought to support increasingly complex perceptual abilities. We recorded the responses of populations of neurons in areas V1, V2, V4, and PIT to radial frequency patterns—a type of global form stimulus. Subjects were three young monkeys (two female, one male) between the ages of 19 and 54 weeks and a single adult animal (male). We found that neurons and neural populations in V4 reliably encoded global form in radial frequency stimuli at the earliest ages we studied, while V1 neurons do not. V2 and PIT populations also showed some degree of selectivity for these patterns at early ages, especially at higher radial frequency values. We did not find significant, systematic changes in neural decoding performance that could account for the improvement in behavioral performance over the same age range in an overlapping group of animals (Rodriguez Deliz et al., 2024). Finally, consistent with our prior behavioral results, neural populations in V4 show highest sensitivity for the higher radial frequency values, which contain the highest concentration of curvature and orientation cues.
KW  - adult
KW  - animal experiment
KW  - article
KW  - contrast sensitivity
KW  - controlled study
KW  - female
KW  - infant
KW  - lateral geniculate body
KW  - Macaca
KW  - male
KW  - nerve cell
KW  - nonhuman
KW  - pattern recognition
KW  - striate cortex
KW  - vision
KW  - visual acuity
KW  - visual cortex
KW  - visual system
KW  - animal
KW  - growth, development and aging
KW  - photostimulation
KW  - physiology
KW  - procedures
KW  - rhesus monkey
KW  - Animals
KW  - Female
KW  - Macaca mulatta
KW  - Male
KW  - Neurons
KW  - Photic Stimulation
KW  - Visual Cortex
ER  - 

TY  - JOUR
ID  - Shakeripour2025Object
AU  - Shakeripour, Alireza
AU  - Bahmani, Zahra
AU  - Aghaomidi, Poorya
AU  - Seyed-Allaei, Shima
TI  - A Novel Object Categorization Decoder from fMRI Signals Using Deep Neural Networks
PY  - 2025
JO  - Frontiers in Biomedical Technologies
T2  - Frontiers in Biomedical Technologies
VL  - 12
IS  - 3
SP  - 617 - 626
DO  - 10.18502/fbt.v12i3.19186
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014260606&doi=10.18502%2Ffbt.v12i3.19186&partnerID=40&md5=bb502e5f650887dc34e61e5d3eff281b
AB  - Purpose: Understanding neural mechanisms is critical for discerning the nature of brain disorders and enhancing treatment methodologies. Functional Magnetic Resonance Imaging (fMRI) plays a vital role in gaining this knowledge by recording various brain regions. In this study, our primary aim was to categorize visual objects based on fMRI data during a natural scene viewing task. We intend to elucidate the challenges and limitations of previous models in order to produce a generalizable model across different subjects using advanced deep-learning methods. Materials and Methods: We’ve designed a new deep-learning model based on transformers for processing fMRI data. The model includes two blocks, the first block receives fMRI data as input and transforms the input data to a set of features called fMRI space. Simultaneously a visual space is extracted from visual images using a pre-trained inceptionv3 network. The model tries to construct the fMRI space similar to the extracted visual space. The other block is a Fully Connected (FC) network for object recognition based on fMRI space. Using transformer capabilities and an overlapping method, the proposed architecture accounts for structural changes across different voxel sizes of the subjects’ brains. Results: A unique model was trained for all subjects with different brain sizes. The results demonstrated that the proposed network achieves an impressive similarity correlation between visual space and fMRI space around 0.86 for train and 0.86 for test dataset. Furthermore, the classification accuracy was about 70.3%. These outcomes underscored the effectiveness of our fMRI transformer network in extracting features from fMRI data. Conclusion: The results indicated the potential of our model for decoding images from the brain activities of new subjects. This unveils a novel direction in image reconstruction from neural activities, an area that has remained relatively uncharted due to its inherent intricacies.
KW  - Article
KW  - brain disease
KW  - brain region
KW  - brain size
KW  - convolutional neural network
KW  - deep learning
KW  - deep neural network
KW  - electric potential
KW  - electroencephalogram
KW  - functional magnetic resonance imaging
KW  - human
KW  - human experiment
KW  - normal human
KW  - retina image
KW  - visual field
ER  - 

TY  - JOUR
ID  - Sharon2025Harnessing
AU  - Sharon, Rini
AU  - Sur, Mriganka
AU  - Murthy, Hema
TI  - Harnessing the Multi-Phasal Nature of Speech-EEG for Enhancing Imagined Speech Recognition
PY  - 2025
JO  - IEEE Open Journal of Signal Processing
T2  - IEEE Open Journal of Signal Processing
VL  - 6
SP  - 78-88
DO  - 10.1109/OJSP.2025.3528368
AB  - Analyzing speech-electroencephalogram (EEG) is pivotal for developing non-invasive and naturalistic brain-computer interfaces. Recognizing that the nature of human communication involves multiple phases like audition, imagination, articulation, and production, this study uncovers the shared cognitive imprints that represent speech cognition across these phases. Regression analysis, using correlation metrics reveal pronounced inter-phasal congruence. This insight promotes a shift from single-phase-centric recognition models to harnessing integrated phase data, thereby enhancing recognition of cognitive speech. Having established the presence of inter-phase associations, a common representation learning feature extractor is introduced, adept at capturing the correlations and replicability across phases. The features so extracted are observed to provide superior discrimination of cognitive speech units. Notably, the proposed approach proves resilient even in the absence of comprehensive multi-phasal data. Through thorough control checks and illustrative topographical visualizations, our observations are substantiated. The findings indicate that the proposed multi-phase approach significantly enhances EEG-based speech recognition, achieving an accuracy gain of 18.2% for 25 cognitive units in continuous speech EEG over models reliant solely on single-phase data.
KW  - Correlation
KW  - Electroencephalography
KW  - Speech recognition
KW  - Feature extraction
KW  - Accuracy
KW  - Image reconstruction
KW  - Training
KW  - Speech enhancement
KW  - Production
KW  - Loss measurement
KW  - Audition
KW  - articulation
KW  - BCI
KW  - imagination
KW  - regression
KW  - speech-EEG correlation
KW  - neural decoding of speech
SN  - 2644-1322
ER  - 

TY  - JOUR
ID  - Shirakawa2025Spurious
AU  - Shirakawa, Ken
AU  - Nagano, Yoshihiro
AU  - Tanaka, Misato
AU  - Aoki, Shuntaro C.
AU  - Muraki, Yusuke
AU  - Majima, Kei
AU  - Kamitani, Yukiyasu
TI  - Spurious reconstruction from brain activity
PY  - 2025
JO  - Neural Networks
T2  - Neural Networks
VL  - 190
DO  - 10.1016/j.neunet.2025.107515
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007598357&doi=10.1016%2Fj.neunet.2025.107515&partnerID=40&md5=cd83086d39c85fb2eb7f840c8f0e20c3
AB  - Advances in brain decoding, particularly in visual image reconstruction, have sparked discussions about the societal implications and ethical considerations of neurotechnology. As reconstruction methods aim to recover visual experiences from brain activity and achieve prediction beyond training samples (zero-shot prediction), it is crucial to assess their capabilities and limitations to inform public expectations and regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (Natural Scenes Dataset, NSD) and text-to-image diffusion models, reveals critical limitations in their generalizability, demonstrated by poor reconstructions on a different dataset. UMAP visualization of the text features from NSD images shows limited diversity with overlapping semantic and visual clusters between training and test sets. We identify that clustered training samples can lead to “output dimension collapse,” restricting predictable output feature dimensions. While diverse training data improves generalization over the entire feature space without requiring exponential scaling, text features alone prove insufficient for mapping to the visual space. Our findings suggest that the apparent realism in current text-guided reconstructions stems from a combination of classification into trained categories and inauthentic image generation (hallucination) through diffusion models, rather than genuine visual reconstruction. We argue that careful selection of datasets and target features, coupled with rigorous evaluation methods, is essential for achieving authentic visual image reconstruction. These insights underscore the importance of grounding interdisciplinary discussions in a thorough understanding of the technology's current capabilities and limitations to ensure responsible development.
KW  - Photointerpretation
KW  - Brain activity
KW  - Brain decoding
KW  - Images reconstruction
KW  - Natural scenes
KW  - Naturalistic approach
KW  - Neuroai
KW  - Reconstruction method
KW  - Training sample
KW  - Visual image
KW  - Visual image reconstruction
KW  - Large datasets
KW  - Article
KW  - artificial intelligence
KW  - brain decoding
KW  - electroencephalogram
KW  - human
KW  - image reconstruction
KW  - naturalistic approach
KW  - visual field
KW  - visual image reconstruction
KW  - brain
KW  - diagnostic imaging
KW  - image processing
KW  - physiology
KW  - procedures
KW  - Brain
KW  - Humans
KW  - Image Processing, Computer-Assisted
ER  - 

TY  - JOUR
ID  - Shoura2025Revealing
AU  - Shoura, Moaz
AU  - Liang, Yong Z.
AU  - Sama, Marco Agazio
AU  - De, Arijit
AU  - Nestor, Adrian
TI  - Revealing the neural representations underlying other-race face perception
PY  - 2025
JO  - Frontiers in Human Neuroscience
T2  - Frontiers in Human Neuroscience
VL  - 19
DO  - 10.3389/fnhum.2025.1543840
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000475153&doi=10.3389%2Ffnhum.2025.1543840&partnerID=40&md5=e05220bd193c9a276a5c1500455b3498
AB  - The other-race effect (ORE) refers to poorer recognition for faces of other races than one’s own. This study investigates the neural and representational basis of ORE in East Asian and White participants using behavioral measures, neural decoding, and image reconstruction based on electroencephalography (EEG) data. Our investigation identifies a reliable neural counterpart of ORE, with reduced decoding accuracy for other-race faces, and it relates this result to higher density of other-race face representations in face space. Then, we characterize the temporal dynamics and the prominence of ORE for individual variability at the neural level. Importantly, we use a data-driven image reconstruction approach to reveal visual biases underlying other-race face perception, including a tendency to perceive other-race faces as more typical, younger, and more expressive. These findings provide neural evidence for a classical account of ORE invoking face space compression for other-race faces. Further, they indicate that ORE involves not only reduced identity information but also broader, systematic distortions in visual representation with considerable cognitive and social implications.
KW  - adult
KW  - article
KW  - Caucasian
KW  - cognition
KW  - compression
KW  - controlled study
KW  - decoding
KW  - diagnosis
KW  - East Asian
KW  - electroencephalogram
KW  - electroencephalography
KW  - facial recognition
KW  - female
KW  - human
KW  - human experiment
KW  - image reconstruction
KW  - male
KW  - normal human
KW  - race
ER  - 

TY  - CONF
ID  - Tian2025Brainguard
AU  - Tian, Zhibo
AU  - Quan, Ruijie
AU  - Ma, Fan
AU  - Zhan, Kun
AU  - Yang, Yi
TI  - BRAINGUARD: Privacy-Preserving Multisubject Image Reconstructions from Brain Activities
PY  - 2025
JO  - Proceedings of the AAAI Conference on Artificial Intelligence
T2  - Proceedings of the AAAI Conference on Artificial Intelligence
VL  - 39
IS  - 13
SP  - 14414 - 14422
DO  - 10.1609/aaai.v39i13.33579
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003904728&doi=10.1609%2Faaai.v39i13.33579&partnerID=40&md5=522cc0d761f22ab451ea7379508297cf
AB  - Reconstructing perceived images from human brain activity forms a crucial link between human and machine learning through Brain-Computer Interfaces. Early methods primarily focused on training separate models for each individual to account for individual variability in brain activity, overlooking valuable cross-subject commonalities. Recent advancements have explored multisubject methods, but these approaches face significant challenges, particularly in data privacy and effectively managing individual variability. To overcome these challenges, we introduce BRAINGUARD, a privacy-preserving collaborative training framework designed to enhance image reconstruction from multisubject fMRI data while safeguarding individual privacy. BRAINGUARD employs a collaborative global-local architecture where individual models are trained on each subject’s local data and operate in conjunction with a shared global model that captures and leverages cross-subject patterns. This architecture eliminates the need to aggregate fMRI data across subjects, thereby ensuring privacy preservation. To tackle the complexity of fMRI data, BRAINGUARD integrates a hybrid synchronization strategy, enabling individual models to dynamically incorporate parameters from the global model. By establishing a secure and collaborative training environment, BRAINGUARD not only protects sensitive brain data but also improves the image reconstructions accuracy. Extensive experiments demonstrate that BRAINGUARD sets a new benchmark in both high-level and low-level metrics, advancing the state-of-the-art in brain decoding through its innovative design.
KW  - Privacy by design
KW  - Brain activity
KW  - Collaborative training
KW  - fMRI data
KW  - Global models
KW  - Human brain
KW  - Human learning
KW  - Images reconstruction
KW  - Individual modeling
KW  - Individual variability
KW  - Privacy preserving
KW  - Active learning
ER  - 

TY  - JOUR
ID  - Veronese2025Optimized
AU  - Veronese, Lorenzo
AU  - Moglia, Andrea
AU  - Pecco, Nicolò
AU  - Anthony Della Rosa, Pasquale
AU  - Scifo, Paola
AU  - Mainardi, Luca Tommaso
AU  - Cerveri, Pietro
TI  - Optimized AI-based neural decoding from BOLD fMRI signal for analyzing visual and semantic ROIs in the human visual system
PY  - 2025
JO  - Journal of Neural Engineering
T2  - Journal of Neural Engineering
VL  - 22
IS  - 4
DO  - 10.1088/1741-2552/adfbc2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014157960&doi=10.1088%2F1741-2552%2Fadfbc2&partnerID=40&md5=a49fd5b7d4d17a7c654076edcea2475b
AB  - Objective. AI-based neural decoding reconstructs visual perception by leveraging generative models to map brain activity measured through functional magnetic resonance imaging (fMRI) into the observed visual stimulus. Approach. Traditionally, ridge linear models transform fMRI into a latent space, which is then decoded using variational autoencoders (VAE) or LDMs. Owing to the complexity and noisiness of fMRI data, newer approaches split the reconstruction into two sequential stages, the first one providing a rough visual approximation using a VAE, the second one incorporating semantic information through the adoption of LDM guided by contrastive language-image pre-training (CLIP) embeddings. This work addressed some key scientific and technical gaps of the two-stage neural decoding by: (1) implementing a gated recurrent unit-based architecture to establish a non-linear mapping between the fMRI signal and the VAE latent space, (2) optimizing the dimensionality of the VAE latent space, (3) systematically evaluating the contribution of the first reconstruction stage, and (4) analyzing the impact of different brain regions of interest (ROIs) on reconstruction quality. Main results. Experiments on the NSD, containing 73 000 unique natural images, along with fMRI of eight subjects, demonstrated that the proposed architecture maintained competitive performance while reducing the complexity of its first stage by 85%. The sensitivity analysis showcased that the first reconstruction stage is essential for preserving high structural similarity in the final reconstructions. Restricting analysis to semantic ROIs, while excluding early visual areas, diminished visual coherence, preserving semantics though. The inter-subject repeatability across ROIs was about 92% and 98% for visual and sematic metrics, respectively. Significance. This study represents a key step toward optimized neural decoding architectures leveraging non-linear models for stimulus prediction. Sensitivity analysis highlighted the interplay between the two reconstruction stages, while ROI-based analysis provided strong evidence that the two-stage AI model reflects the brain’s hierarchical processing of visual information.
KW  - Brain
KW  - Brain mapping
KW  - Contrastive Learning
KW  - Decoding
KW  - Image enhancement
KW  - Image reconstruction
KW  - Latent semantic analysis
KW  - Neurons
KW  - Semantics
KW  - Vision
KW  - Auto encoders
KW  - Functional magnetic resonance imaging
KW  - Functional magnetic resonance imaging imagery
KW  - Generative artificial intelligence
KW  - Human Visual System
KW  - Neural decoding
KW  - Region-of-interest
KW  - Regions of interest
KW  - Sensitivity analyzes
KW  - Visual stimuls
KW  - Sensitivity analysis
KW  - Visual languages
KW  - adult
KW  - Article
KW  - artificial intelligence
KW  - BOLD signal
KW  - brain blood flow
KW  - brain region
KW  - deep neural network
KW  - electroencephalogram
KW  - functional magnetic resonance imaging
KW  - Gaussian noise
KW  - human
KW  - human experiment
KW  - image reconstruction
KW  - mean absolute error
KW  - mean squared error
KW  - normal human
KW  - ridge regression
KW  - semantics
KW  - statistical model
KW  - visual information
KW  - visual stimulation
KW  - visual system
KW  - brain
KW  - brain mapping
KW  - diagnostic imaging
KW  - female
KW  - male
KW  - nuclear magnetic resonance imaging
KW  - photostimulation
KW  - physiology
KW  - procedures
KW  - vision
KW  - visual cortex
KW  - young adult
KW  - Adult
KW  - Artificial Intelligence
KW  - Brain Mapping
KW  - Female
KW  - Humans
KW  - Magnetic Resonance Imaging
KW  - Male
KW  - Photic Stimulation
KW  - Visual Cortex
KW  - Visual Perception
KW  - Young Adult
ER  - 

TY  - JOUR
ID  - Xiong2025Interpretable
AU  - Xiong, Daowen
AU  - Hu, Liangliang
AU  - Jin, Jiahao
AU  - Ding, Yikang
AU  - Tan, Congming
AU  - Zhang, Jing
AU  - Tian, Yin
TI  - Interpretable Cross-Modal Alignment Network for EEG Visual Decoding With Algorithm Unrolling
PY  - 2025
JO  - IEEE Transactions on Neural Networks and Learning Systems
T2  - IEEE Transactions on Neural Networks and Learning Systems
SP  - 1-15
DO  - 10.1109/TNNLS.2025.3592646
AB  - Accurate decoding in electroencephalography (EEG) technology, particularly for rapid visual stimuli, remains challenging due to the low signal-to-noise ratio (SNR). Additionally, existing neural networks struggle with issues related to generalization and interpretability. This article proposes a cross-modal aligned network, E2IVAE, which leverages shared information from multiple modalities for self-supervised alignment of EEG to images for extracting visual perceptual information and features a novel EEG encoder, ISTANet, based on algorithm unrolling. This network framework significantly enhances the accuracy and stability of EEG decoding for object recognition in novel classes while reducing the extensive neural data typically required for training neural decoders. The proposed ISTANet employs algorithm unrolling to transform the multilayer sparse coding algorithm into an end-to-end format, extracting features from noisy EEG signals while incorporating the interpretability of traditional machine learning. The experimental results demonstrate that our method achieves SOTA top-1 accuracy of 62.39% and top-5 accuracy of 88.98% on a comprehensive rapid serial visual presentation (RSVP) dataset for public comparison in a 200-class zero-shot neural decoding task. Additionally, ISTANet enables visualization and analysis of multiscale atom features and overall reconstruction features, exploring biological plausibility across temporal, spatial, and spectral dimensions. On another more challenging RSVP large-scale dataset, the proposed framework also achieves significantly above chance-level performance, proving its robustness and generalization. This research provides critical insights into neural decoding and brain–computer interfaces (BCIs) within the fields of cognitive science and artificial intelligence.
KW  - Electroencephalography
KW  - Decoding
KW  - Feature extraction
KW  - Visualization
KW  - Brain modeling
KW  - Training
KW  - Encoding
KW  - Image reconstruction
KW  - Object recognition
KW  - Data mining
KW  - Algorithm unrolling
KW  - brain–computer interfaces (BCIs)
KW  - electroencephalography (EEG) decoding
KW  - interpretability
SN  - 2162-2388
ER  - 

TY  - JOUR
ID  - Xu2025Brainvision
AU  - Xu, Ting
AU  - Yu, Lianzhi
AU  - Zheng, Yongwei
AU  - Huang, Shuai
TI  - BrainVision: Cross-domain EEG decoding for visual content retrieval and reconstruction
PY  - 2025
JO  - Neuroscience
T2  - Neuroscience
VL  - 584
SP  - 190 - 205
DO  - 10.1016/j.neuroscience.2025.07.047
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013792683&doi=10.1016%2Fj.neuroscience.2025.07.047&partnerID=40&md5=93e752eb9da3c8e30876380bf3397356
AB  - Understanding human visual intent through brain signals remains a fundamental challenge in neuroscience and artificial intelligence. Despite recent advances in brain decoding, existing approaches typically operate within isolated datasets and modalities, limiting their generalization capabilities. This paper introduces BrainVision, a novel framework that bridges visual recognition and emotional EEG datasets to enable comprehensive visual content generation through cross-domain learning. BrainVision addresses the critical challenge of leveraging complementary information across heterogeneous EEG sources by implementing a unified cross-domain alignment strategy. Our framework maps neural patterns from the THINGS-EEG visual recognition dataset and the DEAP emotional response dataset into a shared representation space, enabling three distinct visual output capabilities: (1) accurate content retrieval and classification, (2) detailed linguistic descriptions through adapter-enhanced large language models, and (3) high-fidelity image reconstruction via stable diffusion models. Experimental results demonstrate that BrainVision significantly outperforms single-domain approaches, achieving a 15.3% increase in retrieval accuracy and a 12.7% improvement in structural similarity for reconstructed images compared to state-of-the-art methods. Furthermore, our framework demonstrates robust zero-shot generalization, maintaining 82% of its performance when applied to novel stimuli not seen during training. The multi-modal outputs provide complementary interpretations of neural activity, offering a more comprehensive understanding of visual intent. Our findings establish that integrating diverse neural datasets substantially enhances the capabilities of brain decoding systems, providing a promising direction for developing more intuitive and versatile brain-computer interfaces. BrainVision represents an important step toward bridging the gap between neural activity and rich visual experiences across different cognitive domains.
KW  - Article
KW  - artificial intelligence
KW  - brain signal processing
KW  - brain vision
KW  - Brain2Image
KW  - caption generation quality
KW  - cognition
KW  - cross domain knowledge transfer
KW  - cross domain learning
KW  - cross domain transfer analysis
KW  - data analysis
KW  - data processing
KW  - dataset configuration analysis
KW  - decoding
KW  - electroencephalogram
KW  - emotion
KW  - event related potential
KW  - human
KW  - image reconstruction
KW  - information processing
KW  - information retrieval
KW  - inter subject variability analysis
KW  - knowledge
KW  - large language model
KW  - learning
KW  - learning algorithm
KW  - multi modal visual output generation
KW  - multi-task learning
KW  - nerve cell network
KW  - neuroimaging
KW  - signal processing
KW  - THINGS-EEG dataset
KW  - transfer of learning
KW  - visual adaptation
KW  - visual content retrieval
KW  - visual memory
KW  - adult
KW  - brain
KW  - electroencephalography
KW  - female
KW  - male
KW  - physiology
KW  - procedures
KW  - vision
KW  - Adult
KW  - Brain
KW  - Electroencephalography
KW  - Emotions
KW  - Female
KW  - Humans
KW  - Male
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - Yu2025Robust
AU  - Yu, Zhaofei
AU  - Bu, Tong
AU  - Zhang, Yijun
AU  - Jia, Shanshan
AU  - Huang, Tiejun
AU  - Liu, Jian K.
TI  - Robust Decoding of Rich Dynamical Visual Scenes With Retinal Spikes
PY  - 2025
JO  - IEEE Transactions on Neural Networks and Learning Systems
T2  - IEEE Transactions on Neural Networks and Learning Systems
VL  - 36
IS  - 2
SP  - 3396-3409
DO  - 10.1109/TNNLS.2024.3351120
AB  - Sensory information transmitted to the brain activates neurons to create a series of coping behaviors. Understanding the mechanisms of neural computation and reverse engineering the brain to build intelligent machines requires establishing a robust relationship between stimuli and neural responses. Neural decoding aims to reconstruct the original stimuli that trigger neural responses. With the recent upsurge of artificial intelligence, neural decoding provides an insightful perspective for designing novel algorithms of brain–machine interface. For humans, vision is the dominant contributor to the interaction between the external environment and the brain. In this study, utilizing the retinal neural spike data collected over multi trials with visual stimuli of two movies with different levels of scene complexity, we used a neural network decoder to quantify the decoded visual stimuli with six different metrics for image quality assessment establishing comprehensive inspection of decoding. With the detailed and systematical study of the effect and single and multiple trials of data, different noise in spikes, and blurred images, our results provide an in-depth investigation of decoding dynamical visual scenes using retinal spikes. These results provide insights into the neural coding of visual scenes and services as a guideline for designing next-generation decoding algorithms of neuroprosthesis and other devices of brain–machine interface.
KW  - Decoding
KW  - Visualization
KW  - Retina
KW  - Image reconstruction
KW  - Neurons
KW  - Convolution
KW  - Training
KW  - Deep learning
KW  - image reconstruction
KW  - neural decoding
KW  - neural spikes
KW  - video
KW  - visual scenes
SN  - 2162-2388
DA  - Feb
ER  - 

TY  - JOUR
ID  - Zhu2025Fmriges
AU  - Zhu, Chunzheng
AU  - Shao, Jialin
AU  - Lin, Jianxin
AU  - Wang, Yijun
AU  - Wang, Jing
AU  - Tang, Jinhui
AU  - Li, Kenli
TI  - fMRI2GES: Co-Speech Gesture Reconstruction From fMRI Signal With Dual Brain Decoding Alignment
PY  - 2025
JO  - IEEE Transactions on Circuits and Systems for Video Technology
T2  - IEEE Transactions on Circuits and Systems for Video Technology
VL  - 35
IS  - 9
SP  - 9017-9029
DO  - 10.1109/TCSVT.2025.3558125
AB  - Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired {brain, speech, gesture} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, fMRI2GES, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using Dual Brain Decoding Alignment. This method relies on two key components: 1) observed texts that elicit brain responses, and 2) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.
KW  - Decoding
KW  - Functional magnetic resonance imaging
KW  - Brain modeling
KW  - Image reconstruction
KW  - Training
KW  - Data models
KW  - Recording
KW  - Neuroscience
KW  - Linguistics
KW  - Legged locomotion
KW  - fMRI signal
KW  - diffusoin models
KW  - neurosciences
SN  - 1558-2205
DA  - Sep.
ER  - 

TY  - CONF
ID  - Akbari2024Joint
AU  - Akbari, Ali
AU  - Sanjar, Kosar
AU  - Yousefnezhad, Muhammad
AU  - Mirian, Maryam Sadat
AU  - Arasteh, Emad Malekzadeh
TI  - Joint Learning for Visual Reconstruction from the Brain Activity: Hierarchical Representation of Image Perception with EEG-Vision Transformer
PY  - 2024
JO  - Proceedings of Machine Learning Research
T2  - Proceedings of Machine Learning Research
VL  - 285
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014736717&partnerID=40&md5=95ef7cdfe2da78435a3bfdd9b45b1b61
AB  - Reconstructing visual stimuli from brain activity is a challenging problem, particu-larly when using EEG data, which is more affordable and accessible than fMRI but noisier and lower in spatial resolution. In this paper, we present Hierarchical-ViT, a novel framework designed to improve the quality and precision of EEG-based image reconstruction by integrating hierarchical visual feature extraction, vision transformer-based EEG (EEG-ViT) processing, and CLIP-based joint learning. Inspired by the hierarchical nature of the human visual system, our model progressively captures complex visual features—such as edges, textures, and shapes—through a multi-stage processing approach. These features are aligned with EEG signals processed by the EEG-ViT model, allowing for the creation of a shared latent space that enhances contrastive learning. A StyleGAN is then em-ployed to generate high-resolution images from these aligned representations. We evaluated our method on two benchmark datasets, EEGCVPR40 and ThoughtViz, achieving superior results compared to existing approaches in terms of Inception Score (IS), Kernel Inception Distance (KID), and Frechet Inception Distance (FID) for EEGCVPR, and IS and KID for the ThoughtViz dataset. Through an ablation study, we underscored the feasibility of hierarchical feature extraction, while the multivariate analysis of variance (MANOVA) test confirmed the distinctiveness of the learned feature spaces. In conclusion, our results show the feasibility and uniqueness of using hierarchical filtering of perceived images combined with EEG-ViT-based features to improve brain decoding from EEG data.
KW  - Biomedical signal processing
KW  - Brain
KW  - Computer vision
KW  - Electroencephalography
KW  - Feature extraction
KW  - Image enhancement
KW  - Image reconstruction
KW  - Neurophysiology
KW  - Textures
KW  - Brain activity
KW  - Hierarchical representation
KW  - Human Visual System
KW  - Image perception
KW  - Images reconstruction
KW  - Joint learning
KW  - Spatial resolution
KW  - Visual feature extraction
KW  - Visual reconstruction
KW  - Visual stimulus
KW  - Extraction
KW  - Multivariant analysis
ER  - 

TY  - CONF
ID  - Balisacan2024Neurovis
AU  - Balisacan, Gabriela M.
AU  - Paulo, Anne Therese A.
TI  - Neuro-Vis: Guided Complex Image Reconstruction from Brain Signals Using Multiple Semantic and Perceptual Controls
PY  - 2024
DO  - 10.1145/3661725.3661744
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197256076&doi=10.1145%2F3661725.3661744&partnerID=40&md5=1f14148432beb75c4454987b8e646bac
AB  - The externalization of the state of one's mind, which people refer to as "mind reading"in science fiction, is currently being realized through brain decoding research. This field of study aims to deepen our understanding of the human brain, which is among the least understood known biological structures, and to build better foundations for brain-computer interfaces. With the success of state-of-the-art latent diffusion models in image synthesis, a trend in recent studies is to map fMRI recordings to the image embedding space of these generative models. While this method significantly improved image reconstructions in terms of semantics, preserving perceptual features without losing semantic information remains challenging, especially with complex images of natural scenes. This research introduces Neuro-Vis, a novel fMRI-to-image pipeline based on Stable Diffusion that effectively integrates multiple semantic controls through predicted image embeddings and captions and multiple lightweight perceptual controls through predicted blurry initial images, depth maps, and color palettes. Neuro-Vis outperforms the current state-of-the-art methods in terms of consistency in low-level features while also rivaling them in terms of semantics. Furthermore, ablation experiments demonstrate the effectiveness of each component in Neuro-Vis for fMRI-to-image reconstruction.
KW  - Brain computer interface
KW  - Embeddings
KW  - Functional neuroimaging
KW  - Image enhancement
KW  - Image reconstruction
KW  - Semantic Web
KW  - Semantics
KW  - Brain decoding
KW  - Complex image
KW  - FMRI-to-image reconstruction
KW  - Image captioning
KW  - Image embedding
KW  - Images reconstruction
KW  - Perceptual control
KW  - Perceptual guidance
KW  - Semantic guidance
KW  - Visual decoding
KW  - Decoding
ER  - 

TY  - CONF
ID  - Chen2024Ieeecvf
AU  - Chen, Jiaxuan
AU  - Qi, Yu
AU  - Wang, Yueming
AU  - Pan, Gang
TI  - Mind Artist: Creating Artistic Snapshots with Human Thought
PY  - 2024
BT  - 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
T2  - 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
SP  - 27197-27207
DO  - 10.1109/CVPR52733.2024.02569
AB  - We introduce Mind Artist (MindArt), a novel and efficient neural decoding architecture to snap artistic photographs from our mind in a controllable manner. Recently, progress has been made in image reconstruction with non-invasive brain recordings, but it's still difficult to generate realistic images with high semantic fidelity due to the scarcity of data annotations. Unlike previous methods, this work casts the neural decoding into optimal transport (OT) and representation decoupling problems. Specifically, under discrete OT theory, we design a graph matching-guided neural representation learning framework to seek the underlying correspondences between conceptual semantics and neural signals, which yields a natural and meaningful self-supervisory task. Moreover, the proposed MindArt, structured with multiple stand-alone modal branches, enables the seamless incorporation of semantic representation into any visual style information, thus leaving it to have multi-modal reconstruction and training-free semantic editing capabilities. By doing so, the reconstructed images of MindArt have phenomenal realism both in terms of semantics and appearance. We compare our MindArt with leading alternatives, and achieve SOTA performance in different decoding tasks. Importantly, our approach can directly generate a series of stylized “mind snapshots” w/o extra optimizations, which may open up more potential applications. Code is available at https://github.com/JxuanC/MindArt.
KW  - Visualization
KW  - Solid modeling
KW  - Three-dimensional displays
KW  - Computational modeling
KW  - Semantics
KW  - Linguistics
KW  - Decoding
KW  - Neural decoding
KW  - Representation learning
KW  - Multimodal learning
SN  - 2575-7075
DA  - June
ER  - 

TY  - JOUR
ID  - Dado2024Braingan
AU  - Dado, Thirza
AU  - Papale, Paolo
AU  - Lozano, Antonio M.
AU  - Le, Lynn
AU  - Wang, Feng
AU  - Van Gerven, Marcel A.J.
AU  - Roelfsema, Pieter Roelf
AU  - Güçlütürk, Yaǧmur
AU  - Güçlü, Umut
TI  - Brain2GAN: Feature-disentangled neural encoding and decoding of visual perception in the primate brain
PY  - 2024
JO  - PLOS Computational Biology
T2  - PLOS Computational Biology
VL  - 20
IS  - 5 May
DO  - 10.1371/journal.pcbi.1012058
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193012537&doi=10.1371%2Fjournal.pcbi.1012058&partnerID=40&md5=466d61606677adf237eeb00bac6088f7
AB  - A challenging goal of neural coding is to characterize the neural representations underlying visual perception. To this end, multi-unit activity (MUA) of macaque visual cortex was recorded in a passive fixation task upon presentation of faces and natural images. We analyzed the relationship between MUA and latent representations of state-of-the-art deep generative models, including the conventional and feature-disentangled representations of generative adversarial networks (GANs) (i.e., z- and w-latents of StyleGAN, respectively) and language-contrastive representations of latent diffusion networks (i.e., CLIP-latents of Stable Diffusion). A mass univariate neural encoding analysis of the latent representations showed that feature-disentangled w representations outperform both z and CLIP representations in explaining neural responses. Further, w-latent features were found to be positioned at the higher end of the complexity gradient which indicates that they capture visual information relevant to high-level neural activity. Subsequently, a multivariate neural decoding analysis of the feature-disentangled representations resulted in state-of-the-art spatiotemporal reconstructions of visual perception. Taken together, our results not only highlight the important role of feature-disentanglement in shaping high-level neural representations underlying visual perception but also serve as an important benchmark for the future of neural coding.
KW  - Decoding
KW  - Encoding (symbols)
KW  - Neurons
KW  - Signal encoding
KW  - Vision
KW  - Encoding and decoding
KW  - Face images
KW  - Multi-unit activity
KW  - Neural coding
KW  - Neural decoding
KW  - Neural encoding
KW  - Neural representations
KW  - State of the art
KW  - Visual cortexes
KW  - Visual perception
KW  - Generative adversarial networks
KW  - article
KW  - benchmarking
KW  - controlled study
KW  - diagnosis
KW  - diffusion
KW  - generative model
KW  - human
KW  - human experiment
KW  - Macaca
KW  - nerve potential
KW  - nonhuman
KW  - primate
KW  - vision
KW  - visual cortex
KW  - visual information
KW  - animal
KW  - artificial neural network
KW  - bioinformatics
KW  - biological model
KW  - brain
KW  - male
KW  - nerve cell
KW  - photostimulation
KW  - physiology
KW  - rhesus monkey
KW  - Animals
KW  - Brain
KW  - Computational Biology
KW  - Macaca mulatta
KW  - Male
KW  - Models, Neurological
KW  - Neural Networks, Computer
KW  - Photic Stimulation
KW  - Visual Cortex
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - DeLuca2024Predicting
AU  - De Luca, Daniela
AU  - Moccia, Sara
AU  - Lupori, Leonardo
AU  - Mazziotti, Raffaele
AU  - Pizzorusso, T.
AU  - Micera, Silvestro
TI  - Predicting Visual Stimuli From Cortical Response Recorded With Wide-Field Imaging in a Mouse
PY  - 2024
JO  - IEEE Sensors Journal
T2  - IEEE Sensors Journal
VL  - 24
IS  - 6
SP  - 7299-7307
DO  - 10.1109/JSEN.2023.3335613
AB  - Neural decoding of the visual system is a subject of research interest, both to understand how the visual system works and to be able to use this knowledge in areas, such as computer vision or brain–computer interfaces. Spike-based decoding is often used, but it is difficult to record data from the whole visual cortex, and it requires proper preprocessing. We here propose a decoding method that combines wide-field calcium brain imaging, which allows us to obtain large-scale visualization of cortical activity with a high signal-to-noise ratio (SNR), and convolutional neural networks (CNNs). A mouse was presented with ten different visual stimuli, and the activity from its primary visual cortex (V1) was recorded. A CNN we designed was then compared with other existing commonly used CNNs, that were trained to classify the visual stimuli from wide-field calcium imaging images, obtaining a weighted $F1$ score of more than 0.70 on the test set, showing it is possible to automatically detect what is present in the visual field of the animal.
KW  - Visualization
KW  - Convolutional neural networks
KW  - Decoding
KW  - Calcium
KW  - Imaging
KW  - Fluorescence
KW  - Deep learning
KW  - Transfer learning
KW  - Prosthetics
KW  - Neural activity
KW  - Deep learning
KW  - transfer learning
KW  - visual cortex
KW  - visual prostheses
KW  - wide-field imaging
SN  - 1558-1748
DA  - March
ER  - 

TY  - JOUR
ID  - Ferrante2024Retrieving
AU  - Ferrante, Matteo
AU  - Boccato, Tommaso
AU  - Passamonti, Luca
AU  - Toschi, Nicola
TI  - Retrieving and reconstructing conceptually similar images from fMRI with latent diffusion models and a neuro-inspired brain decoding model
PY  - 2024
JO  - Journal of Neural Engineering
T2  - Journal of Neural Engineering
VL  - 21
IS  - 4
DO  - 10.1088/1741-2552/ad593c
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197363165&doi=10.1088%2F1741-2552%2Fad593c&partnerID=40&md5=be30a60850031c296abea98f3281e465
AB  - Objective. Brain decoding is a field of computational neuroscience that aims to infer mental states or internal representations of perceptual inputs from measurable brain activity. This study proposes a novel approach to brain decoding that relies on semantic and contextual similarity. Approach. We use several functional magnetic resonance imaging (fMRI) datasets of natural images as stimuli and create a deep learning decoding pipeline inspired by the bottom-up and top-down processes in human vision. Our pipeline includes a linear brain-to-feature model that maps fMRI activity to semantic visual stimuli features. We assume that the brain projects visual information onto a space that is homeomorphic to the latent space of last layer of a pretrained neural network, which summarizes and highlights similarities and differences between concepts. These features are categorized in the latent space using a nearest-neighbor strategy, and the results are used to retrieve images or condition a generative latent diffusion model to create novel images. Main results. We demonstrate semantic classification and image retrieval on three different fMRI datasets: Generic Object Decoding (vision perception and imagination), BOLD5000, and NSD. In all cases, a simple mapping between fMRI and a deep semantic representation of the visual stimulus resulted in meaningful classification and retrieved or generated images. We assessed quality using quantitative metrics and a human evaluation experiment that reproduces the multiplicity of conscious and unconscious criteria that humans use to evaluate image similarity. Our method achieved correct evaluation in over 80% of the test set. Significance. Our study proposes a novel approach to brain decoding that relies on semantic and contextual similarity. The results demonstrate that measurable neural correlates can be linearly mapped onto the latent space of a neural network to synthesize images that match the original content. These findings have implications for both cognitive neuroscience and artificial intelligence.
KW  - Brain
KW  - Classification (of information)
KW  - Decoding
KW  - Deep learning
KW  - Diffusion
KW  - Image reconstruction
KW  - Magnetic resonance imaging
KW  - Multilayer neural networks
KW  - Pipelines
KW  - Brain decoding
KW  - Computational neuroscience
KW  - Diffusion model
KW  - Functional magnetic resonance imaging
KW  - Latent
KW  - Mental state
KW  - Neural-networks
KW  - Similar image
KW  - State representation
KW  - Visual stimulus
KW  - Semantics
KW  - Article
KW  - artificial intelligence
KW  - autoencoder
KW  - brain decoding model
KW  - clustering algorithm
KW  - conceptual model
KW  - controlled study
KW  - convolutional neural network
KW  - deep learning
KW  - diffusion
KW  - electroencephalogram
KW  - functional magnetic resonance imaging
KW  - hemodynamics
KW  - human
KW  - image reconstruction
KW  - image retrieval
KW  - magnetic field
KW  - model
KW  - neurobiology
KW  - neuroscience
KW  - visual cortex
KW  - visual stimulation
KW  - artificial neural network
KW  - biological model
KW  - brain
KW  - brain mapping
KW  - diagnostic imaging
KW  - image processing
KW  - nuclear magnetic resonance imaging
KW  - photostimulation
KW  - physiology
KW  - procedures
KW  - semantics
KW  - vision
KW  - Brain Mapping
KW  - Deep Learning
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Magnetic Resonance Imaging
KW  - Models, Neurological
KW  - Neural Networks, Computer
KW  - Photic Stimulation
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - Ferrante2024Decoding
AU  - Ferrante, Matteo
AU  - Boccato, Tommaso
AU  - Bargione, Stefano
AU  - Toschi, Nicola
TI  - Decoding visual brain representations from electroencephalography through knowledge distillation and latent diffusion models
PY  - 2024
JO  - Computers in Biology and Medicine
T2  - Computers in Biology and Medicine
VL  - 178
DO  - 10.1016/j.compbiomed.2024.108701
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196144814&doi=10.1016%2Fj.compbiomed.2024.108701&partnerID=40&md5=6a50ce04957122140b02e8e8c30bb254
AB  - Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain–computer interfaces. Our study presents an innovative method that employs knowledge distillation to train an EEG classifier and reconstruct images from the ImageNet and THINGS-EEG 2 datasets using only electroencephalography (EEG) data from participants who have viewed the images themselves (i.e. “brain decoding”). We analyzed EEG recordings from 6 participants for the ImageNet dataset and 10 for the THINGS-EEG 2 dataset, exposed to images spanning unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 87%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism based on pre-trained latent diffusion models, which allowed us to generate an estimate of the images that had elicited EEG activity. Therefore, our architecture not only decodes images from neural activity but also offers a credible image reconstruction from EEG only, paving the way for, e.g., swift, individualized feedback experiments.
KW  - Brain
KW  - Brain computer interface
KW  - Classification (of information)
KW  - Convolution
KW  - Decoding
KW  - Distillation
KW  - Electroencephalography
KW  - Electrophysiology
KW  - Image classification
KW  - Neurons
KW  - Personnel training
KW  - Semantics
KW  - BCI vision
KW  - Brain activity
KW  - Brain decoding
KW  - Convolutional neural network
KW  - Diffusion model
KW  - Electroencephalography decoding
KW  - Human brain
KW  - Images reconstruction
KW  - Research domains
KW  - Visual representations
KW  - Image reconstruction
KW  - Article
KW  - classification algorithm
KW  - classifier
KW  - clustering algorithm
KW  - comparative study
KW  - computer vision
KW  - contrastive language image pre training
KW  - controlled study
KW  - convolutional neural network
KW  - deep learning
KW  - diffusion
KW  - distillation
KW  - electroencephalogram
KW  - electroencephalography
KW  - follow up
KW  - human
KW  - image reconstruction
KW  - k means clustering
KW  - knowledge
KW  - knowledge distillation
KW  - latent period
KW  - logistic regression analysis
KW  - long short term memory network
KW  - measurement accuracy
KW  - probability
KW  - recurrent neural network
KW  - short time Fourier transform
KW  - signal processing
KW  - time frequency decomposition
KW  - time series analysis
KW  - visual attention
KW  - visual memory
KW  - adult
KW  - artificial neural network
KW  - brain
KW  - brain computer interface
KW  - female
KW  - image processing
KW  - male
KW  - physiology
KW  - procedures
KW  - Adult
KW  - Brain-Computer Interfaces
KW  - Female
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Male
KW  - Neural Networks, Computer
KW  - Signal Processing, Computer-Assisted
ER  - 

TY  - JOUR
ID  - Greenidge2024Efficient
AU  - Greenidge, C. Daniel
AU  - Scholl, Benjamin
AU  - Yates, Jacob L.
AU  - Pillow, Jonathan W.
TI  - Efficient Decoding of Large-Scale Neural Population Responses With Gaussian-Process Multiclass Regression
PY  - 2024
JO  - Neural Computation
T2  - Neural Computation
VL  - 36
IS  - 2
SP  - 175-226
DO  - 10.1162/neco_a_01630
AB  - Neural decoding methods provide a powerful tool for quantifying the information content of neural population codes and the limits imposed by correlations in neural activity. However, standard decoding methods are prone to overfitting and scale poorly to high-dimensional settings. Here, we introduce a novel decoding method to overcome these limitations. Our approach, the gaussian process multiclass decoder (GPMD), is well suited to decoding a continuous low-dimensional variable from high-dimensional population activity and provides a platform for assessing the importance of correlations in neural population codes. The GPMD is a multinomial logistic regression model with a gaussian process prior over the decoding weights. The prior includes hyperparameters that govern the smoothness of each neuron's decoding weights, allowing automatic pruning of uninformative neurons during inference. We provide a variational inference method for fitting the GPMD to data, which scales to hundreds or thousands of neurons and performs well even in data sets with more neurons than trials. We apply the GPMD to recordings from primary visual cortex in three species: monkey, ferret, and mouse. Our decoder achieves state-of-the-art accuracy on all three data sets and substantially outperforms independent Bayesian decoding, showing that knowledge of the correlation structure is essential for optimal decoding in all three species.
SN  - 0899-7667
DA  - Jan
ER  - 

TY  - JOUR
ID  - Ikegawa2024Text
AU  - Ikegawa, Yuya
AU  - Fukuma, Ryohei
AU  - Sugano, Hidenori Sugano
AU  - Oshino, Satoru
AU  - Tani, Naoki
AU  - Tamura, Kentaro
AU  - Iimura, Yasushi Iimura
AU  - Suzuki, Hiroharu
AU  - Yamamoto, Shota
AU  - Fujita, Yuya
AU  - Nishimoto, Shinji
AU  - Kishima, Haruhiko
AU  - Yanagisawa, Takufumi
TI  - Text and image generation from intracranial electroencephalography using an embedding space for text and images
PY  - 2024
JO  - Journal of Neural Engineering
T2  - Journal of Neural Engineering
VL  - 21
IS  - 3
DO  - 10.1088/1741-2552/ad417a
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193773877&doi=10.1088%2F1741-2552%2Fad417a&partnerID=40&md5=6944c52692d205487b14736a17db2770
AB  - Objective. Invasive brain-computer interfaces (BCIs) are promising communication devices for severely paralyzed patients. Recent advances in intracranial electroencephalography (iEEG) coupled with natural language processing have enhanced communication speed and accuracy. It should be noted that such a speech BCI uses signals from the motor cortex. However, BCIs based on motor cortical activities may experience signal deterioration in users with motor cortical degenerative diseases such as amyotrophic lateral sclerosis. An alternative approach to using iEEG of the motor cortex is necessary to support patients with such conditions. Approach. In this study, a multimodal embedding of text and images was used to decode visual semantic information from iEEG signals of the visual cortex to generate text and images. We used contrastive language-image pretraining (CLIP) embedding to represent images presented to 17 patients implanted with electrodes in the occipital and temporal cortices. A CLIP image vector was inferred from the high-γ power of the iEEG signals recorded while viewing the images. Main results. Text was generated by CLIPCAP from the inferred CLIP vector with better-than-chance accuracy. Then, an image was created from the generated text using StableDiffusion with significant accuracy. Significance. The text and images generated from iEEG through the CLIP embedding vector can be used for improved communication.
KW  - Brain computer interface
KW  - Decoding
KW  - Deterioration
KW  - Electrophysiology
KW  - Embeddings
KW  - Image enhancement
KW  - Medical computing
KW  - Natural language processing systems
KW  - Neurodegenerative diseases
KW  - Semantics
KW  - Communication device
KW  - Image generations
KW  - Intracranial EEG
KW  - Motor-cortex
KW  - Natural languages
KW  - Neural decoding
KW  - Paralyzed patients
KW  - Pre-training
KW  - Text generations
KW  - Electroencephalography
KW  - Article
KW  - clinical article
KW  - contrastive language image pretraining
KW  - controlled study
KW  - electrocorticography
KW  - electroencephalography
KW  - embedding
KW  - human
KW  - multilayer perceptron
KW  - nested cross validation
KW  - occipital cortex
KW  - temporal cortex
KW  - adult
KW  - brain computer interface
KW  - electrode implant
KW  - female
KW  - male
KW  - middle aged
KW  - photostimulation
KW  - procedures
KW  - young adult
KW  - Adult
KW  - Brain-Computer Interfaces
KW  - Electrocorticography
KW  - Electrodes, Implanted
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Photic Stimulation
KW  - Young Adult
ER  - 

TY  - CONF
ID  - Ishizaki2024Joint
AU  - Ishizaki, Fumi
AU  - Kobayashi, Ichiro
TI  - A Study on Brain Decoding of Image Stimuli Using a Diffusion Model
PY  - 2024
BT  - 2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCIS&ISIS)
T2  - 2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCIS&ISIS)
SP  - 1-4
DO  - 10.1109/SCISISIS61014.2024.10759955
AB  - Humans recognize the external world by processing information received by the eyes and other sensory organs in the brain. Understanding how the human brain processes complex information from the outside world is expected to improve the performance of image and speech recognition technologies, which have made remarkable progress in recent years. In this study, we focus on brain activity decoding of visual experience, aim to read what humans are looking at by predicting image features from brain activity data, and further attempt to develop a method to output high-definition and semantically valid images by generating images from predicted features using a diffusion model. As a result, similarity to the stimulus image was confirmed in the image generated by the training data, but the evaluation data confirmed that there is still room for further study.
KW  - Visualization
KW  - Noise reduction
KW  - Training data
KW  - Speech recognition
KW  - Predictive models
KW  - Diffusion models
KW  - Brain modeling
KW  - Data models
KW  - Decoding
KW  - Image reconstruction
KW  - Brain decoding
KW  - diffusion models
KW  - image stimu-lation
DA  - Nov
ER  - 

TY  - JOUR
ID  - KoideMajima2024Mental
AU  - Koide-Majima, Naoko
AU  - Nishimoto, Shinji
AU  - Majima, Kei
TI  - Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based Bayesian estimation
PY  - 2024
JO  - Neural Networks
T2  - Neural Networks
VL  - 170
SP  - 349 - 363
DO  - 10.1016/j.neunet.2023.11.024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178352755&doi=10.1016%2Fj.neunet.2023.11.024&partnerID=40&md5=0c0563507dbd1c34cef2ea6a0fa4f2fd
AB  - Visual images observed by humans can be reconstructed from their brain activity. However, the visualization (externalization) of mental imagery is challenging. Only a few studies have reported successful visualization of mental imagery, and their visualizable images have been limited to specific domains such as human faces or alphabetical letters. Therefore, visualizing mental imagery for arbitrary natural images stands as a significant milestone. In this study, we achieved this by enhancing a previous method. Specifically, we demonstrated that the visual image reconstruction method proposed in the seminal study by Shen et al. (2019) heavily relied on low-level visual information decoded from the brain and could not efficiently utilize the semantic information that would be recruited during mental imagery. To address this limitation, we extended the previous method to a Bayesian estimation framework and introduced the assistance of semantic information into it. Our proposed framework successfully reconstructed both seen images (i.e., those observed by the human eye) and imagined images from brain activity. Quantitative evaluation showed that our framework could identify seen and imagined images highly accurately compared to the chance accuracy (seen: 90.7%, imagery: 75.6%, chance accuracy: 50.0%). In contrast, the previous method could only identify seen images (seen: 64.3%, imagery: 50.4%). These results suggest that our framework would provide a unique tool for directly investigating the subjective contents of the brain such as illusions, hallucinations, and dreams.
KW  - Bayesian networks
KW  - Decoding
KW  - Deep neural networks
KW  - Image reconstruction
KW  - Neurophysiology
KW  - Semantics
KW  - Visualization
KW  - Bayesian estimations
KW  - Brain activity
KW  - Brain decoding
KW  - Human brain
KW  - Images reconstruction
KW  - Mental imagery
KW  - Mental images
KW  - Semantic representation
KW  - Semantics Information
KW  - Visual image
KW  - Brain
KW  - adult
KW  - Article
KW  - Bayesian network
KW  - comparative study
KW  - deep neural network
KW  - dream
KW  - electroencephalogram
KW  - female
KW  - hallucination
KW  - human
KW  - illusion
KW  - image reconstruction
KW  - male
KW  - measurement accuracy
KW  - neuroimaging
KW  - semantics
KW  - visual information
KW  - young adult
KW  - artificial neural network
KW  - Bayes theorem
KW  - brain
KW  - brain mapping
KW  - diagnostic imaging
KW  - image processing
KW  - imagination
KW  - nuclear magnetic resonance imaging
KW  - procedures
KW  - Bayes Theorem
KW  - Brain Mapping
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Imagination
KW  - Magnetic Resonance Imaging
KW  - Neural Networks, Computer
ER  - 

TY  - JOUR
ID  - Kuang2024Natural
AU  - Kuang, Mei
AU  - Zhan, Zongyi
AU  - Gao, Shaobing
TI  - Natural Image Reconstruction from fMRI Based on Node–Edge Interaction and Multi–Scale Constraint
PY  - 2024
JO  - Brain Sciences
T2  - Brain Sciences
VL  - 14
IS  - 3
DO  - 10.3390/brainsci14030234
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188703271&doi=10.3390%2Fbrainsci14030234&partnerID=40&md5=10192334498ce21b29ead1e64fa8fdc9
AB  - Reconstructing natural stimulus images using functional magnetic resonance imaging (fMRI) is one of the most challenging problems in brain decoding and is also the crucial component of a brain–computer interface. Previous methods cannot fully exploit the information about interactions among brain regions. In this paper, we propose a natural image reconstruction method based on node–edge interaction and a multi–scale constraint. Inspired by the extensive information interactions in the brain, a novel graph neural network block with node–edge interaction (NEI–GNN block) is presented, which can adequately model the information exchange between brain areas via alternatively updating the nodes and edges. Additionally, to enhance the quality of reconstructed images in terms of both global structure and local detail, we employ a multi–stage reconstruction network that restricts the reconstructed images in a coarse–to–fine manner across multiple scales. Qualitative experiments on the generic object decoding (GOD) dataset demonstrate that the reconstructed images contain accurate structural information and rich texture details. Furthermore, the proposed method surpasses the existing state–of–the–art methods in terms of accuracy in the commonly used n–way evaluation. Our approach achieves 82.00%, 59.40%, 45.20% in n–way mean squared error (MSE) evaluation and 83.50%, 61.80%, 46.00% in n–way structural similarity index measure (SSIM) evaluation, respectively. Our experiments reveal the importance of information interaction among brain areas and also demonstrate the potential for developing visual–decoding brain–computer interfaces.
KW  - Article
KW  - brain region
KW  - deep learning
KW  - event related potential
KW  - functional magnetic resonance imaging
KW  - human
KW  - human experiment
KW  - image quality
KW  - image reconstruction
KW  - image segmentation
KW  - imagery
KW  - mean squared error
KW  - nerve cell differentiation
KW  - nerve cell network
KW  - speech perception
KW  - stimulus
KW  - support vector machine
KW  - task performance
KW  - total quality management
KW  - training
ER  - 

TY  - JOUR
ID  - Li2024Multiscale
AU  - Li, Ziyu
AU  - Li, Qing
AU  - Zhu, Zhiyuan
AU  - Hu, Zhongyi
AU  - Wu, Xia
TI  - Multi-Scale Spatio-Temporal Fusion With Adaptive Brain Topology Learning for fMRI Based Neural Decoding
PY  - 2024
JO  - IEEE Journal of Biomedical and Health Informatics
T2  - IEEE Journal of Biomedical and Health Informatics
VL  - 28
IS  - 1
SP  - 262-272
DO  - 10.1109/JBHI.2023.3327023
AB  - Neural decoding aims to extract information from neurons' activities to reveal how the brain functions. Due to the inherent spatial and temporal characteristics of brain signals, spatio-temporal computing has become a hot topic for neural decoding. However, the extant spatio-temporal decoding methods usually use static brain topology, ignoring the dynamic patterns of the interaction between brain regions. Further, they do not identify the hierarchical organization of brain topology, leading to only superficial insight into brain spatio-temporal interactions. Therefore, here we propose a novel framework, the Multi-Scale Spatio-Temporal framework with Adaptive Brain Topology Learning (MSST-ABTL), for neural decoding. It includes two new capabilities to enhance spatio-temporal decoding: i) ABTL module, which learns dynamic brain topology while updating specific patterns of brain regions, ii) MSST module, which captures the association of spatial pattern and temporal evolution, and further enhances the interpretability of the learned dynamic topology from multi-scale perspective. We evaluated the framework on the public Human Connectome Project (HCP) dataset (resting-state and task-related fMRI data). The extensive experiments show that the proposed MSST-ABTL outperforms state-of-the-art methods on four evaluation metrics, and also can renew the neuroscientific discoveries in the brain's hierarchical patterns.
KW  - Topology
KW  - Decoding
KW  - Network topology
KW  - Correlation
KW  - Brain modeling
KW  - Functional magnetic resonance imaging
KW  - Task analysis
KW  - Neural decoding
KW  - adaptive
KW  - brain topology
KW  - multi-scale
KW  - spatio-temporal
SN  - 2168-2208
DA  - Jan
ER  - 

TY  - CONF
ID  - Liu2024Robrain
AU  - Liu, Che
AU  - Du, Changde
AU  - He, Huiguang
TI  - RoBrain: Towards Robust Brain-to-Image Reconstruction via Cross-Domain Contrastive Learning
PY  - 2024
JO  - Lecture Notes in Computer Science
T2  - Lecture Notes in Computer Science
VL  - 14449 LNCS
SP  - 227 - 238
DO  - 10.1007/978-981-99-8067-3_17
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177851235&doi=10.1007%2F978-981-99-8067-3_17&partnerID=40&md5=9bc837a7a91ba8feeccfeb7eef9928a8
AB  - With the development of neuroimaging technology and deep learning methods, neural decoding with functional Magnetic Resonance Imaging (fMRI) of human brain has attracted more and more attention. Neural reconstruction task, which intends to reconstruct stimulus images from fMRI, is one of the most challenging tasks in neural decoding. Due to the instability of neural signals, trials of fMRI collected under the same stimulus prove to be very different, which leads to the poor robustness and generalization ability of the existing models. In this work, we propose a robust brain-to-image model based on cross-domain contrastive learning. With deep neural network (DNN) features as paradigms, our model can extract features of stimulus stably and generate reconstructed images via DCGAN. Experiments on the benchmark Deep Image Reconstruction dataset show that our method can enhance the robustness of reconstruction significantly.
KW  - Decoding
KW  - Deep neural networks
KW  - Functional neuroimaging
KW  - Image enhancement
KW  - Learning systems
KW  - Magnetic resonance imaging
KW  - Brain-to-image reconstruction
KW  - Contrastive learning
KW  - Cross-domain
KW  - Functional magnetic resonance imaging
KW  - Generalization ability
KW  - Human brain
KW  - Images reconstruction
KW  - Learning methods
KW  - Neural decoding
KW  - Neural signals
KW  - Image reconstruction
ER  - 

TY  - JOUR
ID  - Meng2024Semanticsguided
AU  - Meng, Lu
AU  - Yang, Chuanhao
TI  - Semantics-Guided Hierarchical Feature Encoding Generative Adversarial Network for Visual Image Reconstruction From Brain Activity
PY  - 2024
JO  - IEEE Transactions on Neural Systems and Rehabilitation Engineering
T2  - IEEE Transactions on Neural Systems and Rehabilitation Engineering
VL  - 32
SP  - 1267-1283
DO  - 10.1109/TNSRE.2024.3377698
AB  - The utilization of deep learning techniques for decoding visual perception images from brain activity recorded by functional magnetic resonance imaging (fMRI) has garnered considerable attention in recent research. However, reconstructed images from previous studies still suffer from low quality or unreliability. Moreover, the complexity inherent to fMRI data, characterized by high dimensionality and low signal-to-noise ratio, poses significant challenges in extracting meaningful visual information for perceptual reconstruction. In this regard, we proposes a novel neural decoding model, named the hierarchical semantic generative adversarial network (HS-GAN), inspired by the hierarchical encoding of the visual cortex and the homology theory of convolutional neural networks (CNNs), which is capable of reconstructing perceptual images from fMRI data by leveraging the hierarchical and semantic representations. The experimental results demonstrate that HS-GAN achieved the best performance on Horikawa2017 dataset (histogram similarity: 0.447, SSIM-Acc: 78.9%, Peceptual-Acc: 95.38%, AlexNet(2): 96.24% and AlexNet(5): 94.82%) over existing advanced methods, indicating improved naturalness and fidelity of the reconstructed image. The versatility of the HS-GAN was also highlighted, as it demonstrated promising generalization capabilities in reconstructing handwritten digits, achieving the highest SSIM (0.783±0.038), thus extending its application beyond training solely on natural images.
KW  - Visualization
KW  - Functional magnetic resonance imaging
KW  - Image reconstruction
KW  - Decoding
KW  - Feature extraction
KW  - Semantics
KW  - Training
KW  - Visual decoding
KW  - image reconstruction
KW  - generative adversarial network
KW  - fMRI
SN  - 1558-0210
ER  - 

TY  - JOUR
ID  - Meng2024Reconstruction
AU  - Meng, Lu
AU  - Tang, Zhenxuan
AU  - Liu, Yangqian
TI  - Reconstruction of natural images from human fMRI using a three-stage multi-level deep fusion model
PY  - 2024
JO  - Journal of Neuroscience Methods
T2  - Journal of Neuroscience Methods
VL  - 411
DO  - 10.1016/j.jneumeth.2024.110269
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203189040&doi=10.1016%2Fj.jneumeth.2024.110269&partnerID=40&md5=c41acc6a7e11d44b433e36d01ae9bd4f
AB  - Background: Image reconstruction is a critical task in brain decoding research, primarily utilizing functional magnetic resonance imaging (fMRI) data. However, due to challenges such as limited samples in fMRI data, the quality of reconstruction results often remains poor. New method: We proposed a three-stage multi-level deep fusion model (TS-ML-DFM). The model employed a three-stage training process, encompassing components such as image encoders, generators, discriminators, and fMRI encoders. In this method, we incorporated distinct supplementary features derived separately from depth images and original images. Additionally, the method integrated several components, including a random shift module, dual attention module, and multi-level feature fusion module. Results: In both qualitative and quantitative comparisons on the Horikawa17 and VanGerven10 datasets, our method exhibited excellent performance. Comparison with existing methods: For example, on the primary Horikawa17 dataset, our method was compared with other leading methods based on metrics the average hash value, histogram similarity, mutual information, structural similarity accuracy, AlexNet(2), AlexNet(5), and pairwise human perceptual similarity accuracy. Compared to the second-ranked results in each metric, the proposed method achieved improvements of 0.99 %, 3.62 %, 3.73 %, 2.45 %, 3.51 %, 0.62 %, and 1.03 %, respectively. In terms of the SwAV top-level semantic metric, a substantial improvement of 10.53 % was achieved compared to the second-ranked result in the pixel-level reconstruction methods. Conclusions: The TS-ML-DFM method proposed in this study, when applied to decoding brain visual patterns using fMRI data, has outperformed previous algorithms, thereby facilitating further advancements in research within this field.
KW  - Article
KW  - brain region
KW  - comparative study
KW  - controlled study
KW  - deconvolution
KW  - functional magnetic resonance imaging
KW  - histogram
KW  - human
KW  - image reconstruction
KW  - model
KW  - multi level deep fusion model model
KW  - nonhuman
KW  - validity
KW  - brain
KW  - brain mapping
KW  - deep learning
KW  - diagnostic imaging
KW  - image processing
KW  - nuclear magnetic resonance imaging
KW  - physiology
KW  - procedures
KW  - Brain
KW  - Brain Mapping
KW  - Deep Learning
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Magnetic Resonance Imaging
ER  - 

TY  - JOUR
ID  - Pan2024Reconstructing
AU  - Pan, Hongguang
AU  - Li, Zhuoyi
AU  - Fu, Yunpeng
AU  - Qin, Xuebin
AU  - Hu, Jianchen
TI  - Reconstructing Visual Stimulus Representation From EEG Signals Based on Deep Visual Representation Model
PY  - 2024
JO  - IEEE Transactions on Human-Machine Systems
T2  - IEEE Transactions on Human-Machine Systems
VL  - 54
IS  - 6
SP  - 711-722
DO  - 10.1109/THMS.2024.3407875
AB  - Reconstructing visual stimulus representation is a significant task in neural decoding. Until now, most studies have considered functional magnetic resonance imaging (fMRI) as the signal source. However, fMRI-based image reconstruction methods are challenging to apply widely due to the complexity and high cost of acquisition equipment. Taking into account the advantages of the low cost and easy portability of electroencephalogram (EEG) acquisition equipment, we propose a novel image reconstruction method based on EEG signals in this article. First, to meet the high recognizability of visual stimulus images in a fast-switching manner, we construct a visual stimuli image dataset and obtain the corresponding EEG dataset through EEG signals collection experiment. Second, we introduce the deep visual representation model (DVRM), comprising a primary encoder and a subordinate decoder, to reconstruct visual stimuli representation. The encoder is designed based on residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images. Meanwhile, the decoder is designed using a deep neural network to reconstruct the visual stimulus representation from the learned deep visual representation. The DVRM can accommodate the deep and multiview visual features of the human natural state, resulting in more precise reconstructed images. Finally, we evaluate the DVRM based on the quality of the generated images using our EEG dataset. The results demonstrate that the DVRM exhibits an excellent performance in learning deep visual representation from EEG signals, generating reconstructed representation of images that are realistic and highly resemble the original images.
KW  - Visualization
KW  - Electroencephalography
KW  - Image reconstruction
KW  - Feature extraction
KW  - Brain modeling
KW  - Functional magnetic resonance imaging
KW  - Decoding
KW  - Deep visual representation model (DVRM)
KW  - electroencephalogram (EEG) dataset
KW  - image reconstruction
KW  - neural decoding
SN  - 2168-2305
DA  - Dec
ER  - 

TY  - JOUR
ID  - Ren2024Braindriven
AU  - Ren, Ziqi
AU  - Li, Jie
AU  - Wu, Lukun
AU  - Xue, Xuetong
AU  - Li, Xin
AU  - Yang, Fan
AU  - Jiao, Zhicheng
AU  - Gao, Xinbo Bo
TI  - Brain-driven facial image reconstruction via StyleGAN inversion with improved identity consistency
PY  - 2024
JO  - Pattern Recognition
T2  - Pattern Recognition
VL  - 150
DO  - 10.1016/j.patcog.2024.110331
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185398640&doi=10.1016%2Fj.patcog.2024.110331&partnerID=40&md5=af8e5a17f253c16101dc9d5cd250c8cb
AB  - The reconstruction of visual stimuli from fMRI data represents a major technological and scientific challenge at the forefront of contemporary neuroscience research. Deep learning techniques have played a critical role in advancing decoding models for visual stimulus reconstruction from fMRI data. Particularly, the use of advanced GANs has resulted in significant improvements in the quality of image generation, providing a powerful tool for addressing the challenges of this complex task. However, none of these studies have taken into account the inherent characteristics of the stimulus contents themselves; This, in turn, leads to unsatisfactory outcomes, as demonstrated by the inconsistent identity between reconstructed faces and ground truth in the decoding of facial images. In order to tackle this challenge, we introduce a new framework aimed at enhancing the accuracy of reconstructing facial images from fMRI data. Our key innovation involves extracting and disentangling multi-level visual information from brain signals in the latent space and optimizing high-level features for facial identity control using identity loss. Specifically, our framework uses StyleGAN inversion to extract hierarchical latent codes from images, which are then bridged to fMRI data through transformation blocks. Additionally, we introduce a multi-stage refinement method to enhance the accuracy of reconstructed faces, which involves progressively updating fMRI latent codes with custom loss functions designed for both feature- and image-wise optimization. Our experimental results demonstrate that our proposed framework effectively achieves two critical objectives: (1) accurate facial image reconstruction from fMRI data and (2) preservation of identity characteristics with a high level of consistency.
KW  - Data mining
KW  - Decoding
KW  - Deep learning
KW  - Image enhancement
KW  - Learning systems
KW  - Metadata
KW  - Brain-driven image reconstruction
KW  - Cross-modal
KW  - Facial images
KW  - fMRI data
KW  - Images reconstruction
KW  - Learning techniques
KW  - Neural decoding
KW  - Stimulus reconstruction
KW  - Stylegan inversion
KW  - Visual stimulus
KW  - Image reconstruction
ER  - 

TY  - CONF
ID  - Song2024Ieee
AU  - Song, Yahao
AU  - Liu, Tianyi
AU  - Sun, Chao
AU  - Zhang, Yuwei
AU  - Zheng, Minqian
AU  - Zhang, Milin
TI  - A Low-Power Wireless 128-Channel Neural Interface Circuit for Multiregional Brain Recording
PY  - 2024
BT  - 2024 IEEE Biomedical Circuits and Systems Conference (BioCAS)
T2  - 2024 IEEE Biomedical Circuits and Systems Conference (BioCAS)
SP  - 1-5
DO  - 10.1109/BioCAS61083.2024.10798169
AB  - This paper proposes a low-power, wireless 128channel neural interface circuit, comprising a 40 nm recorder circuit and a WiFi module. The proposed design enables realtime monitoring and neural signal decoding during behavioral studies, facilitating high-density, long-term tracking of neural signals in freely moving subjects. The recorder circuit integrates 16 -bit resolution at a maximum rate of 32 k samples per second (sps), with a power consumption of 3.43 mW and an area of $6.27 \mathrm{~mm}^{2}$,achieving the lowest normalized power consumption of $0.84 \mu \mathrm{~W} / \mathrm{ch} / \mathrm{ksps}$ among state-of-the-art works. The VoltageControlled Oscillator (VCO) Analog Front Ends (AFEs) feature an input-referred noise of $0.66 \mu \mathrm{~V}_{\text {rms}}$ within the $0.5-60 \mathrm{~Hz}$ range. The digital backend allows flexible channel configuration and includes an on-chip CIC filter to reduce out-of-band noise. The WiFi module, connected via QSPI, facilitates wireless control and data transmission. The proposed circuit was validated through electrocorticogram (ECoG) measurements in rodent subjects, demonstrating its reliability and flexibility.
KW  - Wireless communication
KW  - Power demand
KW  - Voltage-controlled oscillators
KW  - Noise
KW  - Recording
KW  - Decoding
KW  - System-on-chip
KW  - Data communication
KW  - Integrated circuit reliability
KW  - Wireless fidelity
KW  - Neural Signal Recording
KW  - Biomedical Signal Processing
KW  - ECoG
KW  - Wireless
SN  - 2766-4465
DA  - Oct
ER  - 

TY  - CONF
ID  - Sugimoto2024Ieee
AU  - Sugimoto, Yuma
AU  - Pongthanisorn, Goragod
AU  - Capi, Genci
TI  - Image Generation using EEG data: A Contrastive Learning based Approach
PY  - 2024
BT  - 2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)
T2  - 2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)
SP  - 794-798
DO  - 10.1109/CCECE59415.2024.10667256
AB  - In recent years, there has been active research on Neural Decoding aims to decrypt perceptual cognitive content, recall content, and motor information directly from brain signals. Simultaneously, deep learning has been widely implemented especially in generative models. Diverse architectures and learning methods have been formulated and applied across numerous fields. In this work, we focus on generating perceptual and cognitive contents using brain electroencephalography (EEG) data. We propose and demonstrate the efficacy of contrastive learning to generate images only from EEG data. We compare the performance of two electrode settings 1) visual cortex and 2) motor cortex.
KW  - Learning systems
KW  - Electrodes
KW  - Deep learning
KW  - Visualization
KW  - Image synthesis
KW  - Contrastive learning
KW  - Motors
KW  - Visual Imagery
KW  - EEG signals
KW  - Contrastive Learning
SN  - 2576-7046
DA  - Aug
ER  - 

TY  - CONF
ID  - Wang2024Ieeecvf
AU  - Wang, Shizun
AU  - Liu, Songhua
AU  - Tan, Zhenxiong
AU  - Wang, Xinchao
TI  - MindBridge: A Cross-Subject Brain Decoding Framework
PY  - 2024
BT  - 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
T2  - 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
SP  - 11333-11342
DO  - 10.1109/CVPR52733.2024.01077
AB  - Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI). Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained. This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited data availability for new subjects in real-world scenarios hampers the performance of decoding models. In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model. Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant representation learning. Notably, by cycle re-construction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo data augmentation. Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject. Experimental results demonstrate MindBridge's ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models. Fur-thermore, with limited data for a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models. This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI data in real-world scenarios. Project page: https://littlepure2333.github.io/MindBridge
KW  - Representation learning
KW  - Adaptation models
KW  - Neuroscience
KW  - Accuracy
KW  - Biological system modeling
KW  - Functional magnetic resonance imaging
KW  - Brain modeling
KW  - Brain decoding
KW  - Cross-subject
KW  - diffusion model
SN  - 2575-7075
DA  - June
ER  - 

TY  - CONF
ID  - Wang2024Photonics
AU  - Wang, Yaqi
AU  - Gui, Renzhou
AU  - Zhu, Wenbo
AU  - Yin, Yumiao
AU  - Tong, Meisong
TI  - VTVBrain: A Two-stage Brain Encoding Model for Decoding Key Neural Responses in Multimodal Contexts
PY  - 2024
BT  - 2024 Photonics & Electromagnetics Research Symposium (PIERS)
T2  - 2024 Photonics & Electromagnetics Research Symposium (PIERS)
SP  - 1-9
DO  - 10.1109/PIERS62282.2024.10618584
AB  - In the field of cognitive neuroscience, understanding how the brain processes multimodal complex stimuli is a long-standing and complex challenge. In this study, we propose a novel two-stage brain coding model called "VTVBrain" that focuses on decoding key neural responses in multimodal environments. In the first stage, we built a high-dimensional multimodal latent space pre-training model using an attentional mechanism-based variational autoencoder, aiming to capture and encode the main perceptual processes of observers when faced with multimodal stimuli integrating textual and visual information. In the second stage, the Versatile Diffusion model is utilized for image reconstruction of the first stage primary representational images for high-level perception. The proposed model further refines the latent space representation approach, explores the relationship between the brain’s neural responses and complex natural scenes, and provides insight into how the brain integrates and processes information from different senses at a higher level. The introduction of the VTVBrain model not only brings a new perspective to the field of neural decoding, but also has a great potential for application in the development of brain-like artificial intelligence and future cognitive neuroscience research. The introduction of the VTVBrain model not only brings a new perspective to the field of neural decoding, but also has great potential for the development of brain-like artificial intelligence and future cognitive neuroscience research.
KW  - Cognitive neuroscience
KW  - Semantics
KW  - Brain modeling
KW  - Diffusion models
KW  - Encoding
KW  - Decoding
KW  - Space exploration
SN  - 2831-5804
DA  - April
ER  - 

TY  - JOUR
ID  - Wang2024Efficient
AU  - Wang, Yun
TI  - Efficient Neural Decoding Based on Multimodal Training
PY  - 2024
JO  - Brain Sciences
T2  - Brain Sciences
VL  - 14
IS  - 10
DO  - 10.3390/brainsci14100988
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207646135&doi=10.3390%2Fbrainsci14100988&partnerID=40&md5=dcea41147346389c36e5c9e10c1b8362
AB  - Background/Objectives: Neural decoding methods are often limited by the performance of brain encoders, which map complex brain signals into a latent representation space of perception information. These brain encoders are constrained by the limited amount of paired brain and stimuli data available for training, making it challenging to learn rich neural representations. Methods: To address this limitation, we present a novel multimodal training approach using paired image and functional magnetic resonance imaging (fMRI) data to establish a brain masked autoencoder that learns the interactions between images and brain activities. Subsequently, we employ a diffusion model conditioned on brain data to decode realistic images. Results: Our method achieves high-quality decoding results in semantic contents and low-level visual attributes, outperforming previous methods both qualitatively and quantitatively, while maintaining computational efficiency. Additionally, our method is applied to decode artificial patterns across region of interests (ROIs) to explore their functional properties. We not only validate existing knowledge concerning ROIs but also unveil new insights, such as the synergy between early visual cortex and higher-level scene ROIs, as well as the competition within the higher-level scene ROIs. Conclusions: These findings provide valuable insights for future directions in the field of neural decoding.
KW  - adult
KW  - article
KW  - autoencoder
KW  - diffusion
KW  - electroencephalogram
KW  - female
KW  - functional magnetic resonance imaging
KW  - human
KW  - human experiment
KW  - male
KW  - nerve cell
KW  - nonhuman
KW  - training
KW  - visual cortex
ER  - 

TY  - CONF
ID  - Wei2024Multimodal
AU  - Wei, Yayun
AU  - Cao, Lei
AU  - Li, Hao
AU  - Dong, Yilin
TI  - MB2C: Multimodal Bidirectional Cycle Consistency for Learning Robust Visual Neural Representations
PY  - 2024
SP  - 8992 - 9000
DO  - 10.1145/3664647.3681292
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206805009&doi=10.1145%2F3664647.3681292&partnerID=40&md5=6b793a9799d08c0190b95dab6eaeb4d0
AB  - Decoding human visual representations from brain activity data is a challenging but arguably essential task with an understanding of the real world and the human visual system. However, decoding semantically similar visual representations from brain recordings is difficult, especially for electroencephalography (EEG), which has excellent temporal resolution but suffers from spatial precision. Prevailing methods mainly focus on matching brain activity data with corresponding stimuli-responses using contrastive learning. They rely on massive and high-quality paired data and omit semantically aligned modalities distributed in distinct regions of the latent space. This paper proposes a novel Multimodal Bidirectional Cycle Consistency (MB2C) framework for learning robust visual neural representations. Specifically, we utilize dual-GAN to generate modality-related features and inversely translate back to the corresponding semantic latent space to close the modality gap and guarantee that embeddings from different modalities with similar semantics are in the same region of representation space. We perform zero-shot tasks on the ThingsEEG dataset. Additionally, we conduct EEG classification and image reconstruction on both the ThingsEEG and EEGCVPR40 datasets, achieving state-of-the-art performance compared to other baselines.
KW  - Contrastive Learning
KW  - Semantic Segmentation
KW  - Zero-shot learning
KW  - Brain activity
KW  - Cycle consistency
KW  - Human visual
KW  - Images reconstruction
KW  - Multi-modal
KW  - Multi-modal learning
KW  - Neural decoding
KW  - Neural representations
KW  - Real-world
KW  - Visual representations
KW  - Electroencephalography
ER  - 

TY  - CONF
ID  - Xie2024Brainram
AU  - Xie, Dian
AU  - Zhao, Peiang
AU  - Zhang, Jiarui
AU  - Wei, Kangqi
AU  - Ni, Xiaobao
AU  - Xia, Jiong
TI  - BrainRAM: Cross-Modality Retrieval-Augmented Image Reconstruction from Human Brain Activity
PY  - 2024
SP  - 3994 - 4003
DO  - 10.1145/3664647.3681296
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209774663&doi=10.1145%2F3664647.3681296&partnerID=40&md5=a26f218cbd721d6e49a0715bb0a714cf
AB  - Reconstructing visual stimuli from brain activities is crucial for deciphering the underlying mechanism of the human visual system. While recent studies have achieved notable results by leveraging deep generative models, challenges persist due to the lack of large-scale datasets and the inherent noise from non-invasive measurement methods. In this study, we draw inspiration from the mechanism of human memory and propose BrainRAM, a novel two-stage dual-guided framework for visual stimuli reconstruction. BrainRAM incorporates a Retrieval-Augmented Module (RAM) and diffusion prior to enhance the quality of reconstructed images from the brain. Specifically, in stage I, we transform fMRI voxels into the latent space of image and text embeddings via diffusion priors, obtaining preliminary estimates of the visual stimuli's semantics and structure. In stage II, based on previous estimates, we retrieve data from the LAION-2B-en dataset and employ the proposed RAM to refine them, yielding high-quality reconstruction results. Extensive experiments demonstrate that our BrainRAM outperforms current state-of-the-art methods both qualitatively and quantitatively, providing a new perspective for visual stimuli reconstruction.
KW  - Brain mapping
KW  - Image denoising
KW  - Image enhancement
KW  - Image retrieval
KW  - Noninvasive medical procedures
KW  - Augmented images
KW  - Brain activity
KW  - Cross modality
KW  - Human brain
KW  - Human Visual System
KW  - Images reconstruction
KW  - Neural decoding
KW  - Retrieval-augmented generation
KW  - Stimulus reconstruction
KW  - Visual stimulus
KW  - Image reconstruction
ER  - 

TY  - JOUR
ID  - Yang2024Functional
AU  - Yang, Lingxiao
AU  - Zhen, Hui
AU  - Li, Le
AU  - Li, Yuanning
AU  - Zhang, Han
AU  - Xie, Xiaohua
AU  - Zhang, Ruyuan
TI  - Functional diversity of visual cortex improves constraint-free natural image reconstruction from human brain activity
PY  - 2024
JO  - Fundamental Research
T2  - Fundamental Research
DO  - 10.1016/j.fmre.2023.08.010
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181810312&doi=10.1016%2Fj.fmre.2023.08.010&partnerID=40&md5=0d6239849fc8fc173d179885262f582a
AB  - Previous brain decoding studies using functional magnetic resonance imaging (fMRI) have greatly advanced our understanding of human visual coding and non-invasive brain-machine interfaces. However, most of these studies focus on classifying a limited number of image categories or reconstructing visual images with additional information, e.g., semantic categories and textual cues. Constraint-free visual reconstruction remains scarce. Here, we propose a generative network based on the functional diversity of the human visual cortex (FDGen) that takes multivariate brain activity as input and directly reconstructs natural images perceived by observers without any additional cues (semantic categories or textual description). Our FDGen is augmented by two bio-inspired computational modules. Based on the functional specializations of the human visual cortex, we propose a new function-based input module (FIM) that projects responses from different brain regions into separate feature spaces. Second, inspired by human attention, we construct a computational module to derive attentive feature weights at the function level to refine the feature map. These function-selection modules (FSMs) allow the network to dynamically select multiscale visual information during the generation process. We test FDGen on the popular fMRI datasets of natural images and achieve highly robust performance. Our work represents an important step forward in the development of fMRI-based brain decoding algorithms and highlights the utility of neuroscience theories in the design of deep learning models.
KW  - Brain function-related model
KW  - Brain-computer interfaces
KW  - Conditional generative network
KW  - fMRI decoding
KW  - Natural image reconstruction
ER  - 

TY  - JOUR
ID  - Yang2024Reconstructing
AU  - Yang, Xiaomeng
AU  - Xiong, Xinzhu
AU  - Li, Xufei
AU  - Lian, Qi
AU  - Zhu, Junming
AU  - Zhang, Jianmin
AU  - Qi, Yu
AU  - Wang, Yueming
TI  - Reconstructing Multi-Stroke Characters From Brain Signals Toward Generalizable Handwriting Brain–Computer Interfaces
PY  - 2024
JO  - IEEE Transactions on Neural Systems and Rehabilitation Engineering
T2  - IEEE Transactions on Neural Systems and Rehabilitation Engineering
VL  - 32
SP  - 4230-4239
DO  - 10.1109/TNSRE.2024.3492191
AB  - Handwriting Brain-Computer Interfaces (BCIs) provides a promising communication avenue for individuals with paralysis. While English-based handwriting BCIs have achieved rapid typewriting with 26 lowercase letters (mostly containing one stroke each), it is difficult to extend to complex characters, especially those with multiple strokes and large character sets. The Chinese characters, including over 3500 commonly used characters with 10.3 strokes per character on average, represent a highly complex writing system. This paper proposes a Chinese handwriting BCI system, which reconstructs multi-stroke handwriting trajectories from brain signals. Through the recording of cortical neural signals from the motor cortex, we reveal distinct neural representations for stroke-writing and pen-lift phases. Leveraging this finding, we propose a stroke-aware approach to decode stroke-writing trajectories and pen-lift movements individually, which can reconstruct recognizable characters (accuracy of 86% with 400 characters). Our approach demonstrates high stability over 5 months, shedding light on generalized and adaptable handwriting BCIs.
KW  - Writing
KW  - Decoding
KW  - Trajectory
KW  - Character recognition
KW  - Long short term memory
KW  - Stroke (medical condition)
KW  - Recording
KW  - Image reconstruction
KW  - Monitoring
KW  - Micromechanical devices
KW  - Brain-computer interfaces (BCIs)
KW  - neural signal decoding
KW  - handwriting reconstruction
KW  - multi-stroke characters
SN  - 1558-0210
ER  - 

TY  - JOUR
ID  - Yu2024Deep
AU  - Yu, Haitao
AU  - Hu, Zhiwen
AU  - Zhao, Quanfa
AU  - Liu, Jing
TI  - Deep source transfer learning for the estimation of internal brain dynamics using scalp EEG
PY  - 2024
JO  - Cognitive Neurodynamics
T2  - Cognitive Neurodynamics
VL  - 18
IS  - 6
SP  - 3507 - 3520
DO  - 10.1007/s11571-024-10149-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198650072&doi=10.1007%2Fs11571-024-10149-2&partnerID=40&md5=6f56db1fa6866da24b4ca6c732f0b10f
AB  - Electroencephalography (EEG) provides high temporal resolution neural data for brain-computer interfacing via noninvasive electrophysiological recording. Estimating the internal brain activity by means of source imaging techniques can further improve the spatial resolution of EEG and enhance the reliability of neural decoding and brain-computer interaction. In this work, we propose a novel EEG data-driven source imaging scheme for precise and efficient estimation of macroscale spatiotemporal brain dynamics across thalamus and cortical regions with deep learning methods. A deep source imaging framework with a convolutional-recurrent neural network is designed to estimate the internal brain dynamics from high-density EEG recordings. Moreover, a brain model including 210 cortical regions and 16 thalamic nuclei is established based on human brain connectome to provide synthetic training data, which manifests intrinsic characteristics of underlying brain dynamics in spontaneous, stimulation-evoked, and pathological states. Transfer learning algorithm is further applied to the trained network to reduce the dynamical differences between synthetic and realistic EEG. Extensive experiments exhibit that the proposed deep-learning method can accurately estimate the spatial and temporal activity of brain sources and achieves superior performance compared to the state-of-the-art approaches. Moreover, the EEG data-driven source imaging framework is effective in the location of seizure onset zone in epilepsy and reconstruction of dynamical thalamocortical interactions during sensory processing of acupuncture stimulation, implying its applicability in brain-computer interfacing for neuroscience research and clinical applications.
KW  - Article
KW  - brain cortex
KW  - connectome
KW  - convolutional neural network
KW  - deep learning
KW  - electroencephalogram
KW  - image reconstruction
KW  - learning algorithm
KW  - neuroimaging
KW  - recurrent neural network
KW  - thalamocortical tract
KW  - thalamus
KW  - transfer learning algorithm
ER  - 

TY  - JOUR
ID  - Berezutskaya2023Direct
AU  - Berezutskaya, Julia
AU  - Freudenburg, Zachary V.
AU  - Vansteensel, Mariska J.
AU  - Aarnoutse, Erik J.
AU  - Ramsey, Nick Franciscus
AU  - Van Gerven, Marcel A.J.
TI  - Direct speech reconstruction from sensorimotor brain activity with optimized deep learning models
PY  - 2023
JO  - Journal of Neural Engineering
T2  - Journal of Neural Engineering
VL  - 20
IS  - 5
DO  - 10.1088/1741-2552/ace8be
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171900901&doi=10.1088%2F1741-2552%2Face8be&partnerID=40&md5=698777bb411c75962425db25d45880b5
AB  - Objective. Development of brain-computer interface (BCI) technology is key for enabling communication in individuals who have lost the faculty of speech due to severe motor paralysis. A BCI control strategy that is gaining attention employs speech decoding from neural data. Recent studies have shown that a combination of direct neural recordings and advanced computational models can provide promising results. Understanding which decoding strategies deliver best and directly applicable results is crucial for advancing the field. Approach. In this paper, we optimized and validated a decoding approach based on speech reconstruction directly from high-density electrocorticography recordings from sensorimotor cortex during a speech production task. Main results. We show that (1) dedicated machine learning optimization of reconstruction models is key for achieving the best reconstruction performance; (2) individual word decoding in reconstructed speech achieves 92%-100% accuracy (chance level is 8%); (3) direct reconstruction from sensorimotor brain activity produces intelligible speech. Significance. These results underline the need for model optimization in achieving best speech decoding results and highlight the potential that reconstruction-based speech decoding from sensorimotor cortex can offer for development of next-generation BCI technology for communication.
KW  - Audio recordings
KW  - Brain
KW  - Brain computer interface
KW  - Decoding
KW  - Electroencephalography
KW  - Electrophysiology
KW  - Learning systems
KW  - Neurophysiology
KW  - Speech communication
KW  - Audio reconstruction
KW  - Brain activity
KW  - Electrocorticography
KW  - Interface control
KW  - Interface technology
KW  - Learning models
KW  - Neural decoding
KW  - Sensorimotor cortex
KW  - Sensorimotors
KW  - Speech decoding
KW  - Deep neural networks
KW  - adult
KW  - Article
KW  - artificial neural network
KW  - audio recording
KW  - classification algorithm
KW  - comparative study
KW  - convolutional neural network
KW  - deep learning
KW  - electrocorticography
KW  - electroencephalogram
KW  - female
KW  - human
KW  - human tissue
KW  - image reconstruction
KW  - leave one out cross validation
KW  - logistic regression analysis
KW  - machine learning
KW  - male
KW  - middle aged
KW  - multilayer perceptron
KW  - normal human
KW  - random forest
KW  - recurrent neural network
KW  - sensorimotor cortex
KW  - speech analysis
KW  - speech intelligibility
KW  - speech perception
KW  - word recognition
KW  - young adult
KW  - interpersonal communication
KW  - procedures
KW  - speech
KW  - Brain-Computer Interfaces
KW  - Communication
KW  - Deep Learning
KW  - Humans
KW  - Sensorimotor Cortex
KW  - Speech
ER  - 

TY  - JOUR
ID  - Cheng2023Reconstructing
AU  - Cheng, Fan L.
AU  - Horikawa, Tomoyasu
AU  - Majima, Kei
AU  - Tanaka, Misato
AU  - Abdelhack, Mohamed
AU  - Aoki, Shuntaro C.
AU  - Hirano, Jin
AU  - Kamitani, Yukiyasu
TI  - Reconstructing visual illusory experiences from human brain activity
PY  - 2023
JO  - Science Advances
T2  - Science Advances
VL  - 9
IS  - 46
DO  - 10.1126/sciadv.adj3906
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178499453&doi=10.1126%2Fsciadv.adj3906&partnerID=40&md5=b735cb085d5fa94c35af612a86ce7fab
AB  - Visual illusions provide valuable insights into the brain’s interpretation of the world given sensory inputs. However, the precise manner in which brain activity translates into illusory experiences remains largely unknown. Here, we leverage a brain decoding technique combined with deep neural network (DNN) representations to reconstruct illusory percepts as images from brain activity. The reconstruction model was trained on natural images to establish a link between brain activity and perceptual features and then tested on two types of illusions: illusory lines and neon color spreading. Reconstructions revealed lines and colors consistent with illusory experiences, which varied across the source visual cortical areas. This framework offers a way to materialize subjective experiences, shedding light on the brain’s internal representations of the world.
KW  - Deep neural networks
KW  - Image reconstruction
KW  - Neurophysiology
KW  - Brain activity
KW  - Brain decoding
KW  - Decoding techniques
KW  - Human brain
KW  - Natural images
KW  - Neon color spreading
KW  - Network representation
KW  - Perceptual feature
KW  - Sensory input
KW  - Visual illusions
KW  - Brain
KW  - artificial neural network
KW  - brain
KW  - human
KW  - illusion
KW  - pattern recognition
KW  - vision
KW  - visual cortex
KW  - Form Perception
KW  - Humans
KW  - Illusions
KW  - Neural Networks, Computer
KW  - Visual Cortex
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - Du2023Review
AU  - Du, Changde
AU  - Zhou, Qiongyi
AU  - Liu, Che
AU  - He, Huiguang
TI  - Review of visual neural encoding and decoding methods in fMRI; fMRI 的视觉神经信息编解码方法综述
PY  - 2023
JO  - Journal of Image and Graphics
T2  - Journal of Image and Graphics
VL  - 28
IS  - 2
SP  - 372 - 384
DO  - 10.11834/jig.220525
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151501435&doi=10.11834%2Fjig.220525&partnerID=40&md5=761f69e1ead77263bb8d338d08462edb
AB  - The relationship between human visual experience and evoked neural activity is central to the field of computational neuroscience. The purpose of visual neural encoding and decoding is to study the relationship between visual stimuli and the evoked neural activity by using neuroimaging data such as functional magnetic resonance imaging (fMRI). Neural encoding researches attempt to predict the brain activity according to the presented external stimuli, which contributes to the development of brain science and brain-like artificial intelligence. Neural decoding researches attempt to predict the information about external stimuli by analyzing the observed brain activities, which can interpret the state of human visual perception and promote the development of brain computer interface (BCI). Therefore, fMRI based visual neural encoding and decoding researches have important scientific significance and engineering value. Typically, the encoding models are based on the specific computations that are thought to underlie the observed brain responses for specific visual stimuli. Early studies of visual neural encoding relied heavily on Gabor wavelet features because these features are very good at modeling brain responses in the primary visual cortex. Recently, given the success of deep neural networks (DNNs) in classifying objects in natural images, the representations within these networks have been used to build encoding models of cortical responses to complex visual stimuli. Most of the existing decoding studies are based on multi-voxel pattern analysis (MVPA) method, but brain connectivity pattern is also a key feature of the brain state and can be used for brain decoding. Although recent studies have demonstrated the feasibility of decoding the identity of binary contrast patterns, handwritten characters, human facial images, natural picture/video stimuli and dreams from the corresponding brain activation patterns, the accurate reconstruction of the visual stimuli from fMRI still lacks adequate examination and requires plenty of efforts to improve. On the basis of summarizing the key technologies and research progress of fMRI based visual neural encoding and decoding, this paper further analyzes the limitations of existing visual neural encoding and decoding methods. In terms of visual neural encoding, the development process of population receptive field (pRF) estimation method is introduced in detail. In terms of visual neural decoding, it is divided into semantic classification, image identification and image reconstruction according to task types, and the representative research work of each part and the methods used are described in detail. From the perspective of machine learning, semantic classification is a single label or multi-label classification problem. Simple visual stimuli only contain a single object, while natural visual stimuli often contain multiple semantic labels. For example, an image may contain flowers, water, trees, cars, etc. Predicting one or more semantic labels of the visual stimulus from the brain signal is called semantic decoding. Image retrieval based on brain signal is also a common visual decoding task where the model is created to “decode” neural activity by retrieving a picture of what a person has just seen or imagined. In particular, the reconstruction techniques of simple image, face image and complex natural image based on deep generative models (including variational auto-encoders (VAEs) and generative adversarial networks (GANs)) are introduced in the part of image reconstruction. Secondly, 10 open source datasets commonly used in this field were statistically sorted out, and the sample size, number of subjects, types of stimuli, research purposes and download links of the datasets were summarized in detail. These datasets have made important contributions to the development of this field. Finally, we introduce the commonly used measurement metrics of visual neural encoding and decoding model in detail, analyze the shortcomings of current visual neural encoding and decoding methods, propose feasible suggestions for improvement, and show the future development directions. Specifically, for neural encoding, the existing methods still have the following shortcomings: 1) the computational models are mostly based on the existing neural network architecture, which cannot reflect the real biological visual information flow; 2) due to the selective attention of each person in the visual perception and the inevitable noise in the fMRI data collection, individual differences are significant; 3) the sample size of the existing fMRI data set is insufficient; 4) most researchers construct feature spaces of neural encoding models based on fixed types of pre-trained neural networks (such as AlexNet), causing problems such as insufficient diversity of visual features. On the other hand, although the existing visual neural decoding methods perform well in the semantic classification and image identification tasks, it is still very difficult to establish an accurate mapping between visual stimuli and visual neural signals, and the results of image reconstruction are often blurry and lack of clear semantics. Moreover, most of the existing visual neural decoding methods are based on linear transformation or deep network transformation of visual images, lacking exploration of new visual features. Factors that hinder researchers from effectively decoding visual information and reconstructing images or videos mainly include high dimension of fMRI data, small sample size and serious noise. In the future, more advanced artificial intelligence technology should be used to develop more effective methods of neural encoding and decoding, and try to translate brain signals into images, video, voice, text and other multimedia content, so as to achieve more BCI applications. The significant research directions include 1) multi-modal neural encoding and decoding based on the union of image and text; 2) brain-guided computer vision model training and enhancement; 3) visual neural encoding and decoding based on the high efficient features of large-scale pre-trained models. In addition, since brain signals are characterized by complexity, high dimension, large individual diversity, high dynamic nature and small sample size, future research needs to combine computational neuroscience and artificial intelligence theories to develop visual neural encoding and decoding methods with higher robustness, adaptability and interpretability.
KW  - brain computer interface (BCI)
KW  - deep learning
KW  - image reconstruction
KW  - neural decoding
KW  - neural encoding
KW  - visual cognitive computing
ER  - 

TY  - CONF
ID  - Ferrante2023Multimodal
AU  - Ferrante, Matteo
AU  - Boccato, Tommaso
AU  - Ozcelik, Furkan
AU  - VanRullen, Rufin
AU  - Toschi, Nicola
TI  - Multimodal decoding of human brain activity into images and text
PY  - 2023
JO  - Proceedings of Machine Learning Research
T2  - Proceedings of Machine Learning Research
VL  - 243
SP  - 11 - 26
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196100263&partnerID=40&md5=d96db970d5caf79649c6b9e8f4366bc8
AB  - Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation. We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline based on latent diffusion models. The method involves training regularized linear regression models between brain activity and extracted features. Additionally, we incorporated depth maps from the ControlNet model to further guide the reconstruction process. We propose a multimodal based approach that leverages similarities between neural and deep learning representations and by learning alignment between these spaces, we produce textual description and image reconstruction from brain activity. We evaluate our methods using quantitative metrics for both generated captions and images. Our brain captioning approach outperforms existing methods, while our image reconstruction pipeline generates plausible images with improved spatial relationships. In conclusion, we demonstrate significant progress in brain decoding, showcasing the enormous potential of integrating vision and language to better understand human cognition. Our approach provides a flexible platform for future research, with potential applications based on a combination of high-level semantic information coming from text and low-level image shape information coming from depth maps and initial guess images.
KW  - Decoding
KW  - Deep learning
KW  - Diffusion
KW  - Image enhancement
KW  - Image reconstruction
KW  - Magnetic resonance imaging
KW  - Neurophysiology
KW  - Pipelines
KW  - Regression analysis
KW  - Semantics
KW  - Brain activity
KW  - Brain decoding
KW  - Brain process
KW  - Depthmap
KW  - Diffusion model
KW  - Functional magnetic resonance imaging
KW  - Human brain
KW  - Images reconstruction
KW  - Multi-modal
KW  - Visual information
KW  - Brain
ER  - 

TY  - CONF
ID  - Gu2023Decoding
AU  - Gu, Zijin
AU  - Jamison, Keith Wakefield
AU  - Kuceyeski, Amy F.
AU  - Rory Sabuncu, Mert Rory
TI  - Decoding natural image stimuli from fMRI data with a surface-based convolutional network
PY  - 2023
JO  - Proceedings of Machine Learning Research
T2  - Proceedings of Machine Learning Research
VL  - 227
SP  - 107 - 118
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189347544&partnerID=40&md5=35091dda16c7655448012c6cd5e5c285
AB  - Due to the low signal-to-noise ratio and limited resolution of functional MRI data, and the high complexity of natural images, reconstructing a visual stimulus from human brain fMRI measurements is a challenging task. In this work, we propose a novel approach for this task, which we call Cortex2Image, to decode visual stimuli with high semantic fidelity and rich fine-grained detail. In particular, we train a surface-based convolutional network model that maps from brain response to semantic image features first (Cortex2Semantic). We then combine this model with a high-quality image generator (Instance-Conditioned GAN) to train another mapping from brain response to fine-grained image features using a variational approach (Cortex2Detail). Image reconstructions obtained by our proposed method achieve state-of-the-art semantic fidelity, while yielding good fine-grained similarity with the ground-truth stimulus. Our code is available on https://github.com/zijin-gu/meshconv-decoding.git.
KW  - Brain mapping
KW  - Convolution
KW  - Decoding
KW  - Magnetic resonance imaging
KW  - Semantics
KW  - Signal to noise ratio
KW  - Brain response
KW  - Convolutional networks
KW  - Fine grained
KW  - Functional MRI
KW  - Image features
KW  - Images reconstruction
KW  - Natural images
KW  - Neural decoding
KW  - Surface-based
KW  - Visual stimulus
KW  - Image reconstruction
ER  - 

TY  - CONF
ID  - Gui2023Photonics
AU  - Gui, Renzhou
AU  - Zhang, Aobo
AU  - Liu, Shuai
AU  - Tong, Mei Song
TI  - Analysis of Functional Areas of Human Brain Based on Reconstructed Images of DMFG-generated Countermeasure Network
PY  - 2023
BT  - 2023 Photonics & Electromagnetics Research Symposium (PIERS)
T2  - 2023 Photonics & Electromagnetics Research Symposium (PIERS)
SP  - 1131-1138
DO  - 10.1109/PIERS59004.2023.10221339
AB  - The structure of human brain is complex, and fMRI data can be used to reveal the working mechanism of human brain. We construct a generative confrontation deep learning network based on DMFG-loss function. Using this network, we can not only reconstruct the simple scene images perceived and imagined by human brain with high precision, but also achieve good results for the restoration and reconstruction of complex natural images. In addition, we propose to set the detection threshold based on the constant false alarm algorithm. Further, we explore the distribution of brain sensitive areas, and make a deep analysis of the impact of different regions on image reconstruction. The contribution ratio of specific brain regions to the image reconstruction of human brain is gived. This will help to explore the unknown areas of human brain and reveal the mechanism of human brain operation. It has broad application prospects in brain computer interaction and human brain decoding.
KW  - Training
KW  - Deep learning
KW  - Support vector machines
KW  - Visualization
KW  - Image color analysis
KW  - Fitting
KW  - Functional magnetic resonance imaging
SN  - 2831-5804
DA  - July
ER  - 

TY  - JOUR
ID  - Hua2023Neural
AU  - Hua, Lin
AU  - Gao, Fei
AU  - Leong, Chantat
AU  - Yuan, Zhen
TI  - Neural decoding dissociates perceptual grouping between proximity and similarity in visual perception
PY  - 2023
JO  - Cerebral Cortex
T2  - Cerebral Cortex
VL  - 33
IS  - 7
SP  - 3803 - 3815
DO  - 10.1093/cercor/bhac308
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159549724&doi=10.1093%2Fcercor%2Fbhac308&partnerID=40&md5=d59ef52a17392dc7345eadb6c638313c
AB  - Unlike single grouping principle, cognitive neural mechanism underlying the dissociation across two or more grouping principles is still unclear. In this study, a dimotif lattice paradigm that can adjust the strength of one grouping principle was used to inspect how, when, and where the processing of two grouping principles (proximity and similarity) were carried out in human brain. Our psychophysical findings demonstrated that similarity grouping effect was enhanced with reduced proximity effect when the grouping cues of proximity and similarity were presented simultaneously. Meanwhile, EEG decoding was performed to reveal the specific cognitive patterns involved in each principle by using time-resolved MVPA. More importantly, the onsets of dissociation between 2 grouping principles coincided within 3 time windows: the early-stage proximity-defined local visual element arrangement in middle occipital cortex, the middle-stage processing for feature selection modulating low-level visual cortex such as inferior occipital cortex and fusiform cortex, and the high-level cognitive integration to make decisions for specific grouping preference in the parietal areas. In addition, it was discovered that the brain responses were highly correlated with behavioral grouping. Therefore, our study provides direct evidence for a link between the human perceptual space of grouping decision-making and neural space of brain activation patterns.
KW  - article
KW  - brain cortex
KW  - decision making
KW  - dissociation
KW  - electroencephalogram
KW  - feature selection
KW  - human
KW  - human experiment
KW  - occipital cortex
KW  - vision
KW  - visual cortex
ER  - 

TY  - JOUR
ID  - Luo2023Visual
AU  - Luo, Jie
AU  - Cui, Weigang
AU  - Liu, Jingyu
AU  - Li, Yang
AU  - Guo, Yuzhu
AU  - Xu, Song
AU  - Wang, Lina
TI  - Visual Image Decoding of Brain Activities Using a Dual Attention Hierarchical Latent Generative Network With Multiscale Feature Fusion
PY  - 2023
JO  - IEEE Transactions on Cognitive and Developmental Systems
T2  - IEEE Transactions on Cognitive and Developmental Systems
VL  - 15
IS  - 2
SP  - 761-773
DO  - 10.1109/TCDS.2022.3181469
AB  - Reconstructing visual stimulus from human brain activity measured with functional magnetic resonance imaging (fMRI) is a challenging decoding task for revealing the visual system. Recent deep learning approaches commonly neglect the relationship between hierarchical image features and different regions of the visual cortex, and fail to use global and local image features in reconstructing visual stimulus. To address these issues, in this article, a novel neural decoding framework is proposed by using a dual attention (DA) hierarchical latent generative network with multiscale feature fusion (DA-HLGN-MSFF) method. Specifically, the fMRI data are first encoded to hierarchical features of our image encoder network, which employs a multikernel convolution block to extract the multiscale spatial information of images. In order to reconstruct the perceived images and further improve the performance of our generator network, a DA block based on the channel-spatial attention mechanism is then proposed to exploit the interchannel relationships and spatial long-range dependencies of features. Moreover, a multiscale feature fusion block is finally adopted to aggregate the global and local information of features at different scales and synthesize the final reconstructed images in the generator network. Competitive experimental results on two public fMRI data sets demonstrate that our method is able to achieve promising reconstructing performance compared with the state-of-the-art methods. The codes of our proposed DA-HLGN-MSFF method will be open access on https://github.com/ljbuaa/HLDAGN.
KW  - Image reconstruction
KW  - Functional magnetic resonance imaging
KW  - Visualization
KW  - Generators
KW  - Decoding
KW  - Generative adversarial networks
KW  - Feature extraction
KW  - Deep neural network (DNN)
KW  - functional magnetic resonance imaging (fMRI) decoding
KW  - generative adversarial network (GAN)
KW  - image reconstruction
KW  - visual cortex
SN  - 2379-8939
DA  - June
ER  - 

TY  - CONF
ID  - Luo2023Ieee
AU  - Luo, Ying
AU  - Kobayashi, Ichiro
TI  - BrainLM: Estimation of Brain Activity Evoked Linguistic Stimuli Utilizing Large Language Models
PY  - 2023
BT  - 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
T2  - 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
SP  - 1904-1909
DO  - 10.1109/SMC53992.2023.10394227
AB  - In recent years, with the recent remarkable development of large-scale language models in natural language processing research, there has been an increasing number of studies employing large-scale language models to investigate the information processing mechanisms of encoding and decoding in the brain. In this study, we developed a new pre-trained language model, BrainLM, which incorporates paired data of brain activity induced by text and stimuli, and verified the accuracy of estimating brain states from natural language in multiple NLP tasks. In essence, our research has achieved several noteworthy accomplishments. Firstly, we successfully developed a multimodal model that incorporates both brain and text. Subsequently, we conducted bi-directional experiments to validate the model and ensure the reliability of both brain encoding and decoding processes. Furthermore, we performed meticulous comparative experiments, wherein we introduced 20 state-of-the-art (SOTA) language models as a control group. Our findings reveal that our proposed model outperforms superior brain encoding ability compared to the control group. Lastly, we designed a discrete Autoencoder module that extracts brain features. This module can be utilized independently to extract brain features in a wider range of brain decoding studies beyond fMRI.
KW  - Brain modeling
KW  - Feature extraction
KW  - Encoding
KW  - Data models
KW  - Natural language processing
KW  - Decoding
KW  - Task analysis
SN  - 2577-1655
DA  - Oct
ER  - 

TY  - CONF
ID  - Meng2023International
AU  - Meng, Lu
AU  - Yang, Chuanhao
TI  - Semantics-guided hierarchical feature encoding generative adversarial network for natural image reconstruction from brain activities
PY  - 2023
BT  - 2023 International Joint Conference on Neural Networks (IJCNN)
T2  - 2023 International Joint Conference on Neural Networks (IJCNN)
SP  - 1-9
DO  - 10.1109/IJCNN54540.2023.10191903
AB  - The use of deep learning methods to decode visual perception images from brain activity recorded by fMRI has received a lot of attention. However, limited fMRI data make the task of visual reconstruction challenging. Inspired by hierarchical encoding of the visual cortex and the theory of brain homology with convolutional neural networks (CNNs), we propose a novel neural decoding model called hierarchical semantic generative adversarial network (HS-GAN). Specifically, we use CNN-based image encoder to extract hierarchical and semantic features of visually stimulus images. Then a neural decoder is used to decode hierarchical and semantic features from fMRI. In order to take full advantage of the information from different visual cortexes, we construct a generator with self-attention modules and skip connections to fuse the image features of different layers. In model training, adversarial learning is introduced to realize more natural image reconstruction. Compared to existing advanced methods, our method significantly improves the naturalness and fidelity of reconstructed images.
KW  - Visualization
KW  - Image coding
KW  - Semantics
KW  - Functional magnetic resonance imaging
KW  - Generative adversarial networks
KW  - Feature extraction
KW  - Brain modeling
KW  - brain decoding
KW  - fMRI
KW  - GAN
KW  - image reconstruction
KW  - deep learning
SN  - 2161-4407
DA  - June
ER  - 

TY  - JOUR
ID  - Meng2023Dualguided
AU  - Meng, Lu
AU  - Yang, Chuanhao
TI  - Dual-Guided Brain Diffusion Model: Natural Image Reconstruction from Human Visual Stimulus fMRI
PY  - 2023
JO  - Bioengineering
T2  - Bioengineering
VL  - 10
IS  - 10
DO  - 10.3390/bioengineering10101117
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175057353&doi=10.3390%2Fbioengineering10101117&partnerID=40&md5=da8e4b76f7248f7cc2b802c7e3fa4a7b
AB  - The reconstruction of visual stimuli from fMRI signals, which record brain activity, is a challenging task with crucial research value in the fields of neuroscience and machine learning. Previous studies tend to emphasize reconstructing pixel-level features (contours, colors, etc.) or semantic features (object category) of the stimulus image, but typically, these properties are not reconstructed together. In this context, we introduce a novel three-stage visual reconstruction approach called the Dual-guided Brain Diffusion Model (DBDM). Initially, we employ the Very Deep Variational Autoencoder (VDVAE) to reconstruct a coarse image from fMRI data, capturing the underlying details of the original image. Subsequently, the Bootstrapping Language-Image Pre-training (BLIP) model is utilized to provide a semantic annotation for each image. Finally, the image-to-image generation pipeline of the Versatile Diffusion (VD) model is utilized to recover natural images from the fMRI patterns guided by both visual and semantic information. The experimental results demonstrate that DBDM surpasses previous approaches in both qualitative and quantitative comparisons. In particular, the best performance is achieved by DBDM in reconstructing the semantic details of the original image; the Inception, CLIP and SwAV distances are 0.611, 0.225 and 0.405, respectively. This confirms the efficacy of our model and its potential to advance visual decoding research.
KW  - brain decoding
KW  - diffusion model
KW  - fMRI
KW  - visual reconstruction
ER  - 

TY  - CONF
ID  - Miliotou2023Generative
AU  - Miliotou, Eleni
AU  - Kyriakis, Panagiotis
AU  - Hinman, Jason D.
AU  - Irimia, Andrei
AU  - Bogdan, Paul
TI  - Generative Decoding of Visual Stimuli
PY  - 2023
JO  - Proceedings of Machine Learning Research
T2  - Proceedings of Machine Learning Research
VL  - 202
SP  - 24775 - 24784
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174388441&partnerID=40&md5=1d206fb358ae273c8bbaa5f3a0b4fadb
AB  - Reconstructing real-world images from fMRI recordings is a challenging task of great importance in neuroscience. The current architectures are bottlenecked because they fail to effectively capture the hierarchical processing of visual stimuli that takes place in the human brain. Motivated by that fact, we introduce a novel neural network architecture for the problem of neural decoding. Our architecture uses Hierarchical Variational Autoencoders (HVAEs) to learn meaningful representations of real-world images and leverages their latent space hierarchy to learn voxel-to-image mappings. By mapping the early stages of the visual pathway to the first set of latent variables and the higher visual cortex areas to the deeper layers in the latent hierarchy, we are able to construct a latent variable neural decoding model that replicates the hierarchical visual information processing. Our model achieves better reconstructions compared to the state of the art and our ablation study indicates that the hierarchical structure of the latent space is responsible for that performance.
KW  - Decoding
KW  - Machine learning
KW  - Network architecture
KW  - Neural networks
KW  - 'current
KW  - Hierarchical processing
KW  - Human brain
KW  - Latent variable
KW  - Learn+
KW  - Neural decoding
KW  - Neural network architecture
KW  - Novel neural network
KW  - Real-world image
KW  - Visual stimulus
KW  - Mapping
ER  - 

TY  - JOUR
ID  - Pan2023Images
AU  - Pan, Honggguang
AU  - Fu, Yunpeng
AU  - Li, Zhuoyi
AU  - Wen, Fan
AU  - Hu, Jianchen
AU  - Wu, Bo
TI  - Images Reconstruction from Functional Magnetic Resonance Imaging Patterns Based on the Improved Deep Generative Multiview Model
PY  - 2023
JO  - Neuroscience
T2  - Neuroscience
VL  - 509
SP  - 103 - 112
DO  - 10.1016/j.neuroscience.2022.11.021
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145571633&doi=10.1016%2Fj.neuroscience.2022.11.021&partnerID=40&md5=6b69e10a82387ac87fe9d1484abe6dbe
AB  - Reconstructing visual stimulus images from the brain activity signals is an important research task in the field of brain decoding. Many methods of reconstructing visual stimulus images mainly focus on how to use deep learning to classify the brain activities measured by functional magnetic resonance imaging or identify visual stimulus images. Accurate reconstruction of visual stimulus images by using deep learning still remains challenging. This paper proposes an improved deep generative multiview model to further promote the accuracy of reconstructing visual stimulus images. Firstly, an encoder based on residual-in-residual dense blocks is designed to fit the deep and multiview visual features of human natural state, and extract the features of visual stimulus images. Secondly, the structure of original decoder is extended to a deeper network in the deep generative multiview model, which makes the features obtained by each deconvolution layer more distinguishable. Finally, we configure the parameters of the optimizer and compare the performance of various optimizers under different parameter values, and then the one with the best performance is chosen and adopted to the whole model. The performance evaluations conducted on two publicly available datasets demonstrate that the improved model has more accurate reconstruction effectiveness than the original deep generative multiview model.
KW  - accuracy
KW  - Article
KW  - deconvolution
KW  - deep learning
KW  - electroencephalogram
KW  - functional magnetic resonance imaging
KW  - human
KW  - image reconstruction
KW  - visual stimulation
KW  - brain
KW  - diagnostic imaging
KW  - head
KW  - image processing
KW  - nuclear magnetic resonance imaging
KW  - procedures
KW  - Brain
KW  - Head
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Magnetic Resonance Imaging
ER  - 

TY  - JOUR
ID  - Wang2023Enhancing
AU  - Wang, Hanlin
AU  - Kuo, Yunting
AU  - Lo, Yuchun
AU  - Kuo, Chao Hung
AU  - Chen, Bowei
AU  - Wang, Chingfu
AU  - Wu, Zuyu
AU  - Lee, Chi En
AU  - Yang, Shih Hung
AU  - Lin, Shenghuang
AU  - Chen, Pochuan
AU  - Chen, Youyin
TI  - Enhancing Prediction of Forelimb Movement Trajectory through a Calibrating-Feedback Paradigm Incorporating RAT Primary Motor and Agranular Cortical Ensemble Activity in the Goal-Directed Reaching Task
PY  - 2023
JO  - International Journal of Neural Systems
T2  - International Journal of Neural Systems
VL  - 33
IS  - 10
DO  - 10.1142/S012906572350051X
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170231923&doi=10.1142%2FS012906572350051X&partnerID=40&md5=9b94e6a3054a654440ecdae01d735b52
AB  - Complete reaching movements involve target sensing, motor planning, and arm movement execution, and this process requires the integration and communication of various brain regions. Previously, reaching movements have been decoded successfully from the motor cortex (M1) and applied to prosthetic control. However, most studies attempted to decode neural activities from a single brain region, resulting in reduced decoding accuracy during visually guided reaching motions. To enhance the decoding accuracy of visually guided forelimb reaching movements, we propose a parallel computing neural network using both M1 and medial agranular cortex (AGm) neural activities of rats to predict forelimb-reaching movements. The proposed network decodes M1 neural activities into the primary components of the forelimb movement and decodes AGm neural activities into internal feedforward information to calibrate the forelimb movement in a goal-reaching movement. We demonstrate that using AGm neural activity to calibrate M1 predicted forelimb movement can improve decoding performance significantly compared to neural decoders without calibration. We also show that the M1 and AGm neural activities contribute to controlling forelimb movement during goal-reaching movements, and we report an increase in the power of the local field potential (LFP) in beta and gamma bands over AGm in response to a change in the target distance, which may involve sensorimotor transformation and communication between the visual cortex and AGm when preparing for an upcoming reaching movement. The proposed parallel computing neural network with the internal feedback model improves prediction accuracy for goal-reaching movements.
KW  - Brain
KW  - Electrophysiology
KW  - Feedback
KW  - Forecasting
KW  - Neurons
KW  - Brain regions
KW  - Cortexes
KW  - Goal-reaching movement
KW  - Internal feedback
KW  - Medial agranular cortex
KW  - Neural activity
KW  - Neural decoding
KW  - Primary motor cortex
KW  - Reaching movements
KW  - Target distance
KW  - Decoding
KW  - animal
KW  - feedback system
KW  - forelimb
KW  - motivation
KW  - movement (physiology)
KW  - physiology
KW  - upper limb
KW  - Animals
KW  - Forelimb
KW  - Goals
KW  - Movement
KW  - Upper Extremity
ER  - 

TY  - JOUR
ID  - Wang2023Decoding
AU  - Wang, Pengpai
AU  - Gong, Peiliang
AU  - Zhou, Yueying
AU  - Wen, Xuyun
AU  - Zhang, Daoqiang
TI  - Decoding the Continuous Motion Imagery Trajectories of Upper Limb Skeleton Points for EEG-Based Brain–Computer Interface
PY  - 2023
JO  - IEEE Transactions on Instrumentation and Measurement
T2  - IEEE Transactions on Instrumentation and Measurement
VL  - 72
SP  - 1-12
DO  - 10.1109/TIM.2022.3224991
AB  - In the field of brain–computer interface (BCI), brain decoding using electroencephalography (EEG) is an essential direction, and motion imagery EEG-based BCI can not only help rehabilitation of patients with physical disabilities, but also enhance the endurance and power of people. Most of the existing MI-based BCI studies are limited to discrete EEG classification or 3-D directional limb trajectory reconstruction. To suitable for the requirements of BCI systems in practical applications, here, we explored the decoding of trajectories of continuous nondirectional motion imagination in 3-D space based on Chinese sign language. We propose a motion imagery trajectory reconstruction Transformer (MITRT) model to decode the EEG signals of the subjects performing motion imagery, and obtain the positional changes in the 3-D space of the shoulder, elbow, and wrist skeleton points in the neural activity. We add the geometric constraint features of upper limb skeleton points to the model, and the MITRT decoding model can obtain prior knowledge to improve the reconstruction accuracy of spatial positions. To verify the decoding performance of our proposed model, we collected motor imagery (MI) EEG signals of 20 subjects based on Chinese sign language for experiments. The experimental results show that the average Pearson correlation coefficient of the six skeleton points was 0.975, which was significantly higher than the contrast models. This study is the first attempt to reconstruct multidirectional continuous nondirectional upper limb MI trajectories based on Chinese sign language. The experimental results show that it is feasible to decode and reconstruct imagined 3-D trajectories of human upper limb skeleton points from scalp EEG.
KW  - Electroencephalography
KW  - Trajectory
KW  - Image reconstruction
KW  - Decoding
KW  - Gesture recognition
KW  - Assistive technologies
KW  - Skeleton
KW  - Brain–computer interface (BCI)
KW  - electroencephalography (EEG)
KW  - limb motion decoding
KW  - motor imagery (MI)
KW  - trajectory reconstruction
SN  - 1557-9662
ER  - 

TY  - CONF
ID  - Chen2022International
AU  - Chen, Kai
AU  - Ma, Yongqiang
AU  - Sheng, Mingyang
AU  - Zheng, Nanning
TI  - Foreground-attention in neural decoding: Guiding Loop-Enc-Dec to reconstruct visual stimulus images from fMRI
PY  - 2022
BT  - 2022 International Joint Conference on Neural Networks (IJCNN)
T2  - 2022 International Joint Conference on Neural Networks (IJCNN)
SP  - 1-8
DO  - 10.1109/IJCNN55064.2022.9892276
AB  - The reconstruction of visual stimulus images from functional Magnetic Resonance Imaging (fMRI) has received extensive attention in recent years, which provides a possibility to interpret the human brain. Due to the high-dimensional and high-noise characteristics of fMRI data, how to extract stable, reliable and useful information from fMRI data for image reconstruction has become a challenging problem. Inspired by the mechanism of human visual attention, in this paper, we propose a novel method of reconstructing visual stimulus images, which first decodes human visual salient region from fMRI, we define human visual salient region as foreground attention (F-attention), and then reconstructs the visual images guided by F-attention. Because the human brain is strongly wound into sulci and gyri, some spatially adjacent voxels are not connected in practice. Therefore, it is necessary to consider the global information when decoding fMRI, so we introduce the self-attention module for capturing global information into the process of decoding F-attention. In addition, in order to obtain more loss constraints in the training process of encoder-decoder, we also propose a new training strategy called Loop-Enc-Dec. The experimental results show that the F-attention decoder decodes the visual attention from fMRI successfully, and the Loop-Enc-Dec guided by F-attention can also well reconstruct the visual stimulus images.
KW  - Training
KW  - Visualization
KW  - Neuroscience
KW  - Shape
KW  - Functional magnetic resonance imaging
KW  - Decoding
KW  - Reliability
KW  - visual reconstruction
KW  - Foreground-attention
KW  - fMRI
KW  - neural decoding
SN  - 2161-4407
DA  - July
ER  - 

TY  - JOUR
ID  - Du2022Structured
AU  - Du, Changde
AU  - Du, Changying
AU  - Huang, Lijie
AU  - Wang, Haibao
AU  - He, Huiguang
TI  - Structured Neural Decoding With Multitask Transfer Learning of Deep Neural Network Representations
PY  - 2022
JO  - IEEE Transactions on Neural Networks and Learning Systems
T2  - IEEE Transactions on Neural Networks and Learning Systems
VL  - 33
IS  - 2
SP  - 600-614
DO  - 10.1109/TNNLS.2020.3028167
AB  - The reconstruction of visual information from human brain activity is a very important research topic in brain decoding. Existing methods ignore the structural information underlying the brain activities and the visual features, which severely limits their performance and interpretability. Here, we propose a hierarchically structured neural decoding framework by using multitask transfer learning of deep neural network (DNN) representations and a matrix-variate Gaussian prior. Our framework consists of two stages, Voxel2Unit and Unit2Pixel. In Voxel2Unit, we decode the functional magnetic resonance imaging (fMRI) data to the intermediate features of a pretrained convolutional neural network (CNN). In Unit2Pixel, we further invert the predicted CNN features back to the visual images. Matrix-variate Gaussian prior allows us to take into account the structures between feature dimensions and between regression tasks, which are useful for improving decoding effectiveness and interpretability. This is in contrast with the existing single-output regression models that usually ignore these structures. We conduct extensive experiments on two real-world fMRI data sets, and the results show that our method can predict CNN features more accurately and reconstruct the perceived natural images and faces with higher quality.
KW  - Decoding
KW  - Image reconstruction
KW  - Functional magnetic resonance imaging
KW  - Visualization
KW  - Task analysis
KW  - Brain
KW  - Correlation
KW  - Deep neural network (DNN)
KW  - functional magnetic resonance imaging (fMRI)
KW  - image reconstruction
KW  - multioutput regression
KW  - neural decoding
SN  - 2162-2388
DA  - Feb
ER  - 

TY  - CONF
ID  - Jin2022Ieee
AU  - Jin, Yingxin
AU  - Shang, Shaohua
AU  - Tang, Liwei
AU  - He, Lianzhua
AU  - Zhou, MengChu
TI  - EEG channel selection algorithm based on Reinforcement Learning
PY  - 2022
BT  - 2022 IEEE International Conference on Networking, Sensing and Control (ICNSC)
T2  - 2022 IEEE International Conference on Networking, Sensing and Control (ICNSC)
SP  - 1-6
DO  - 10.1109/ICNSC55942.2022.10004161
AB  - Multichannel EEG is generally used to collect brain activities from various locations across the brain. However, BCIs using lesser channels will be more convenient for subjects. What's more, information acquired from adjacent channels is usually inter-correlated or irrelevant to the task. And some channels are noisy. This paper proposes a novel channel selection algorithm based on reinforcement learning. It can adaptively transform the full-channel EEG data to the optimal-channel-number EEG format conditioned on different input trials to make a trade-off between brain decoding accuracy and efficiency. Experimen-tal results showed that the proposed model can improve the classification accuracy by 2% ~ 6% compared to channel set $\{C3,C4,Cz\}$.
KW  - Knowledge engineering
KW  - Reinforcement learning
KW  - Transforms
KW  - Feature extraction
KW  - Electroencephalography
KW  - Decoding
KW  - Classification algorithms
KW  - EEG
KW  - channel selection
KW  - reinforcement learning
DA  - Dec
ER  - 

TY  - CONF
ID  - K2022International
AU  - K, Sowmya
AU  - S, Sushitha
TI  - An Interpretation on Brain Gate System Network and Technology- A Study
PY  - 2022
BT  - 2022 International Conference on Automation, Computing and Renewable Systems (ICACRS)
T2  - 2022 International Conference on Automation, Computing and Renewable Systems (ICACRS)
SP  - 868-873
DO  - 10.1109/ICACRS55517.2022.10029097
AB  - The process of thoughts-to-movement is a scientific discovery that makes a severely disabled human to manipulate a device using thoughts. These were achieved, in a great portion through the brain gate network technology. The network system uses a different kind of Brain neural signal which regulates the language of the neuron's sensing, transmission, perception, and implementation. The working idea of the brain gate system is that brainwave patterns are produced with brain function and not transmitted to the body. It is a brain implant device developed at Brown University in partnership with the biotech firm‚ Cyber-Kinetics Neuroscience. Even though effective BCI research has been arrived in the past 25 years, this paper aims to study the working, core principles and research aspects of brain gate technology in depth.
KW  - Digital control
KW  - Computer interfaces
KW  - Renewable energy sources
KW  - Neuroscience
KW  - Epilepsy
KW  - Neural implants
KW  - Logic gates
KW  - Neurotechnologies
KW  - Brain Computer Interface (BCI)
KW  - Electro-magnetic Neurons
KW  - Motor Cortex
KW  - Neural Decoding
KW  - Central Nervus System(CNS)
DA  - Dec
ER  - 

TY  - CONF
ID  - Karam2022International
AU  - Karam, Andrew
AU  - Boles, Kirollos
AU  - Raouf, Mario
AU  - Yousef, Mina Atef
AU  - Khoriba, Ghada
TI  - Deep learning approaches for analysing visual stimuli from human fMRI
PY  - 2022
BT  - 2022 2nd International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)
T2  - 2022 2nd International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)
SP  - 449-458
DO  - 10.1109/MIUCC55081.2022.9781770
AB  - Decoding brain activities corresponding to an external stimulus is an excellent challenge because of the complexity of brain activities data and the understood of the activity of the brain is not yet complete. This research paper focuses on the functional magnetic resonance imaging (fMRI) data of different people corresponding to external visual stimulus images. The images consist of three main types: letters, artificial shapes, and natural images. The proposed model provides an analysis and classification of the three main types of images using Support Vector Machine and Convolutional Neural Networks by proving that the brain is affected differently by each type. In addition, the identification of 200 natural image categories was analyzed by calculating the similarity between the features for each category by using the features of visual images using CNN and fMRI using the regression model. Also, we used deep convolution generative adversarial networks (DCGANs) for image reconstruction.
KW  - Support vector machines
KW  - Visualization
KW  - Analytical models
KW  - Functional magnetic resonance imaging
KW  - Predictive models
KW  - Brain modeling
KW  - Data models
KW  - Brain Decoding
KW  - Visual Image Reconstruction
KW  - Deep Neural Networks
KW  - GAN
KW  - fMRI
DA  - May
ER  - 

TY  - JOUR
ID  - Khaleghi2022Visual
AU  - Khaleghi, Nastaran
AU  - Yousefi Rezaii, Tohid
AU  - Beheshti, Soosan
AU  - Meshgini, Saeed
AU  - Sheykhivand, Sobhan
AU  - Daneshvar, Sabalan
TI  - Visual Saliency and Image Reconstruction from EEG Signals via an Effective Geometric Deep Network-Based Generative Adversarial Network
PY  - 2022
JO  - Electronics (Switzerland)
T2  - Electronics (Switzerland)
VL  - 11
IS  - 21
DO  - 10.3390/electronics11213637
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141696795&doi=10.3390%2Felectronics11213637&partnerID=40&md5=62ebda7f159fabddc49ee42a27519fd7
AB  - Reaching out the function of the brain in perceiving input data from the outside world is one of the great targets of neuroscience. Neural decoding helps us to model the connection between brain activities and the visual stimulation. The reconstruction of images from brain activity can be achieved through this modelling. Recent studies have shown that brain activity is impressed by visual saliency, the important parts of an image stimuli. In this paper, a deep model is proposed to reconstruct the image stimuli from electroencephalogram (EEG) recordings via visual saliency. To this end, the proposed geometric deep network-based generative adversarial network (GDN-GAN) is trained to map the EEG signals to the visual saliency maps corresponding to each image. The first part of the proposed GDN-GAN consists of Chebyshev graph convolutional layers. The input of the GDN part of the proposed network is the functional connectivity-based graph representation of the EEG channels. The output of the GDN is imposed to the GAN part of the proposed network to reconstruct the image saliency. The proposed GDN-GAN is trained using the Google Colaboratory Pro platform. The saliency metrics validate the viability and efficiency of the proposed saliency reconstruction network. The weights of the trained network are used as initial weights to reconstruct the grayscale image stimuli. The proposed network realizes the image reconstruction from EEG signals.
KW  - electroencephalogram
KW  - generative adversarial network
KW  - geometric deep network
KW  - image reconstruction
KW  - visual saliency
ER  - 

TY  - JOUR
ID  - Laino2022Generative
AU  - Laino, Maria Elena
AU  - Cancian, Pierandrea
AU  - Politi, Letterio S.
AU  - della Porta, Matteo Giovanni
AU  - Saba, Luca
AU  - Savevski, Victor
TI  - Generative Adversarial Networks in Brain Imaging: A Narrative Review
PY  - 2022
JO  - Journal of Imaging
T2  - Journal of Imaging
VL  - 8
IS  - 4
DO  - 10.3390/jimaging8040083
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127651324&doi=10.3390%2Fjimaging8040083&partnerID=40&md5=9c35e224d5a618f71b5ac9c0b0ed0b74
AB  - Artificial intelligence (AI) is expected to have a major effect on radiology as it demon-strated remarkable progress in many clinical tasks, mostly regarding the detection, segmentation, classification, monitoring, and prediction of diseases. Generative Adversarial Networks have been proposed as one of the most exciting applications of deep learning in radiology. GANs are a new approach to deep learning that leverages adversarial learning to tackle a wide array of computer vision challenges. Brain radiology was one of the first fields where GANs found their application. In neuroradiology, indeed, GANs open unexplored scenarios, allowing new processes such as image-to-image and cross-modality synthesis, image reconstruction, image segmentation, image synthesis, data augmentation, disease progression models, and brain decoding. In this narrative review, we will provide an introduction to GANs in brain imaging, discussing the clinical potential of GANs, future clinical applications, as well as pitfalls that radiologists should be aware of.
KW  - brain imaging
KW  - CT
KW  - fMRI
KW  - generative adversarial networks
KW  - MRI
KW  - PET
ER  - 

TY  - JOUR
ID  - Layton2022Estimating
AU  - Layton, Oliver W.
AU  - Powell, Nathaniel V.
AU  - Steinmetz, Scott T.
AU  - Fajen, Brett R.
TI  - Estimating curvilinear self-motion from optic flow with a biologically inspired neural system
PY  - 2022
JO  - Bioinspiration and Biomimetics
T2  - Bioinspiration and Biomimetics
VL  - 17
IS  - 4
DO  - 10.1088/1748-3190/ac709b
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131772037&doi=10.1088%2F1748-3190%2Fac709b&partnerID=40&md5=d47990b22cb979f8665de7f762d1882c
AB  - Optic flow provides rich information about world-relative self-motion and is used by many animals to guide movement. For example, self-motion along linear, straight paths without eye movements, generates optic flow that radiates from a singularity that specifies the direction of travel (heading). Many neural models of optic flow processing contain heading detectors that are tuned to the position of the singularity, the design of which is informed by brain area MSTd of primate visual cortex that has been linked to heading perception. Such biologically inspired models could be useful for efficient self-motion estimation in robots, but existing systems are tailored to the limited scenario of linear self-motion and neglect sensitivity to self-motion along more natural curvilinear paths. The observer in this case experiences more complex motion patterns, the appearance of which depends on the radius of the curved path (path curvature) and the direction of gaze. Indeed, MSTd neurons have been shown to exhibit tuning to optic flow patterns other than radial expansion, a property that is rarely captured in neural models. We investigated in a computational model whether a population of MSTd-like sensors tuned to radial, spiral, ground, and other optic flow patterns could support the accurate estimation of parameters describing both linear and curvilinear self-motion. We used deep learning to decode self-motion parameters from the signals produced by the diverse population of MSTd-like units. We demonstrate that this system is capable of accurately estimating curvilinear path curvature, clockwise/counterclockwise sign, and gaze direction relative to the path tangent in both synthetic and naturalistic videos of simulated self-motion. Estimates remained stable over time while rapidly adapting to dynamic changes in the observer's curvilinear self-motion. Our results show that coupled biologically inspired and artificial neural network systems hold promise as a solution for robust vision-based self-motion estimation in robots.
KW  - Biomimetics
KW  - Decoding
KW  - Eye movements
KW  - Flow patterns
KW  - Motion estimation
KW  - Neural networks
KW  - Optical flows
KW  - Biologically-inspired
KW  - Curvilinear path
KW  - Deep learning
KW  - Heading
KW  - Neural decoding
KW  - Neural modelling
KW  - Optic flow
KW  - Path curvature
KW  - Self motion
KW  - Self-motion estimation
KW  - animal
KW  - motion
KW  - movement perception
KW  - nerve cell
KW  - optic flow
KW  - physiology
KW  - visual cortex
KW  - Animals
KW  - Motion
KW  - Motion Perception
KW  - Neurons
KW  - Optic Flow
KW  - Visual Cortex
ER  - 

TY  - CONF
ID  - Ozcelik2022International
AU  - Ozcelik, Furkan
AU  - Choksi, Bhavin
AU  - Mozafari, Milad
AU  - Reddy, Leila
AU  - VanRullen, Rufin
TI  - Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs
PY  - 2022
BT  - 2022 International Joint Conference on Neural Networks (IJCNN)
T2  - 2022 International Joint Conference on Neural Networks (IJCNN)
SP  - 1-8
DO  - 10.1109/IJCNN55064.2022.9892673
AB  - Reconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRI-predicted variables. The generated images presented state-of-the-art results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regions-of-interest in the human brain.
KW  - Visualization
KW  - Image synthesis
KW  - Semantics
KW  - Self-supervised learning
KW  - Functional magnetic resonance imaging
KW  - Predictive models
KW  - Feature extraction
KW  - Natural Image Reconstruction
KW  - fMRI Decoding
KW  - IC-GAN
KW  - Brain-Computer Interface
SN  - 2161-4407
DA  - July
ER  - 

TY  - CONF
ID  - Taguchi2022Joint
AU  - Taguchi, Haruka
AU  - Nishida, Satoshi
AU  - Nishimoto, Shinji
AU  - Kobayashi, Ichiro
TI  - Validation of the Role of Attention Mechanism in Predicting Brain Activity
PY  - 2022
BT  - 2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems (SCIS&ISIS)
T2  - 2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems (SCIS&ISIS)
SP  - 1-5
DO  - 10.1109/SCISISIS55246.2022.10001915
AB  - In this study, we estimate the state of the human brain from visual stimuli by regressing the brain activity state from image features extracted from an image identification deep learning model. We introduce Attention Branch Network, which enhances to capture the features of the identified target by attention when extracting image features, into the image identification deep learning model and estimate the brain activity state from the image features weighted or unweighted by attention. Through experiments, we aim to verify the role of attention mechanism in estimating brain activity state from visual stimuli. As a result, we confirmed that the introduction of Attention did not have a significant effect on the estimation accuracy, but that differences were observed in the areas where the estimation accuracy was higher.
KW  - Deep learning
KW  - Visualization
KW  - Decision making
KW  - Estimation
KW  - Information representation
KW  - Predictive models
KW  - Functional magnetic resonance imaging
KW  - brain decoding
KW  - attention branch network
KW  - fMRI
KW  - ResNet
DA  - Nov
ER  - 

TY  - CONF
ID  - Weng2022Asilomar
AU  - Weng, Geyu
AU  - Akbarian, Amir
AU  - Noudoost, Behrad
AU  - Nategh, Neda
TI  - Modeling the Relationship between Perisaccadic Neural Responses and Location Information
PY  - 2022
BT  - 2022 56th Asilomar Conference on Signals, Systems, and Computers
T2  - 2022 56th Asilomar Conference on Signals, Systems, and Computers
SP  - 451-454
DO  - 10.1109/IEEECONF56349.2022.10051903
AB  - Eye movements are essential for the brain to collect visual information from the environment. Visual images projected on the retina change abruptly during rapid ballistic eye movements (saccades), but our perception of the visual world is continuous. To generate a stable visual perception, the spatiotemporal sensitivity of visual neurons needs to change quickly prior to and during saccades. This study uses a modeling framework to characterize the fast dynamics of neuronal responses across saccades, thereby quantifying the contribution of perisaccadic response dynamics to the readout of location information during saccades. We apply this approach to neuronal responses recorded from the visual cortex of nonhuman primates during a visually-guided saccade task with visual stimulations. Using the model-predicted responses and a classification method, we measure the spatial discriminability performance of neurons at pre-saccadic and post-saccadic receptive field locations. Characterizing the readout of perisaccadic spatial information and its precise time course can provide insights into how neurons integrate spatial information across saccades to generate a continuous visual experience.
KW  - Visualization
KW  - Sensitivity
KW  - Neurons
KW  - Electrophysiology
KW  - Brain modeling
KW  - Retina
KW  - Time measurement
KW  - computational models
KW  - neural decoding
KW  - eye movement
KW  - visual cortex
SN  - 2576-2303
DA  - Oct
ER  - 

TY  - JOUR
ID  - Wu2022Featurespecific
AU  - Wu, Hao
AU  - Zheng, Nanning
AU  - Chen, Badong
TI  - Feature-Specific Denoising of Neural Activity for Natural Image Identification
PY  - 2022
JO  - IEEE Transactions on Cognitive and Developmental Systems
T2  - IEEE Transactions on Cognitive and Developmental Systems
VL  - 14
IS  - 2
SP  - 629-638
DO  - 10.1109/TCDS.2021.3062067
AB  - Decoding the content in neural activity through voxelwise encoding plays an important role in investigating cognitive functions of the human brain. However, unlike multivoxel pattern analysis (MVPA), voxelwise encoding builds a model for each individual voxel; therefore, ignores the interactions between voxels and is sensitive to noise. In this work, we propose the feature-specific denoise (FSdenoise), a noise reduction method for encoding-based models to improve their decoding performance. FSdenoise considers the response of a voxel to a stimulus as a combination of two components: 1) feature-relevant component, which can be predicted from stimulus features and 2) feature-irrelevant component, which shows no direct relation to the concerned features. Exploiting the correlations between voxels, FSdenoise reduces the feature-irrelevant component in voxels that exhibit more feature-relevant component, enhancing their predictive power from stimulus features. Decoding performance with the denoised voxels would be improved in consequence. We validate the FSdenoise on two functional magnetic resonance imaging data sets and the results demonstrate that FSdenoise can efficiently improve the decoding accuracy for encoding-based approaches. Moreover, the encoding-based approaches combined with FSdenoise can even outperform the MVPA-based approach in brain decoding.
KW  - Decoding
KW  - Predictive models
KW  - Image coding
KW  - Feature extraction
KW  - Noise reduction
KW  - Encoding
KW  - Brain modeling
KW  - Brain decoding
KW  - denoising
KW  - functional magnetic resonance imaging (fMRI)
KW  - visual cognition
KW  - voxelwise encoding
SN  - 2379-8939
DA  - June
ER  - 

TY  - CONF
ID  - Xie2022Influence
AU  - Xie, Donghong
TI  - On the Influence of Feature Selection and Regression Models on the Decoding Accuracy of Seen and Imagery Objects Using Hierarchical Visual Features
PY  - 2022
JO  - ACM International Conference Proceeding Series
T2  - ACM International Conference Proceeding Series
SP  - 7 - 15
DO  - 10.1145/3560470.3560472
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142471677&doi=10.1145%2F3560470.3560472&partnerID=40&md5=df9073c0d690dbcc8e135137a2767472
AB  - With the development in brain decoding and encoding, the seen and imagery object identification tasks achieve increasingly higher accuracy by using more intricate features. However, the experiment always requires a huge amount of data to process which will consume a lot of time. With available seen and imagery BOLD signals, we leverage the Generic Brain Decoding model to fit and then predict visual feature vectors extracted from neural networks. Given the hierarchy of AlexNet and that of the human visual cortex, the accuracies of the couplings between low layers of AlexNet and human visual cortex and those between high layers outperform other coupling cases. Using a different number of feature dimensions during feature selection induces a significant difference in the decoding performance at the high layers. Moreover, the shallow neural network achieves higher accuracies at the high layers. The results demonstrate that when the number of available data samples is not sufficient, reducing the input feature dimensions can still ensure a comparable decoding accuracy.
KW  - Decoding
KW  - Feature Selection
KW  - Multilayer neural networks
KW  - Regression analysis
KW  - Brain decoding
KW  - Brain visual decoding
KW  - Features selection
KW  - High-accuracy
KW  - Human visual cortex
KW  - Identification accuracy
KW  - Neural-networks
KW  - Objects recognition
KW  - Shallow neural network
KW  - Visual feature
KW  - Object recognition
ER  - 

TY  - JOUR
ID  - Xu2022Robust
AU  - Xu, Qi
AU  - Shen, Jiangrong
AU  - Ran, Xuming
AU  - Tang, Huajin
AU  - Pan, Gang
AU  - Liu, Jian K.
TI  - Robust Transcoding Sensory Information With Neural Spikes
PY  - 2022
JO  - IEEE Transactions on Neural Networks and Learning Systems
T2  - IEEE Transactions on Neural Networks and Learning Systems
VL  - 33
IS  - 5
SP  - 1935-1946
DO  - 10.1109/TNNLS.2021.3107449
AB  - Neural coding, including encoding and decoding, is one of the key problems in neuroscience for understanding how the brain uses neural signals to relate sensory perception and motor behaviors with neural systems. However, most of the existed studies only aim at dealing with the continuous signal of neural systems, while lacking a unique feature of biological neurons, termed spike, which is the fundamental information unit for neural computation as well as a building block for brain–machine interface. Aiming at these limitations, we propose a transcoding framework to encode multi-modal sensory information into neural spikes and then reconstruct stimuli from spikes. Sensory information can be compressed into 10% in terms of neural spikes, yet re-extract 100% of information by reconstruction. Our framework can not only feasibly and accurately reconstruct dynamical visual and auditory scenes, but also rebuild the stimulus patterns from functional magnetic resonance imaging (fMRI) brain activities. More importantly, it has a superb ability of noise immunity for various types of artificial noises and background signals. The proposed framework provides efficient ways to perform multimodal feature representation and reconstruction in a high-throughput fashion, with potential usage for efficient neuromorphic computing in a noisy environment.
KW  - Decoding
KW  - Neurons
KW  - Image reconstruction
KW  - Biological information theory
KW  - Transcoding
KW  - Visualization
KW  - Computational modeling
KW  - Cross-multimodal
KW  - decoding
KW  - denoising
KW  - neural spikes
KW  - reconstruction
KW  - spatio-temporal representations
SN  - 2162-2388
DA  - May
ER  - 

TY  - JOUR
ID  - Zhang2022Decoding
AU  - Zhang, Yijun
AU  - Bu, Tong
AU  - Zhang, Jiyuan
AU  - Tang, Shiming
AU  - Yu, Zhaofei
AU  - Liu, Jian K.
AU  - Huang, Tiejun
TI  - Decoding Pixel-Level Image Features From Two-Photon Calcium Signals of Macaque Visual Cortex
PY  - 2022
JO  - Neural Computation
T2  - Neural Computation
VL  - 34
IS  - 6
SP  - 1369-1397
DO  - 10.1162/neco_a_01498
AB  - Images of visual scenes comprise essential features important for visual cognition of the brain. The complexity of visual features lies at different levels, from simple artificial patterns to natural images with different scenes. It has been a focus of using stimulus images to predict neural responses. However, it remains unclear how to extract features from neuronal responses. Here we address this question by leveraging two-photon calcium neural data recorded from the visual cortex of awake macaque monkeys. With stimuli including various categories of artificial patterns and diverse scenes of natural images, we employed a deep neural network decoder inspired by image segmentation technique. Consistent with the notation of sparse coding for natural images, a few neurons with stronger responses dominated the decoding performance, whereas decoding of ar tificial patterns needs a large number of neurons. When natural images using the model pretrained on artificial patterns are decoded, salient features of natural scenes can be extracted, as well as the conventional category information. Altogether, our results give a new perspective on studying neural encoding principles using reverse-engineering decoding strategies.
SN  - 0899-7667
DA  - May
ER  - 

TY  - JOUR
ID  - Hallenbeck2021Working
AU  - Hallenbeck, Grace E.
AU  - Sprague, Thomas C.
AU  - Rahmati, Masih
AU  - Sreenivasan, Kartik K.
AU  - Curtis, Clayton E.
TI  - Working memory representations in visual cortex mediate distraction effects
PY  - 2021
JO  - Nature Communications
T2  - Nature Communications
VL  - 12
IS  - 1
DO  - 10.1038/s41467-021-24973-1
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111995233&doi=10.1038%2Fs41467-021-24973-1&partnerID=40&md5=15c5d1faf642c742d5cee6f504ff062a
AB  - Although the contents of working memory can be decoded from visual cortex activity, these representations may play a limited role if they are not robust to distraction. We used model-based fMRI to estimate the impact of distracting visual tasks on working memory representations in several visual field maps in visual and frontoparietal association cortex. Here, we show distraction causes the fidelity of working memory representations to briefly dip when both the memorandum and distractor are jointly encoded by the population activities. Distraction induces small biases in memory errors which can be predicted by biases in neural decoding in early visual cortex, but not other regions. Although distraction briefly disrupts working memory representations, the widespread redundancy with which working memory information is encoded may protect against catastrophic loss. In early visual cortex, the neural representation of information in working memory and behavioral performance are intertwined, solidifying its importance in visual memory.
KW  - brain
KW  - memory
KW  - visual analysis
KW  - article
KW  - association cortex
KW  - functional magnetic resonance imaging
KW  - visual cortex
KW  - visual field
KW  - visual memory
KW  - working memory
KW  - adult
KW  - attention
KW  - biological model
KW  - brain mapping
KW  - female
KW  - functional neuroimaging
KW  - human
KW  - male
KW  - middle aged
KW  - nuclear magnetic resonance imaging
KW  - photostimulation
KW  - physiology
KW  - psychological model
KW  - short term memory
KW  - task performance
KW  - Adult
KW  - Attention
KW  - Brain Mapping
KW  - Female
KW  - Functional Neuroimaging
KW  - Humans
KW  - Magnetic Resonance Imaging
KW  - Male
KW  - Memory, Short-Term
KW  - Middle Aged
KW  - Models, Neurological
KW  - Models, Psychological
KW  - Photic Stimulation
KW  - Task Performance and Analysis
KW  - Visual Cortex
ER  - 

TY  - CONF
ID  - Hu2021Decoding
AU  - Hu, Lulu
AU  - Li, Jingwei
AU  - Zhang, Chi
AU  - Tong, Li
TI  - Decoding Categories from Human Brain Activity in the Human Visual Cortex Using the Triplet Network
PY  - 2021
SP  - 128 - 134
DO  - 10.1145/3448748.3448769
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103519138&doi=10.1145%2F3448748.3448769&partnerID=40&md5=870f91f23c3ed87aadcda4515aa1d84e
AB  - Decoding visual stimuli from functional magnetic resonance imaging (fMRI) is of great significance for understanding the neural mechanism of the visual information processing in the human brain. How to extract effective information from massive voxel data in the brain to predict the brain state is a problem worth discussing in fMRI. However, the inherent characteristics of small quantity and high dimensionality in fMRI data limited the performance of brain decoding. As an effective way to acquire visual information, people usually compare with the prior knowledge learned when recognizing objects, and does not need to have a complete understanding of visual information. In this paper, we proposed a new visual classification model to decode the stimulus categories from the visual information of the brain based on the triplet network. The triplet network is a model framework with a comparison mechanism similar to that of human visual recognition objects, contains three-branches weight sharing subnetworks, which are composed of fully connected networks in our model. Our results showed that the decoding accuracy is 57.5±1.86% and 44.17±1.31% for S1 and S2, respectively. S1 was about 6% higher than the best traditional machine learning classifier SVM, while S2 was nearly 3.5% higher than SVM. Our results fully confirmed the validity of comparing the differences between samples in fMRI data with small quantity.
KW  - Bioinformatics
KW  - Brain
KW  - Classification (of information)
KW  - Data reduction
KW  - Decoding
KW  - Intelligent computing
KW  - Learning systems
KW  - Magnetic resonance imaging
KW  - Support vector machines
KW  - Fully connected networks
KW  - Functional magnetic resonance imaging
KW  - High dimensionality
KW  - Human visual cortex
KW  - Inherent characteristics
KW  - Visual classification
KW  - Visual information
KW  - Visual information processing
KW  - Functional neuroimaging
ER  - 

TY  - JOUR
ID  - Huang2021Neural
AU  - Huang, Wei
AU  - Yan, Hongmei
AU  - Cheng, Kaiwen
AU  - Wang, Chong
AU  - Li, Jiyi
AU  - Wang, Yuting
AU  - Li, Chen
AU  - Li, Chaorong
AU  - Li, Yunhan
AU  - Zuo, Zhentao
AU  - Chen, Huafu
TI  - A neural decoding algorithm that generates language from visual activity evoked by natural images
PY  - 2021
JO  - Neural Networks
T2  - Neural Networks
VL  - 144
SP  - 90 - 100
DO  - 10.1016/j.neunet.2021.08.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113990280&doi=10.1016%2Fj.neunet.2021.08.006&partnerID=40&md5=045538c96a65b28da54eaac9b0444645
AB  - Transforming neural activities into language is revolutionary for human–computer interaction as well as functional restoration of aphasia. Present rapid development of artificial intelligence makes it feasible to decode the neural signals of human visual activities. In this paper, a novel Progressive Transfer Language Decoding Model (PT-LDM) is proposed to decode visual fMRI signals into phrases or sentences when natural images are being watched. The PT-LDM consists of an image-encoder, a fMRI encoder and a language-decoder. The results showed that phrases and sentences were successfully generated from visual activities. Similarity analysis showed that three often-used evaluation indexes BLEU, ROUGE and CIDEr reached 0.182, 0.197 and 0.680 averagely between the generated texts and the corresponding annotated texts in the testing set respectively, significantly higher than the baseline. Moreover, we found that higher visual areas usually had better performance than lower visual areas and the contribution curve of visual response patterns in language decoding varied at successively different time points. Our findings demonstrate that the neural representations elicited in visual cortices when scenes are being viewed have already contained semantic information that can be utilized to generate human language. Our study shows potential application of language-based brain–machine interfaces in the future, especially for assisting aphasics in communicating more efficiently with fMRI signals.
KW  - Brain
KW  - Human computer interaction
KW  - Semantics
KW  - Signal encoding
KW  - Visual languages
KW  - Computer interaction
KW  - Decoding algorithm
KW  - Language decoding
KW  - Natural images
KW  - Neural activity
KW  - Neural decoding
KW  - Neural signals
KW  - Progressive transfer
KW  - Visual activity
KW  - Visual areas
KW  - Decoding
KW  - adult
KW  - algorithm
KW  - Article
KW  - electroencephalogram
KW  - female
KW  - functional magnetic resonance imaging
KW  - human
KW  - human computer interaction
KW  - language
KW  - language processing
KW  - male
KW  - model
KW  - semantics
KW  - vision
KW  - visual cortex
KW  - visual evoked potential
KW  - artificial intelligence
KW  - brain mapping
KW  - nuclear magnetic resonance imaging
KW  - Algorithms
KW  - Artificial Intelligence
KW  - Brain Mapping
KW  - Humans
KW  - Language
KW  - Magnetic Resonance Imaging
ER  - 

TY  - JOUR
ID  - Huang2021Deep
AU  - Huang, Wei
AU  - Yan, Hongmei
AU  - Wang, Chong
AU  - Yang, Xiaoqing
AU  - Li, Jiyi
AU  - Zuo, Zhentao
AU  - Zhang, Jiang
AU  - Chen, Huafu
TI  - Deep Natural Image Reconstruction from Human Brain Activity Based on Conditional Progressively Growing Generative Adversarial Networks
PY  - 2021
JO  - Neuroscience Bulletin
T2  - Neuroscience Bulletin
VL  - 37
IS  - 3
SP  - 369 - 379
DO  - 10.1007/s12264-020-00613-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096395474&doi=10.1007%2Fs12264-020-00613-4&partnerID=40&md5=11fc49b4432eb636b70a84b65ef5cd1a
AB  - Brain decoding based on functional magnetic resonance imaging has recently enabled the identification of visual perception and mental states. However, due to the limitations of sample size and the lack of an effective reconstruction model, accurate reconstruction of natural images is still a major challenge. The current, rapid development of deep learning models provides the possibility of overcoming these obstacles. Here, we propose a deep learning-based framework that includes a latent feature extractor, a latent feature decoder, and a natural image generator, to achieve the accurate reconstruction of natural images from brain activity. The latent feature extractor is used to extract the latent features of natural images. The latent feature decoder predicts the latent features of natural images based on the response signals from the higher visual cortex. The natural image generator is applied to generate reconstructed images from the predicted latent features of natural images and the response signals from the visual cortex. Quantitative and qualitative evaluations were conducted with test images. The results showed that the reconstructed image achieved comparable, accurate reproduction of the presented image in both high-level semantic category information and low-level pixel information. The framework we propose shows promise for decoding the brain activity.
KW  - article
KW  - brain function
KW  - deep learning
KW  - functional magnetic resonance imaging
KW  - human
KW  - human experiment
KW  - image reconstruction
KW  - qualitative analysis
KW  - quantitative analysis
KW  - reproduction
KW  - visual cortex
KW  - brain
KW  - diagnostic imaging
KW  - image processing
KW  - nuclear magnetic resonance imaging
KW  - Brain
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Magnetic Resonance Imaging
KW  - Neural Networks, Computer
KW  - Visual Cortex
ER  - 

TY  - JOUR
ID  - Nonaka2021Brain
AU  - Nonaka, Soma
AU  - Majima, Kei
AU  - Aoki, Shuntaro C.
AU  - Kamitani, Yukiyasu
TI  - Brain hierarchy score: Which deep neural networks are hierarchically brain-like?
PY  - 2021
JO  - iScience
T2  - iScience
VL  - 24
IS  - 9
DO  - 10.1016/j.isci.2021.103013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122826845&doi=10.1016%2Fj.isci.2021.103013&partnerID=40&md5=cc8ea22a5576d4f8f355de24c3ee9e2d
AB  - Achievement of human-level image recognition by deep neural networks (DNNs) has spurred interest in whether and how DNNs are brain-like. Both DNNs and the visual cortex perform hierarchical processing, and correspondence has been shown between hierarchical visual areas and DNN layers in representing visual features. Here, we propose the brain hierarchy (BH) score as a metric to quantify the degree of hierarchical correspondence based on neural decoding and encoding analyses where DNN unit activations and human brain activity are predicted from each other. We find that BH scores for 29 pre-trained DNNs with various architectures are negatively correlated with image recognition performance, thus indicating that recently developed high-performance DNNs are not necessarily brain-like. Experimental manipulations of DNN models suggest that single-path sequential feedforward architecture with broad spatial integration is critical to brain-like hierarchy. Our method may provide new ways to design DNNs in light of their representational homology to the brain.
KW  - Human-centered computing
KW  - Neural networks
KW  - Neuroscience
ER  - 

TY  - JOUR
ID  - Rakhimberdina2021Natural
AU  - Rakhimberdina, Zarina
AU  - Jodelet, Quentin
AU  - Liu, Xin
AU  - Murata, Tsuyoshi
TI  - Natural Image Reconstruction From fMRI Using Deep Learning: A Survey
PY  - 2021
JO  - Frontiers in Neuroscience
T2  - Frontiers in Neuroscience
VL  - 15
DO  - 10.3389/fnins.2021.795488
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122311969&doi=10.3389%2Ffnins.2021.795488&partnerID=40&md5=abde4ee9d4039110d10d258e09c66ea1
AB  - With the advent of brain imaging techniques and machine learning tools, much effort has been devoted to building computational models to capture the encoding of visual information in the human brain. One of the most challenging brain decoding tasks is the accurate reconstruction of the perceived natural images from brain activities measured by functional magnetic resonance imaging (fMRI). In this work, we survey the most recent deep learning methods for natural image reconstruction from fMRI. We examine these methods in terms of architectural design, benchmark datasets, and evaluation metrics and present a fair performance evaluation across standardized evaluation metrics. Finally, we discuss the strengths and limitations of existing studies and present potential future directions.
KW  - brain
KW  - deep learning
KW  - functional magnetic resonance imaging
KW  - human
KW  - human experiment
KW  - image reconstruction
KW  - review
ER  - 

TY  - CONF
ID  - Takada2021Ieee
AU  - Takada, Saya
AU  - Togo, Ren
AU  - Ogawa, Takahiro
AU  - Haseyama, Miki
TI  - Estimating Imagined Images from Brain Activities via Visual Question Answering
PY  - 2021
BT  - 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)
T2  - 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)
SP  - 35-36
DO  - 10.1109/GCCE53005.2021.9622077
AB  - Investigating human mental contents has been a topic for many years, but its ambiguous property has made the analysis difficult. We propose a neural decoding method via a machine learning model that predicts the imagined content based on measuring brain activity in this paper. This technique uses brain activity and computer vision models to discover the association between human functional magnetic resonance imaging (fMRI) activity and imagined contents. Decoding models based on neural networks learned using stimulus-induced brain activity in the visual cortex region showed an accurate estimation of the content. We provide a means of revealing subjective mental content by analysis with visual question answering. This result shows that a mental experience relates to brain activity patterns.
KW  - Visualization
KW  - Image color analysis
KW  - Computational modeling
KW  - Estimation
KW  - Machine learning
KW  - Functional magnetic resonance imaging
KW  - Predictive models
SN  - 2378-8143
DA  - Oct
ER  - 

TY  - JOUR
ID  - Tripathy2021Decoding
AU  - Tripathy, Kalyan
AU  - Markow, Zachary E.
AU  - Fishell, Andrew K.
AU  - Sherafati, Arefeh
AU  - Burns-Yocum, Tracy M.
AU  - Schroeder, Mariel Lee
AU  - Svoboda, Alexandra M.
AU  - Eggebrecht, Adam T.
AU  - Anastasio, Mark A.
AU  - Schlaggar, Bradley L.
AU  - Culver, Joseph P.
TI  - Decoding visual information from high-density diffuse optical tomography neuroimaging data
PY  - 2021
JO  - NeuroImage
T2  - NeuroImage
VL  - 226
DO  - 10.1016/j.neuroimage.2020.117516
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097338427&doi=10.1016%2Fj.neuroimage.2020.117516&partnerID=40&md5=ffaf23af600c0cfeb45496579d3f7a55
AB  - Background: Neural decoding could be useful in many ways, from serving as a neuroscience research tool to providing a means of augmented communication for patients with neurological conditions. However, applications of decoding are currently constrained by the limitations of traditional neuroimaging modalities. Electrocorticography requires invasive neurosurgery, magnetic resonance imaging (MRI) is too cumbersome for uses like daily communication, and alternatives like functional near-infrared spectroscopy (fNIRS) offer poor image quality. High-density diffuse optical tomography (HD-DOT) is an emerging modality that uses denser optode arrays than fNIRS to combine logistical advantages of optical neuroimaging with enhanced image quality. Despite the resulting promise of HD-DOT for facilitating field applications of neuroimaging, decoding of brain activity as measured by HD-DOT has yet to be evaluated. Objective: To assess the feasibility and performance of decoding with HD-DOT in visual cortex. Methods and Results: To establish the feasibility of decoding at the single-trial level with HD-DOT, a template matching strategy was used to decode visual stimulus position. A receiver operating characteristic (ROC) analysis was used to quantify the sensitivity, specificity, and reproducibility of binary visual decoding. Mean areas under the curve (AUCs) greater than 0.97 across 10 imaging sessions in a highly sampled participant were observed. ROC analyses of decoding across 5 participants established both reproducibility in multiple individuals and the feasibility of inter-individual decoding (mean AUCs > 0.7), although decoding performance varied between individuals. Phase-encoded checkerboard stimuli were used to assess more complex, non-binary decoding with HD-DOT. Across 3 highly sampled participants, the phase of a 60° wide checkerboard wedge rotating 10° per second through 360° was decoded with a within-participant error of 25.8±24.7°. Decoding between participants was also feasible based on permutation-based significance testing. Conclusions: Visual stimulus information can be decoded accurately, reproducibly, and across a range of detail (for both binary and non-binary outcomes) at the single-trial level (without needing to block-average test data) using HD-DOT data. These results lay the foundation for future studies of more complex decoding with HD-DOT and applications in clinical populations.
KW  - adult
KW  - area under the curve
KW  - article
KW  - brain function
KW  - clinical article
KW  - controlled study
KW  - feasibility study
KW  - female
KW  - functional near-infrared spectroscopy
KW  - functional neuroimaging
KW  - human
KW  - human experiment
KW  - human tissue
KW  - image quality
KW  - male
KW  - optical tomography
KW  - quantitative analysis
KW  - receiver operating characteristic
KW  - reproducibility
KW  - sensitivity and specificity
KW  - visual cortex
KW  - visual information
KW  - visual stimulation
KW  - image processing
KW  - middle aged
KW  - physiology
KW  - procedures
KW  - vision
KW  - Adult
KW  - Female
KW  - Functional Neuroimaging
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Male
KW  - Middle Aged
KW  - Tomography, Optical
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - Wakita2021Photorealistic
AU  - Wakita, Suguru
AU  - Orima, Taiki
AU  - Motoyoshi, Isamu
TI  - Photorealistic Reconstruction of Visual Texture From EEG Signals
PY  - 2021
JO  - Frontiers in Computational Neuroscience
T2  - Frontiers in Computational Neuroscience
VL  - 15
DO  - 10.3389/fncom.2021.754587
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120531286&doi=10.3389%2Ffncom.2021.754587&partnerID=40&md5=0ab74dc3e7fe164f9c559c8075732fcf
AB  - Recent advances in brain decoding have made it possible to classify image categories based on neural activity. Increasing numbers of studies have further attempted to reconstruct the image itself. However, because images of objects and scenes inherently involve spatial layout information, the reconstruction usually requires retinotopically organized neural data with high spatial resolution, such as fMRI signals. In contrast, spatial layout does not matter in the perception of “texture,” which is known to be represented as spatially global image statistics in the visual cortex. This property of “texture” enables us to reconstruct the perceived image from EEG signals, which have a low spatial resolution. Here, we propose an MVAE-based approach for reconstructing texture images from visual evoked potentials measured from observers viewing natural textures such as the textures of various surfaces and object ensembles. This approach allowed us to reconstruct images that perceptually resemble the original textures with a photographic appearance. The present approach can be used as a method for decoding the highly detailed “impression” of sensory stimuli from brain activity.
KW  - Brain
KW  - Decoding
KW  - Electrophysiology
KW  - Image coding
KW  - Image reconstruction
KW  - Image resolution
KW  - Image texture
KW  - Network coding
KW  - Neurons
KW  - Textures
KW  - Auto encoders
KW  - Brain decoding
KW  - Deep neural network
KW  - EEG signals
KW  - Multi-modal
KW  - Multimodal variational auto encoder
KW  - Object and scenes
KW  - Photo-realistic
KW  - Spatial layout
KW  - Visual texture
KW  - Deep neural networks
KW  - article
KW  - autoencoder
KW  - brain function
KW  - deep neural network
KW  - electroencephalogram
KW  - human
KW  - perception
KW  - visual cortex
KW  - visual evoked potential
ER  - 

TY  - CONF
ID  - Wang2021Ieee
AU  - Wang, Sixian
AU  - Dai, Jincheng
AU  - Yao, Shengshi
AU  - Niu, Kai
AU  - Zhang, Ping
TI  - A Novel Deep Learning Architecture for Wireless Image Transmission
PY  - 2021
BT  - 2021 IEEE Global Communications Conference (GLOBECOM)
T2  - 2021 IEEE Global Communications Conference (GLOBECOM)
SP  - 1-6
DO  - 10.1109/GLOBECOM46510.2021.9685036
AB  - In this paper, the problem of neural compression based image transmission over wireless channels is studied. Since all procedures are considered over wireless links, the quality of training is affected by wireless factors such as packet errors. In the considered model, compressed data given by the neural source encoder (NSE) are fed into an error-control channel encoder and modulated as discrete symbols sent over a memoryless channel. In the receiving end, the channel decoder and the neural source decoder (NSD) forms an iterative structure to reconstruct the original image. Since all neural compressed data are transmitted over wireless channels, the training of NSD is affected by wireless channel factors such as residual bit errors given by the channel decoder. Meanwhile, during outer-loop iterations, the NSD needs to match the variant of information reliability output by the channel decoder so as to build a global optimal receiver. To this end, a refiner neural network is first attached after the NSD to adjust its output as the format of a priori information sent into the channel decoder. Then, the extrinsic information transfer (EXIT) functions of channel decoder and NSD are derived. At each iteration, the reliability of messages sent into the NSD is explicitly predicted by using the EXIT chart. By this means, the NSD can be trained in a residual bit error aware manner, and we realize a joint learning and iterative decoding framework to ensure the quality of neural image transmission over realistic wireless channels.
KW  - Wireless communication
KW  - Training
KW  - Image coding
KW  - Image communication
KW  - Neural networks
KW  - Receivers
KW  - Reliability engineering
DA  - Dec
ER  - 

TY  - JOUR
ID  - Wu2021Encoding
AU  - Wu, Hao
AU  - Zhu, Ziyu
AU  - Wang, Jiayi
AU  - Zheng, Nanning
AU  - Chen, Badong
TI  - An Encoding Framework With Brain Inner State for Natural Image Identification
PY  - 2021
JO  - IEEE Transactions on Cognitive and Developmental Systems
T2  - IEEE Transactions on Cognitive and Developmental Systems
VL  - 13
IS  - 3
SP  - 453-464
DO  - 10.1109/TCDS.2020.2987352
AB  - Neural encoding and decoding, which aim to characterize the relationship between stimuli and brain activities, have emerged as an important area in cognitive neuroscience. Traditional encoding models, which focus on feature extraction and mapping, consider the brain as an input-output mapper without inner states. In this article, inspired by the fact that the human brain acts like a state machine, we proposed a novel encoding framework that combines information from both the external world and the inner state to predict brain activity. The framework comprises two parts: 1) forward encoding model that deals with visual stimuli and 2) inner state model that captures influence from intrinsic connections in the brain. The forward model can be any traditional encoding model, making the framework flexible. The inner state model is a linear model to utilize information in the prediction residuals of the forward model. The proposed encoding framework achieved much better performance on natural image identification than forward-only models, with a maximum identification accuracy of 100%. The identification accuracy decreased slightly with the data set size increasing, but remained relatively stable with different identification methods. The results confirm that the new encoding framework is effective and robust when used for brain decoding.
KW  - Encoding
KW  - Brain modeling
KW  - Decoding
KW  - Predictive models
KW  - Visualization
KW  - Feature extraction
KW  - Connectivity
KW  - decoding
KW  - functional magnetic resonance imaging (fMRI)
KW  - perception
KW  - voxelwise encoding
SN  - 2379-8939
DA  - Sep.
ER  - 

TY  - JOUR
ID  - Yang2021Revealing
AU  - Yang, Qianli
AU  - Walker, Edgar Y.
AU  - Cotton, Ronald James
AU  - Tolias, Andreas Savas
AU  - Pitkow, Xaq
TI  - Revealing nonlinear neural decoding by analyzing choices
PY  - 2021
JO  - Nature Communications
T2  - Nature Communications
VL  - 12
IS  - 1
DO  - 10.1038/s41467-021-26793-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119093419&doi=10.1038%2Fs41467-021-26793-9&partnerID=40&md5=9977d514944407fd931324b10f53526a
AB  - Sensory data about most natural task-relevant variables are entangled with task-irrelevant nuisance variables. The neurons that encode these relevant signals typically constitute a nonlinear population code. Here we present a theoretical framework for quantifying how the brain uses or decodes its nonlinear information. Our theory obeys fundamental mathematical limitations on information content inherited from the sensory periphery, describing redundant codes when there are many more cortical neurons than primary sensory neurons. The theory predicts that if the brain uses its nonlinear population codes optimally, then more informative patterns should be more correlated with choices. More specifically, the theory predicts a simple, easily computed quantitative relationship between fluctuating neural activity and behavioral choices that reveals the decoding efficiency. This relationship holds for optimal feedforward networks of modest complexity, when experiments are performed under natural nuisance variation. We analyze recordings from primary visual cortex of monkeys discriminating the distribution from which oriented stimuli were drawn, and find these data are consistent with the hypothesis of near-optimal nonlinear decoding.
KW  - brain
KW  - data set
KW  - experimental study
KW  - sensory system
KW  - animal cell
KW  - animal experiment
KW  - article
KW  - brain cell
KW  - conceptual framework
KW  - Haplorhini
KW  - nonhuman
KW  - quantitative analysis
KW  - sensory nerve cell
KW  - striate cortex
KW  - algorithm
KW  - animal
KW  - biological model
KW  - metabolism
KW  - nerve cell
KW  - theoretical model
KW  - Algorithms
KW  - Animals
KW  - Brain
KW  - Models, Neurological
KW  - Models, Theoretical
KW  - Neurons
KW  - Primary Visual Cortex
ER  - 

TY  - CONF
ID  - Awangga2020Literature
AU  - Awangga, Rolly Maulana
AU  - Mengko, Tati Latifah E.R.
AU  - Utama, Nugraha Priya
TI  - A literature review of brain decoding research
PY  - 2020
JO  - IOP Conference Series: Materials Science and Engineering
T2  - IOP Conference Series: Materials Science and Engineering
VL  - 830
IS  - 3
DO  - 10.1088/1757-899X/830/3/032049
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086339669&doi=10.1088%2F1757-899X%2F830%2F3%2F032049&partnerID=40&md5=71d4124a3b70c284072b55b8ee16c6bc
AB  - Brain Decoding is a popular topic in neuroscience. The purpose is how to reconstruct an object that came from a sensory system using brain activity data. There is three brain area generally use in brain decoding research. The somatosensory area generally using mice and touch they whisker. Auditory area using different sound frequency as stimuli. The visual area using shape, random image, and video. Take one example in the visual cortex. Using the retinotopic mapping concept, the object possible to reconstruct using visual cortex activity recorded by fMRI. Retinotopic mapping focus is to relate fMRI records into visual objects seen by the subject. This brain possibilities of decoding research come to the next level when combining using deep learning. The image seen by the subject can be reconstructed by using visual cortex activity. Make reconstruction come faster and realistic to predict the stimuli. This opportunity is opening the era of the brain-computer interface. Combine a method to analyze brain functionality related to the human sensory. Bring hope and increased human quality of life. This paper reviews research in the field of brain encoding. Divide into three sections, the first section is brain decoding research in somatosensory. The second section is brain decoding in the auditory cortex. For the last section, explain visual cortex reconstruction. Every section includes equipment devices to record brain activity and the source of datasets and methods to get the brain activity data.
ER  - 

TY  - JOUR
ID  - Huang2020Long
AU  - Huang, Wei
AU  - Yan, Hongmei
AU  - Wang, Chong
AU  - Li, Jiyi
AU  - Yang, Xiaoqing
AU  - Li, Liang
AU  - Zuo, Zhentao
AU  - Zhang, Jiang
AU  - Chen, Huafu
TI  - Long short-term memory-based neural decoding of object categories evoked by natural images
PY  - 2020
JO  - Human Brain Mapping
T2  - Human Brain Mapping
VL  - 41
IS  - 15
SP  - 4442 - 4453
DO  - 10.1002/hbm.25136
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087712424&doi=10.1002%2Fhbm.25136&partnerID=40&md5=693d7f7134d0c12c6eccdaefa87b5dc0
AB  - Visual perceptual decoding is one of the important and challenging topics in cognitive neuroscience. Building a mapping model between visual response signals and visual contents is the key point of decoding. Most previous studies used peak response signals to decode object categories. However, brain activities measured by functional magnetic resonance imaging are a dynamic process with time dependence, so peak signals cannot fully represent the whole process, which may affect the performance of decoding. Here, we propose a decoding model based on long short-term memory (LSTM) network to decode five object categories from multitime response signals evoked by natural images. Experimental results show that the average decoding accuracy using the multitime (2–6 s) response signals is 0.540 from the five subjects, which is significantly higher than that using the peak ones (6 s; accuracy: 0.492; p '.05). In addition, from the perspective of different durations, methods and visual areas, the decoding performances of the five object categories are deeply and comprehensively explored. The analysis of different durations and decoding methods reveals that the LSTM-based decoding model with sequence simulation ability can fit the time dependence of the multitime visual response signals to achieve higher decoding performance. The comparative analysis of different visual areas demonstrates that the higher visual cortex (VC) contains more semantic category information needed for visual perceptual decoding than lower VC.
KW  - adult
KW  - article
KW  - controlled study
KW  - deep learning
KW  - female
KW  - functional magnetic resonance imaging
KW  - human
KW  - human experiment
KW  - long short term memory network
KW  - major clinical study
KW  - male
KW  - simulation
KW  - visual cortex
KW  - brain mapping
KW  - concept formation
KW  - diagnostic imaging
KW  - long term memory
KW  - nerve cell network
KW  - nuclear magnetic resonance imaging
KW  - pattern recognition
KW  - physiology
KW  - short term memory
KW  - theoretical model
KW  - young adult
KW  - Adult
KW  - Brain Mapping
KW  - Concept Formation
KW  - Female
KW  - Humans
KW  - Magnetic Resonance Imaging
KW  - Male
KW  - Memory, Long-Term
KW  - Memory, Short-Term
KW  - Models, Theoretical
KW  - Nerve Net
KW  - Pattern Recognition, Visual
KW  - Visual Cortex
KW  - Young Adult
ER  - 

TY  - JOUR
ID  - Huang2020Perceptiontoimage
AU  - Huang, Wei
AU  - Yan, Hongmei
AU  - Wang, Chong
AU  - Li, Jiyi
AU  - Zuo, Zhentao
AU  - Zhang, Jiang
AU  - Shen, Zhan
AU  - Chen, Huafu
TI  - Perception-to-Image: Reconstructing Natural Images from the Brain Activity of Visual Perception
PY  - 2020
JO  - Annals of Biomedical Engineering
T2  - Annals of Biomedical Engineering
VL  - 48
IS  - 9
SP  - 2323 - 2332
DO  - 10.1007/s10439-020-02502-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083795890&doi=10.1007%2Fs10439-020-02502-3&partnerID=40&md5=cbc780cb64570a975f6e4e551917b31c
AB  - The reappearance of human visual perception is a challenging topic in the field of brain decoding. Due to the complexity of visual stimuli and the constraints of fMRI data collection, the present decoding methods can only reconstruct the basic outline or provide similar figures/features of the perceived natural stimuli. To achieve a high-quality and high-resolution reconstruction of natural images from brain activity, this paper presents an end-to-end perception reconstruction model called the similarity-conditions generative adversarial network (SC-GAN), where visually perceptible images are reconstructed based on human visual cortex responses. The SC-GAN extracts the high-level semantic features of natural images and corresponding visual cortical responses and then introduces the semantic features as conditions of generative adversarial networks (GANs) to realize the perceptual reconstruction of visual images. The experimental results show that the semantic features extracted from SC-GAN play a key role in the reconstruction of natural images. The similarity between the presented and reconstructed images obtained by the SC-GAN is significantly higher than that obtained by a condition generative adversarial network (C-GAN). The model we proposed offers a potential perspective for decoding the brain activity of complex natural stimuli.
KW  - Brain
KW  - Complex networks
KW  - Decoding
KW  - Neurophysiology
KW  - Semantics
KW  - Vision
KW  - Adversarial networks
KW  - High-level semantic features
KW  - High-resolution reconstruction
KW  - Human visual cortex
KW  - Human visual perception
KW  - Reconstructed image
KW  - Semantic features
KW  - Visual perception
KW  - Image reconstruction
KW  - article
KW  - brain function
KW  - deep learning
KW  - human
KW  - human experiment
KW  - retina image
KW  - visual cortex
KW  - adult
KW  - clinical trial
KW  - diagnostic imaging
KW  - female
KW  - image processing
KW  - male
KW  - nuclear magnetic resonance imaging
KW  - physiology
KW  - vision
KW  - Adult
KW  - Female
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Magnetic Resonance Imaging
KW  - Male
KW  - Neural Networks, Computer
KW  - Visual Cortex
KW  - Visual Perception
ER  - 

TY  - JOUR
ID  - Kauvar2020Cortical
AU  - Kauvar, Isaac V.
AU  - MacHado, Timothy A.
AU  - Yuen, Elle
AU  - Kochalka, John
AU  - Choi, Minseung
AU  - Allen, William E.
AU  - Wetzstein, Gordon
AU  - Deisseroth, Karl
TI  - Cortical Observation by Synchronous Multifocal Optical Sampling Reveals Widespread Population Encoding of Actions
PY  - 2020
JO  - Neuron
T2  - Neuron
VL  - 107
IS  - 2
SP  - 351 - 367.e19
DO  - 10.1016/j.neuron.2020.04.023
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085762879&doi=10.1016%2Fj.neuron.2020.04.023&partnerID=40&md5=2f82c09bd7643398f91a254b0d47d8cf
AB  - Kauvar, Machado, et al. have developed a new method, COSMOS, to simultaneously record neural dynamics at ∼30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice. With COSMOS, they observe cortex-spanning population encoding of actions during a three-option lick-to-target task.; To advance the measurement of distributed neuronal population representations of targeted motor actions on single trials, we developed an optical method (COSMOS) for tracking neural activity in a largely uncharacterized spatiotemporal regime. COSMOS allowed simultaneous recording of neural dynamics at ∼30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice during a three-option lick-to-target task. We identified spatially distributed neuronal population representations spanning the dorsal cortex that precisely encoded ongoing motor actions on single trials. Neuronal correlations measured at video rate using unaveraged, whole-session data had localized spatial structure, whereas trial-averaged data exhibited widespread correlations. Separable modes of neural activity encoded history-guided motor plans, with similar population dynamics in individual areas throughout cortex. These initial experiments illustrate how COSMOS enables investigation of large-scale cortical dynamics and that information about motor actions is widely shared between areas, potentially underlying distributed computations.
KW  - animal experiment
KW  - animal tissue
KW  - Article
KW  - controlled study
KW  - cortical observation by synchronous multifocal optical sampling
KW  - female
KW  - Go No Go task
KW  - licking
KW  - male
KW  - motor control
KW  - motor cortex
KW  - motor performance
KW  - mouse
KW  - neocortex
KW  - nonhuman
KW  - optogenetics
KW  - parietal cortex
KW  - priority journal
KW  - retrosplenial cortex
KW  - signal noise ratio
KW  - somatosensory cortex
KW  - stimulus response
KW  - visual cortex
KW  - visual orientation
KW  - visual stimulation
KW  - algorithm
KW  - animal
KW  - animal behavior
KW  - brain cortex
KW  - brain mapping
KW  - craniotomy
KW  - cytology
KW  - devices
KW  - instrumental conditioning
KW  - nerve cell
KW  - neuroimaging
KW  - observation
KW  - physiology
KW  - procedures
KW  - psychomotor performance
KW  - robot assisted surgery
KW  - Algorithms
KW  - Animals
KW  - Behavior, Animal
KW  - Brain Mapping
KW  - Cerebral Cortex
KW  - Conditioning, Operant
KW  - Craniotomy
KW  - Mice
KW  - Neocortex
KW  - Neuroimaging
KW  - Neurons
KW  - Observation
KW  - Optogenetics
KW  - Psychomotor Performance
KW  - Robotic Surgical Procedures
KW  - Signal-To-Noise Ratio
ER  - 

TY  - JOUR
ID  - MaimonMor2020Artificial
AU  - Maimon-Mor, Roni O.
AU  - Makin, Tamar R.
TI  - Is an artificial limb embodied as a hand? Brain decoding in prosthetic limb users
PY  - 2020
JO  - PLOS Biology
T2  - PLOS Biology
VL  - 18
IS  - 6
DO  - 10.1371/journal.pbio.3000729
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086748893&doi=10.1371%2Fjournal.pbio.3000729&partnerID=40&md5=e6246dafd3dc190cd6fee7d0f02d8f67
AB  - The potential ability of the human brain to represent an artificial limb as a body part (embodiment) has been inspiring engineers, clinicians, and scientists as a means to optimise human-machine interfaces. Using functional MRI (fMRI), we studied whether neural embodiment actually occurs in prosthesis users' occipitotemporal cortex (OTC). Compared with controls, different prostheses types were visually represented more similarly to each other, relative to hands and tools, indicating the emergence of a dissociated prosthesis categorisation. Greater daily life prosthesis usage correlated positively with greater prosthesis categorisation. Moreover, when comparing prosthesis users' representation of their own prosthesis to controls' representation of a similar looking prosthesis, prosthesis users represented their own prosthesis more dissimilarly to hands, challenging current views of visual prosthesis embodiment. Our results reveal a use-dependent neural correlate for wearable technology adoption, demonstrating adaptive use-related plasticity within the OTC. Because these neural correlates were independent of the prostheses' appearance and control, our findings offer new opportunities for prosthesis design by lifting restrictions imposed by the embodiment theory for artificial limbs.
KW  - Article
KW  - body image
KW  - brain cortex
KW  - brain decoding
KW  - brain function
KW  - functional magnetic resonance imaging
KW  - human
KW  - mental representation
KW  - nerve cell plasticity
KW  - occipitotemporal cortex
KW  - perception
KW  - prosthesis design
KW  - adult
KW  - brain
KW  - cluster analysis
KW  - female
KW  - hand
KW  - limb prosthesis
KW  - male
KW  - middle aged
KW  - photostimulation
KW  - physiology
KW  - visual cortex
KW  - young adult
KW  - Adult
KW  - Artificial Limbs
KW  - Brain
KW  - Cluster Analysis
KW  - Female
KW  - Hand
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Photic Stimulation
KW  - Visual Cortex
KW  - Young Adult
ER  - 

TY  - CONF
ID  - Mali2020Data
AU  - Mali, Ankur
AU  - Ororbia, Alexander G.
AU  - Giles, C Lee
TI  - The Sibling Neural Estimator: Improving Iterative Image Decoding with Gradient Communication
PY  - 2020
BT  - 2020 Data Compression Conference (DCC)
T2  - 2020 Data Compression Conference (DCC)
SP  - 23-32
DO  - 10.1109/DCC47342.2020.00010
AB  - For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that "help" each other reconstruct the same target image patches using complementary portions of the spatial context, communicating with each other via gradient signals. This dual agent system builds upon prior work that proposed an iterative refinement algorithm for recurrent neural network (RNN) based decoding. Our approach works with any neural or non-neural encoder. Our system progressively reduces image patch reconstruction error over a fixed number of steps. Experiments with variations of RNN memory cells show that our system consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe gains of 1:64 decibel (dB) over JPEG, a 1:46 dB over JPEG2000, a 1:34 dB over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder.
KW  - Measurement
KW  - Visualization
KW  - Image coding
KW  - Recurrent neural networks
KW  - Nonlinear distortion
KW  - Transform coding
KW  - Rate-distortion
KW  - Decoding
KW  - Iterative decoding
KW  - Image reconstruction
KW  - Compression
KW  - decoder
KW  - RNN
KW  - Neural Networks
SN  - 2375-0359
DA  - March
ER  - 

TY  - CONF
ID  - Nategh2020Asilomar
AU  - Nategh, Neda
TI  - Decoding Neural Activity to Anticipate Eye Movements
PY  - 2020
BT  - 2020 54th Asilomar Conference on Signals, Systems, and Computers
T2  - 2020 54th Asilomar Conference on Signals, Systems, and Computers
SP  - 375-378
DO  - 10.1109/IEEECONF51394.2020.9443415
AB  - Neural interfaces for motor control read out motor planning activity in the brain and use it to control a computer or physical device in order to restore or replace motor functions of the brain. Applications involving the readout of intended eye movements however remain comparatively undeveloped. In this study, we aim to develop decoding algorithms that can predict eye movement signals from neural responses during the planning period, before the animal makes an eye movement. The single-trial local field potentials of neurons in Frontal Eye Field (FEF), a cortical area that contributes to the control of eye movements, as well as, in area V4 of nonhuman primates are used to train the eye movement decoders. The algorithms identified and optimized in this study can facilitate the development of brain machine interface systems for eye movements, with an ultimate goal of providing assistive technologies to correct for the impaired gaze control in patients with eye movement disorders.
KW  - Motor drives
KW  - Animals
KW  - Assistive technology
KW  - Neurons
KW  - Neural activity
KW  - Prediction algorithms
KW  - Brain-computer interfaces
KW  - Neural decoding
KW  - eye movement
KW  - microsaccade
KW  - local field potential
KW  - visual cortex
SN  - 2576-2303
DA  - Nov
ER  - 

TY  - JOUR
ID  - Nestor2020Face
AU  - Nestor, Adrian
AU  - Lee, Andy C.H.
AU  - Plaut, David C.
AU  - Behrmann, Marlene Behrmann
TI  - The Face of Image Reconstruction: Progress, Pitfalls, Prospects
PY  - 2020
JO  - Trends in Cognitive Sciences
T2  - Trends in Cognitive Sciences
VL  - 24
IS  - 9
SP  - 747 - 759
DO  - 10.1016/j.tics.2020.06.006
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087940586&doi=10.1016%2Fj.tics.2020.06.006&partnerID=40&md5=9fcf699627ba8a7ce99109d98f655ef6
AB  - Recent research has demonstrated that neural and behavioral data acquired in response to viewing face images can be used to reconstruct the images themselves. However, the theoretical implications, promises, and challenges of this direction of research remain unclear. We evaluate the potential of this research for elucidating the visual representations underlying face recognition. Specifically, we outline complementary and converging accounts of the visual content, the representational structure, and the neural dynamics of face processing. We illustrate how this research addresses fundamental questions in the study of normal and impaired face recognition, and how image reconstruction provides a powerful framework for uncovering face representations, for unifying multiple types of empirical data, and for facilitating both theoretical and methodological progress.
KW  - Face recognition
KW  - Behavioral data
KW  - Empirical data
KW  - Face processing
KW  - Face representations
KW  - Neural dynamics
KW  - Recent researches
KW  - Visual content
KW  - Visual representations
KW  - Image reconstruction
KW  - facial recognition
KW  - image reconstruction
KW  - review
KW  - theoretical study
KW  - brain mapping
KW  - human
KW  - image processing
KW  - nuclear magnetic resonance imaging
KW  - pattern recognition
KW  - Brain Mapping
KW  - Facial Recognition
KW  - Humans
KW  - Image Processing, Computer-Assisted
KW  - Magnetic Resonance Imaging
KW  - Pattern Recognition, Visual
ER  - 

TY  - JOUR
ID  - Sharon2020Neural
AU  - Sharon, Rini A.
AU  - Narayanan, Shrikanth S.
AU  - Sur, Mriganka
AU  - Murthy, A. Hema
TI  - Neural Speech Decoding During Audition, Imagination and Production
PY  - 2020
JO  - IEEE Access
T2  - IEEE Access
VL  - 8
SP  - 149714-149729
DO  - 10.1109/ACCESS.2020.3016756
AB  - Interpretation of neural signals to a form that is as intelligible as speech facilitates the development of communication mediums for the otherwise speech/motor-impaired individuals. Speech perception, production, and imagination often constitute phases of human communication. The primary goal of this article is to analyze the similarity between these three phases by studying electroencephalogram(EEG) patterns across these modalities, in order to establish their usefulness for brain computer interfaces. Neural decoding of speech using such non-invasive techniques necessitates the optimal choice of signal analysis and translation protocols. By employing selection-by-exclusion based temporal modeling algorithms, we discover fundamental syllable-like units that reveal similar set of signal signatures across all the three phases. Significantly higher than chance accuracies are recorded for single trial multi-unit EEG classification using machine learning approaches over three datasets across 30 subjects. Repeatability and subject independence tests performed at every step of the analysis further strengthens the findings and holds promise for translating brain signals to speech non-invasively.
KW  - Electroencephalography
KW  - Production
KW  - Protocols
KW  - Image segmentation
KW  - Brain modeling
KW  - Correlation
KW  - Image reconstruction
KW  - Assistive technology
KW  - brain computer interface
KW  - EEG
KW  - imagined speech
KW  - speech-EEG correlation
KW  - unit classification
SN  - 2169-3536
ER  - 

TY  - JOUR
ID  - Vetter2020Decoding
AU  - Vetter, Philipp
AU  - Bola, Łukasz
AU  - Reich, Lior
AU  - Bennett, Matthew A.
AU  - Muckli, Lars F.
AU  - Amedi, Amir
TI  - Decoding Natural Sounds in Early “Visual” Cortex of Congenitally Blind Individuals
PY  - 2020
JO  - Current Biology
T2  - Current Biology
VL  - 30
IS  - 15
SP  - 3039 - 3044.e2
DO  - 10.1016/j.cub.2020.05.071
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088361291&doi=10.1016%2Fj.cub.2020.05.071&partnerID=40&md5=46fcdc07af48bbc3e02ba1034be35fd7
AB  - Complex natural sounds, such as bird singing, people talking, or traffic noise, induce decodable fMRI activation patterns in early visual cortex of sighted blindfolded participants [1]. That is, early visual cortex receives non-visual and potentially predictive information from audition. However, it is unclear whether the transfer of auditory information to early visual areas is an epiphenomenon of visual imagery or, alternatively, whether it is driven by mechanisms independent from visual experience. Here, we show that we can decode natural sounds from activity patterns in early “visual” areas of congenitally blind individuals who lack visual imagery. Thus, visual imagery is not a prerequisite of auditory feedback to early visual cortex. Furthermore, the spatial pattern of sound decoding accuracy in early visual cortex was remarkably similar in blind and sighted individuals, with an increasing decoding accuracy gradient from foveal to peripheral regions. This suggests that the typical organization by eccentricity of early visual cortex develops for auditory feedback, even in the lifelong absence of vision. The same feedback to early visual cortex might support visual perception in the sighted [1] and drive the recruitment of this area for non-visual functions in blind individuals [2, 3].; Natural sounds can be distinguished based on early visual cortex activity in sighted people. Is this effect driven by visual imagery? Vetter et al. report successful sound decoding, increasing from fovea to periphery, in people blind from birth, proving that visual imagery is not necessary for sound representation in these early visual areas.
KW  - auditory stimulation
KW  - blindness
KW  - diagnostic imaging
KW  - human
KW  - nuclear magnetic resonance imaging
KW  - pathophysiology
KW  - physiology
KW  - sensory feedback
KW  - sound
KW  - visual cortex
KW  - Acoustic Stimulation
KW  - Blindness
KW  - Feedback, Sensory
KW  - Humans
KW  - Magnetic Resonance Imaging
KW  - Sound
KW  - Visual Cortex
ER  - 

TY  - JOUR
ID  - Zhang2020Reconstruction
AU  - Zhang, Yichen
AU  - Jia, Shanshan
AU  - Zheng, Yajing
AU  - Yu, Zhaofei
AU  - Tian, Yonghong
AU  - Ma, Siwei
AU  - Huang, Tie Jun
AU  - Liu, Jian K.
TI  - Reconstruction of natural visual scenes from neural spikes with deep neural networks
PY  - 2020
JO  - Neural Networks
T2  - Neural Networks
VL  - 125
SP  - 19 - 30
DO  - 10.1016/j.neunet.2020.01.033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079328270&doi=10.1016%2Fj.neunet.2020.01.033&partnerID=40&md5=f96edc7f6152742fb50abe7d06f9c3a0
AB  - Neural coding is one of the central questions in systems neuroscience for understanding how the brain processes stimulus from the environment, moreover, it is also a cornerstone for designing algorithms of brain–machine interface, where decoding incoming stimulus is highly demanded for better performance of physical devices. Traditionally researchers have focused on functional magnetic resonance imaging (fMRI) data as the neural signals of interest for decoding visual scenes. However, our visual perception operates in a fast time scale of millisecond in terms of an event termed neural spike. There are few studies of decoding by using spikes. Here we fulfill this aim by developing a novel decoding framework based on deep neural networks, named spike-image decoder (SID), for reconstructing natural visual scenes, including static images and dynamic videos, from experimentally recorded spikes of a population of retinal ganglion cells. The SID is an end-to-end decoder with one end as neural spikes and the other end as images, which can be trained directly such that visual scenes are reconstructed from spikes in a highly accurate fashion. Our SID also outperforms on the reconstruction of visual stimulus compared to existing fMRI decoding models. In addition, with the aid of a spike encoder, we show that SID can be generalized to arbitrary visual scenes by using the image datasets of MNIST, CIFAR10, and CIFAR100. Furthermore, with a pre-trained SID, one can decode any dynamic videos to achieve real-time encoding and decoding of visual scenes by spikes. Altogether, our results shed new light on neuromorphic computing for artificial visual systems, such as event-based visual cameras and visual neuroprostheses.
KW  - Aldehydes
KW  - Decoding
KW  - Deep learning
KW  - Deep neural networks
KW  - Image reconstruction
KW  - Magnetic resonance imaging
KW  - Neural networks
KW  - Signal encoding
KW  - Video signal processing
KW  - Vision
KW  - Artificial retinas
KW  - Functional magnetic resonance imaging
KW  - Natural scenes
KW  - Neural decoding
KW  - Neural spike
KW  - Neuromorphic computing
KW  - Real-time encoding
KW  - Retinal ganglion cells
KW  - Functional neuroimaging
KW  - Article
KW  - deep learning
KW  - deep neural network
KW  - functional magnetic resonance imaging
KW  - human
KW  - longitudinal study
KW  - mathematical computing
KW  - priority journal
KW  - retina
KW  - retina ganglion cell
KW  - spike image decoder
KW  - vision
KW  - visual system
KW  - animal
KW  - biological model
KW  - brain computer interface
KW  - diagnostic imaging
KW  - nuclear magnetic resonance imaging
KW  - physiology
KW  - visual cortex
KW  - visual evoked potential
KW  - Animals
KW  - Brain-Computer Interfaces
KW  - Evoked Potentials, Visual
KW  - Magnetic Resonance Imaging
KW  - Models, Neurological
KW  - Neural Networks, Computer
KW  - Retinal Ganglion Cells
KW  - Visual Cortex
KW  - Visual Perception
ER  - 

TY  - CONF
ID  - Zhou2020International
AU  - Zhou, Qiongyi
AU  - Du, Changde
AU  - Li, Dan
AU  - Wang, Haibao
AU  - Liu, Jian K.
AU  - He, Huiguang
TI  - Simultaneous Neural Spike Encoding and Decoding Based on Cross-modal Dual Deep Generative Model
PY  - 2020
BT  - 2020 International Joint Conference on Neural Networks (IJCNN)
T2  - 2020 International Joint Conference on Neural Networks (IJCNN)
SP  - 1-8
DO  - 10.1109/IJCNN48605.2020.9207466
AB  - Neural encoding and decoding of retinal ganglion cells (RGCs) have been attached great importance in the research work of brain-machine interfaces. Much effort has been invested to mimic RGC and get insight into RGC signals to reconstruct stimuli. However, there remain two challenges. On the one hand, complex nonlinear processes in retinal neural circuits hinder encoding models from enhancing their ability to fit the natural stimuli and modelling RGCs accurately. On the other hand, current research of the decoding process is separate from that of the encoding process, in which the liaison of mutual promotion between them is neglected. In order to alleviate the above problems, we propose a cross-modal dual deep generative model (CDDG) in this paper. CDDG treats the RGC spike signals and the stimuli as two modalities, which learns a shared latent representation for the concatenated modality and two modal-specific latent representations. Then, it imposes distribution consistency restriction on different latent space, cross-consistency and cycle-consistency constraints on the generated variables. Thus, our model ensures cross-modal generation from RGC spike signals to stimuli and vice versa. In our framework, the generation from stimuli to RGC spike signals is equivalent to neural encoding while the inverse process is equivalent to neural decoding. Hence, the proposed method integrates neural encoding and decoding and exploits the reciprocity between them. The experimental results demonstrate that our proposed method can achieve excellent encoding and decoding performance compared with the state-of-the-art methods on three salamander RGC spike datasets with natural stimuli.
KW  - Decoding
KW  - Encoding
KW  - Retina
KW  - Visualization
KW  - Brain modeling
KW  - Image reconstruction
KW  - Bidirectional control
KW  - dual learning
KW  - cross-modal generation
KW  - retinal ganglion cells
KW  - neural encoding
KW  - neural decoding
SN  - 2161-4407
DA  - July
ER  - 

TY  - CONF
ID  - Baluja2019Ieee
AU  - Baluja, Shumeet
AU  - Marwood, David
AU  - Johnston, Nick
AU  - Covell, Michele
TI  - Learning to Render Better Image Previews
PY  - 2019
BT  - 2019 IEEE International Conference on Image Processing (ICIP)
T2  - 2019 IEEE International Conference on Image Processing (ICIP)
SP  - 1700-1704
DO  - 10.1109/ICIP.2019.8803147
AB  - A rapidly increasing portion of Internet traffic is dominated by requests from mobile devices with limited and metered bandwidth constraints. To satisfy these requests, it has become standard practice for websites to transmit small and extremely compressed image previews as part of the initial page-load process. Recent work, based on an adaptive triangulation of the target image, has performed well at extreme compression rates: 200 bytes or less. Gains have been shown, in terms of PSNR and SSIM, over both JPEG and WebP standards. However, qualitative assessments and preservation of semantic content can be less favorable. We present a novel method to significantly improve the reconstruction quality of the original image that requires no changes to the encoded information. Our neural-based decoding triples the amount of semantic-level content preservation while also improving both SSIM and PSNR scores. In addition, by keeping the same encoding stream, our solution is completely inter-operable with the original, and remains suitable for small-device deployment.
KW  - Decoding
KW  - Image coding
KW  - Image color analysis
KW  - Transform coding
KW  - Standards
KW  - Image reconstruction
KW  - Image edge detection
KW  - Compression
KW  - Semantic Quality Measurement
KW  - Image Triangulation
KW  - Deep Neural Networks
SN  - 2381-8549
DA  - Sep.
ER  - 

TY  - JOUR
ID  - Du2019Reconstructing
AU  - Du, Changde
AU  - Du, Changying
AU  - Huang, Lijie
AU  - He, Huiguang
TI  - Reconstructing Perceived Images From Human Brain Activities With Bayesian Deep Multiview Learning
PY  - 2019
JO  - IEEE Transactions on Neural Networks and Learning Systems
T2  - IEEE Transactions on Neural Networks and Learning Systems
VL  - 30
IS  - 8
SP  - 2310-2323
DO  - 10.1109/TNNLS.2018.2882456
AB  - Neural decoding, which aims to predict external visual stimuli information from evoked brain activities, plays an important role in understanding human visual system. Many existing methods are based on linear models, and most of them only focus on either the brain activity pattern classification or visual stimuli identification. Accurate reconstruction of the perceived images from the measured human brain activities still remains challenging. In this paper, we propose a novel deep generative multiview model for the accurate visual image reconstruction from the human brain activities measured by functional magnetic resonance imaging (fMRI). Specifically, we model the statistical relationships between the two views (i.e., the visual stimuli and the evoked fMRI) by using two view-specific generators with a shared latent space. On the one hand, we adopt a deep neural network architecture for visual image generation, which mimics the stages of human visual processing. On the other hand, we design a sparse Bayesian linear model for fMRI activity generation, which can effectively capture voxel correlations, suppress data noise, and avoid overfitting. Furthermore, we devise an efficient mean-field variational inference method to train the proposed model. The proposed method can accurately reconstruct visual images via Bayesian inference. In particular, we exploit a posterior regularization technique in the Bayesian inference to regularize the model posterior. The quantitative and qualitative evaluations conducted on multiple fMRI data sets demonstrate the proposed method can reconstruct visual images more accurately than the state of the art.
KW  - Visualization
KW  - Brain modeling
KW  - Functional magnetic resonance imaging
KW  - Image reconstruction
KW  - Decoding
KW  - Bayes methods
KW  - Deep neural network (DNN)
KW  - image reconstruction
KW  - multiview learning
KW  - neural decoding
KW  - variational Bayesian inference
SN  - 2162-2388
DA  - Aug
ER  - 

TY  - CONF
ID  - Lee2019Ieee
AU  - Lee, Seo-Hyun
AU  - Lee, Minji
AU  - Jeong, Ji-Hoon
AU  - Lee, Seong-Whan
TI  - Towards an EEG-based Intuitive BCI Communication System Using Imagined Speech and Visual Imagery
PY  - 2019
BT  - 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)
T2  - 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)
SP  - 4409-4414
DO  - 10.1109/SMC.2019.8914645
AB  - Communication using brain-computer interface (BCI) has developed in attempts toward an intuitive system by decoding the imagined speech or visual imagery. However, discrimination between the two paradigms may be ambiguous because the user intention contains their original meaning. A clear distinction between the two paradigms may facilitate the active use of them leading to an intuitive BCI conversation system. In this study, we compared imagined speech and visual imagery in the perspective of its presence, spatial features, and classification performance based on electroencephalography. Seven subjects performed both imagined speech and visual imagery of twelve words/phrases. We showed the presence of the two paradigms, having distinct brain region from each other. The maximum thirteen-class classification accuracy including rest class was 34.2 % for imagined speech and 26.7 % for visual imagery. Therefore, we investigated the possibility of multiclass classification of more than ten classes in both paradigms, showing the potential of them to be used in the real world communication system. These findings could further be utilized in the intuitive communication for locked-in patients sending commands to the external world simply by thinking of `the very thing' that the user wants to deliver.
KW  - Visualization
KW  - Task analysis
KW  - Electroencephalography
KW  - Brain
KW  - Decoding
KW  - Radio frequency
SN  - 2577-1655
DA  - Oct
ER  - 

TY  - CONF
ID  - Ororbia2019Data
AU  - Ororbia, Alexander G.
AU  - Mali, Ankur
AU  - Wu, Jian
AU  - O'Connell, Scott
AU  - Dreese, William
AU  - Miller, David
AU  - Giles, C. Lee
TI  - Learned Neural Iterative Decoding for Lossy Image Compression Systems
PY  - 2019
BT  - 2019 Data Compression Conference (DCC)
T2  - 2019 Data Compression Conference (DCC)
SP  - 3-12
DO  - 10.1109/DCC.2019.00008
AB  - For lossy image compression systems, we develop an algorithm, iterative refinement, to improve the decoder's reconstruction compared to standard decoding techniques. Specifically, we propose a recurrent neural network approach for nonlinear, iterative decoding. Our decoder, which works with any encoder, employs self-connected memory units that make use of causal and non-causal spatial context information to progressively reduce reconstruction error over a fixed number of steps. We experiment with variants of our estimator and find that iterative refinement consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a 0.971 dB gain over a competitive neural model.
KW  - Decoding
KW  - Iterative decoding
KW  - Image reconstruction
KW  - Image coding
KW  - Transform coding
KW  - Computational modeling
KW  - Iterative algorithms
KW  - iterative refinement
KW  - compression
KW  - recurrent neural networks
SN  - 2375-0359
DA  - March
ER  - 

TY  - JOUR
ID  - Yu2019Brain
AU  - Yu, Siyu
AU  - Zheng, Nanning
AU  - Ma, Yongqiang
AU  - Wu, Hao
AU  - Chen, Badong
TI  - A Novel Brain Decoding Method: A Correlation Network Framework for Revealing Brain Connections
PY  - 2019
JO  - IEEE Transactions on Cognitive and Developmental Systems
T2  - IEEE Transactions on Cognitive and Developmental Systems
VL  - 11
IS  - 1
SP  - 95-106
DO  - 10.1109/TCDS.2018.2854274
AB  - Brain decoding is a hot spot in cognitive science, which focuses on reconstructing perceptual images from brain activities. Analyzing the correlations of collected data from human brain activities and representing activity patterns are two key problems in brain decoding based on functional magnetic resonance imaging signals. However, existing correlation analysis methods mainly focus on the strength information of voxel, which reveals functional connectivity in the cerebral cortex. They tend to neglect the structural information that implies the intracortical or intrinsic connections; that is, structural connectivity. Hence, the effective connectivity inferred by these methods is relatively unilateral. Therefore, we propose in this paper a correlation network (CorrNet) framework that could be flexibly combined with diverse pattern representation models. In the CorrNet framework, the topological correlation is introduced to reveal structural information. Rich correlations can be obtained, which contribute to specifying the underlying effective connectivity. We also combine the CorrNet framework with a linear support vector machine and a dynamic evolving spike neuron network for pattern representation separately, thus provide a novel method for decoding cognitive activity patterns. Experimental results verify the reliability and robustness of our CorrNet framework, and demonstrate that the new method can achieve significant improvement in brain decoding over comparable methods.
KW  - Correlation
KW  - Brain modeling
KW  - Functional magnetic resonance imaging
KW  - Decoding
KW  - Image reconstruction
KW  - Visualization
KW  - Brain decoding
KW  - connection
KW  - correlation network (CorrNet) framework
KW  - functional magnetic resonance imaging (fMRI)
KW  - pattern representation
SN  - 2379-8939
DA  - March
ER  - 

