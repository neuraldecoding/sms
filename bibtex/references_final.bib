% BibTeX References - Merged and Deduplicated
% Generated: 2025-10-10 22:23:16
% Tool: BibTeX Merger + Deduplicator v2.0
%
% Statistics:
%   Files processed: 2
%   Total imported: 140
%   Duplicates removed: 22
%   Final unique entries: 118
%
% Duplicate detection method:
%   Priority 1: DOI matching
%   Priority 2: Title + Year matching
%

@article{Battula2025Latentneuronet,
	author  = {Battula, Shreyas and Kirithivasan, Shyam Krishna and Soori, Aditi and Ramesh, Richa and Srinath, Ramamoorthy},
	title  = {LatentNeuroNet: A Text-Conditioned Stable Diffusion Framework for Reconstructing Visual Stimuli from fMRI},
	year  = {2025},
	journal  = {Communications in Computer and Information Science},
	volume  = {2194 CCIS},
	pages  = {225 - 235},
	doi  = {10.1007/978-3-031-70906-7_19},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208060558&doi=10.1007%2F978-3-031-70906-7_19&partnerID=40&md5=ea6463f1ed2c398fb520530f9bc270f5},
	abstract  = {The human brain, among the most complex and mysterious aspects of the body, harbours vast potential for extensive exploration. Unravelling these enigmas, especially within neural perception and cognition, delves into the realm of neural decoding. Harnessing advancements in generative AI, particularly in the Image Processing domain, seeks to elucidate how the brain comprehends visual stimuli perceived by humans. The paper endeavours to reconstruct human-perceived visual stimuli using Functional Magnetic Resonance Imaging (fMRI). This fMRI data is then processed through pre-trained deep-learning models to recreate the stimuli. Introducing a new architecture named LatentNeuroNet, the aim is to achieve the utmost semantic fidelity in stimuli reconstruction. The approach employs a Latent Diffusion Model (LDM), emphasizing semantic accuracy and generating superior-quality outputs. Text conditioning within the LDM’s denoising process is handled by extracting text from the brain’s ventral visual cortex region. This extracted text undergoes processing through a Bootstrapping Language-Image Pre-training (BLIP) encoder before it is injected into the denoising process. In conclusion, a successful architecture is developed that reconstructs the visual stimuli perceived and finally, this research provides us with enough evidence to identify the most substantial regions of the brain responsible for perception. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Deep learning; Functional neuroimaging; Image coding; Image denoising; Magnetic resonance imaging; Neurons; Semantic Segmentation; Visual languages; Bootstrapping language-image pre-training; De-noising; Diffusion model; Extensive explorations; Functional magnetic resonance imaging; Human brain; Latent diffusion model; Neural perception; Pre-training; Visual stimulus; Semantics},
	type  = {Conference paper}}





@article{Chen2025Mindgpt,
  author  = {Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang},
  journal  = {IEEE Transactions on Image Processing}, 
  title  = {MindGPT: Interpreting What You See With Non-Invasive Brain Recordings}, 
  year  = {2025},
  volume  = {34},
  number  = {},
  pages  = {3281--3293},
  abstract  = {Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed MindGPT, which interprets perceived visual stimuli into natural languages from functional Magnetic Resonance Imaging (fMRI) signals in an end-to-end manner. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism. By the collaborative use of data augmentation techniques, this architecture permits us to guide latent neural representations towards a desired language semantic direction in a self-supervised fashion. Through doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. The source code for the MindGPT model is publicly available at https://github.com/JxuanC/MindGPT.},
  keywords  = {Visualization;Semantics;Functional magnetic resonance imaging;Image reconstruction;Decoding;Brain modeling;Recording;Training;Data augmentation;Predictive models;Neural decoding;functional magnetic resonance imaging;text reconstruction;self-supervised learning;multimodal representation learning},
  doi  = {10.1109/TIP.2025.3572784},
  issn  = {1941-0042},
  month  = {}}



@inproceedings{Choi2025International,
  author  = {Choi, Yebin and Kim, Jun-Mo and Choi, WooHyeok and Ji, Chang-Hoon and Oh, Ji-Hye and Kam, Tae-Eui},
  booktitle  = {2025 13th International Conference on Brain-Computer Interface (BCI)}, 
  title  = {Visual Decoding Using a Learnable Wavelet-Based Spatial-Spectral-Temporal EEG Embedding}, 
  year  = {2025},
  volume  = {},
  number  = {},
  pages  = {1--5},
  abstract  = {Visual decoding seeks to identify or reconstruct visual stimuli perceived by individuals based on neural activity. Although functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have achieved remarkable success in visual decoding, their high costs, limited portability, and real-time processing challenges necessitate alternative approaches. Electroencephalography (EEG) provides a promising solution due to its cost-effectiveness, high temporal resolution, and suitability for real-time applications. However, conventional EEG-based encoders often rely on simplistic architectures, which limits their ability to fully capture the spatial, spectral, and temporal (SST) features of EEG signals, leading to suboptimal performance. In this study, we propose a novel brain decoding framework utilizing a state-of-the-art EEG encoder specifically designed to capture the SST characteristics of EEG signals. The proposed framework was evaluated on the THINGS-EEG dataset. It achieved a mean top-l accuracy of 19.7% and a top-5 accuracy of 50.7% in zero-shot retrieval tasks, outperforming conventional EEG encoders. These results demonstrate the potential of our method in advancing EEG-based visual decoding task.},
  keywords  = {Visualization;Accuracy;Functional magnetic resonance imaging;Feature extraction;Electroencephalography;Real-time systems;Decoding;Spatial resolution;Image reconstruction;Visual perception;Brain Decoding;Visual Decoding;Elec-troencephalogram;Learnable Wavelet Kernel;Spectral-spatial-temporal Representation},
  doi  = {10.1109/BCI65088.2025.10931751},
  issn  = {2572-7672},
  month  = {Feb}}



@article{Deng2025Image,
	author  = {Deng, Xin and Bao, Feiyang and Liu, Bin and Li, Yijia and Zhang, Lianhua},
	title  = {A Study on Image Reconstruction Based on Decoding fMRI Through Extracting Image Depth Features},
	year  = {2025},
	journal  = {Communications in Computer and Information Science},
	volume  = {2181 CCIS},
	pages  = {449 - 462},
	doi  = {10.1007/978-981-97-7001-4_32},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205387927&doi=10.1007%2F978-981-97-7001-4_32&partnerID=40&md5=adf7dbff53ef2b9f5d7e095c4252a434},
	abstract  = {Visualizing perceived content through functional magnetic resonance imaging (fMRI) analysis is a captivating research area in brain decoding. Previous studies have primarily focused on restoring either high-level semantic features or low-level semantic features from fMRI data, but rarely achieved effective restoration of both. This study proposes a novel approach for decoding the visual cortex activity measured by fMRI into the layered visual features that share the hierarchical information from the corresponding images. By iteratively optimizing the relationship between the layered visual features and the image’s depth features extracted by a visual transformer, the method in this research significantly improves the reconstruction of the image’s deep features. Furthermore, by incorporating the prior natural image information through a deep generator network, this work enhances the reconstruction process, resulting in richer semantic details. Experimental results verify the effectiveness of our methodology in restoring both high-level and low-level semantic features of the perceived images, ultimately enhancing the overall visual fidelity of the reconstructed image. Importantly, our model demonstrates successful generalization to reconstruct artificial shapes, indicating that the performance of our model is not simply achieved by relying on extensive sample datasets. These findings prove the efficacy of the method in effectively reconstructing the perceived content based on the hierarchical neural representations, providing a new method to study the brain’s underlying mechanisms. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain mapping; Deep neural networks; Image denoising; Image enhancement; Magnetic resonance imaging; Restoration; Depth features; Features extraction; Functional magnetic resonance imaging; Image depth; Images reconstruction; Imaging analysis; Neural decoding; Semantic features; Visual feature; Visual transformer; Image reconstruction},
	type  = {Conference paper}}





@article{Dong2025High,
	author  = {Dong, Zhen and Xiang, Yingjie and Wang, Songwei},
	title  = {High - quality decoding of RGB images from the neuronal signals of the pigeon optic tectum},
	year  = {2025},
	journal  = {Journal of Neuroscience Methods},
	volume  = {424},
	pages  = {},
	doi  = {10.1016/j.jneumeth.2025.110595},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017292462&doi=10.1016%2Fj.jneumeth.2025.110595&partnerID=40&md5=7a3f32ae1426799a38dde09a1d2d602d},
	abstract  = {Background: Decoding neural activity to reverse-engineer sensory inputs advances understanding of neural encoding and boosts brain-computer interface and visual prosthesis tech. A major challenge is high-quality RGB image reconstruction from natural scenes, which this study tackles using pigeon optic tectum neurons. New method: We built a neural response dataset via microelectrode arrays capturing tectal neurons' ON-OFF responses to RGB images. A modular decoding algorithm, integrating a convolutional encoding network, linear decoder, and image enhancement network, enabled inverse RGB image reconstruction from neural signals. Results: Experimental results confirmed high-quality RGB image reconstruction by the proposed algorithm. For all test set reconstructions, average metrics were: correlation coefficient (R) of 0.853, structural similarity index (SSIM) of 0.618, peak signal-to-noise ratio (PSNR) of 19.94 dB, and feature similarity index (FSIMc) of 0.801. These results confirm accurate recapitulation of both color and contour details of the original images. Comparison with existing methods: In terms of key quantitative metrics, the proposed algorithm achieves a significant improvement over traditional linear reconstruction methods, with the correlation coefficient (R) increased by 12.65 %, the structural similarity index (SSIM) increased by 38.92 %, the peak signal-to-noise ratio (PSNR) increased by 12.65 %, and the feature similarity index (FSIMc) increased by 9.28 %. Conclusions: This research provides a novel technical pathway for high-quality visual neural decoding, with robust experimental metrics validating its effectiveness. It also offers experimental evidence to support investigations into the information processing mechanisms of the avian visual pathway. © 2025 Elsevier B.V., All rights reserved.},	type  = {Article}}





@article{Gao2025Mindd,
  author  = {Gao, Jianxiong and Fu, Yanwei and Fu, Yuqian and Wang, Yun and Qian, Xuelin and Feng, Jianfeng},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title  = {MinD-3D++: Advancing fMRI-Based 3D Reconstruction With High-Quality Textured Mesh Generation and a Comprehensive Dataset}, 
  year  = {2025},
  volume  = {},
  number  = {},
  pages  = {1--15},
  abstract  = {Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4,768 3D objects. The dataset consists of two components: fMRI-Shape, previously introduced and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape , and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the core set in fMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Moreover, we propose MinD-3D++, a novel framework for decoding textured 3D visual information from fMRI signals. The framework evaluates the feasibility of not only reconstructing 3D objects from the human mind but also generating, for the first time, 3D textured meshes with detailed textures from fMRI data. We establish new benchmarks by designing metrics at the semantic, structural, and textured levels to evaluate model performance. Furthermore, we assess the model's effectiveness in out-of-distribution settings and analyze the attribution of the proposed 3D pari fMRI dataset in visual regions of interest (ROIs) in fMRI signals. Our experiments demonstrate that MinD-3D++ not only reconstructs 3D objects with high semantic and spatial accuracy but also provides deeper insights into how the human brain processes 3D visual information. Project page: https://jianxgao.github.io/MinD-3D.},
  keywords  = {Three-dimensional displays;Functional magnetic resonance imaging;Feature extraction;Visualization;Image reconstruction;Diffusion models;Decoding;Solid modeling;Semantics;Brain modeling;3D vision;dataset;diffusion model;FMRI decoding},
  doi  = {10.1109/TPAMI.2025.3599860},
  issn  = {1939-3539},
  month  = {}}



@article{Gao2025Reduced,
	author  = {Gao, Zhiyao and Duberg, Katherine and Warren, Stacie L. and Zheng, Li and Hinshaw, Stephen P. and Menon, Vinod and Cai, Weidong},
	title  = {Reduced temporal and spatial stability of neural activity patterns predict cognitive control deficits in children with ADHD},
	year  = {2025},
	journal  = {Nature Communications},
	volume  = {16},
	number  = {1},
	pages  = {},
	doi  = {10.1038/s41467-025-57685-x},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000716747&doi=10.1038%2Fs41467-025-57685-x&partnerID=40&md5=159b263002eee810dab32981eac96e11},
	abstract  = {This study investigates the neural underpinnings of cognitive control deficits in attention-deficit/hyperactivity disorder (ADHD), focusing on trial-level variability of neural coding. Using fMRI, we apply a computational approach to single-trial neural decoding on a cued stop-signal task, probing proactive and reactive control within the dual control model. Reactive control involves suppressing an automatic response when interference is detected, and proactive control involves implementing preparatory strategies based on prior information. In contrast to typically developing children (TD), children with ADHD show disrupted neural coding during both proactive and reactive control, characterized by increased temporal variability and diminished spatial stability in neural responses in salience and frontal-parietal network regions. This variability correlates with fluctuating task performance and ADHD symptoms. Additionally, children with ADHD exhibit more heterogeneous neural response patterns across individuals compared to TD children. Our findings underscore the significance of modeling trial-wise neural variability in understanding cognitive control deficits in ADHD. © 2025 Elsevier B.V., All rights reserved.},
	keywords  = {activity pattern; brain; child health; disease control; mental disorder; anterior insula; Article; attention deficit hyperactivity disorder; child; child development; cognitive defect; controlled study; diffusion kurtosis imaging; evoked response; executive function; female; frontoparietal network; functional magnetic resonance imaging; head movement; human; inferior frontal gyrus; inhibitory control; intelligence quotient; lateral prefrontal cortex; major clinical study; male; medial temporal lobe; middle frontal gyrus; motor cortex; nerve potential; neuroimaging; parietal cortex; phenotype; posterior cingulate; posterior parietal cortex; primary motor cortex; superior frontal gyrus; task performance; thalamus; visual cortex; adolescent; brain mapping; cognition; diagnostic imaging; nuclear magnetic resonance imaging; pathophysiology; physiology; procedures; reaction time; Adolescent; Attention Deficit Disorder with Hyperactivity; Brain; Brain Mapping; Child; Cognition; Female; Humans; Magnetic Resonance Imaging; Male; Reaction Time},
	type  = {Article}}





@article{Huo2025Neuropictor,
	author  = {Huo, Jingyang and Wang, Yikai and Wang, Yun and Qian, Xuelin and Li, Chong and Fu, Yanwei and Feng, Jianfeng},
	title  = {NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation},
	year  = {2025},
	journal  = {Lecture Notes in Computer Science},
	volume  = {15109 LNCS},
	pages  = {56 - 73},
	doi  = {10.1007/978-3-031-72983-6_4},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209888834&doi=10.1007%2F978-3-031-72983-6_4&partnerID=40&md5=ddd07c13b4cac354e218a7141045dfb4},
	abstract  = {Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent multi-subject training; ii) fMRI-to-image multi-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with about 67,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Our code and model are available at https://jingyanghuo.github.io/neuropictor/. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Benchmarking; Direct process refining; Image coding; Image compression; Image denoising; Neuroimaging; Stereo image processing; Complex information; Condition; Diffusion model; FMRI-to-image; High quality images; Image captures; Images reconstruction; Multilevels; Neural decoding; Pre-training; Image reconstruction},
	type  = {Conference paper}}





@article{Kamitani2025Visual,
	author  = {Kamitani, Yukiyasu and Tanaka, Misato and Shirakawa, Ken},
	title  = {Visual Image Reconstruction from Brain Activity via Latent Representation},
	year  = {2025},
	journal  = {Annual Review of Vision Science},
	volume  = {11},
	number  = {1},
	pages  = {611 - 634},
	doi  = {10.1146/annurev-vision-110423-023616},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016350828&doi=10.1146%2Fannurev-vision-110423-023616&partnerID=40&md5=6adf018e19b322acae793ccaa10af9ce},
	abstract  = {Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field’s evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain–machine interfaces. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {artificial neural network; brain; brain computer interface; human; image processing; physiology; procedures; vision; Brain; Brain-Computer Interfaces; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Visual Perception},
	type  = {Review}}





@article{Letafati2025Deep,
  author  = {Letafati, Mehdi and Amirhossein Ameli Kalkhoran, Seyyed and Erdemir, Ecenaz and Hossein Khalaj, Babak and Behroozi, Hamid and Gündüz, Deniz},
  journal  = {IEEE Transactions on Machine Learning in Communications and Networking}, 
  title  = {Deep Joint Source Channel Coding for Privacy-Aware End-to-End Image Transmission}, 
  year  = {2025},
  volume  = {3},
  number  = {},
  pages  = {568--584},
  abstract  = {Deep neural network (DNN)-based joint source and channel coding is proposed for privacy-aware end-to-end image transmission against multiple eavesdroppers. Both scenarios of colluding and non-colluding eavesdroppers are considered. Unlike prior works that assume perfectly known and independent identically distributed (i.i.d.) source and channel statistics, the proposed scheme operates under unknown and non-i.i.d. conditions, making it more applicable to real-world scenarios. The goal is to transmit images with minimum distortion, while simultaneously preventing eavesdroppers from inferring certain private attributes of images. Simultaneously generalizing the ideas of privacy funnel and wiretap coding, a multi-objective optimization framework is expressed that characterizes the trade-off between image reconstruction quality and information leakage to eavesdroppers, taking into account the structural similarity index (SSIM) for improving the perceptual quality of image reconstruction. Extensive experiments on the CIFAR-10 and CelebA, along with ablation studies, demonstrate significant performance improvements in terms of SSIM, adversarial accuracy, and the mutual information leakage compared to benchmarks. Experiments show that the proposed scheme restrains the adversarially-trained eavesdroppers from intercepting privatized data for both cases of eavesdropping a common secret, as well as the case in which eavesdroppers are interested in different secrets. Furthermore, useful insights on the privacy-utility trade-off are also provided.},
  keywords  = {Communication system security;Security;Image communication;Wireless sensor networks;Image reconstruction;Eavesdropping;Channel coding;Autoencoders;Wireless networks;Training;DeepJSCC;secure image transmission;end-to-end learning;privacy-utility trade-off;adversarial neural networks;deep learning},
  doi  = {10.1109/TMLCN.2025.3564907},
  issn  = {2831-316X},
  month  = {}}



@article{Li2025Deep,
  author  = {Li, Wei and Zhao, Penglu and Xu, Cheng and Hou, Yingting and Jiang, Wenhao and Song, Aiguo},
  journal  = {IEEE Transactions on Biomedical Engineering}, 
  title  = {Deep Learning for EEG-Based Visual Classification and Reconstruction: Panorama, Trends, Challenges and Opportunities}, 
  year  = {2025},
  volume  = {},
  number  = {},
  pages  = {1--17},
  abstract  = {Deep learning has significantly enhanced the research on the emerging issue of Electroencephalogram (EEG)-based visual classification and reconstruction, which has gained a growth of attention and concern recently. To promote the research progress, at this critical moment, a review work on the deep learning methodology for the issue becomes necessary and important. However, such a work seems absent in the literature. This paper provides the first review on EEG-based visual classification and reconstruction, whose contents can be categorized into the following four main parts: 1) comprehensively summarizing and systematically analyzing the representative deep learning methods from both feature encoding and decoding perspectives; 2) introducing the available benchmark datasets, describing the experimental paradigms, and displaying the method performances; 3) proposing the methodological essences and neuroscientific insights as well as the dynamic closed-loop interaction and promotion between them, which are potentially beneficial for technological innovations and academic progress; 4) discussing the potential challenges of current research and the prospective opportunities in future trends. We expect that this work can shed light on the technological directions and also enlighten the academic breakthroughs for the issue in the not-so-far future.},
  keywords  = {Visualization;Electroencephalography;Reviews;Image reconstruction;Deep learning;Decoding;Functional magnetic resonance imaging;Convolutional neural networks;Feature extraction;Transformers;Electroencephalogram;Deep Learning;Visual Classification;Visual Reconstruction},
  doi  = {10.1109/TBME.2025.3568282},
  issn  = {1558-2531},
  month  = {}}



@article{Lotey2025Eegbased,
	author  = {Lotey, Taveena and Verma, Aman and Roy, Partha Pratim},
	title  = {EEG-Based Mental Imagery Task Adaptation via Ensemble of Weight-Decomposed Low-Rank Adapters},
	year  = {2025},
	journal  = {Lecture Notes in Computer Science},
	volume  = {15311 LNCS},
	pages  = {309 - 324},
	doi  = {10.1007/978-3-031-78195-7_21},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211963733&doi=10.1007%2F978-3-031-78195-7_21&partnerID=40&md5=7bd691071a1a2856dea639b3b0043761},
	abstract  = {Electroencephalography (EEG) is widely researched for neural decoding in Brain Computer Interfaces (BCIs) as it is non-invasive, portable, and economical. However, EEG signals suffer from inter- and intra-subject variability, leading to poor performance. Recent technological advancements have led to deep learning (DL) models that have achieved high performance in various fields. However, such large models are compute- and resource-intensive and are a bottleneck for real-time neural decoding. Data distribution shift can be handled with the help of domain adaptation techniques of transfer learning (fine-tuning) and adversarial training that requires model parameter updates according to the target domain. One such recent technique is Parameter-efficient fine-tuning (PEFT), which requires only a small fraction of the total trainable parameters compared to fine-tuning the whole model. Therefore, we explored PEFT methods for adapting EEG-based mental imagery tasks. We considered two mental imagery tasks: speech imagery and motor imagery, as both of these tasks are instrumental in post-stroke neuro-rehabilitation. We proposed a novel ensemble of weight-decomposed low-rank adaptation methods, EDoRA, for parameter-efficient mental imagery task adaptation through EEG signal classification. The performance of the proposed PEFT method is validated on two publicly available datasets, one speech imagery, and the other motor imagery dataset. In extensive experiments and analysis, the proposed method has performed better than full fine-tune and state-of-the-art PEFT methods for mental imagery EEG classification. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Adversarial machine learning; Brain mapping; Deep learning; Image analysis; Image reconstruction; Learning to rank; Neurons; Transfer learning; Fine tuning; Fine-tuning methods; Imagery task; Low-rank adaptation.; Mental imagery; Neural decoding; Performance; Task adaptation; Brain computer interface},
	type  = {Conference paper}}





@article{Ma2025Brainclip,
  author  = {Ma, Yongqiang and Liu, Yulong and Chen, Liangjun and Zhu, Guibo and Chen, Badong and Zheng, Nanning},
  journal  = {IEEE Transactions on Medical Imaging}, 
  title  = {BrainCLIP: Brain Representation via CLIP for Generic Natural Visual Stimulus Decoding}, 
  year  = {2025},
  volume  = {},
  number  = {},
  pages  = {1--1},
  abstract  = {Functional Magnetic Resonance Imaging (fMRI) presents challenges due to limited paired samples and low signal-to-noise ratios, particularly in tasks involving reconstructing natural images or decoding their semantic content. To address these challenges, we introduce BrainCLIP, an innovative fMRI-based brain decoding model. BrainCLIP leverages Contrastive Language-Image Pre-training’s (CLIP) cross-modal generalization abilities to bridge brain activity, images, and text for the first time. Our experiments demonstrate CLIP’s effectiveness in diverse brain decoding tasks, including zero-shot visual category decoding, fMRI-image/text alignment, and fMRI-to-image generation. The core objective of BrainCLIP is to train a mapping network that translates fMRI patterns into a unified CLIP embedding space, achieved through visual and textual supervision integration. Our experiments highlight that this approach significantly enhances performance in tasks such as fMRI-text alignment and fMRI-based image generation. Notably, BrainCLIP surpasses BraVL, a recent multi-modal method, in zero-shot visual category decoding. Moreover, BrainCLIP demonstrates strong capability in reconstructing visual stimuli with high semantic fidelity, competing favorably with state-of-the-art methods in capturing high-level semantic features during fMRI-based natural image reconstruction.},
  keywords  = {Visualization;Decoding;Functional magnetic resonance imaging;Brain modeling;Semantics;Image reconstruction;Training;Brain;Feature extraction;Contrastive learning;Brain decoding;CLIP;Visual-Linguistic representation;Cross-modal},
  doi  = {10.1109/TMI.2025.3537287},
  issn  = {1558-254X},
  month  = {}}



@article{Mai2025Brainconditional,
  author  = {Mai, Weijian and Zhang, Jian and Fang, Pengfei and Zhang, Zhijun},
  journal  = {IEEE Transactions on Artificial Intelligence}, 
  title  = {Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy}, 
  year  = {2025},
  volume  = {6},
  number  = {5},
  pages  = {1080--1099},
  abstract  = {In the era of artificial intelligence generated content (AIGC), conditional multimodal synthesis technologies (e.g., text-to-image) are dynamically reshaping the natural content. Brain signals, serving as potential reflections of how the brain interprets external information, exhibit a distinctive one-to-many correspondence with various external modalities. This correspondence makes brain signals emerge as a promising guiding condition for multimodal synthesis (e.g., image, text, and audio), which is crucial for developing practical brain–computer interface systems and unraveling complex mechanisms underlying human perception. This survey comprehensively examines the emerging field of brain-conditional multimodal synthesis, termed AIGC-brain, to delineate the current landscape and future directions. To begin, related neuroimaging datasets and generative models are introduced as the foundation of AIGC-brain decoding and analysis. Next, we present a comprehensive taxonomy according to AIGC-brain methodologies, followed by task-specific representative work and implementation details to facilitate in-depth comparison and analysis. Quality assessments are then introduced for both qualitative and quantitative evaluation. Finally, this survey explores insights gained, outlining current challenges and prospects of AIGC-brain. As a pioneering survey, this article paves the way for future advances in AIGC-brain research.},
  keywords  = {Functional magnetic resonance imaging;Decoding;Surveys;Neuroimaging;Electroencephalography;Brain modeling;Artificial intelligence;Taxonomy;Music;Faces;Artificial intelligence generated content;brain–computer interface;brain decoding;multimodal synthesis},
  doi  = {10.1109/TAI.2024.3516698},
  issn  = {2691-4581},
  month  = {May}}



@article{McKilliam2025Aphantasia,
	author  = {McKilliam, Andy and Kirberg, Manuela},
	title  = {Aphantasia and the unconscious imagery hypothesis},
	year  = {2025},
	journal  = {Consciousness and Cognition},
	volume  = {135},
	pages  = {},
	doi  = {10.1016/j.concog.2025.103924},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014611288&doi=10.1016%2Fj.concog.2025.103924&partnerID=40&md5=62e678a14dbb01adcb81f69816e5736d},
	abstract  = {Until recently, mental imagery has largely been regarded as an exclusively conscious phenomenon. However, recent empirical results suggest that mental imagery can also occur unconsciously. People who report having no experiences of mental imagery often perform similar to controls on behavioural tasks thought to require imagery. A surprising number of them also display significant levels of imagery-based priming, and recent neural decoding studies have shown that imagery-related information is being processed in their visual cortex. However, investigating unconscious imagery empirically is not straightforward. One challenge is to establish that imagery is genuinely unconscious as opposed to merely going unreported due to response biases. Another is to clarify how imagistic and indirect perceptual processing needs to be to qualify as imagery. In this paper, we take a closer look at the evidence for unconscious imagery, argue that it is not as compelling as it initially appears, and outline a strategy for advancing research on this question. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {aphantasia; Article; guided imagery; human; imagery; mental disease; mental performance; mental task; unconscious (psychology); unconscious imagery; visual cortex},
	type  = {Article}}





@article{Nigam2025Eegdepth,
	author  = {Nigam, Jyoti and Prakash, Aditya and Uthamkumar, M. and Salgotra, Samvaidan and Bhavsar, Arnav V.},
	title  = {EEG-Depth: Learning Structural Information from Visual Brain Decoding via Depth Estimation},
	year  = {2025},
	journal  = {Lecture Notes in Computer Science},
	volume  = {15614 LNCS},
	pages  = {253 - 264},
	doi  = {10.1007/978-3-031-87657-8_18},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005571568&doi=10.1007%2F978-3-031-87657-8_18&partnerID=40&md5=3095c110122497f20029f3dd27e9e5e6},
	abstract  = {Electroencephalography (EEG) is a crucial tool for recording the brain’s electrical activity, providing insights into neural processes. However, extracting meaningful information of a complex visual stimulus (e.g. a scene image) from EEG data is challenging due to a large domain shift and the signal’s complexity. Recent research efforts focus on decoding visual information from EEG using advanced models and techniques. In our work, we address structure and locality estimation from EEG signals with a particular focus on depth perception, which is a fundamental aspect of the visual processing pathway and could serve as a key intermediate ground for visual decoding models. Thus, we focus on the task of reconstruction of depth maps from EEG data, corresponding to the images shown to subjects. Our work involves a contrastive learning framework in an attempt to align GNN based EEG embeddings to that of and depth map embeddings. We also perform experiments to draw some insights about the importance of EEG channels in such a EEG to Depth map reconstruction. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Brain decoding; Depth Estimation; Depthmap; Electrical activities; Electroencephalography-imagenet dataset; Embeddings; Images reconstruction; Neural process; Structural information; Visual brain decoding; Image analysis},
	type  = {Conference paper}}





@article{Olza2025Domain,
	author  = {Olza, Alexander and Soto, David and Santana, Roberto},
	title  = {Domain Adaptation-enhanced searchlight: enabling classification of brain states from visual perception to mental imagery},
	year  = {2025},
	journal  = {Brain Informatics},
	volume  = {12},
	number  = {1},
	pages  = {},
	doi  = {10.1186/s40708-025-00263-0},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009369560&doi=10.1186%2Fs40708-025-00263-0&partnerID=40&md5=50f9c4637a30abcde8bf4dd9eb4dab48},
	abstract  = {In cognitive neuroscience and brain-computer interface research, accurately predicting imagined stimuli is crucial. This study investigates the effectiveness of Domain Adaptation (DA) in enhancing imagery prediction using primarily visual data from fMRI scans of 18 subjects. Initially, we train a baseline model on visual stimuli to predict imagined stimuli, utilizing data from 14 brain regions. We then develop several models to improve imagery prediction, comparing different DA methods. Our results demonstrate that DA significantly enhances imagery prediction in binary classification on our dataset, as well as in multiclass classification on a publicly available dataset. We then conduct a DA-enhanced searchlight analysis, followed by permutation-based statistical tests to identify brain regions where imagery decoding is consistently above chance across subjects. Our DA-enhanced searchlight predicts imagery contents in a highly distributed set of brain regions, including the visual cortex and the frontoparietal cortex, thereby outperforming standard cross-domain classification methods. The complete code and data for this paper have been made openly available for the use of the scientific community. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Classification (of information); Forecasting; Interfaces (computer); Tunneling (excavation); Baseline models; Brain decoding; Brain regions; Brain state; Cognitive neurosciences; Domain adaptation; FMRI; Mental imagery; Visual data; Visual perception; Brain computer interface; adaptation; Article; binary classification; brain; brain region; cognitive neuroscience; deep learning; Domain Adaptation; electroencephalogram; frontoparietal cortex; functional magnetic resonance imaging; fusiform gyrus; hemodynamics; human; image segmentation; imagery; logistic regression analysis; machine learning; mental imagery; neuroimaging; support vector machine; vision; visual cortex},
	type  = {Article}}





@article{Peng2025Decoding,
	author  = {Peng, Jing and Jia, Shanshan and Zhang, Jiyuan and Wang, Yongxing and Yu, Zhaofei and Liu, Jian K.},
	title  = {Decoding natural visual scenes via learnable representations of neural spiking sequences},
	year  = {2025},
	journal  = {Neural Networks},
	volume  = {192},
	pages  = {},
	doi  = {10.1016/j.neunet.2025.107863},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011203775&doi=10.1016%2Fj.neunet.2025.107863&partnerID=40&md5=c4f0bf1845f8cf856f76834d9b94c665},
	abstract  = {Visual input underpins cognitive function by providing the brain with essential environmental information. Neural decoding of visual scenes seeks to reconstruct pixel-level images from neural activity, a vital capability for vision restoration via brain-computer interfaces. However, extracting visual content from time-resolved spiking activity remains a significant challenge. Here, we introduce the Wavelet-Informed Spike Augmentation (WISA) model, which applies multilevel wavelet transforms to spike trains to learn compact representations that can be directly fed into deep reconstruction networks. When tested on recorded retinal spike data responding to natural video stimuli, WISA substantially improves reconstruction accuracy, especially in recovering fine-grained details. These results emphasize the value of temporal spike patterns for high-fidelity visual decoding and demonstrate WISA as a promising model for visual decoding. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Brain computer interface; Decoding; Deep neural networks; Image reconstruction; Neurons; Wavelet transforms; Cognitive functions; Deep learning; Environmental information; Neural decoding; Neural spike; Neural-networks; Pixel level; Video; Visual scene; Wavelet; Vision; Article; cognition; decoding; deep learning; nerve cell; nerve cell network; retina; spike; videorecording; wavelet transform},
	type  = {Article}}





@inproceedings{PossoMurillo2025Ieee,
  author  = {Posso-Murillo, Santiago and Sanchez-Giraldo, Luis G. and Bae, Jihye},
  booktitle  = {2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title  = {Semantic Reconstruction from Fnirs Using Recurrent Neural Networks}, 
  year  = {2025},
  volume  = {},
  number  = {},
  pages  = {1--5},
  abstract  = {Semantic reconstruction of language aims to decode the meaning of words or sentences from neural activity. Previous studies have demonstrated that functional near-infrared spectroscopy (fNIRS) contains information suitable for language decoding. However, most of the existing work on fNIRS-based neural decoding relies on traditional machine learning algorithms such as linear models and support vector machines, and it has been limited to classification of limited set of words. To address these shortcomings, we examine 4 recurrent neural networks (RNNs) that learn features to decode semantic representations from fNIRS: the Elman recurrent neural network (ERNN), long short-term memory (LSTM), and bidirectional version of them (BiERNN and BiLSTM). Using a publicly available fNIRS dataset, we performed within-category, between-category, and leave-two-out tests. The decoding performance was measured by computing the matching score, a pairwise metric that assesses the model's ability to distinguish between two concepts. The results show that ERNN and BiLSTM models consistently outperform linear decoder models. Specifically, ERNN shows better performance for 4 out of 7 subjects in the between-category test, and BiLSTM performs better for 6 out of 7 subjects in the within-category test and 4 out of 7 subjects in the leave-two-out test. Notably, in between-category experiment, the BiLSTM scored 61 % matching score for subject 3, representing a 9% improvement, and ERNN achieved an 80% matching score for subject 2, marking a significant 33% improvement. These promising results encourage the use of advanced machine learning models for semantic reconstruction from fNIRS. Code is available at https://github.com/sposso/Semantic-Reconstruction-using-fNIRS-signal.},
  keywords  = {Support vector machines;Neuroimaging;Machine learning algorithms;Computational modeling;Semantics;Neural activity;Bidirectional long short term memory;Decoding;Functional near-infrared spectroscopy;Image reconstruction;fNIRS;RNN;Semantic Reconstruction},
  doi  = {10.1109/ISBI60581.2025.10981123},
  issn  = {1945-8452},
  month  = {April}}



@article{Qu2025Uncovering,
	author  = {Qu, Youzhi and Xia, Junfeng and Jian, Xinyao and Li, Wendu and Peng, Kaining and Liang, Zhichao and Wu, Haiyan and Liu, Quanying},
	title  = {Uncovering Cognitive Taskonomy Through Transfer Learning in Masked Autoencoder-Based fMRI Reconstruction},
	year  = {2025},
	journal  = {Communications in Computer and Information Science},
	volume  = {2438 CCIS},
	pages  = {35 - 50},
	doi  = {10.1007/978-981-96-4001-0_3},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003635208&doi=10.1007%2F978-981-96-4001-0_3&partnerID=40&md5=0edd25d5ce8f0957c6d5ce215c2efd54},
	abstract  = {Data reconstruction is a widely used pre-training task to learn the generalized features for many downstream tasks. Although reconstruction tasks have been applied to neural signal completion and denoising, neural signal reconstruction is less studied. Here, we employ the masked autoencoder (MAE) model to reconstruct functional magnetic resonance imaging (fMRI) data, and utilize a transfer learning framework to obtain the cognitive taskonomy, a matrix to quantify the similarity between cognitive tasks. Our experimental results demonstrate that the MAE model effectively captures the temporal dynamics patterns and interactions within the brain regions, enabling robust cross-subject fMRI signal reconstruction. The cognitive taskonomy derived from the transfer learning framework reveals the relationships among cognitive tasks, highlighting subtask correlations within motor tasks and similarities between emotion, social, and gambling tasks. Our study suggests that the fMRI reconstruction with MAE model can uncover the latent representation and the obtained taskonomy offers guidance for selecting source tasks in neural decoding tasks for improving the decoding performance on target tasks. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Amplitude modulation; Frequency modulation; Image coding; Image compression; Image denoising; Image segmentation; Light modulation; Signal modulation; Transfer learning; Auto encoders; Cognitive task; FMRI reconstruction; Functional magnetic resonance imaging; Imaging reconstruction; Masked autoencoder; Neural signals; Signals reconstruction; Taskonomy; Image reconstruction},
	type  = {Conference paper}}





@article{RodrguezDeliz2025Neural,
	author  = {Rodríguez Deliz, Charlie L. and Lee, Gerick M. and Bushnell, Brittany N. and Majaj, Najib J. and Anthony Movshon, J. and Kiorpes, Lynne},
	title  = {Neural Sensitivity to Radial Frequency Patterns in the Visual Cortex of Developing Macaques},
	year  = {2025},
	journal  = {Journal of Neuroscience},
	volume  = {45},
	number  = {34},
	pages  = {},
	doi  = {10.1523/JNEUROSCI.0179-25.2025},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013674585&doi=10.1523%2FJNEUROSCI.0179-25.2025&partnerID=40&md5=604be766464fbcf8ac8be2a46d136012},
	abstract  = {Visual resolution, contrast sensitivity, and form perception improve gradually with age. In nonhuman primates, the sensitivity and resolution of cells in the retina, lateral geniculate nucleus, and primary visual cortex (V1) also improve, but not enough to account for the perceptual changes. So, what aspects of visual system development limit visual sensitivity in infants? Improvements in behavioral sensitivity might arise from maturation of regions downstream of V1 such as V2, V4, and IT, which are thought to support increasingly complex perceptual abilities. We recorded the responses of populations of neurons in areas V1, V2, V4, and PIT to radial frequency patterns—a type of global form stimulus. Subjects were three young monkeys (two female, one male) between the ages of 19 and 54 weeks and a single adult animal (male). We found that neurons and neural populations in V4 reliably encoded global form in radial frequency stimuli at the earliest ages we studied, while V1 neurons do not. V2 and PIT populations also showed some degree of selectivity for these patterns at early ages, especially at higher radial frequency values. We did not find significant, systematic changes in neural decoding performance that could account for the improvement in behavioral performance over the same age range in an overlapping group of animals (Rodriguez Deliz et al., 2024). Finally, consistent with our prior behavioral results, neural populations in V4 show highest sensitivity for the higher radial frequency values, which contain the highest concentration of curvature and orientation cues. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {adult; animal experiment; article; contrast sensitivity; controlled study; female; infant; lateral geniculate body; Macaca; male; nerve cell; nonhuman; pattern recognition; striate cortex; vision; visual acuity; visual cortex; visual system; animal; growth, development and aging; photostimulation; physiology; procedures; rhesus monkey; Animals; Female; Macaca mulatta; Male; Neurons; Photic Stimulation; Visual Cortex},
	type  = {Article}}





@article{Shakeripour2025Object,
	author  = {Shakeripour, Alireza and Bahmani, Zahra and Aghaomidi, Poorya and Seyed-Allaei, Shima},
	title  = {A Novel Object Categorization Decoder from fMRI Signals Using Deep Neural Networks},
	year  = {2025},
	journal  = {Frontiers in Biomedical Technologies},
	volume  = {12},
	number  = {3},
	pages  = {617 - 626},
	doi  = {10.18502/fbt.v12i3.19186},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014260606&doi=10.18502%2Ffbt.v12i3.19186&partnerID=40&md5=bb502e5f650887dc34e61e5d3eff281b},
	abstract  = {Purpose: Understanding neural mechanisms is critical for discerning the nature of brain disorders and enhancing treatment methodologies. Functional Magnetic Resonance Imaging (fMRI) plays a vital role in gaining this knowledge by recording various brain regions. In this study, our primary aim was to categorize visual objects based on fMRI data during a natural scene viewing task. We intend to elucidate the challenges and limitations of previous models in order to produce a generalizable model across different subjects using advanced deep-learning methods. Materials and Methods: We’ve designed a new deep-learning model based on transformers for processing fMRI data. The model includes two blocks, the first block receives fMRI data as input and transforms the input data to a set of features called fMRI space. Simultaneously a visual space is extracted from visual images using a pre-trained inceptionv3 network. The model tries to construct the fMRI space similar to the extracted visual space. The other block is a Fully Connected (FC) network for object recognition based on fMRI space. Using transformer capabilities and an overlapping method, the proposed architecture accounts for structural changes across different voxel sizes of the subjects’ brains. Results: A unique model was trained for all subjects with different brain sizes. The results demonstrated that the proposed network achieves an impressive similarity correlation between visual space and fMRI space around 0.86 for train and 0.86 for test dataset. Furthermore, the classification accuracy was about 70.3%. These outcomes underscored the effectiveness of our fMRI transformer network in extracting features from fMRI data. Conclusion: The results indicated the potential of our model for decoding images from the brain activities of new subjects. This unveils a novel direction in image reconstruction from neural activities, an area that has remained relatively uncharted due to its inherent intricacies. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Article; brain disease; brain region; brain size; convolutional neural network; deep learning; deep neural network; electric potential; electroencephalogram; functional magnetic resonance imaging; human; human experiment; normal human; retina image; visual field},
	type  = {Article}}





@article{Sharon2025Harnessing,
  author  = {Sharon, Rini and Sur, Mriganka and Murthy, Hema},
  journal  = {IEEE Open Journal of Signal Processing}, 
  title  = {Harnessing the Multi-Phasal Nature of Speech-EEG for Enhancing Imagined Speech Recognition}, 
  year  = {2025},
  volume  = {6},
  number  = {},
  pages  = {78--88},
  abstract  = {Analyzing speech-electroencephalogram (EEG) is pivotal for developing non-invasive and naturalistic brain-computer interfaces. Recognizing that the nature of human communication involves multiple phases like audition, imagination, articulation, and production, this study uncovers the shared cognitive imprints that represent speech cognition across these phases. Regression analysis, using correlation metrics reveal pronounced inter-phasal congruence. This insight promotes a shift from single-phase-centric recognition models to harnessing integrated phase data, thereby enhancing recognition of cognitive speech. Having established the presence of inter-phase associations, a common representation learning feature extractor is introduced, adept at capturing the correlations and replicability across phases. The features so extracted are observed to provide superior discrimination of cognitive speech units. Notably, the proposed approach proves resilient even in the absence of comprehensive multi-phasal data. Through thorough control checks and illustrative topographical visualizations, our observations are substantiated. The findings indicate that the proposed multi-phase approach significantly enhances EEG-based speech recognition, achieving an accuracy gain of 18.2% for 25 cognitive units in continuous speech EEG over models reliant solely on single-phase data.},
  keywords  = {Correlation;Electroencephalography;Speech recognition;Feature extraction;Accuracy;Image reconstruction;Training;Speech enhancement;Production;Loss measurement;Audition;articulation;BCI;imagination;regression;speech-EEG correlation;neural decoding of speech},
  doi  = {10.1109/OJSP.2025.3528368},
  issn  = {2644-1322},
  month  = {}}



@article{Shirakawa2025Spurious,
	author  = {Shirakawa, Ken and Nagano, Yoshihiro and Tanaka, Misato and Aoki, Shuntaro C. and Muraki, Yusuke and Majima, Kei and Kamitani, Yukiyasu},
	title  = {Spurious reconstruction from brain activity},
	year  = {2025},
	journal  = {Neural Networks},
	volume  = {190},
	pages  = {},
	doi  = {10.1016/j.neunet.2025.107515},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007598357&doi=10.1016%2Fj.neunet.2025.107515&partnerID=40&md5=cd83086d39c85fb2eb7f840c8f0e20c3},
	abstract  = {Advances in brain decoding, particularly in visual image reconstruction, have sparked discussions about the societal implications and ethical considerations of neurotechnology. As reconstruction methods aim to recover visual experiences from brain activity and achieve prediction beyond training samples (zero-shot prediction), it is crucial to assess their capabilities and limitations to inform public expectations and regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (Natural Scenes Dataset, NSD) and text-to-image diffusion models, reveals critical limitations in their generalizability, demonstrated by poor reconstructions on a different dataset. UMAP visualization of the text features from NSD images shows limited diversity with overlapping semantic and visual clusters between training and test sets. We identify that clustered training samples can lead to “output dimension collapse,” restricting predictable output feature dimensions. While diverse training data improves generalization over the entire feature space without requiring exponential scaling, text features alone prove insufficient for mapping to the visual space. Our findings suggest that the apparent realism in current text-guided reconstructions stems from a combination of classification into trained categories and inauthentic image generation (hallucination) through diffusion models, rather than genuine visual reconstruction. We argue that careful selection of datasets and target features, coupled with rigorous evaluation methods, is essential for achieving authentic visual image reconstruction. These insights underscore the importance of grounding interdisciplinary discussions in a thorough understanding of the technology's current capabilities and limitations to ensure responsible development. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Photointerpretation; Brain activity; Brain decoding; Images reconstruction; Natural scenes; Naturalistic approach; Neuroai; Reconstruction method; Training sample; Visual image; Visual image reconstruction; Large datasets; Article; artificial intelligence; brain decoding; electroencephalogram; human; image reconstruction; naturalistic approach; visual field; visual image reconstruction; brain; diagnostic imaging; image processing; physiology; procedures; Brain; Humans; Image Processing, Computer-Assisted},
	type  = {Article}}





@article{Shoura2025Revealing,
	author  = {Shoura, Moaz and Liang, Yong Z. and Sama, Marco Agazio and De, Arijit and Nestor, Adrian},
	title  = {Revealing the neural representations underlying other-race face perception},
	year  = {2025},
	journal  = {Frontiers in Human Neuroscience},
	volume  = {19},
	pages  = {},
	doi  = {10.3389/fnhum.2025.1543840},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000475153&doi=10.3389%2Ffnhum.2025.1543840&partnerID=40&md5=e05220bd193c9a276a5c1500455b3498},
	abstract  = {The other-race effect (ORE) refers to poorer recognition for faces of other races than one’s own. This study investigates the neural and representational basis of ORE in East Asian and White participants using behavioral measures, neural decoding, and image reconstruction based on electroencephalography (EEG) data. Our investigation identifies a reliable neural counterpart of ORE, with reduced decoding accuracy for other-race faces, and it relates this result to higher density of other-race face representations in face space. Then, we characterize the temporal dynamics and the prominence of ORE for individual variability at the neural level. Importantly, we use a data-driven image reconstruction approach to reveal visual biases underlying other-race face perception, including a tendency to perceive other-race faces as more typical, younger, and more expressive. These findings provide neural evidence for a classical account of ORE invoking face space compression for other-race faces. Further, they indicate that ORE involves not only reduced identity information but also broader, systematic distortions in visual representation with considerable cognitive and social implications. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {adult; article; Caucasian; cognition; compression; controlled study; decoding; diagnosis; East Asian; electroencephalogram; electroencephalography; facial recognition; female; human; human experiment; image reconstruction; male; normal human; race},
	type  = {Article}}





@conference{Tian2025Brainguard,
	author  = {Tian, Zhibo and Quan, Ruijie and Ma, Fan and Zhan, Kun and Yang, Yi},
	title  = {BRAINGUARD: Privacy-Preserving Multisubject Image Reconstructions from Brain Activities},
	year  = {2025},
	journal  = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume  = {39},
	number  = {13},
	pages  = {14414 - 14422},
	doi  = {10.1609/aaai.v39i13.33579},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003904728&doi=10.1609%2Faaai.v39i13.33579&partnerID=40&md5=522cc0d761f22ab451ea7379508297cf},
	abstract  = {Reconstructing perceived images from human brain activity forms a crucial link between human and machine learning through Brain-Computer Interfaces. Early methods primarily focused on training separate models for each individual to account for individual variability in brain activity, overlooking valuable cross-subject commonalities. Recent advancements have explored multisubject methods, but these approaches face significant challenges, particularly in data privacy and effectively managing individual variability. To overcome these challenges, we introduce BRAINGUARD, a privacy-preserving collaborative training framework designed to enhance image reconstruction from multisubject fMRI data while safeguarding individual privacy. BRAINGUARD employs a collaborative global-local architecture where individual models are trained on each subject’s local data and operate in conjunction with a shared global model that captures and leverages cross-subject patterns. This architecture eliminates the need to aggregate fMRI data across subjects, thereby ensuring privacy preservation. To tackle the complexity of fMRI data, BRAINGUARD integrates a hybrid synchronization strategy, enabling individual models to dynamically incorporate parameters from the global model. By establishing a secure and collaborative training environment, BRAINGUARD not only protects sensitive brain data but also improves the image reconstructions accuracy. Extensive experiments demonstrate that BRAINGUARD sets a new benchmark in both high-level and low-level metrics, advancing the state-of-the-art in brain decoding through its innovative design. © 2025 Elsevier B.V., All rights reserved.},
	keywords  = {Privacy by design; Brain activity; Collaborative training; fMRI data; Global models; Human brain; Human learning; Images reconstruction; Individual modeling; Individual variability; Privacy preserving; Active learning},
	type  = {Conference paper}}





@article{Veronese2025Optimized,
	author  = {Veronese, Lorenzo and Moglia, Andrea and Pecco, Nicolò and Anthony Della Rosa, Pasquale and Scifo, Paola and Mainardi, Luca Tommaso and Cerveri, Pietro},
	title  = {Optimized AI-based neural decoding from BOLD fMRI signal for analyzing visual and semantic ROIs in the human visual system},
	year  = {2025},
	journal  = {Journal of Neural Engineering},
	volume  = {22},
	number  = {4},
	pages  = {},
	doi  = {10.1088/1741-2552/adfbc2},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014157960&doi=10.1088%2F1741-2552%2Fadfbc2&partnerID=40&md5=a49fd5b7d4d17a7c654076edcea2475b},
	abstract  = {Objective. AI-based neural decoding reconstructs visual perception by leveraging generative models to map brain activity measured through functional magnetic resonance imaging (fMRI) into the observed visual stimulus. Approach. Traditionally, ridge linear models transform fMRI into a latent space, which is then decoded using variational autoencoders (VAE) or LDMs. Owing to the complexity and noisiness of fMRI data, newer approaches split the reconstruction into two sequential stages, the first one providing a rough visual approximation using a VAE, the second one incorporating semantic information through the adoption of LDM guided by contrastive language-image pre-training (CLIP) embeddings. This work addressed some key scientific and technical gaps of the two-stage neural decoding by: (1) implementing a gated recurrent unit-based architecture to establish a non-linear mapping between the fMRI signal and the VAE latent space, (2) optimizing the dimensionality of the VAE latent space, (3) systematically evaluating the contribution of the first reconstruction stage, and (4) analyzing the impact of different brain regions of interest (ROIs) on reconstruction quality. Main results. Experiments on the NSD, containing 73 000 unique natural images, along with fMRI of eight subjects, demonstrated that the proposed architecture maintained competitive performance while reducing the complexity of its first stage by 85%. The sensitivity analysis showcased that the first reconstruction stage is essential for preserving high structural similarity in the final reconstructions. Restricting analysis to semantic ROIs, while excluding early visual areas, diminished visual coherence, preserving semantics though. The inter-subject repeatability across ROIs was about 92% and 98% for visual and sematic metrics, respectively. Significance. This study represents a key step toward optimized neural decoding architectures leveraging non-linear models for stimulus prediction. Sensitivity analysis highlighted the interplay between the two reconstruction stages, while ROI-based analysis provided strong evidence that the two-stage AI model reflects the brain’s hierarchical processing of visual information. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Brain mapping; Contrastive Learning; Decoding; Image enhancement; Image reconstruction; Latent semantic analysis; Neurons; Semantics; Vision; Auto encoders; Functional magnetic resonance imaging; Functional magnetic resonance imaging imagery; Generative artificial intelligence; Human Visual System; Neural decoding; Region-of-interest; Regions of interest; Sensitivity analyzes; Visual stimuls; Sensitivity analysis; Visual languages; adult; Article; artificial intelligence; BOLD signal; brain blood flow; brain region; deep neural network; electroencephalogram; functional magnetic resonance imaging; Gaussian noise; human; human experiment; image reconstruction; mean absolute error; mean squared error; normal human; ridge regression; semantics; statistical model; visual information; visual stimulation; visual system; brain; brain mapping; diagnostic imaging; female; male; nuclear magnetic resonance imaging; photostimulation; physiology; procedures; vision; visual cortex; young adult; Adult; Artificial Intelligence; Brain Mapping; Female; Humans; Magnetic Resonance Imaging; Male; Photic Stimulation; Visual Cortex; Visual Perception; Young Adult},
	type  = {Article}}





@article{Xiong2025Interpretable,
  author  = {Xiong, Daowen and Hu, Liangliang and Jin, Jiahao and Ding, Yikang and Tan, Congming and Zhang, Jing and Tian, Yin},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems}, 
  title  = {Interpretable Cross-Modal Alignment Network for EEG Visual Decoding With Algorithm Unrolling}, 
  year  = {2025},
  volume  = {},
  number  = {},
  pages  = {1--15},
  abstract  = {Accurate decoding in electroencephalography (EEG) technology, particularly for rapid visual stimuli, remains challenging due to the low signal-to-noise ratio (SNR). Additionally, existing neural networks struggle with issues related to generalization and interpretability. This article proposes a cross-modal aligned network, E2IVAE, which leverages shared information from multiple modalities for self-supervised alignment of EEG to images for extracting visual perceptual information and features a novel EEG encoder, ISTANet, based on algorithm unrolling. This network framework significantly enhances the accuracy and stability of EEG decoding for object recognition in novel classes while reducing the extensive neural data typically required for training neural decoders. The proposed ISTANet employs algorithm unrolling to transform the multilayer sparse coding algorithm into an end-to-end format, extracting features from noisy EEG signals while incorporating the interpretability of traditional machine learning. The experimental results demonstrate that our method achieves SOTA top-1 accuracy of 62.39% and top-5 accuracy of 88.98% on a comprehensive rapid serial visual presentation (RSVP) dataset for public comparison in a 200-class zero-shot neural decoding task. Additionally, ISTANet enables visualization and analysis of multiscale atom features and overall reconstruction features, exploring biological plausibility across temporal, spatial, and spectral dimensions. On another more challenging RSVP large-scale dataset, the proposed framework also achieves significantly above chance-level performance, proving its robustness and generalization. This research provides critical insights into neural decoding and brain–computer interfaces (BCIs) within the fields of cognitive science and artificial intelligence.},
  keywords  = {Electroencephalography;Decoding;Feature extraction;Visualization;Brain modeling;Training;Encoding;Image reconstruction;Object recognition;Data mining;Algorithm unrolling;brain–computer interfaces (BCIs);electroencephalography (EEG) decoding;interpretability},
  doi  = {10.1109/TNNLS.2025.3592646},
  issn  = {2162-2388},
  month  = {}}



@article{Xu2025Brainvision,
	author  = {Xu, Ting and Yu, Lianzhi and Zheng, Yongwei and Huang, Shuai},
	title  = {BrainVision: Cross-domain EEG decoding for visual content retrieval and reconstruction},
	year  = {2025},
	journal  = {Neuroscience},
	volume  = {584},
	pages  = {190 - 205},
	doi  = {10.1016/j.neuroscience.2025.07.047},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013792683&doi=10.1016%2Fj.neuroscience.2025.07.047&partnerID=40&md5=93e752eb9da3c8e30876380bf3397356},
	abstract  = {Understanding human visual intent through brain signals remains a fundamental challenge in neuroscience and artificial intelligence. Despite recent advances in brain decoding, existing approaches typically operate within isolated datasets and modalities, limiting their generalization capabilities. This paper introduces BrainVision, a novel framework that bridges visual recognition and emotional EEG datasets to enable comprehensive visual content generation through cross-domain learning. BrainVision addresses the critical challenge of leveraging complementary information across heterogeneous EEG sources by implementing a unified cross-domain alignment strategy. Our framework maps neural patterns from the THINGS-EEG visual recognition dataset and the DEAP emotional response dataset into a shared representation space, enabling three distinct visual output capabilities: (1) accurate content retrieval and classification, (2) detailed linguistic descriptions through adapter-enhanced large language models, and (3) high-fidelity image reconstruction via stable diffusion models. Experimental results demonstrate that BrainVision significantly outperforms single-domain approaches, achieving a 15.3% increase in retrieval accuracy and a 12.7% improvement in structural similarity for reconstructed images compared to state-of-the-art methods. Furthermore, our framework demonstrates robust zero-shot generalization, maintaining 82% of its performance when applied to novel stimuli not seen during training. The multi-modal outputs provide complementary interpretations of neural activity, offering a more comprehensive understanding of visual intent. Our findings establish that integrating diverse neural datasets substantially enhances the capabilities of brain decoding systems, providing a promising direction for developing more intuitive and versatile brain-computer interfaces. BrainVision represents an important step toward bridging the gap between neural activity and rich visual experiences across different cognitive domains. © 2025 Elsevier B.V., All rights reserved.},	keywords  = {Article; artificial intelligence; brain signal processing; brain vision; Brain2Image; caption generation quality; cognition; cross domain knowledge transfer; cross domain learning; cross domain transfer analysis; data analysis; data processing; dataset configuration analysis; decoding; electroencephalogram; emotion; event related potential; human; image reconstruction; information processing; information retrieval; inter subject variability analysis; knowledge; large language model; learning; learning algorithm; multi modal visual output generation; multi-task learning; nerve cell network; neuroimaging; signal processing; THINGS-EEG dataset; transfer of learning; visual adaptation; visual content retrieval; visual memory; adult; brain; electroencephalography; female; male; physiology; procedures; vision; Adult; Brain; Electroencephalography; Emotions; Female; Humans; Male; Visual Perception},
	type  = {Article}}





@article{Yu2025Robust,
  author  = {Yu, Zhaofei and Bu, Tong and Zhang, Yijun and Jia, Shanshan and Huang, Tiejun and Liu, Jian K.},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems}, 
  title  = {Robust Decoding of Rich Dynamical Visual Scenes With Retinal Spikes}, 
  year  = {2025},
  volume  = {36},
  number  = {2},
  pages  = {3396--3409},
  abstract  = {Sensory information transmitted to the brain activates neurons to create a series of coping behaviors. Understanding the mechanisms of neural computation and reverse engineering the brain to build intelligent machines requires establishing a robust relationship between stimuli and neural responses. Neural decoding aims to reconstruct the original stimuli that trigger neural responses. With the recent upsurge of artificial intelligence, neural decoding provides an insightful perspective for designing novel algorithms of brain–machine interface. For humans, vision is the dominant contributor to the interaction between the external environment and the brain. In this study, utilizing the retinal neural spike data collected over multi trials with visual stimuli of two movies with different levels of scene complexity, we used a neural network decoder to quantify the decoded visual stimuli with six different metrics for image quality assessment establishing comprehensive inspection of decoding. With the detailed and systematical study of the effect and single and multiple trials of data, different noise in spikes, and blurred images, our results provide an in-depth investigation of decoding dynamical visual scenes using retinal spikes. These results provide insights into the neural coding of visual scenes and services as a guideline for designing next-generation decoding algorithms of neuroprosthesis and other devices of brain–machine interface.},
  keywords  = {Decoding;Visualization;Retina;Image reconstruction;Neurons;Convolution;Training;Deep learning;image reconstruction;neural decoding;neural spikes;video;visual scenes},
  doi  = {10.1109/TNNLS.2024.3351120},
  issn  = {2162-2388},
  month  = {Feb}}



@article{Zhu2025Fmriges,
  author  = {Zhu, Chunzheng and Shao, Jialin and Lin, Jianxin and Wang, Yijun and Wang, Jing and Tang, Jinhui and Li, Kenli},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology}, 
  title  = {fMRI2GES: Co-Speech Gesture Reconstruction From fMRI Signal With Dual Brain Decoding Alignment}, 
  year  = {2025},
  volume  = {35},
  number  = {9},
  pages  = {9017--9029},
  abstract  = {Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired {brain, speech, gesture} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, fMRI2GES, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using Dual Brain Decoding Alignment. This method relies on two key components: 1) observed texts that elicit brain responses, and 2) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.},
  keywords  = {Decoding;Functional magnetic resonance imaging;Brain modeling;Image reconstruction;Training;Data models;Recording;Neuroscience;Linguistics;Legged locomotion;fMRI signal;diffusoin models;neurosciences},
  doi  = {10.1109/TCSVT.2025.3558125},
  issn  = {1558-2205},
  month  = {Sep.}}



@conference{Akbari2024Joint,
	author  = {Akbari, Ali and Sanjar, Kosar and Yousefnezhad, Muhammad and Mirian, Maryam Sadat and Arasteh, Emad Malekzadeh},
	title  = {Joint Learning for Visual Reconstruction from the Brain Activity: Hierarchical Representation of Image Perception with EEG-Vision Transformer},
	year  = {2024},
	journal  = {Proceedings of Machine Learning Research},
	volume  = {285},
	pages  = {},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014736717&partnerID=40&md5=95ef7cdfe2da78435a3bfdd9b45b1b61},
	abstract  = {Reconstructing visual stimuli from brain activity is a challenging problem, particu-larly when using EEG data, which is more affordable and accessible than fMRI but noisier and lower in spatial resolution. In this paper, we present Hierarchical-ViT, a novel framework designed to improve the quality and precision of EEG-based image reconstruction by integrating hierarchical visual feature extraction, vision transformer-based EEG (EEG-ViT) processing, and CLIP-based joint learning. Inspired by the hierarchical nature of the human visual system, our model progressively captures complex visual features—such as edges, textures, and shapes—through a multi-stage processing approach. These features are aligned with EEG signals processed by the EEG-ViT model, allowing for the creation of a shared latent space that enhances contrastive learning. A StyleGAN is then em-ployed to generate high-resolution images from these aligned representations. We evaluated our method on two benchmark datasets, EEGCVPR40 and ThoughtViz, achieving superior results compared to existing approaches in terms of Inception Score (IS), Kernel Inception Distance (KID), and Frechet Inception Distance (FID) for EEGCVPR, and IS and KID for the ThoughtViz dataset. Through an ablation study, we underscored the feasibility of hierarchical feature extraction, while the multivariate analysis of variance (MANOVA) test confirmed the distinctiveness of the learned feature spaces. In conclusion, our results show the feasibility and uniqueness of using hierarchical filtering of perceived images combined with EEG-ViT-based features to improve brain decoding from EEG data. © 2025 Elsevier B.V., All rights reserved.},
	keywords  = {Biomedical signal processing; Brain; Computer vision; Electroencephalography; Feature extraction; Image enhancement; Image reconstruction; Neurophysiology; Textures; Brain activity; Hierarchical representation; Human Visual System; Image perception; Images reconstruction; Joint learning; Spatial resolution; Visual feature extraction; Visual reconstruction; Visual stimulus; Extraction; Multivariant analysis},
	type  = {Conference paper}}





@conference{Balisacan2024Neurovis,
	author  = {Balisacan, Gabriela M. and Paulo, Anne Therese A.},
	title  = {Neuro-Vis: Guided Complex Image Reconstruction from Brain Signals Using Multiple Semantic and Perceptual Controls},
	year  = {2024},
	pages  = {},
	doi  = {10.1145/3661725.3661744},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197256076&doi=10.1145%2F3661725.3661744&partnerID=40&md5=1f14148432beb75c4454987b8e646bac},
	abstract  = {The externalization of the state of one's mind, which people refer to as "mind reading"in science fiction, is currently being realized through brain decoding research. This field of study aims to deepen our understanding of the human brain, which is among the least understood known biological structures, and to build better foundations for brain-computer interfaces. With the success of state-of-the-art latent diffusion models in image synthesis, a trend in recent studies is to map fMRI recordings to the image embedding space of these generative models. While this method significantly improved image reconstructions in terms of semantics, preserving perceptual features without losing semantic information remains challenging, especially with complex images of natural scenes. This research introduces Neuro-Vis, a novel fMRI-to-image pipeline based on Stable Diffusion that effectively integrates multiple semantic controls through predicted image embeddings and captions and multiple lightweight perceptual controls through predicted blurry initial images, depth maps, and color palettes. Neuro-Vis outperforms the current state-of-the-art methods in terms of consistency in low-level features while also rivaling them in terms of semantics. Furthermore, ablation experiments demonstrate the effectiveness of each component in Neuro-Vis for fMRI-to-image reconstruction. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain computer interface; Embeddings; Functional neuroimaging; Image enhancement; Image reconstruction; Semantic Web; Semantics; Brain decoding; Complex image; FMRI-to-image reconstruction; Image captioning; Image embedding; Images reconstruction; Perceptual control; Perceptual guidance; Semantic guidance; Visual decoding; Decoding},
	type  = {Conference paper}}





@inproceedings{Chen2024Ieeecvf,
  author  = {Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang},
  booktitle  = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title  = {Mind Artist: Creating Artistic Snapshots with Human Thought}, 
  year  = {2024},
  volume  = {},
  number  = {},
  pages  = {27197--27207},
  abstract  = {We introduce Mind Artist (MindArt), a novel and efficient neural decoding architecture to snap artistic photographs from our mind in a controllable manner. Recently, progress has been made in image reconstruction with non-invasive brain recordings, but it's still difficult to generate realistic images with high semantic fidelity due to the scarcity of data annotations. Unlike previous methods, this work casts the neural decoding into optimal transport (OT) and representation decoupling problems. Specifically, under discrete OT theory, we design a graph matching-guided neural representation learning framework to seek the underlying correspondences between conceptual semantics and neural signals, which yields a natural and meaningful self-supervisory task. Moreover, the proposed MindArt, structured with multiple stand-alone modal branches, enables the seamless incorporation of semantic representation into any visual style information, thus leaving it to have multi-modal reconstruction and training-free semantic editing capabilities. By doing so, the reconstructed images of MindArt have phenomenal realism both in terms of semantics and appearance. We compare our MindArt with leading alternatives, and achieve SOTA performance in different decoding tasks. Importantly, our approach can directly generate a series of stylized “mind snapshots” w/o extra optimizations, which may open up more potential applications. Code is available at https://github.com/JxuanC/MindArt.},
  keywords  = {Visualization;Solid modeling;Three-dimensional displays;Computational modeling;Semantics;Linguistics;Decoding;Neural decoding;Representation learning;Multimodal learning},
  doi  = {10.1109/CVPR52733.2024.02569},
  issn  = {2575-7075},
  month  = {June}}



@article{Dado2024Braingan,
	author  = {Dado, Thirza and Papale, Paolo and Lozano, Antonio M. and Le, Lynn and Wang, Feng and Van Gerven, Marcel A.J. and Roelfsema, Pieter Roelf and Güçlütürk, Yaǧmur and Güçlü, Umut},
	title  = {Brain2GAN: Feature-disentangled neural encoding and decoding of visual perception in the primate brain},
	year  = {2024},
	journal  = {PLOS Computational Biology},
	volume  = {20},
	number  = {5 May},
	pages  = {},
	doi  = {10.1371/journal.pcbi.1012058},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193012537&doi=10.1371%2Fjournal.pcbi.1012058&partnerID=40&md5=466d61606677adf237eeb00bac6088f7},
	abstract  = {A challenging goal of neural coding is to characterize the neural representations underlying visual perception. To this end, multi-unit activity (MUA) of macaque visual cortex was recorded in a passive fixation task upon presentation of faces and natural images. We analyzed the relationship between MUA and latent representations of state-of-the-art deep generative models, including the conventional and feature-disentangled representations of generative adversarial networks (GANs) (i.e., z- and w-latents of StyleGAN, respectively) and language-contrastive representations of latent diffusion networks (i.e., CLIP-latents of Stable Diffusion). A mass univariate neural encoding analysis of the latent representations showed that feature-disentangled w representations outperform both z and CLIP representations in explaining neural responses. Further, w-latent features were found to be positioned at the higher end of the complexity gradient which indicates that they capture visual information relevant to high-level neural activity. Subsequently, a multivariate neural decoding analysis of the feature-disentangled representations resulted in state-of-the-art spatiotemporal reconstructions of visual perception. Taken together, our results not only highlight the important role of feature-disentanglement in shaping high-level neural representations underlying visual perception but also serve as an important benchmark for the future of neural coding. © 2024 Elsevier B.V., All rights reserved.},
	keywords  = {Decoding; Encoding (symbols); Neurons; Signal encoding; Vision; Encoding and decoding; Face images; Multi-unit activity; Neural coding; Neural decoding; Neural encoding; Neural representations; State of the art; Visual cortexes; Visual perception; Generative adversarial networks; article; benchmarking; controlled study; diagnosis; diffusion; generative model; human; human experiment; Macaca; nerve potential; nonhuman; primate; vision; visual cortex; visual information; animal; artificial neural network; bioinformatics; biological model; brain; male; nerve cell; photostimulation; physiology; rhesus monkey; Animals; Brain; Computational Biology; Macaca mulatta; Male; Models, Neurological; Neural Networks, Computer; Photic Stimulation; Visual Cortex; Visual Perception},
	type  = {Article}}





@article{DeLuca2024Predicting,
  author  = {De Luca, Daniela and Moccia, Sara and Lupori, Leonardo and Mazziotti, Raffaele and Pizzorusso, T. and Micera, Silvestro},
  journal  = {IEEE Sensors Journal}, 
  title  = {Predicting Visual Stimuli From Cortical Response Recorded With Wide-Field Imaging in a Mouse}, 
  year  = {2024},
  volume  = {24},
  number  = {6},
  pages  = {7299--7307},
  abstract  = {Neural decoding of the visual system is a subject of research interest, both to understand how the visual system works and to be able to use this knowledge in areas, such as computer vision or brain–computer interfaces. Spike-based decoding is often used, but it is difficult to record data from the whole visual cortex, and it requires proper preprocessing. We here propose a decoding method that combines wide-field calcium brain imaging, which allows us to obtain large-scale visualization of cortical activity with a high signal-to-noise ratio (SNR), and convolutional neural networks (CNNs). A mouse was presented with ten different visual stimuli, and the activity from its primary visual cortex (V1) was recorded. A CNN we designed was then compared with other existing commonly used CNNs, that were trained to classify the visual stimuli from wide-field calcium imaging images, obtaining a weighted  $F1$  score of more than 0.70 on the test set, showing it is possible to automatically detect what is present in the visual field of the animal.},
  keywords  = {Visualization;Convolutional neural networks;Decoding;Calcium;Imaging;Fluorescence;Deep learning;Transfer learning;Prosthetics;Neural activity;Deep learning;transfer learning;visual cortex;visual prostheses;wide-field imaging},
  doi  = {10.1109/JSEN.2023.3335613},
  issn  = {1558-1748},
  month  = {March}}



@article{Ferrante2024Retrieving,
	author  = {Ferrante, Matteo and Boccato, Tommaso and Passamonti, Luca and Toschi, Nicola},
	title  = {Retrieving and reconstructing conceptually similar images from fMRI with latent diffusion models and a neuro-inspired brain decoding model},
	year  = {2024},
	journal  = {Journal of Neural Engineering},
	volume  = {21},
	number  = {4},
	pages  = {},
	doi  = {10.1088/1741-2552/ad593c},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197363165&doi=10.1088%2F1741-2552%2Fad593c&partnerID=40&md5=be30a60850031c296abea98f3281e465},
	abstract  = {Objective. Brain decoding is a field of computational neuroscience that aims to infer mental states or internal representations of perceptual inputs from measurable brain activity. This study proposes a novel approach to brain decoding that relies on semantic and contextual similarity. Approach. We use several functional magnetic resonance imaging (fMRI) datasets of natural images as stimuli and create a deep learning decoding pipeline inspired by the bottom-up and top-down processes in human vision. Our pipeline includes a linear brain-to-feature model that maps fMRI activity to semantic visual stimuli features. We assume that the brain projects visual information onto a space that is homeomorphic to the latent space of last layer of a pretrained neural network, which summarizes and highlights similarities and differences between concepts. These features are categorized in the latent space using a nearest-neighbor strategy, and the results are used to retrieve images or condition a generative latent diffusion model to create novel images. Main results. We demonstrate semantic classification and image retrieval on three different fMRI datasets: Generic Object Decoding (vision perception and imagination), BOLD5000, and NSD. In all cases, a simple mapping between fMRI and a deep semantic representation of the visual stimulus resulted in meaningful classification and retrieved or generated images. We assessed quality using quantitative metrics and a human evaluation experiment that reproduces the multiplicity of conscious and unconscious criteria that humans use to evaluate image similarity. Our method achieved correct evaluation in over 80% of the test set. Significance. Our study proposes a novel approach to brain decoding that relies on semantic and contextual similarity. The results demonstrate that measurable neural correlates can be linearly mapped onto the latent space of a neural network to synthesize images that match the original content. These findings have implications for both cognitive neuroscience and artificial intelligence. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Classification (of information); Decoding; Deep learning; Diffusion; Image reconstruction; Magnetic resonance imaging; Multilayer neural networks; Pipelines; Brain decoding; Computational neuroscience; Diffusion model; Functional magnetic resonance imaging; Latent; Mental state; Neural-networks; Similar image; State representation; Visual stimulus; Semantics; Article; artificial intelligence; autoencoder; brain decoding model; clustering algorithm; conceptual model; controlled study; convolutional neural network; deep learning; diffusion; electroencephalogram; functional magnetic resonance imaging; hemodynamics; human; image reconstruction; image retrieval; magnetic field; model; neurobiology; neuroscience; visual cortex; visual stimulation; artificial neural network; biological model; brain; brain mapping; diagnostic imaging; image processing; nuclear magnetic resonance imaging; photostimulation; physiology; procedures; semantics; vision; Brain Mapping; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Models, Neurological; Neural Networks, Computer; Photic Stimulation; Visual Perception},
	type  = {Article}}





@article{Ferrante2024Decoding,
	author  = {Ferrante, Matteo and Boccato, Tommaso and Bargione, Stefano and Toschi, Nicola},
	title  = {Decoding visual brain representations from electroencephalography through knowledge distillation and latent diffusion models},
	year  = {2024},
	journal  = {Computers in Biology and Medicine},
	volume  = {178},
	pages  = {},
	doi  = {10.1016/j.compbiomed.2024.108701},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196144814&doi=10.1016%2Fj.compbiomed.2024.108701&partnerID=40&md5=6a50ce04957122140b02e8e8c30bb254},
	abstract  = {Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain–computer interfaces. Our study presents an innovative method that employs knowledge distillation to train an EEG classifier and reconstruct images from the ImageNet and THINGS-EEG 2 datasets using only electroencephalography (EEG) data from participants who have viewed the images themselves (i.e. “brain decoding”). We analyzed EEG recordings from 6 participants for the ImageNet dataset and 10 for the THINGS-EEG 2 dataset, exposed to images spanning unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 87%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism based on pre-trained latent diffusion models, which allowed us to generate an estimate of the images that had elicited EEG activity. Therefore, our architecture not only decodes images from neural activity but also offers a credible image reconstruction from EEG only, paving the way for, e.g., swift, individualized feedback experiments. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Brain computer interface; Classification (of information); Convolution; Decoding; Distillation; Electroencephalography; Electrophysiology; Image classification; Neurons; Personnel training; Semantics; BCI vision; Brain activity; Brain decoding; Convolutional neural network; Diffusion model; Electroencephalography decoding; Human brain; Images reconstruction; Research domains; Visual representations; Image reconstruction; Article; classification algorithm; classifier; clustering algorithm; comparative study; computer vision; contrastive language image pre training; controlled study; convolutional neural network; deep learning; diffusion; distillation; electroencephalogram; electroencephalography; follow up; human; image reconstruction; k means clustering; knowledge; knowledge distillation; latent period; logistic regression analysis; long short term memory network; measurement accuracy; probability; recurrent neural network; short time Fourier transform; signal processing; time frequency decomposition; time series analysis; visual attention; visual memory; adult; artificial neural network; brain; brain computer interface; female; image processing; male; physiology; procedures; Adult; Brain-Computer Interfaces; Female; Humans; Image Processing, Computer-Assisted; Male; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	type  = {Article}}





@article{Greenidge2024Efficient,
  author  = {Greenidge, C. Daniel and Scholl, Benjamin and Yates, Jacob L. and Pillow, Jonathan W.},
  journal  = {Neural Computation}, 
  title  = {Efficient Decoding of Large-Scale Neural Population Responses With Gaussian-Process Multiclass Regression}, 
  year  = {2024},
  volume  = {36},
  number  = {2},
  pages  = {175--226},
  abstract  = {Neural decoding methods provide a powerful tool for quantifying the information content of neural population codes and the limits imposed by correlations in neural activity. However, standard decoding methods are prone to overfitting and scale poorly to high-dimensional settings. Here, we introduce a novel decoding method to overcome these limitations. Our approach, the gaussian process multiclass decoder (GPMD), is well suited to decoding a continuous low-dimensional variable from high-dimensional population activity and provides a platform for assessing the importance of correlations in neural population codes. The GPMD is a multinomial logistic regression model with a gaussian process prior over the decoding weights. The prior includes hyperparameters that govern the smoothness of each neuron's decoding weights, allowing automatic pruning of uninformative neurons during inference. We provide a variational inference method for fitting the GPMD to data, which scales to hundreds or thousands of neurons and performs well even in data sets with more neurons than trials. We apply the GPMD to recordings from primary visual cortex in three species: monkey, ferret, and mouse. Our decoder achieves state-of-the-art accuracy on all three data sets and substantially outperforms independent Bayesian decoding, showing that knowledge of the correlation structure is essential for optimal decoding in all three species.},
  keywords  = {},
  doi  = {10.1162/neco_a_01630},
  issn  = {0899-7667},
  month  = {Jan}}



@article{Ikegawa2024Text,
	author  = {Ikegawa, Yuya and Fukuma, Ryohei and Sugano, Hidenori Sugano and Oshino, Satoru and Tani, Naoki and Tamura, Kentaro and Iimura, Yasushi Iimura and Suzuki, Hiroharu and Yamamoto, Shota and Fujita, Yuya and Nishimoto, Shinji and Kishima, Haruhiko and Yanagisawa, Takufumi},
	title  = {Text and image generation from intracranial electroencephalography using an embedding space for text and images},
	year  = {2024},
	journal  = {Journal of Neural Engineering},
	volume  = {21},
	number  = {3},
	pages  = {},
	doi  = {10.1088/1741-2552/ad417a},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193773877&doi=10.1088%2F1741-2552%2Fad417a&partnerID=40&md5=6944c52692d205487b14736a17db2770},
	abstract  = {Objective. Invasive brain-computer interfaces (BCIs) are promising communication devices for severely paralyzed patients. Recent advances in intracranial electroencephalography (iEEG) coupled with natural language processing have enhanced communication speed and accuracy. It should be noted that such a speech BCI uses signals from the motor cortex. However, BCIs based on motor cortical activities may experience signal deterioration in users with motor cortical degenerative diseases such as amyotrophic lateral sclerosis. An alternative approach to using iEEG of the motor cortex is necessary to support patients with such conditions. Approach. In this study, a multimodal embedding of text and images was used to decode visual semantic information from iEEG signals of the visual cortex to generate text and images. We used contrastive language-image pretraining (CLIP) embedding to represent images presented to 17 patients implanted with electrodes in the occipital and temporal cortices. A CLIP image vector was inferred from the high-γ power of the iEEG signals recorded while viewing the images. Main results. Text was generated by CLIPCAP from the inferred CLIP vector with better-than-chance accuracy. Then, an image was created from the generated text using StableDiffusion with significant accuracy. Significance. The text and images generated from iEEG through the CLIP embedding vector can be used for improved communication. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain computer interface; Decoding; Deterioration; Electrophysiology; Embeddings; Image enhancement; Medical computing; Natural language processing systems; Neurodegenerative diseases; Semantics; Communication device; Image generations; Intracranial EEG; Motor-cortex; Natural languages; Neural decoding; Paralyzed patients; Pre-training; Text generations; Electroencephalography; Article; clinical article; contrastive language image pretraining; controlled study; electrocorticography; electroencephalography; embedding; human; multilayer perceptron; nested cross validation; occipital cortex; temporal cortex; adult; brain computer interface; electrode implant; female; male; middle aged; photostimulation; procedures; young adult; Adult; Brain-Computer Interfaces; Electrocorticography; Electrodes, Implanted; Female; Humans; Male; Middle Aged; Photic Stimulation; Young Adult},
	type  = {Article}}





@inproceedings{Ishizaki2024Joint,
  author  = {Ishizaki, Fumi and Kobayashi, Ichiro},
  booktitle  = {2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCIS&ISIS)}, 
  title  = {A Study on Brain Decoding of Image Stimuli Using a Diffusion Model}, 
  year  = {2024},
  volume  = {},
  number  = {},
  pages  = {1--4},
  abstract  = {Humans recognize the external world by processing information received by the eyes and other sensory organs in the brain. Understanding how the human brain processes complex information from the outside world is expected to improve the performance of image and speech recognition technologies, which have made remarkable progress in recent years. In this study, we focus on brain activity decoding of visual experience, aim to read what humans are looking at by predicting image features from brain activity data, and further attempt to develop a method to output high-definition and semantically valid images by generating images from predicted features using a diffusion model. As a result, similarity to the stimulus image was confirmed in the image generated by the training data, but the evaluation data confirmed that there is still room for further study.},
  keywords  = {Visualization;Noise reduction;Training data;Speech recognition;Predictive models;Diffusion models;Brain modeling;Data models;Decoding;Image reconstruction;Brain decoding;diffusion models;image stimu-lation},
  doi  = {10.1109/SCISISIS61014.2024.10759955},
  issn  = {},
  month  = {Nov}}



@article{KoideMajima2024Mental,
	author  = {Koide-Majima, Naoko and Nishimoto, Shinji and Majima, Kei},
	title  = {Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based Bayesian estimation},
	year  = {2024},
	journal  = {Neural Networks},
	volume  = {170},
	pages  = {349 - 363},
	doi  = {10.1016/j.neunet.2023.11.024},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178352755&doi=10.1016%2Fj.neunet.2023.11.024&partnerID=40&md5=0c0563507dbd1c34cef2ea6a0fa4f2fd},
	abstract  = {Visual images observed by humans can be reconstructed from their brain activity. However, the visualization (externalization) of mental imagery is challenging. Only a few studies have reported successful visualization of mental imagery, and their visualizable images have been limited to specific domains such as human faces or alphabetical letters. Therefore, visualizing mental imagery for arbitrary natural images stands as a significant milestone. In this study, we achieved this by enhancing a previous method. Specifically, we demonstrated that the visual image reconstruction method proposed in the seminal study by Shen et al. (2019) heavily relied on low-level visual information decoded from the brain and could not efficiently utilize the semantic information that would be recruited during mental imagery. To address this limitation, we extended the previous method to a Bayesian estimation framework and introduced the assistance of semantic information into it. Our proposed framework successfully reconstructed both seen images (i.e., those observed by the human eye) and imagined images from brain activity. Quantitative evaluation showed that our framework could identify seen and imagined images highly accurately compared to the chance accuracy (seen: 90.7%, imagery: 75.6%, chance accuracy: 50.0%). In contrast, the previous method could only identify seen images (seen: 64.3%, imagery: 50.4%). These results suggest that our framework would provide a unique tool for directly investigating the subjective contents of the brain such as illusions, hallucinations, and dreams. © 2023 Elsevier B.V., All rights reserved.},	keywords  = {Bayesian networks; Decoding; Deep neural networks; Image reconstruction; Neurophysiology; Semantics; Visualization; Bayesian estimations; Brain activity; Brain decoding; Human brain; Images reconstruction; Mental imagery; Mental images; Semantic representation; Semantics Information; Visual image; Brain; adult; Article; Bayesian network; comparative study; deep neural network; dream; electroencephalogram; female; hallucination; human; illusion; image reconstruction; male; measurement accuracy; neuroimaging; semantics; visual information; young adult; artificial neural network; Bayes theorem; brain; brain mapping; diagnostic imaging; image processing; imagination; nuclear magnetic resonance imaging; procedures; Bayes Theorem; Brain Mapping; Humans; Image Processing, Computer-Assisted; Imagination; Magnetic Resonance Imaging; Neural Networks, Computer},
	type  = {Article}}





@article{Kuang2024Natural,
	author  = {Kuang, Mei and Zhan, Zongyi and Gao, Shaobing},
	title  = {Natural Image Reconstruction from fMRI Based on Node–Edge Interaction and Multi–Scale Constraint},
	year  = {2024},
	journal  = {Brain Sciences},
	volume  = {14},
	number  = {3},
	pages  = {},
	doi  = {10.3390/brainsci14030234},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188703271&doi=10.3390%2Fbrainsci14030234&partnerID=40&md5=10192334498ce21b29ead1e64fa8fdc9},
	abstract  = {Reconstructing natural stimulus images using functional magnetic resonance imaging (fMRI) is one of the most challenging problems in brain decoding and is also the crucial component of a brain–computer interface. Previous methods cannot fully exploit the information about interactions among brain regions. In this paper, we propose a natural image reconstruction method based on node–edge interaction and a multi–scale constraint. Inspired by the extensive information interactions in the brain, a novel graph neural network block with node–edge interaction (NEI–GNN block) is presented, which can adequately model the information exchange between brain areas via alternatively updating the nodes and edges. Additionally, to enhance the quality of reconstructed images in terms of both global structure and local detail, we employ a multi–stage reconstruction network that restricts the reconstructed images in a coarse–to–fine manner across multiple scales. Qualitative experiments on the generic object decoding (GOD) dataset demonstrate that the reconstructed images contain accurate structural information and rich texture details. Furthermore, the proposed method surpasses the existing state–of–the–art methods in terms of accuracy in the commonly used n–way evaluation. Our approach achieves 82.00%, 59.40%, 45.20% in n–way mean squared error (MSE) evaluation and 83.50%, 61.80%, 46.00% in n–way structural similarity index measure (SSIM) evaluation, respectively. Our experiments reveal the importance of information interaction among brain areas and also demonstrate the potential for developing visual–decoding brain–computer interfaces. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Article; brain region; deep learning; event related potential; functional magnetic resonance imaging; human; human experiment; image quality; image reconstruction; image segmentation; imagery; mean squared error; nerve cell differentiation; nerve cell network; speech perception; stimulus; support vector machine; task performance; total quality management; training},
	type  = {Article}}





@article{Li2024Multiscale,
  author  = {Li, Ziyu and Li, Qing and Zhu, Zhiyuan and Hu, Zhongyi and Wu, Xia},
  journal  = {IEEE Journal of Biomedical and Health Informatics}, 
  title  = {Multi-Scale Spatio-Temporal Fusion With Adaptive Brain Topology Learning for fMRI Based Neural Decoding}, 
  year  = {2024},
  volume  = {28},
  number  = {1},
  pages  = {262--272},
  abstract  = {Neural decoding aims to extract information from neurons' activities to reveal how the brain functions. Due to the inherent spatial and temporal characteristics of brain signals, spatio-temporal computing has become a hot topic for neural decoding. However, the extant spatio-temporal decoding methods usually use static brain topology, ignoring the dynamic patterns of the interaction between brain regions. Further, they do not identify the hierarchical organization of brain topology, leading to only superficial insight into brain spatio-temporal interactions. Therefore, here we propose a novel framework, the Multi-Scale Spatio-Temporal framework with Adaptive Brain Topology Learning (MSST-ABTL), for neural decoding. It includes two new capabilities to enhance spatio-temporal decoding: i) ABTL module, which learns dynamic brain topology while updating specific patterns of brain regions, ii) MSST module, which captures the association of spatial pattern and temporal evolution, and further enhances the interpretability of the learned dynamic topology from multi-scale perspective. We evaluated the framework on the public Human Connectome Project (HCP) dataset (resting-state and task-related fMRI data). The extensive experiments show that the proposed MSST-ABTL outperforms state-of-the-art methods on four evaluation metrics, and also can renew the neuroscientific discoveries in the brain's hierarchical patterns.},
  keywords  = {Topology;Decoding;Network topology;Correlation;Brain modeling;Functional magnetic resonance imaging;Task analysis;Neural decoding;adaptive;brain topology;multi-scale;spatio-temporal},
  doi  = {10.1109/JBHI.2023.3327023},
  issn  = {2168-2208},
  month  = {Jan}}



@article{Liu2024Robrain,
	author  = {Liu, Che and Du, Changde and He, Huiguang},
	title  = {RoBrain: Towards Robust Brain-to-Image Reconstruction via Cross-Domain Contrastive Learning},
	year  = {2024},
	journal  = {Lecture Notes in Computer Science},
	volume  = {14449 LNCS},
	pages  = {227 - 238},
	doi  = {10.1007/978-981-99-8067-3_17},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177851235&doi=10.1007%2F978-981-99-8067-3_17&partnerID=40&md5=9bc837a7a91ba8feeccfeb7eef9928a8},
	abstract  = {With the development of neuroimaging technology and deep learning methods, neural decoding with functional Magnetic Resonance Imaging (fMRI) of human brain has attracted more and more attention. Neural reconstruction task, which intends to reconstruct stimulus images from fMRI, is one of the most challenging tasks in neural decoding. Due to the instability of neural signals, trials of fMRI collected under the same stimulus prove to be very different, which leads to the poor robustness and generalization ability of the existing models. In this work, we propose a robust brain-to-image model based on cross-domain contrastive learning. With deep neural network (DNN) features as paradigms, our model can extract features of stimulus stably and generate reconstructed images via DCGAN. Experiments on the benchmark Deep Image Reconstruction dataset show that our method can enhance the robustness of reconstruction significantly. © 2023 Elsevier B.V., All rights reserved.},	keywords  = {Decoding; Deep neural networks; Functional neuroimaging; Image enhancement; Learning systems; Magnetic resonance imaging; Brain-to-image reconstruction; Contrastive learning; Cross-domain; Functional magnetic resonance imaging; Generalization ability; Human brain; Images reconstruction; Learning methods; Neural decoding; Neural signals; Image reconstruction},
	type  = {Conference paper}}





@article{Meng2024Semanticsguided,
  author  = {Meng, Lu and Yang, Chuanhao},
  journal  = {IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title  = {Semantics-Guided Hierarchical Feature Encoding Generative Adversarial Network for Visual Image Reconstruction From Brain Activity}, 
  year  = {2024},
  volume  = {32},
  number  = {},
  pages  = {1267--1283},
  abstract  = {The utilization of deep learning techniques for decoding visual perception images from brain activity recorded by functional magnetic resonance imaging (fMRI) has garnered considerable attention in recent research. However, reconstructed images from previous studies still suffer from low quality or unreliability. Moreover, the complexity inherent to fMRI data, characterized by high dimensionality and low signal-to-noise ratio, poses significant challenges in extracting meaningful visual information for perceptual reconstruction. In this regard, we proposes a novel neural decoding model, named the hierarchical semantic generative adversarial network (HS-GAN), inspired by the hierarchical encoding of the visual cortex and the homology theory of convolutional neural networks (CNNs), which is capable of reconstructing perceptual images from fMRI data by leveraging the hierarchical and semantic representations. The experimental results demonstrate that HS-GAN achieved the best performance on Horikawa2017 dataset (histogram similarity: 0.447, SSIM-Acc: 78.9%, Peceptual-Acc: 95.38%, AlexNet(2): 96.24% and AlexNet(5): 94.82%) over existing advanced methods, indicating improved naturalness and fidelity of the reconstructed image. The versatility of the HS-GAN was also highlighted, as it demonstrated promising generalization capabilities in reconstructing handwritten digits, achieving the highest SSIM (0.783±0.038), thus extending its application beyond training solely on natural images.},
  keywords  = {Visualization;Functional magnetic resonance imaging;Image reconstruction;Decoding;Feature extraction;Semantics;Training;Visual decoding;image reconstruction;generative adversarial network;fMRI},
  doi  = {10.1109/TNSRE.2024.3377698},
  issn  = {1558-0210},
  month  = {}}



@article{Meng2024Reconstruction,
	author  = {Meng, Lu and Tang, Zhenxuan and Liu, Yangqian},
	title  = {Reconstruction of natural images from human fMRI using a three-stage multi-level deep fusion model},
	year  = {2024},
	journal  = {Journal of Neuroscience Methods},
	volume  = {411},
	pages  = {},
	doi  = {10.1016/j.jneumeth.2024.110269},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203189040&doi=10.1016%2Fj.jneumeth.2024.110269&partnerID=40&md5=c41acc6a7e11d44b433e36d01ae9bd4f},
	abstract  = {Background: Image reconstruction is a critical task in brain decoding research, primarily utilizing functional magnetic resonance imaging (fMRI) data. However, due to challenges such as limited samples in fMRI data, the quality of reconstruction results often remains poor. New method: We proposed a three-stage multi-level deep fusion model (TS-ML-DFM). The model employed a three-stage training process, encompassing components such as image encoders, generators, discriminators, and fMRI encoders. In this method, we incorporated distinct supplementary features derived separately from depth images and original images. Additionally, the method integrated several components, including a random shift module, dual attention module, and multi-level feature fusion module. Results: In both qualitative and quantitative comparisons on the Horikawa17 and VanGerven10 datasets, our method exhibited excellent performance. Comparison with existing methods: For example, on the primary Horikawa17 dataset, our method was compared with other leading methods based on metrics the average hash value, histogram similarity, mutual information, structural similarity accuracy, AlexNet(2), AlexNet(5), and pairwise human perceptual similarity accuracy. Compared to the second-ranked results in each metric, the proposed method achieved improvements of 0.99 %, 3.62 %, 3.73 %, 2.45 %, 3.51 %, 0.62 %, and 1.03 %, respectively. In terms of the SwAV top-level semantic metric, a substantial improvement of 10.53 % was achieved compared to the second-ranked result in the pixel-level reconstruction methods. Conclusions: The TS-ML-DFM method proposed in this study, when applied to decoding brain visual patterns using fMRI data, has outperformed previous algorithms, thereby facilitating further advancements in research within this field. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Article; brain region; comparative study; controlled study; deconvolution; functional magnetic resonance imaging; histogram; human; image reconstruction; model; multi level deep fusion model model; nonhuman; validity; brain; brain mapping; deep learning; diagnostic imaging; image processing; nuclear magnetic resonance imaging; physiology; procedures; Brain; Brain Mapping; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging},
	type  = {Article}}





@article{Pan2024Reconstructing,
  author  = {Pan, Hongguang and Li, Zhuoyi and Fu, Yunpeng and Qin, Xuebin and Hu, Jianchen},
  journal  = {IEEE Transactions on Human-Machine Systems}, 
  title  = {Reconstructing Visual Stimulus Representation From EEG Signals Based on Deep Visual Representation Model}, 
  year  = {2024},
  volume  = {54},
  number  = {6},
  pages  = {711--722},
  abstract  = {Reconstructing visual stimulus representation is a significant task in neural decoding. Until now, most studies have considered functional magnetic resonance imaging (fMRI) as the signal source. However, fMRI-based image reconstruction methods are challenging to apply widely due to the complexity and high cost of acquisition equipment. Taking into account the advantages of the low cost and easy portability of electroencephalogram (EEG) acquisition equipment, we propose a novel image reconstruction method based on EEG signals in this article. First, to meet the high recognizability of visual stimulus images in a fast-switching manner, we construct a visual stimuli image dataset and obtain the corresponding EEG dataset through EEG signals collection experiment. Second, we introduce the deep visual representation model (DVRM), comprising a primary encoder and a subordinate decoder, to reconstruct visual stimuli representation. The encoder is designed based on residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images. Meanwhile, the decoder is designed using a deep neural network to reconstruct the visual stimulus representation from the learned deep visual representation. The DVRM can accommodate the deep and multiview visual features of the human natural state, resulting in more precise reconstructed images. Finally, we evaluate the DVRM based on the quality of the generated images using our EEG dataset. The results demonstrate that the DVRM exhibits an excellent performance in learning deep visual representation from EEG signals, generating reconstructed representation of images that are realistic and highly resemble the original images.},
  keywords  = {Visualization;Electroencephalography;Image reconstruction;Feature extraction;Brain modeling;Functional magnetic resonance imaging;Decoding;Deep visual representation model (DVRM);electroencephalogram (EEG) dataset;image reconstruction;neural decoding},
  doi  = {10.1109/THMS.2024.3407875},
  issn  = {2168-2305},
  month  = {Dec}}



@article{Ren2024Braindriven,
	author  = {Ren, Ziqi and Li, Jie and Wu, Lukun and Xue, Xuetong and Li, Xin and Yang, Fan and Jiao, Zhicheng and Gao, Xinbo Bo},
	title  = {Brain-driven facial image reconstruction via StyleGAN inversion with improved identity consistency},
	year  = {2024},
	journal  = {Pattern Recognition},
	volume  = {150},
	pages  = {},
	doi  = {10.1016/j.patcog.2024.110331},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185398640&doi=10.1016%2Fj.patcog.2024.110331&partnerID=40&md5=af8e5a17f253c16101dc9d5cd250c8cb},
	abstract  = {The reconstruction of visual stimuli from fMRI data represents a major technological and scientific challenge at the forefront of contemporary neuroscience research. Deep learning techniques have played a critical role in advancing decoding models for visual stimulus reconstruction from fMRI data. Particularly, the use of advanced GANs has resulted in significant improvements in the quality of image generation, providing a powerful tool for addressing the challenges of this complex task. However, none of these studies have taken into account the inherent characteristics of the stimulus contents themselves; This, in turn, leads to unsatisfactory outcomes, as demonstrated by the inconsistent identity between reconstructed faces and ground truth in the decoding of facial images. In order to tackle this challenge, we introduce a new framework aimed at enhancing the accuracy of reconstructing facial images from fMRI data. Our key innovation involves extracting and disentangling multi-level visual information from brain signals in the latent space and optimizing high-level features for facial identity control using identity loss. Specifically, our framework uses StyleGAN inversion to extract hierarchical latent codes from images, which are then bridged to fMRI data through transformation blocks. Additionally, we introduce a multi-stage refinement method to enhance the accuracy of reconstructed faces, which involves progressively updating fMRI latent codes with custom loss functions designed for both feature- and image-wise optimization. Our experimental results demonstrate that our proposed framework effectively achieves two critical objectives: (1) accurate facial image reconstruction from fMRI data and (2) preservation of identity characteristics with a high level of consistency. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Data mining; Decoding; Deep learning; Image enhancement; Learning systems; Metadata; Brain-driven image reconstruction; Cross-modal; Facial images; fMRI data; Images reconstruction; Learning techniques; Neural decoding; Stimulus reconstruction; Stylegan inversion; Visual stimulus; Image reconstruction},
	type  = {Article}}





@inproceedings{Song2024Ieee,
  author  = {Song, Yahao and Liu, Tianyi and Sun, Chao and Zhang, Yuwei and Zheng, Minqian and Zhang, Milin},
  booktitle  = {2024 IEEE Biomedical Circuits and Systems Conference (BioCAS)}, 
  title  = {A Low-Power Wireless 128-Channel Neural Interface Circuit for Multiregional Brain Recording}, 
  year  = {2024},
  volume  = {},
  number  = {},
  pages  = {1--5},
  abstract  = {This paper proposes a low-power, wireless 128channel neural interface circuit, comprising a 40 nm recorder circuit and a WiFi module. The proposed design enables realtime monitoring and neural signal decoding during behavioral studies, facilitating high-density, long-term tracking of neural signals in freely moving subjects. The recorder circuit integrates 16 -bit resolution at a maximum rate of 32 k samples per second (sps), with a power consumption of 3.43 mW and an area of $6.27 \mathrm{~mm}^{2}$,achieving the lowest normalized power consumption of $0.84 \mu \mathrm{~W} / \mathrm{ch} / \mathrm{ksps}$ among state-of-the-art works. The VoltageControlled Oscillator (VCO) Analog Front Ends (AFEs) feature an input-referred noise of $0.66 \mu \mathrm{~V}_{\text {rms}}$ within the $0.5-60 \mathrm{~Hz}$ range. The digital backend allows flexible channel configuration and includes an on-chip CIC filter to reduce out-of-band noise. The WiFi module, connected via QSPI, facilitates wireless control and data transmission. The proposed circuit was validated through electrocorticogram (ECoG) measurements in rodent subjects, demonstrating its reliability and flexibility.},
  keywords  = {Wireless communication;Power demand;Voltage-controlled oscillators;Noise;Recording;Decoding;System-on-chip;Data communication;Integrated circuit reliability;Wireless fidelity;Neural Signal Recording;Biomedical Signal Processing;ECoG;Wireless},
  doi  = {10.1109/BioCAS61083.2024.10798169},
  issn  = {2766-4465},
  month  = {Oct}}



@inproceedings{Sugimoto2024Ieee,
  author  = {Sugimoto, Yuma and Pongthanisorn, Goragod and Capi, Genci},
  booktitle  = {2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title  = {Image Generation using EEG data: A Contrastive Learning based Approach}, 
  year  = {2024},
  volume  = {},
  number  = {},
  pages  = {794--798},
  abstract  = {In recent years, there has been active research on Neural Decoding aims to decrypt perceptual cognitive content, recall content, and motor information directly from brain signals. Simultaneously, deep learning has been widely implemented especially in generative models. Diverse architectures and learning methods have been formulated and applied across numerous fields. In this work, we focus on generating perceptual and cognitive contents using brain electroencephalography (EEG) data. We propose and demonstrate the efficacy of contrastive learning to generate images only from EEG data. We compare the performance of two electrode settings 1) visual cortex and 2) motor cortex.},
  keywords  = {Learning systems;Electrodes;Deep learning;Visualization;Image synthesis;Contrastive learning;Motors;Visual Imagery;EEG signals;Contrastive Learning},
  doi  = {10.1109/CCECE59415.2024.10667256},
  issn  = {2576-7046},
  month  = {Aug}}



@inproceedings{Wang2024Ieeecvf,
  author  = {Wang, Shizun and Liu, Songhua and Tan, Zhenxiong and Wang, Xinchao},
  booktitle  = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title  = {MindBridge: A Cross-Subject Brain Decoding Framework}, 
  year  = {2024},
  volume  = {},
  number  = {},
  pages  = {11333--11342},
  abstract  = {Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI). Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained. This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited data availability for new subjects in real-world scenarios hampers the performance of decoding models. In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model. Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant representation learning. Notably, by cycle re-construction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo data augmentation. Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject. Experimental results demonstrate MindBridge's ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models. Fur-thermore, with limited data for a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models. This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI data in real-world scenarios. Project page: https://littlepure2333.github.io/MindBridge},
  keywords  = {Representation learning;Adaptation models;Neuroscience;Accuracy;Biological system modeling;Functional magnetic resonance imaging;Brain modeling;Brain decoding;Cross-subject;diffusion model},
  doi  = {10.1109/CVPR52733.2024.01077},
  issn  = {2575-7075},
  month  = {June}}



@inproceedings{Wang2024Photonics,
  author  = {Wang, Yaqi and Gui, Renzhou and Zhu, Wenbo and Yin, Yumiao and Tong, Meisong},
  booktitle  = {2024 Photonics & Electromagnetics Research Symposium (PIERS)}, 
  title  = {VTVBrain: A Two-stage Brain Encoding Model for Decoding Key Neural Responses in Multimodal Contexts}, 
  year  = {2024},
  volume  = {},
  number  = {},
  pages  = {1--9},
  abstract  = {In the field of cognitive neuroscience, understanding how the brain processes multimodal complex stimuli is a long-standing and complex challenge. In this study, we propose a novel two-stage brain coding model called "VTVBrain" that focuses on decoding key neural responses in multimodal environments. In the first stage, we built a high-dimensional multimodal latent space pre-training model using an attentional mechanism-based variational autoencoder, aiming to capture and encode the main perceptual processes of observers when faced with multimodal stimuli integrating textual and visual information. In the second stage, the Versatile Diffusion model is utilized for image reconstruction of the first stage primary representational images for high-level perception. The proposed model further refines the latent space representation approach, explores the relationship between the brain’s neural responses and complex natural scenes, and provides insight into how the brain integrates and processes information from different senses at a higher level. The introduction of the VTVBrain model not only brings a new perspective to the field of neural decoding, but also has a great potential for application in the development of brain-like artificial intelligence and future cognitive neuroscience research. The introduction of the VTVBrain model not only brings a new perspective to the field of neural decoding, but also has great potential for the development of brain-like artificial intelligence and future cognitive neuroscience research.},
  keywords  = {Cognitive neuroscience;Semantics;Brain modeling;Diffusion models;Encoding;Decoding;Space exploration},
  doi  = {10.1109/PIERS62282.2024.10618584},
  issn  = {2831-5804},
  month  = {April}}



@article{Wang2024Efficient,
	author  = {Wang, Yun},
	title  = {Efficient Neural Decoding Based on Multimodal Training},
	year  = {2024},
	journal  = {Brain Sciences},
	volume  = {14},
	number  = {10},
	pages  = {},
	doi  = {10.3390/brainsci14100988},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207646135&doi=10.3390%2Fbrainsci14100988&partnerID=40&md5=dcea41147346389c36e5c9e10c1b8362},
	abstract  = {Background/Objectives: Neural decoding methods are often limited by the performance of brain encoders, which map complex brain signals into a latent representation space of perception information. These brain encoders are constrained by the limited amount of paired brain and stimuli data available for training, making it challenging to learn rich neural representations. Methods: To address this limitation, we present a novel multimodal training approach using paired image and functional magnetic resonance imaging (fMRI) data to establish a brain masked autoencoder that learns the interactions between images and brain activities. Subsequently, we employ a diffusion model conditioned on brain data to decode realistic images. Results: Our method achieves high-quality decoding results in semantic contents and low-level visual attributes, outperforming previous methods both qualitatively and quantitatively, while maintaining computational efficiency. Additionally, our method is applied to decode artificial patterns across region of interests (ROIs) to explore their functional properties. We not only validate existing knowledge concerning ROIs but also unveil new insights, such as the synergy between early visual cortex and higher-level scene ROIs, as well as the competition within the higher-level scene ROIs. Conclusions: These findings provide valuable insights for future directions in the field of neural decoding. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {adult; article; autoencoder; diffusion; electroencephalogram; female; functional magnetic resonance imaging; human; human experiment; male; nerve cell; nonhuman; training; visual cortex},
	type  = {Article}}





@conference{Wei2024Multimodal,
	author  = {Wei, Yayun and Cao, Lei and Li, Hao and Dong, Yilin},
	title  = {MB2C: Multimodal Bidirectional Cycle Consistency for Learning Robust Visual Neural Representations},
	year  = {2024},
	pages  = {8992 - 9000},
	doi  = {10.1145/3664647.3681292},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206805009&doi=10.1145%2F3664647.3681292&partnerID=40&md5=6b793a9799d08c0190b95dab6eaeb4d0},
	abstract  = {Decoding human visual representations from brain activity data is a challenging but arguably essential task with an understanding of the real world and the human visual system. However, decoding semantically similar visual representations from brain recordings is difficult, especially for electroencephalography (EEG), which has excellent temporal resolution but suffers from spatial precision. Prevailing methods mainly focus on matching brain activity data with corresponding stimuli-responses using contrastive learning. They rely on massive and high-quality paired data and omit semantically aligned modalities distributed in distinct regions of the latent space. This paper proposes a novel Multimodal Bidirectional Cycle Consistency (MB2C) framework for learning robust visual neural representations. Specifically, we utilize dual-GAN to generate modality-related features and inversely translate back to the corresponding semantic latent space to close the modality gap and guarantee that embeddings from different modalities with similar semantics are in the same region of representation space. We perform zero-shot tasks on the ThingsEEG dataset. Additionally, we conduct EEG classification and image reconstruction on both the ThingsEEG and EEGCVPR40 datasets, achieving state-of-the-art performance compared to other baselines. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Contrastive Learning; Semantic Segmentation; Zero-shot learning; Brain activity; Cycle consistency; Human visual; Images reconstruction; Multi-modal; Multi-modal learning; Neural decoding; Neural representations; Real-world; Visual representations; Electroencephalography},
	type  = {Conference paper}}





@conference{Xie2024Brainram,
	author  = {Xie, Dian and Zhao, Peiang and Zhang, Jiarui and Wei, Kangqi and Ni, Xiaobao and Xia, Jiong},
	title  = {BrainRAM: Cross-Modality Retrieval-Augmented Image Reconstruction from Human Brain Activity},
	year  = {2024},
	pages  = {3994 - 4003},
	doi  = {10.1145/3664647.3681296},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209774663&doi=10.1145%2F3664647.3681296&partnerID=40&md5=a26f218cbd721d6e49a0715bb0a714cf},
	abstract  = {Reconstructing visual stimuli from brain activities is crucial for deciphering the underlying mechanism of the human visual system. While recent studies have achieved notable results by leveraging deep generative models, challenges persist due to the lack of large-scale datasets and the inherent noise from non-invasive measurement methods. In this study, we draw inspiration from the mechanism of human memory and propose BrainRAM, a novel two-stage dual-guided framework for visual stimuli reconstruction. BrainRAM incorporates a Retrieval-Augmented Module (RAM) and diffusion prior to enhance the quality of reconstructed images from the brain. Specifically, in stage I, we transform fMRI voxels into the latent space of image and text embeddings via diffusion priors, obtaining preliminary estimates of the visual stimuli's semantics and structure. In stage II, based on previous estimates, we retrieve data from the LAION-2B-en dataset and employ the proposed RAM to refine them, yielding high-quality reconstruction results. Extensive experiments demonstrate that our BrainRAM outperforms current state-of-the-art methods both qualitatively and quantitatively, providing a new perspective for visual stimuli reconstruction. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain mapping; Image denoising; Image enhancement; Image retrieval; Noninvasive medical procedures; Augmented images; Brain activity; Cross modality; Human brain; Human Visual System; Images reconstruction; Neural decoding; Retrieval-augmented generation; Stimulus reconstruction; Visual stimulus; Image reconstruction},
	type  = {Conference paper}}





@article{Yang2024Functional,
	author  = {Yang, Lingxiao and Zhen, Hui and Li, Le and Li, Yuanning and Zhang, Han and Xie, Xiaohua and Zhang, Ruyuan},
	title  = {Functional diversity of visual cortex improves constraint-free natural image reconstruction from human brain activity},
	year  = {2024},
	journal  = {Fundamental Research},
	pages  = {},
	doi  = {10.1016/j.fmre.2023.08.010},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181810312&doi=10.1016%2Fj.fmre.2023.08.010&partnerID=40&md5=0d6239849fc8fc173d179885262f582a},
	abstract  = {Previous brain decoding studies using functional magnetic resonance imaging (fMRI) have greatly advanced our understanding of human visual coding and non-invasive brain-machine interfaces. However, most of these studies focus on classifying a limited number of image categories or reconstructing visual images with additional information, e.g., semantic categories and textual cues. Constraint-free visual reconstruction remains scarce. Here, we propose a generative network based on the functional diversity of the human visual cortex (FDGen) that takes multivariate brain activity as input and directly reconstructs natural images perceived by observers without any additional cues (semantic categories or textual description). Our FDGen is augmented by two bio-inspired computational modules. Based on the functional specializations of the human visual cortex, we propose a new function-based input module (FIM) that projects responses from different brain regions into separate feature spaces. Second, inspired by human attention, we construct a computational module to derive attentive feature weights at the function level to refine the feature map. These function-selection modules (FSMs) allow the network to dynamically select multiscale visual information during the generation process. We test FDGen on the popular fMRI datasets of natural images and achieve highly robust performance. Our work represents an important step forward in the development of fMRI-based brain decoding algorithms and highlights the utility of neuroscience theories in the design of deep learning models. © 2024 Elsevier B.V., All rights reserved.},	type  = {Article}}





@article{Yang2024Reconstructing,
  author  = {Yang, Xiaomeng and Xiong, Xinzhu and Li, Xufei and Lian, Qi and Zhu, Junming and Zhang, Jianmin and Qi, Yu and Wang, Yueming},
  journal  = {IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title  = {Reconstructing Multi-Stroke Characters From Brain Signals Toward Generalizable Handwriting Brain–Computer Interfaces}, 
  year  = {2024},
  volume  = {32},
  number  = {},
  pages  = {4230--4239},
  abstract  = {Handwriting Brain-Computer Interfaces (BCIs) provides a promising communication avenue for individuals with paralysis. While English-based handwriting BCIs have achieved rapid typewriting with 26 lowercase letters (mostly containing one stroke each), it is difficult to extend to complex characters, especially those with multiple strokes and large character sets. The Chinese characters, including over 3500 commonly used characters with 10.3 strokes per character on average, represent a highly complex writing system. This paper proposes a Chinese handwriting BCI system, which reconstructs multi-stroke handwriting trajectories from brain signals. Through the recording of cortical neural signals from the motor cortex, we reveal distinct neural representations for stroke-writing and pen-lift phases. Leveraging this finding, we propose a stroke-aware approach to decode stroke-writing trajectories and pen-lift movements individually, which can reconstruct recognizable characters (accuracy of 86% with 400 characters). Our approach demonstrates high stability over 5 months, shedding light on generalized and adaptable handwriting BCIs.},
  keywords  = {Writing;Decoding;Trajectory;Character recognition;Long short term memory;Stroke (medical condition);Recording;Image reconstruction;Monitoring;Micromechanical devices;Brain-computer interfaces (BCIs);neural signal decoding;handwriting reconstruction;multi-stroke characters},
  doi  = {10.1109/TNSRE.2024.3492191},
  issn  = {1558-0210},
  month  = {}}



@article{Yu2024Deep,
	author  = {Yu, Haitao and Hu, Zhiwen and Zhao, Quanfa and Liu, Jing},
	title  = {Deep source transfer learning for the estimation of internal brain dynamics using scalp EEG},
	year  = {2024},
	journal  = {Cognitive Neurodynamics},
	volume  = {18},
	number  = {6},
	pages  = {3507 - 3520},
	doi  = {10.1007/s11571-024-10149-2},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198650072&doi=10.1007%2Fs11571-024-10149-2&partnerID=40&md5=6f56db1fa6866da24b4ca6c732f0b10f},
	abstract  = {Electroencephalography (EEG) provides high temporal resolution neural data for brain-computer interfacing via noninvasive electrophysiological recording. Estimating the internal brain activity by means of source imaging techniques can further improve the spatial resolution of EEG and enhance the reliability of neural decoding and brain-computer interaction. In this work, we propose a novel EEG data-driven source imaging scheme for precise and efficient estimation of macroscale spatiotemporal brain dynamics across thalamus and cortical regions with deep learning methods. A deep source imaging framework with a convolutional-recurrent neural network is designed to estimate the internal brain dynamics from high-density EEG recordings. Moreover, a brain model including 210 cortical regions and 16 thalamic nuclei is established based on human brain connectome to provide synthetic training data, which manifests intrinsic characteristics of underlying brain dynamics in spontaneous, stimulation-evoked, and pathological states. Transfer learning algorithm is further applied to the trained network to reduce the dynamical differences between synthetic and realistic EEG. Extensive experiments exhibit that the proposed deep-learning method can accurately estimate the spatial and temporal activity of brain sources and achieves superior performance compared to the state-of-the-art approaches. Moreover, the EEG data-driven source imaging framework is effective in the location of seizure onset zone in epilepsy and reconstruction of dynamical thalamocortical interactions during sensory processing of acupuncture stimulation, implying its applicability in brain-computer interfacing for neuroscience research and clinical applications. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Article; brain cortex; connectome; convolutional neural network; deep learning; electroencephalogram; image reconstruction; learning algorithm; neuroimaging; recurrent neural network; thalamocortical tract; thalamus; transfer learning algorithm},
	type  = {Article}}





@article{Berezutskaya2023Direct,
	author  = {Berezutskaya, Julia and Freudenburg, Zachary V. and Vansteensel, Mariska J. and Aarnoutse, Erik J. and Ramsey, Nick Franciscus and Van Gerven, Marcel A.J.},
	title  = {Direct speech reconstruction from sensorimotor brain activity with optimized deep learning models},
	year  = {2023},
	journal  = {Journal of Neural Engineering},
	volume  = {20},
	number  = {5},
	pages  = {},
	doi  = {10.1088/1741-2552/ace8be},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171900901&doi=10.1088%2F1741-2552%2Face8be&partnerID=40&md5=698777bb411c75962425db25d45880b5},
	abstract  = {Objective. Development of brain-computer interface (BCI) technology is key for enabling communication in individuals who have lost the faculty of speech due to severe motor paralysis. A BCI control strategy that is gaining attention employs speech decoding from neural data. Recent studies have shown that a combination of direct neural recordings and advanced computational models can provide promising results. Understanding which decoding strategies deliver best and directly applicable results is crucial for advancing the field. Approach. In this paper, we optimized and validated a decoding approach based on speech reconstruction directly from high-density electrocorticography recordings from sensorimotor cortex during a speech production task. Main results. We show that (1) dedicated machine learning optimization of reconstruction models is key for achieving the best reconstruction performance; (2) individual word decoding in reconstructed speech achieves 92%-100% accuracy (chance level is 8%); (3) direct reconstruction from sensorimotor brain activity produces intelligible speech. Significance. These results underline the need for model optimization in achieving best speech decoding results and highlight the potential that reconstruction-based speech decoding from sensorimotor cortex can offer for development of next-generation BCI technology for communication. © 2023 Elsevier B.V., All rights reserved.},	keywords  = {Audio recordings; Brain; Brain computer interface; Decoding; Electroencephalography; Electrophysiology; Learning systems; Neurophysiology; Speech communication; Audio reconstruction; Brain activity; Electrocorticography; Interface control; Interface technology; Learning models; Neural decoding; Sensorimotor cortex; Sensorimotors; Speech decoding; Deep neural networks; adult; Article; artificial neural network; audio recording; classification algorithm; comparative study; convolutional neural network; deep learning; electrocorticography; electroencephalogram; female; human; human tissue; image reconstruction; leave one out cross validation; logistic regression analysis; machine learning; male; middle aged; multilayer perceptron; normal human; random forest; recurrent neural network; sensorimotor cortex; speech analysis; speech intelligibility; speech perception; word recognition; young adult; interpersonal communication; procedures; speech; Brain-Computer Interfaces; Communication; Deep Learning; Humans; Sensorimotor Cortex; Speech},
	type  = {Article}}





@article{Cheng2023Reconstructing,
	author  = {Cheng, Fan L. and Horikawa, Tomoyasu and Majima, Kei and Tanaka, Misato and Abdelhack, Mohamed and Aoki, Shuntaro C. and Hirano, Jin and Kamitani, Yukiyasu},
	title  = {Reconstructing visual illusory experiences from human brain activity},
	year  = {2023},
	journal  = {Science Advances},
	volume  = {9},
	number  = {46},
	pages  = {},
	doi  = {10.1126/sciadv.adj3906},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178499453&doi=10.1126%2Fsciadv.adj3906&partnerID=40&md5=b735cb085d5fa94c35af612a86ce7fab},
	abstract  = {Visual illusions provide valuable insights into the brain’s interpretation of the world given sensory inputs. However, the precise manner in which brain activity translates into illusory experiences remains largely unknown. Here, we leverage a brain decoding technique combined with deep neural network (DNN) representations to reconstruct illusory percepts as images from brain activity. The reconstruction model was trained on natural images to establish a link between brain activity and perceptual features and then tested on two types of illusions: illusory lines and neon color spreading. Reconstructions revealed lines and colors consistent with illusory experiences, which varied across the source visual cortical areas. This framework offers a way to materialize subjective experiences, shedding light on the brain’s internal representations of the world. © 2023 Elsevier B.V., All rights reserved.},
	keywords  = {Deep neural networks; Image reconstruction; Neurophysiology; Brain activity; Brain decoding; Decoding techniques; Human brain; Natural images; Neon color spreading; Network representation; Perceptual feature; Sensory input; Visual illusions; Brain; artificial neural network; brain; human; illusion; pattern recognition; vision; visual cortex; Form Perception; Humans; Illusions; Neural Networks, Computer; Visual Cortex; Visual Perception},
	type  = {Article}}





@article{Du2023Review,
	author  = {Du, Changde and Zhou, Qiongyi and Liu, Che and He, Huiguang},
	title  = {Review of visual neural encoding and decoding methods in fMRI; fMRI 的视觉神经信息编解码方法综述},
	year  = {2023},
	journal  = {Journal of Image and Graphics},
	volume  = {28},
	number  = {2},
	pages  = {372 - 384},
	doi  = {10.11834/jig.220525},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151501435&doi=10.11834%2Fjig.220525&partnerID=40&md5=761f69e1ead77263bb8d338d08462edb},
	abstract  = {The relationship between human visual experience and evoked neural activity is central to the field of computational neuroscience. The purpose of visual neural encoding and decoding is to study the relationship between visual stimuli and the evoked neural activity by using neuroimaging data such as functional magnetic resonance imaging (fMRI). Neural encoding researches attempt to predict the brain activity according to the presented external stimuli, which contributes to the development of brain science and brain-like artificial intelligence. Neural decoding researches attempt to predict the information about external stimuli by analyzing the observed brain activities, which can interpret the state of human visual perception and promote the development of brain computer interface (BCI). Therefore, fMRI based visual neural encoding and decoding researches have important scientific significance and engineering value. Typically, the encoding models are based on the specific computations that are thought to underlie the observed brain responses for specific visual stimuli. Early studies of visual neural encoding relied heavily on Gabor wavelet features because these features are very good at modeling brain responses in the primary visual cortex. Recently, given the success of deep neural networks (DNNs) in classifying objects in natural images, the representations within these networks have been used to build encoding models of cortical responses to complex visual stimuli. Most of the existing decoding studies are based on multi-voxel pattern analysis (MVPA) method, but brain connectivity pattern is also a key feature of the brain state and can be used for brain decoding. Although recent studies have demonstrated the feasibility of decoding the identity of binary contrast patterns, handwritten characters, human facial images, natural picture/video stimuli and dreams from the corresponding brain activation patterns, the accurate reconstruction of the visual stimuli from fMRI still lacks adequate examination and requires plenty of efforts to improve. On the basis of summarizing the key technologies and research progress of fMRI based visual neural encoding and decoding, this paper further analyzes the limitations of existing visual neural encoding and decoding methods. In terms of visual neural encoding, the development process of population receptive field (pRF) estimation method is introduced in detail. In terms of visual neural decoding, it is divided into semantic classification, image identification and image reconstruction according to task types, and the representative research work of each part and the methods used are described in detail. From the perspective of machine learning, semantic classification is a single label or multi-label classification problem. Simple visual stimuli only contain a single object, while natural visual stimuli often contain multiple semantic labels. For example, an image may contain flowers, water, trees, cars, etc. Predicting one or more semantic labels of the visual stimulus from the brain signal is called semantic decoding. Image retrieval based on brain signal is also a common visual decoding task where the model is created to “decode” neural activity by retrieving a picture of what a person has just seen or imagined. In particular, the reconstruction techniques of simple image, face image and complex natural image based on deep generative models (including variational auto-encoders (VAEs) and generative adversarial networks (GANs)) are introduced in the part of image reconstruction. Secondly, 10 open source datasets commonly used in this field were statistically sorted out, and the sample size, number of subjects, types of stimuli, research purposes and download links of the datasets were summarized in detail. These datasets have made important contributions to the development of this field. Finally, we introduce the commonly used measurement metrics of visual neural encoding and decoding model in detail, analyze the shortcomings of current visual neural encoding and decoding methods, propose feasible suggestions for improvement, and show the future development directions. Specifically, for neural encoding, the existing methods still have the following shortcomings: 1) the computational models are mostly based on the existing neural network architecture, which cannot reflect the real biological visual information flow; 2) due to the selective attention of each person in the visual perception and the inevitable noise in the fMRI data collection, individual differences are significant; 3) the sample size of the existing fMRI data set is insufficient; 4) most researchers construct feature spaces of neural encoding models based on fixed types of pre-trained neural networks (such as AlexNet), causing problems such as insufficient diversity of visual features. On the other hand, although the existing visual neural decoding methods perform well in the semantic classification and image identification tasks, it is still very difficult to establish an accurate mapping between visual stimuli and visual neural signals, and the results of image reconstruction are often blurry and lack of clear semantics. Moreover, most of the existing visual neural decoding methods are based on linear transformation or deep network transformation of visual images, lacking exploration of new visual features. Factors that hinder researchers from effectively decoding visual information and reconstructing images or videos mainly include high dimension of fMRI data, small sample size and serious noise. In the future, more advanced artificial intelligence technology should be used to develop more effective methods of neural encoding and decoding, and try to translate brain signals into images, video, voice, text and other multimedia content, so as to achieve more BCI applications. The significant research directions include 1) multi-modal neural encoding and decoding based on the union of image and text; 2) brain-guided computer vision model training and enhancement; 3) visual neural encoding and decoding based on the high efficient features of large-scale pre-trained models. In addition, since brain signals are characterized by complexity, high dimension, large individual diversity, high dynamic nature and small sample size, future research needs to combine computational neuroscience and artificial intelligence theories to develop visual neural encoding and decoding methods with higher robustness, adaptability and interpretability. © 2023 Elsevier B.V., All rights reserved.},	type  = {Article}}





@conference{Ferrante2023Multimodal,
	author  = {Ferrante, Matteo and Boccato, Tommaso and Ozcelik, Furkan and VanRullen, Rufin and Toschi, Nicola},
	title  = {Multimodal decoding of human brain activity into images and text},
	year  = {2023},
	journal  = {Proceedings of Machine Learning Research},
	volume  = {243},
	pages  = {11 - 26},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196100263&partnerID=40&md5=d96db970d5caf79649c6b9e8f4366bc8},
	abstract  = {Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation. We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline based on latent diffusion models. The method involves training regularized linear regression models between brain activity and extracted features. Additionally, we incorporated depth maps from the ControlNet model to further guide the reconstruction process. We propose a multimodal based approach that leverages similarities between neural and deep learning representations and by learning alignment between these spaces, we produce textual description and image reconstruction from brain activity. We evaluate our methods using quantitative metrics for both generated captions and images. Our brain captioning approach outperforms existing methods, while our image reconstruction pipeline generates plausible images with improved spatial relationships. In conclusion, we demonstrate significant progress in brain decoding, showcasing the enormous potential of integrating vision and language to better understand human cognition. Our approach provides a flexible platform for future research, with potential applications based on a combination of high-level semantic information coming from text and low-level image shape information coming from depth maps and initial guess images. © 2024 Elsevier B.V., All rights reserved.},
	keywords  = {Decoding; Deep learning; Diffusion; Image enhancement; Image reconstruction; Magnetic resonance imaging; Neurophysiology; Pipelines; Regression analysis; Semantics; Brain activity; Brain decoding; Brain process; Depthmap; Diffusion model; Functional magnetic resonance imaging; Human brain; Images reconstruction; Multi-modal; Visual information; Brain},
	type  = {Conference paper}}





@conference{Gu2023Decoding,
	author  = {Gu, Zijin and Jamison, Keith Wakefield and Kuceyeski, Amy F. and Rory Sabuncu, Mert Rory},
	title  = {Decoding natural image stimuli from fMRI data with a surface-based convolutional network},
	year  = {2023},
	journal  = {Proceedings of Machine Learning Research},
	volume  = {227},
	pages  = {107 - 118},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189347544&partnerID=40&md5=35091dda16c7655448012c6cd5e5c285},
	abstract  = {Due to the low signal-to-noise ratio and limited resolution of functional MRI data, and the high complexity of natural images, reconstructing a visual stimulus from human brain fMRI measurements is a challenging task. In this work, we propose a novel approach for this task, which we call Cortex2Image, to decode visual stimuli with high semantic fidelity and rich fine-grained detail. In particular, we train a surface-based convolutional network model that maps from brain response to semantic image features first (Cortex2Semantic). We then combine this model with a high-quality image generator (Instance-Conditioned GAN) to train another mapping from brain response to fine-grained image features using a variational approach (Cortex2Detail). Image reconstructions obtained by our proposed method achieve state-of-the-art semantic fidelity, while yielding good fine-grained similarity with the ground-truth stimulus. Our code is available on https://github.com/zijin-gu/meshconv-decoding.git. © 2024 Elsevier B.V., All rights reserved.},	keywords  = {Brain mapping; Convolution; Decoding; Magnetic resonance imaging; Semantics; Signal to noise ratio; Brain response; Convolutional networks; Fine grained; Functional MRI; Image features; Images reconstruction; Natural images; Neural decoding; Surface-based; Visual stimulus; Image reconstruction},
	type  = {Conference paper}}





@inproceedings{Gui2023Photonics,
  author  = {Gui, Renzhou and Zhang, Aobo and Liu, Shuai and Tong, Mei Song},
  booktitle  = {2023 Photonics & Electromagnetics Research Symposium (PIERS)}, 
  title  = {Analysis of Functional Areas of Human Brain Based on Reconstructed Images of DMFG-generated Countermeasure Network}, 
  year  = {2023},
  volume  = {},
  number  = {},
  pages  = {1131--1138},
  abstract  = {The structure of human brain is complex, and fMRI data can be used to reveal the working mechanism of human brain. We construct a generative confrontation deep learning network based on DMFG-loss function. Using this network, we can not only reconstruct the simple scene images perceived and imagined by human brain with high precision, but also achieve good results for the restoration and reconstruction of complex natural images. In addition, we propose to set the detection threshold based on the constant false alarm algorithm. Further, we explore the distribution of brain sensitive areas, and make a deep analysis of the impact of different regions on image reconstruction. The contribution ratio of specific brain regions to the image reconstruction of human brain is gived. This will help to explore the unknown areas of human brain and reveal the mechanism of human brain operation. It has broad application prospects in brain computer interaction and human brain decoding.},
  keywords  = {Training;Deep learning;Support vector machines;Visualization;Image color analysis;Fitting;Functional magnetic resonance imaging},
  doi  = {10.1109/PIERS59004.2023.10221339},
  issn  = {2831-5804},
  month  = {July}}



@article{Hua2023Neural,
	author  = {Hua, Lin and Gao, Fei and Leong, Chantat and Yuan, Zhen},
	title  = {Neural decoding dissociates perceptual grouping between proximity and similarity in visual perception},
	year  = {2023},
	journal  = {Cerebral Cortex},
	volume  = {33},
	number  = {7},
	pages  = {3803 - 3815},
	doi  = {10.1093/cercor/bhac308},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159549724&doi=10.1093%2Fcercor%2Fbhac308&partnerID=40&md5=d59ef52a17392dc7345eadb6c638313c},
	abstract  = {Unlike single grouping principle, cognitive neural mechanism underlying the dissociation across two or more grouping principles is still unclear. In this study, a dimotif lattice paradigm that can adjust the strength of one grouping principle was used to inspect how, when, and where the processing of two grouping principles (proximity and similarity) were carried out in human brain. Our psychophysical findings demonstrated that similarity grouping effect was enhanced with reduced proximity effect when the grouping cues of proximity and similarity were presented simultaneously. Meanwhile, EEG decoding was performed to reveal the specific cognitive patterns involved in each principle by using time-resolved MVPA. More importantly, the onsets of dissociation between 2 grouping principles coincided within 3 time windows: the early-stage proximity-defined local visual element arrangement in middle occipital cortex, the middle-stage processing for feature selection modulating low-level visual cortex such as inferior occipital cortex and fusiform cortex, and the high-level cognitive integration to make decisions for specific grouping preference in the parietal areas. In addition, it was discovered that the brain responses were highly correlated with behavioral grouping. Therefore, our study provides direct evidence for a link between the human perceptual space of grouping decision-making and neural space of brain activation patterns. © 2023 Elsevier B.V., All rights reserved.},	keywords  = {article; brain cortex; decision making; dissociation; electroencephalogram; feature selection; human; human experiment; occipital cortex; vision; visual cortex},
	type  = {Article}}





@article{Luo2023Visual,
  author  = {Luo, Jie and Cui, Weigang and Liu, Jingyu and Li, Yang and Guo, Yuzhu and Xu, Song and Wang, Lina},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems}, 
  title  = {Visual Image Decoding of Brain Activities Using a Dual Attention Hierarchical Latent Generative Network With Multiscale Feature Fusion}, 
  year  = {2023},
  volume  = {15},
  number  = {2},
  pages  = {761--773},
  abstract  = {Reconstructing visual stimulus from human brain activity measured with functional magnetic resonance imaging (fMRI) is a challenging decoding task for revealing the visual system. Recent deep learning approaches commonly neglect the relationship between hierarchical image features and different regions of the visual cortex, and fail to use global and local image features in reconstructing visual stimulus. To address these issues, in this article, a novel neural decoding framework is proposed by using a dual attention (DA) hierarchical latent generative network with multiscale feature fusion (DA-HLGN-MSFF) method. Specifically, the fMRI data are first encoded to hierarchical features of our image encoder network, which employs a multikernel convolution block to extract the multiscale spatial information of images. In order to reconstruct the perceived images and further improve the performance of our generator network, a DA block based on the channel-spatial attention mechanism is then proposed to exploit the interchannel relationships and spatial long-range dependencies of features. Moreover, a multiscale feature fusion block is finally adopted to aggregate the global and local information of features at different scales and synthesize the final reconstructed images in the generator network. Competitive experimental results on two public fMRI data sets demonstrate that our method is able to achieve promising reconstructing performance compared with the state-of-the-art methods. The codes of our proposed DA-HLGN-MSFF method will be open access on https://github.com/ljbuaa/HLDAGN.},
  keywords  = {Image reconstruction;Functional magnetic resonance imaging;Visualization;Generators;Decoding;Generative adversarial networks;Feature extraction;Deep neural network (DNN);functional magnetic resonance imaging (fMRI) decoding;generative adversarial network (GAN);image reconstruction;visual cortex},
  doi  = {10.1109/TCDS.2022.3181469},
  issn  = {2379-8939},
  month  = {June}}



@inproceedings{Luo2023Ieee,
  author  = {Luo, Ying and Kobayashi, Ichiro},
  booktitle  = {2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title  = {BrainLM: Estimation of Brain Activity Evoked Linguistic Stimuli Utilizing Large Language Models}, 
  year  = {2023},
  volume  = {},
  number  = {},
  pages  = {1904--1909},
  abstract  = {In recent years, with the recent remarkable development of large-scale language models in natural language processing research, there has been an increasing number of studies employing large-scale language models to investigate the information processing mechanisms of encoding and decoding in the brain. In this study, we developed a new pre-trained language model, BrainLM, which incorporates paired data of brain activity induced by text and stimuli, and verified the accuracy of estimating brain states from natural language in multiple NLP tasks. In essence, our research has achieved several noteworthy accomplishments. Firstly, we successfully developed a multimodal model that incorporates both brain and text. Subsequently, we conducted bi-directional experiments to validate the model and ensure the reliability of both brain encoding and decoding processes. Furthermore, we performed meticulous comparative experiments, wherein we introduced 20 state-of-the-art (SOTA) language models as a control group. Our findings reveal that our proposed model outperforms superior brain encoding ability compared to the control group. Lastly, we designed a discrete Autoencoder module that extracts brain features. This module can be utilized independently to extract brain features in a wider range of brain decoding studies beyond fMRI.},
  keywords  = {Brain modeling;Feature extraction;Encoding;Data models;Natural language processing;Decoding;Task analysis},
  doi  = {10.1109/SMC53992.2023.10394227},
  issn  = {2577-1655},
  month  = {Oct}}



@inproceedings{Meng2023International,
  author  = {Meng, Lu and Yang, Chuanhao},
  booktitle  = {2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title  = {Semantics-guided hierarchical feature encoding generative adversarial network for natural image reconstruction from brain activities}, 
  year  = {2023},
  volume  = {},
  number  = {},
  pages  = {1--9},
  abstract  = {The use of deep learning methods to decode visual perception images from brain activity recorded by fMRI has received a lot of attention. However, limited fMRI data make the task of visual reconstruction challenging. Inspired by hierarchical encoding of the visual cortex and the theory of brain homology with convolutional neural networks (CNNs), we propose a novel neural decoding model called hierarchical semantic generative adversarial network (HS-GAN). Specifically, we use CNN-based image encoder to extract hierarchical and semantic features of visually stimulus images. Then a neural decoder is used to decode hierarchical and semantic features from fMRI. In order to take full advantage of the information from different visual cortexes, we construct a generator with self-attention modules and skip connections to fuse the image features of different layers. In model training, adversarial learning is introduced to realize more natural image reconstruction. Compared to existing advanced methods, our method significantly improves the naturalness and fidelity of reconstructed images.},
  keywords  = {Visualization;Image coding;Semantics;Functional magnetic resonance imaging;Generative adversarial networks;Feature extraction;Brain modeling;brain decoding;fMRI;GAN;image reconstruction;deep learning},
  doi  = {10.1109/IJCNN54540.2023.10191903},
  issn  = {2161-4407},
  month  = {June}}



@article{Meng2023Dualguided,
	author  = {Meng, Lu and Yang, Chuanhao},
	title  = {Dual-Guided Brain Diffusion Model: Natural Image Reconstruction from Human Visual Stimulus fMRI},
	year  = {2023},
	journal  = {Bioengineering},
	volume  = {10},
	number  = {10},
	pages  = {},
	doi  = {10.3390/bioengineering10101117},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175057353&doi=10.3390%2Fbioengineering10101117&partnerID=40&md5=da8e4b76f7248f7cc2b802c7e3fa4a7b},
	abstract  = {The reconstruction of visual stimuli from fMRI signals, which record brain activity, is a challenging task with crucial research value in the fields of neuroscience and machine learning. Previous studies tend to emphasize reconstructing pixel-level features (contours, colors, etc.) or semantic features (object category) of the stimulus image, but typically, these properties are not reconstructed together. In this context, we introduce a novel three-stage visual reconstruction approach called the Dual-guided Brain Diffusion Model (DBDM). Initially, we employ the Very Deep Variational Autoencoder (VDVAE) to reconstruct a coarse image from fMRI data, capturing the underlying details of the original image. Subsequently, the Bootstrapping Language-Image Pre-training (BLIP) model is utilized to provide a semantic annotation for each image. Finally, the image-to-image generation pipeline of the Versatile Diffusion (VD) model is utilized to recover natural images from the fMRI patterns guided by both visual and semantic information. The experimental results demonstrate that DBDM surpasses previous approaches in both qualitative and quantitative comparisons. In particular, the best performance is achieved by DBDM in reconstructing the semantic details of the original image; the Inception, CLIP and SwAV distances are 0.611, 0.225 and 0.405, respectively. This confirms the efficacy of our model and its potential to advance visual decoding research. © 2023 Elsevier B.V., All rights reserved.},	type  = {Article}}





@conference{Miliotou2023Generative,
	author  = {Miliotou, Eleni and Kyriakis, Panagiotis and Hinman, Jason D. and Irimia, Andrei and Bogdan, Paul},
	title  = {Generative Decoding of Visual Stimuli},
	year  = {2023},
	journal  = {Proceedings of Machine Learning Research},
	volume  = {202},
	pages  = {24775 - 24784},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174388441&partnerID=40&md5=1d206fb358ae273c8bbaa5f3a0b4fadb},
	abstract  = {Reconstructing real-world images from fMRI recordings is a challenging task of great importance in neuroscience. The current architectures are bottlenecked because they fail to effectively capture the hierarchical processing of visual stimuli that takes place in the human brain. Motivated by that fact, we introduce a novel neural network architecture for the problem of neural decoding. Our architecture uses Hierarchical Variational Autoencoders (HVAEs) to learn meaningful representations of real-world images and leverages their latent space hierarchy to learn voxel-to-image mappings. By mapping the early stages of the visual pathway to the first set of latent variables and the higher visual cortex areas to the deeper layers in the latent hierarchy, we are able to construct a latent variable neural decoding model that replicates the hierarchical visual information processing. Our model achieves better reconstructions compared to the state of the art and our ablation study indicates that the hierarchical structure of the latent space is responsible for that performance. © 2023 Elsevier B.V., All rights reserved.},
	keywords  = {Decoding; Machine learning; Network architecture; Neural networks; 'current; Hierarchical processing; Human brain; Latent variable; Learn+; Neural decoding; Neural network architecture; Novel neural network; Real-world image; Visual stimulus; Mapping},
	type  = {Conference paper}}





@article{Pan2023Images,
	author  = {Pan, Honggguang and Fu, Yunpeng and Li, Zhuoyi and Wen, Fan and Hu, Jianchen and Wu, Bo},
	title  = {Images Reconstruction from Functional Magnetic Resonance Imaging Patterns Based on the Improved Deep Generative Multiview Model},
	year  = {2023},
	journal  = {Neuroscience},
	volume  = {509},
	pages  = {103 - 112},
	doi  = {10.1016/j.neuroscience.2022.11.021},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145571633&doi=10.1016%2Fj.neuroscience.2022.11.021&partnerID=40&md5=6b69e10a82387ac87fe9d1484abe6dbe},
	abstract  = {Reconstructing visual stimulus images from the brain activity signals is an important research task in the field of brain decoding. Many methods of reconstructing visual stimulus images mainly focus on how to use deep learning to classify the brain activities measured by functional magnetic resonance imaging or identify visual stimulus images. Accurate reconstruction of visual stimulus images by using deep learning still remains challenging. This paper proposes an improved deep generative multiview model to further promote the accuracy of reconstructing visual stimulus images. Firstly, an encoder based on residual-in-residual dense blocks is designed to fit the deep and multiview visual features of human natural state, and extract the features of visual stimulus images. Secondly, the structure of original decoder is extended to a deeper network in the deep generative multiview model, which makes the features obtained by each deconvolution layer more distinguishable. Finally, we configure the parameters of the optimizer and compare the performance of various optimizers under different parameter values, and then the one with the best performance is chosen and adopted to the whole model. The performance evaluations conducted on two publicly available datasets demonstrate that the improved model has more accurate reconstruction effectiveness than the original deep generative multiview model. © 2023 Elsevier B.V., All rights reserved.},	keywords  = {accuracy; Article; deconvolution; deep learning; electroencephalogram; functional magnetic resonance imaging; human; image reconstruction; visual stimulation; brain; diagnostic imaging; head; image processing; nuclear magnetic resonance imaging; procedures; Brain; Head; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging},
	type  = {Article}}





@article{Wang2023Enhancing,
	author  = {Wang, Hanlin and Kuo, Yunting and Lo, Yuchun and Kuo, Chao Hung and Chen, Bowei and Wang, Chingfu and Wu, Zuyu and Lee, Chi En and Yang, Shih Hung and Lin, Shenghuang and Chen, Pochuan and Chen, Youyin},
	title  = {Enhancing Prediction of Forelimb Movement Trajectory through a Calibrating-Feedback Paradigm Incorporating RAT Primary Motor and Agranular Cortical Ensemble Activity in the Goal-Directed Reaching Task},
	year  = {2023},
	journal  = {International Journal of Neural Systems},
	volume  = {33},
	number  = {10},
	pages  = {},
	doi  = {10.1142/S012906572350051X},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170231923&doi=10.1142%2FS012906572350051X&partnerID=40&md5=9b94e6a3054a654440ecdae01d735b52},
	abstract  = {Complete reaching movements involve target sensing, motor planning, and arm movement execution, and this process requires the integration and communication of various brain regions. Previously, reaching movements have been decoded successfully from the motor cortex (M1) and applied to prosthetic control. However, most studies attempted to decode neural activities from a single brain region, resulting in reduced decoding accuracy during visually guided reaching motions. To enhance the decoding accuracy of visually guided forelimb reaching movements, we propose a parallel computing neural network using both M1 and medial agranular cortex (AGm) neural activities of rats to predict forelimb-reaching movements. The proposed network decodes M1 neural activities into the primary components of the forelimb movement and decodes AGm neural activities into internal feedforward information to calibrate the forelimb movement in a goal-reaching movement. We demonstrate that using AGm neural activity to calibrate M1 predicted forelimb movement can improve decoding performance significantly compared to neural decoders without calibration. We also show that the M1 and AGm neural activities contribute to controlling forelimb movement during goal-reaching movements, and we report an increase in the power of the local field potential (LFP) in beta and gamma bands over AGm in response to a change in the target distance, which may involve sensorimotor transformation and communication between the visual cortex and AGm when preparing for an upcoming reaching movement. The proposed parallel computing neural network with the internal feedback model improves prediction accuracy for goal-reaching movements. © 2023 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Electrophysiology; Feedback; Forecasting; Neurons; Brain regions; Cortexes; Goal-reaching movement; Internal feedback; Medial agranular cortex; Neural activity; Neural decoding; Primary motor cortex; Reaching movements; Target distance; Decoding; animal; feedback system; forelimb; motivation; movement (physiology); physiology; upper limb; Animals; Forelimb; Goals; Movement; Upper Extremity},
	type  = {Article}}





@article{Wang2023Decoding,
  author  = {Wang, Pengpai and Gong, Peiliang and Zhou, Yueying and Wen, Xuyun and Zhang, Daoqiang},
  journal  = {IEEE Transactions on Instrumentation and Measurement}, 
  title  = {Decoding the Continuous Motion Imagery Trajectories of Upper Limb Skeleton Points for EEG-Based Brain–Computer Interface}, 
  year  = {2023},
  volume  = {72},
  number  = {},
  pages  = {1--12},
  abstract  = {In the field of brain–computer interface (BCI), brain decoding using electroencephalography (EEG) is an essential direction, and motion imagery EEG-based BCI can not only help rehabilitation of patients with physical disabilities, but also enhance the endurance and power of people. Most of the existing MI-based BCI studies are limited to discrete EEG classification or 3-D directional limb trajectory reconstruction. To suitable for the requirements of BCI systems in practical applications, here, we explored the decoding of trajectories of continuous nondirectional motion imagination in 3-D space based on Chinese sign language. We propose a motion imagery trajectory reconstruction Transformer (MITRT) model to decode the EEG signals of the subjects performing motion imagery, and obtain the positional changes in the 3-D space of the shoulder, elbow, and wrist skeleton points in the neural activity. We add the geometric constraint features of upper limb skeleton points to the model, and the MITRT decoding model can obtain prior knowledge to improve the reconstruction accuracy of spatial positions. To verify the decoding performance of our proposed model, we collected motor imagery (MI) EEG signals of 20 subjects based on Chinese sign language for experiments. The experimental results show that the average Pearson correlation coefficient of the six skeleton points was 0.975, which was significantly higher than the contrast models. This study is the first attempt to reconstruct multidirectional continuous nondirectional upper limb MI trajectories based on Chinese sign language. The experimental results show that it is feasible to decode and reconstruct imagined 3-D trajectories of human upper limb skeleton points from scalp EEG.},
  keywords  = {Electroencephalography;Trajectory;Image reconstruction;Decoding;Gesture recognition;Assistive technologies;Skeleton;Brain–computer interface (BCI);electroencephalography (EEG);limb motion decoding;motor imagery (MI);trajectory reconstruction},
  doi  = {10.1109/TIM.2022.3224991},
  issn  = {1557-9662},
  month  = {}}




@inproceedings{Chen2022International,
  author  = {Chen, Kai and Ma, Yongqiang and Sheng, Mingyang and Zheng, Nanning},
  booktitle  = {2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title  = {Foreground-attention in neural decoding: Guiding Loop-Enc-Dec to reconstruct visual stimulus images from fMRI}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {1--8},
  abstract  = {The reconstruction of visual stimulus images from functional Magnetic Resonance Imaging (fMRI) has received extensive attention in recent years, which provides a possibility to interpret the human brain. Due to the high-dimensional and high-noise characteristics of fMRI data, how to extract stable, reliable and useful information from fMRI data for image reconstruction has become a challenging problem. Inspired by the mechanism of human visual attention, in this paper, we propose a novel method of reconstructing visual stimulus images, which first decodes human visual salient region from fMRI, we define human visual salient region as foreground attention (F-attention), and then reconstructs the visual images guided by F-attention. Because the human brain is strongly wound into sulci and gyri, some spatially adjacent voxels are not connected in practice. Therefore, it is necessary to consider the global information when decoding fMRI, so we introduce the self-attention module for capturing global information into the process of decoding F-attention. In addition, in order to obtain more loss constraints in the training process of encoder-decoder, we also propose a new training strategy called Loop-Enc-Dec. The experimental results show that the F-attention decoder decodes the visual attention from fMRI successfully, and the Loop-Enc-Dec guided by F-attention can also well reconstruct the visual stimulus images.},
  keywords  = {Training;Visualization;Neuroscience;Shape;Functional magnetic resonance imaging;Decoding;Reliability;visual reconstruction;Foreground-attention;fMRI;neural decoding},
  doi  = {10.1109/IJCNN55064.2022.9892276},
  issn  = {2161-4407},
  month  = {July}}



@article{Du2022Structured,
  author  = {Du, Changde and Du, Changying and Huang, Lijie and Wang, Haibao and He, Huiguang},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems}, 
  title  = {Structured Neural Decoding With Multitask Transfer Learning of Deep Neural Network Representations}, 
  year  = {2022},
  volume  = {33},
  number  = {2},
  pages  = {600--614},
  abstract  = {The reconstruction of visual information from human brain activity is a very important research topic in brain decoding. Existing methods ignore the structural information underlying the brain activities and the visual features, which severely limits their performance and interpretability. Here, we propose a hierarchically structured neural decoding framework by using multitask transfer learning of deep neural network (DNN) representations and a matrix-variate Gaussian prior. Our framework consists of two stages, Voxel2Unit and Unit2Pixel. In Voxel2Unit, we decode the functional magnetic resonance imaging (fMRI) data to the intermediate features of a pretrained convolutional neural network (CNN). In Unit2Pixel, we further invert the predicted CNN features back to the visual images. Matrix-variate Gaussian prior allows us to take into account the structures between feature dimensions and between regression tasks, which are useful for improving decoding effectiveness and interpretability. This is in contrast with the existing single-output regression models that usually ignore these structures. We conduct extensive experiments on two real-world fMRI data sets, and the results show that our method can predict CNN features more accurately and reconstruct the perceived natural images and faces with higher quality.},
  keywords  = {Decoding;Image reconstruction;Functional magnetic resonance imaging;Visualization;Task analysis;Brain;Correlation;Deep neural network (DNN);functional magnetic resonance imaging (fMRI);image reconstruction;multioutput regression;neural decoding},
  doi  = {10.1109/TNNLS.2020.3028167},
  issn  = {2162-2388},
  month  = {Feb}}



@inproceedings{Jin2022Ieee,
  author  = {Jin, Yingxin and Shang, Shaohua and Tang, Liwei and He, Lianzhua and Zhou, MengChu},
  booktitle  = {2022 IEEE International Conference on Networking, Sensing and Control (ICNSC)}, 
  title  = {EEG channel selection algorithm based on Reinforcement Learning}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {1--6},
  abstract  = {Multichannel EEG is generally used to collect brain activities from various locations across the brain. However, BCIs using lesser channels will be more convenient for subjects. What's more, information acquired from adjacent channels is usually inter-correlated or irrelevant to the task. And some channels are noisy. This paper proposes a novel channel selection algorithm based on reinforcement learning. It can adaptively transform the full-channel EEG data to the optimal-channel-number EEG format conditioned on different input trials to make a trade-off between brain decoding accuracy and efficiency. Experimen-tal results showed that the proposed model can improve the classification accuracy by 2% ~ 6% compared to channel set $\{C3,C4,Cz\}$.},
  keywords  = {Knowledge engineering;Reinforcement learning;Transforms;Feature extraction;Electroencephalography;Decoding;Classification algorithms;EEG;channel selection;reinforcement learning},
  doi  = {10.1109/ICNSC55942.2022.10004161},
  issn  = {},
  month  = {Dec}}



@inproceedings{K2022International,
  author  = {K, Sowmya and S, Sushitha},
  booktitle  = {2022 International Conference on Automation, Computing and Renewable Systems (ICACRS)}, 
  title  = {An Interpretation on Brain Gate System Network and Technology- A Study}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {868--873},
  abstract  = {The process of thoughts-to-movement is a scientific discovery that makes a severely disabled human to manipulate a device using thoughts. These were achieved, in a great portion through the brain gate network technology. The network system uses a different kind of Brain neural signal which regulates the language of the neuron's sensing, transmission, perception, and implementation. The working idea of the brain gate system is that brainwave patterns are produced with brain function and not transmitted to the body. It is a brain implant device developed at Brown University in partnership with the biotech firm‚ Cyber-Kinetics Neuroscience. Even though effective BCI research has been arrived in the past 25 years, this paper aims to study the working, core principles and research aspects of brain gate technology in depth.},
  keywords  = {Digital control;Computer interfaces;Renewable energy sources;Neuroscience;Epilepsy;Neural implants;Logic gates;Neurotechnologies;Brain Computer Interface (BCI);Electro-magnetic Neurons;Motor Cortex;Neural Decoding;Central Nervus System(CNS)},
  doi  = {10.1109/ICACRS55517.2022.10029097},
  issn  = {},
  month  = {Dec}}



@inproceedings{Karam2022International,
  author  = {Karam, Andrew and Boles, Kirollos and Raouf, Mario and Yousef, Mina Atef and Khoriba, Ghada},
  booktitle  = {2022 2nd International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)}, 
  title  = {Deep learning approaches for analysing visual stimuli from human fMRI}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {449--458},
  abstract  = {Decoding brain activities corresponding to an external stimulus is an excellent challenge because of the complexity of brain activities data and the understood of the activity of the brain is not yet complete. This research paper focuses on the functional magnetic resonance imaging (fMRI) data of different people corresponding to external visual stimulus images. The images consist of three main types: letters, artificial shapes, and natural images. The proposed model provides an analysis and classification of the three main types of images using Support Vector Machine and Convolutional Neural Networks by proving that the brain is affected differently by each type. In addition, the identification of 200 natural image categories was analyzed by calculating the similarity between the features for each category by using the features of visual images using CNN and fMRI using the regression model. Also, we used deep convolution generative adversarial networks (DCGANs) for image reconstruction.},
  keywords  = {Support vector machines;Visualization;Analytical models;Functional magnetic resonance imaging;Predictive models;Brain modeling;Data models;Brain Decoding;Visual Image Reconstruction;Deep Neural Networks;GAN;fMRI},
  doi  = {10.1109/MIUCC55081.2022.9781770},
  issn  = {},
  month  = {May}}



@article{Khaleghi2022Visual,
	author  = {Khaleghi, Nastaran and Yousefi Rezaii, Tohid and Beheshti, Soosan and Meshgini, Saeed and Sheykhivand, Sobhan and Daneshvar, Sabalan},
	title  = {Visual Saliency and Image Reconstruction from EEG Signals via an Effective Geometric Deep Network-Based Generative Adversarial Network},
	year  = {2022},
	journal  = {Electronics (Switzerland)},
	volume  = {11},
	number  = {21},
	pages  = {},
	doi  = {10.3390/electronics11213637},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141696795&doi=10.3390%2Felectronics11213637&partnerID=40&md5=62ebda7f159fabddc49ee42a27519fd7},
	abstract  = {Reaching out the function of the brain in perceiving input data from the outside world is one of the great targets of neuroscience. Neural decoding helps us to model the connection between brain activities and the visual stimulation. The reconstruction of images from brain activity can be achieved through this modelling. Recent studies have shown that brain activity is impressed by visual saliency, the important parts of an image stimuli. In this paper, a deep model is proposed to reconstruct the image stimuli from electroencephalogram (EEG) recordings via visual saliency. To this end, the proposed geometric deep network-based generative adversarial network (GDN-GAN) is trained to map the EEG signals to the visual saliency maps corresponding to each image. The first part of the proposed GDN-GAN consists of Chebyshev graph convolutional layers. The input of the GDN part of the proposed network is the functional connectivity-based graph representation of the EEG channels. The output of the GDN is imposed to the GAN part of the proposed network to reconstruct the image saliency. The proposed GDN-GAN is trained using the Google Colaboratory Pro platform. The saliency metrics validate the viability and efficiency of the proposed saliency reconstruction network. The weights of the trained network are used as initial weights to reconstruct the grayscale image stimuli. The proposed network realizes the image reconstruction from EEG signals. © 2022 Elsevier B.V., All rights reserved.},	type  = {Article}}





@article{Laino2022Generative,
	author  = {Laino, Maria Elena and Cancian, Pierandrea and Politi, Letterio S. and della Porta, Matteo Giovanni and Saba, Luca and Savevski, Victor},
	title  = {Generative Adversarial Networks in Brain Imaging: A Narrative Review},
	year  = {2022},
	journal  = {Journal of Imaging},
	volume  = {8},
	number  = {4},
	pages  = {},
	doi  = {10.3390/jimaging8040083},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127651324&doi=10.3390%2Fjimaging8040083&partnerID=40&md5=9c35e224d5a618f71b5ac9c0b0ed0b74},
	abstract  = {Artificial intelligence (AI) is expected to have a major effect on radiology as it demon-strated remarkable progress in many clinical tasks, mostly regarding the detection, segmentation, classification, monitoring, and prediction of diseases. Generative Adversarial Networks have been proposed as one of the most exciting applications of deep learning in radiology. GANs are a new approach to deep learning that leverages adversarial learning to tackle a wide array of computer vision challenges. Brain radiology was one of the first fields where GANs found their application. In neuroradiology, indeed, GANs open unexplored scenarios, allowing new processes such as image-to-image and cross-modality synthesis, image reconstruction, image segmentation, image synthesis, data augmentation, disease progression models, and brain decoding. In this narrative review, we will provide an introduction to GANs in brain imaging, discussing the clinical potential of GANs, future clinical applications, as well as pitfalls that radiologists should be aware of. © 2022 Elsevier B.V., All rights reserved.},	type  = {Review}}





@article{Layton2022Estimating,
	author  = {Layton, Oliver W. and Powell, Nathaniel V. and Steinmetz, Scott T. and Fajen, Brett R.},
	title  = {Estimating curvilinear self-motion from optic flow with a biologically inspired neural system},
	year  = {2022},
	journal  = {Bioinspiration and Biomimetics},
	volume  = {17},
	number  = {4},
	pages  = {},
	doi  = {10.1088/1748-3190/ac709b},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131772037&doi=10.1088%2F1748-3190%2Fac709b&partnerID=40&md5=d47990b22cb979f8665de7f762d1882c},
	abstract  = {Optic flow provides rich information about world-relative self-motion and is used by many animals to guide movement. For example, self-motion along linear, straight paths without eye movements, generates optic flow that radiates from a singularity that specifies the direction of travel (heading). Many neural models of optic flow processing contain heading detectors that are tuned to the position of the singularity, the design of which is informed by brain area MSTd of primate visual cortex that has been linked to heading perception. Such biologically inspired models could be useful for efficient self-motion estimation in robots, but existing systems are tailored to the limited scenario of linear self-motion and neglect sensitivity to self-motion along more natural curvilinear paths. The observer in this case experiences more complex motion patterns, the appearance of which depends on the radius of the curved path (path curvature) and the direction of gaze. Indeed, MSTd neurons have been shown to exhibit tuning to optic flow patterns other than radial expansion, a property that is rarely captured in neural models. We investigated in a computational model whether a population of MSTd-like sensors tuned to radial, spiral, ground, and other optic flow patterns could support the accurate estimation of parameters describing both linear and curvilinear self-motion. We used deep learning to decode self-motion parameters from the signals produced by the diverse population of MSTd-like units. We demonstrate that this system is capable of accurately estimating curvilinear path curvature, clockwise/counterclockwise sign, and gaze direction relative to the path tangent in both synthetic and naturalistic videos of simulated self-motion. Estimates remained stable over time while rapidly adapting to dynamic changes in the observer's curvilinear self-motion. Our results show that coupled biologically inspired and artificial neural network systems hold promise as a solution for robust vision-based self-motion estimation in robots. © 2022 Elsevier B.V., All rights reserved.},	keywords  = {Biomimetics; Decoding; Eye movements; Flow patterns; Motion estimation; Neural networks; Optical flows; Biologically-inspired; Curvilinear path; Deep learning; Heading; Neural decoding; Neural modelling; Optic flow; Path curvature; Self motion; Self-motion estimation; animal; motion; movement perception; nerve cell; optic flow; physiology; visual cortex; Animals; Motion; Motion Perception; Neurons; Optic Flow; Visual Cortex},
	type  = {Article}}





@inproceedings{Ozcelik2022International,
  author  = {Ozcelik, Furkan and Choksi, Bhavin and Mozafari, Milad and Reddy, Leila and VanRullen, Rufin},
  booktitle  = {2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title  = {Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {1--8},
  abstract  = {Reconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRI-predicted variables. The generated images presented state-of-the-art results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regions-of-interest in the human brain.},
  keywords  = {Visualization;Image synthesis;Semantics;Self-supervised learning;Functional magnetic resonance imaging;Predictive models;Feature extraction;Natural Image Reconstruction;fMRI Decoding;IC-GAN;Brain-Computer Interface},
  doi  = {10.1109/IJCNN55064.2022.9892673},
  issn  = {2161-4407},
  month  = {July}}



@inproceedings{Taguchi2022Joint,
  author  = {Taguchi, Haruka and Nishida, Satoshi and Nishimoto, Shinji and Kobayashi, Ichiro},
  booktitle  = {2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems (SCIS&ISIS)}, 
  title  = {Validation of the Role of Attention Mechanism in Predicting Brain Activity}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {1--5},
  abstract  = {In this study, we estimate the state of the human brain from visual stimuli by regressing the brain activity state from image features extracted from an image identification deep learning model. We introduce Attention Branch Network, which enhances to capture the features of the identified target by attention when extracting image features, into the image identification deep learning model and estimate the brain activity state from the image features weighted or unweighted by attention. Through experiments, we aim to verify the role of attention mechanism in estimating brain activity state from visual stimuli. As a result, we confirmed that the introduction of Attention did not have a significant effect on the estimation accuracy, but that differences were observed in the areas where the estimation accuracy was higher.},
  keywords  = {Deep learning;Visualization;Decision making;Estimation;Information representation;Predictive models;Functional magnetic resonance imaging;brain decoding;attention branch network;fMRI;ResNet},
  doi  = {10.1109/SCISISIS55246.2022.10001915},
  issn  = {},
  month  = {Nov}}



@inproceedings{Weng2022Asilomar,
  author  = {Weng, Geyu and Akbarian, Amir and Noudoost, Behrad and Nategh, Neda},
  booktitle  = {2022 56th Asilomar Conference on Signals, Systems, and Computers}, 
  title  = {Modeling the Relationship between Perisaccadic Neural Responses and Location Information}, 
  year  = {2022},
  volume  = {},
  number  = {},
  pages  = {451--454},
  abstract  = {Eye movements are essential for the brain to collect visual information from the environment. Visual images projected on the retina change abruptly during rapid ballistic eye movements (saccades), but our perception of the visual world is continuous. To generate a stable visual perception, the spatiotemporal sensitivity of visual neurons needs to change quickly prior to and during saccades. This study uses a modeling framework to characterize the fast dynamics of neuronal responses across saccades, thereby quantifying the contribution of perisaccadic response dynamics to the readout of location information during saccades. We apply this approach to neuronal responses recorded from the visual cortex of nonhuman primates during a visually-guided saccade task with visual stimulations. Using the model-predicted responses and a classification method, we measure the spatial discriminability performance of neurons at pre-saccadic and post-saccadic receptive field locations. Characterizing the readout of perisaccadic spatial information and its precise time course can provide insights into how neurons integrate spatial information across saccades to generate a continuous visual experience.},
  keywords  = {Visualization;Sensitivity;Neurons;Electrophysiology;Brain modeling;Retina;Time measurement;computational models;neural decoding;eye movement;visual cortex},
  doi  = {10.1109/IEEECONF56349.2022.10051903},
  issn  = {2576-2303},
  month  = {Oct}}



@article{Wu2022Featurespecific,
  author  = {Wu, Hao and Zheng, Nanning and Chen, Badong},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems}, 
  title  = {Feature-Specific Denoising of Neural Activity for Natural Image Identification}, 
  year  = {2022},
  volume  = {14},
  number  = {2},
  pages  = {629--638},
  abstract  = {Decoding the content in neural activity through voxelwise encoding plays an important role in investigating cognitive functions of the human brain. However, unlike multivoxel pattern analysis (MVPA), voxelwise encoding builds a model for each individual voxel; therefore, ignores the interactions between voxels and is sensitive to noise. In this work, we propose the feature-specific denoise (FSdenoise), a noise reduction method for encoding-based models to improve their decoding performance. FSdenoise considers the response of a voxel to a stimulus as a combination of two components: 1) feature-relevant component, which can be predicted from stimulus features and 2) feature-irrelevant component, which shows no direct relation to the concerned features. Exploiting the correlations between voxels, FSdenoise reduces the feature-irrelevant component in voxels that exhibit more feature-relevant component, enhancing their predictive power from stimulus features. Decoding performance with the denoised voxels would be improved in consequence. We validate the FSdenoise on two functional magnetic resonance imaging data sets and the results demonstrate that FSdenoise can efficiently improve the decoding accuracy for encoding-based approaches. Moreover, the encoding-based approaches combined with FSdenoise can even outperform the MVPA-based approach in brain decoding.},
  keywords  = {Decoding;Predictive models;Image coding;Feature extraction;Noise reduction;Encoding;Brain modeling;Brain decoding;denoising;functional magnetic resonance imaging (fMRI);visual cognition;voxelwise encoding},
  doi  = {10.1109/TCDS.2021.3062067},
  issn  = {2379-8939},
  month  = {June}}



@conference{Xie2022Influence,
	author  = {Xie, Donghong},
	title  = {On the Influence of Feature Selection and Regression Models on the Decoding Accuracy of Seen and Imagery Objects Using Hierarchical Visual Features},
	year  = {2022},
	journal  = {ACM International Conference Proceeding Series},
	pages  = {7 - 15},
	doi  = {10.1145/3560470.3560472},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142471677&doi=10.1145%2F3560470.3560472&partnerID=40&md5=df9073c0d690dbcc8e135137a2767472},
	abstract  = {With the development in brain decoding and encoding, the seen and imagery object identification tasks achieve increasingly higher accuracy by using more intricate features. However, the experiment always requires a huge amount of data to process which will consume a lot of time. With available seen and imagery BOLD signals, we leverage the Generic Brain Decoding model to fit and then predict visual feature vectors extracted from neural networks. Given the hierarchy of AlexNet and that of the human visual cortex, the accuracies of the couplings between low layers of AlexNet and human visual cortex and those between high layers outperform other coupling cases. Using a different number of feature dimensions during feature selection induces a significant difference in the decoding performance at the high layers. Moreover, the shallow neural network achieves higher accuracies at the high layers. The results demonstrate that when the number of available data samples is not sufficient, reducing the input feature dimensions can still ensure a comparable decoding accuracy. © 2022 Elsevier B.V., All rights reserved.},	keywords  = {Decoding; Feature Selection; Multilayer neural networks; Regression analysis; Brain decoding; Brain visual decoding; Features selection; High-accuracy; Human visual cortex; Identification accuracy; Neural-networks; Objects recognition; Shallow neural network; Visual feature; Object recognition},
	type  = {Conference paper}}





@article{Xu2022Robust,
  author  = {Xu, Qi and Shen, Jiangrong and Ran, Xuming and Tang, Huajin and Pan, Gang and Liu, Jian K.},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems}, 
  title  = {Robust Transcoding Sensory Information With Neural Spikes}, 
  year  = {2022},
  volume  = {33},
  number  = {5},
  pages  = {1935--1946},
  abstract  = {Neural coding, including encoding and decoding, is one of the key problems in neuroscience for understanding how the brain uses neural signals to relate sensory perception and motor behaviors with neural systems. However, most of the existed studies only aim at dealing with the continuous signal of neural systems, while lacking a unique feature of biological neurons, termed spike, which is the fundamental information unit for neural computation as well as a building block for brain–machine interface. Aiming at these limitations, we propose a transcoding framework to encode multi-modal sensory information into neural spikes and then reconstruct stimuli from spikes. Sensory information can be compressed into 10% in terms of neural spikes, yet re-extract 100% of information by reconstruction. Our framework can not only feasibly and accurately reconstruct dynamical visual and auditory scenes, but also rebuild the stimulus patterns from functional magnetic resonance imaging (fMRI) brain activities. More importantly, it has a superb ability of noise immunity for various types of artificial noises and background signals. The proposed framework provides efficient ways to perform multimodal feature representation and reconstruction in a high-throughput fashion, with potential usage for efficient neuromorphic computing in a noisy environment.},
  keywords  = {Decoding;Neurons;Image reconstruction;Biological information theory;Transcoding;Visualization;Computational modeling;Cross-multimodal;decoding;denoising;neural spikes;reconstruction;spatio-temporal representations},
  doi  = {10.1109/TNNLS.2021.3107449},
  issn  = {2162-2388},
  month  = {May}}



@article{Zhang2022Decoding,
  author  = {Zhang, Yijun and Bu, Tong and Zhang, Jiyuan and Tang, Shiming and Yu, Zhaofei and Liu, Jian K. and Huang, Tiejun},
  journal  = {Neural Computation}, 
  title  = {Decoding Pixel-Level Image Features From Two-Photon Calcium Signals of Macaque Visual Cortex}, 
  year  = {2022},
  volume  = {34},
  number  = {6},
  pages  = {1369--1397},
  abstract  = {Images of visual scenes comprise essential features important for visual cognition of the brain. The complexity of visual features lies at different levels, from simple artificial patterns to natural images with different scenes. It has been a focus of using stimulus images to predict neural responses. However, it remains unclear how to extract features from neuronal responses. Here we address this question by leveraging two-photon calcium neural data recorded from the visual cortex of awake macaque monkeys. With stimuli including various categories of artificial patterns and diverse scenes of natural images, we employed a deep neural network decoder inspired by image segmentation technique. Consistent with the notation of sparse coding for natural images, a few neurons with stronger responses dominated the decoding performance, whereas decoding of ar tificial patterns needs a large number of neurons. When natural images using the model pretrained on artificial patterns are decoded, salient features of natural scenes can be extracted, as well as the conventional category information. Altogether, our results give a new perspective on studying neural encoding principles using reverse-engineering decoding strategies.},
  keywords  = {},
  doi  = {10.1162/neco_a_01498},
  issn  = {0899-7667},
  month  = {May}}



@article{Hallenbeck2021Working,
	author  = {Hallenbeck, Grace E. and Sprague, Thomas C. and Rahmati, Masih and Sreenivasan, Kartik K. and Curtis, Clayton E.},
	title  = {Working memory representations in visual cortex mediate distraction effects},
	year  = {2021},
	journal  = {Nature Communications},
	volume  = {12},
	number  = {1},
	pages  = {},
	doi  = {10.1038/s41467-021-24973-1},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111995233&doi=10.1038%2Fs41467-021-24973-1&partnerID=40&md5=15c5d1faf642c742d5cee6f504ff062a},
	abstract  = {Although the contents of working memory can be decoded from visual cortex activity, these representations may play a limited role if they are not robust to distraction. We used model-based fMRI to estimate the impact of distracting visual tasks on working memory representations in several visual field maps in visual and frontoparietal association cortex. Here, we show distraction causes the fidelity of working memory representations to briefly dip when both the memorandum and distractor are jointly encoded by the population activities. Distraction induces small biases in memory errors which can be predicted by biases in neural decoding in early visual cortex, but not other regions. Although distraction briefly disrupts working memory representations, the widespread redundancy with which working memory information is encoded may protect against catastrophic loss. In early visual cortex, the neural representation of information in working memory and behavioral performance are intertwined, solidifying its importance in visual memory. © 2022 Elsevier B.V., All rights reserved.},
	keywords  = {brain; memory; visual analysis; article; association cortex; functional magnetic resonance imaging; visual cortex; visual field; visual memory; working memory; adult; attention; biological model; brain mapping; female; functional neuroimaging; human; male; middle aged; nuclear magnetic resonance imaging; photostimulation; physiology; psychological model; short term memory; task performance; Adult; Attention; Brain Mapping; Female; Functional Neuroimaging; Humans; Magnetic Resonance Imaging; Male; Memory, Short-Term; Middle Aged; Models, Neurological; Models, Psychological; Photic Stimulation; Task Performance and Analysis; Visual Cortex},
	type  = {Article}}





@conference{Hu2021Decoding,
	author  = {Hu, Lulu and Li, Jingwei and Zhang, Chi and Tong, Li},
	title  = {Decoding Categories from Human Brain Activity in the Human Visual Cortex Using the Triplet Network},
	year  = {2021},
	pages  = {128 - 134},
	doi  = {10.1145/3448748.3448769},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103519138&doi=10.1145%2F3448748.3448769&partnerID=40&md5=870f91f23c3ed87aadcda4515aa1d84e},
	abstract  = {Decoding visual stimuli from functional magnetic resonance imaging (fMRI) is of great significance for understanding the neural mechanism of the visual information processing in the human brain. How to extract effective information from massive voxel data in the brain to predict the brain state is a problem worth discussing in fMRI. However, the inherent characteristics of small quantity and high dimensionality in fMRI data limited the performance of brain decoding. As an effective way to acquire visual information, people usually compare with the prior knowledge learned when recognizing objects, and does not need to have a complete understanding of visual information. In this paper, we proposed a new visual classification model to decode the stimulus categories from the visual information of the brain based on the triplet network. The triplet network is a model framework with a comparison mechanism similar to that of human visual recognition objects, contains three-branches weight sharing subnetworks, which are composed of fully connected networks in our model. Our results showed that the decoding accuracy is 57.5±1.86% and 44.17±1.31% for S1 and S2, respectively. S1 was about 6% higher than the best traditional machine learning classifier SVM, while S2 was nearly 3.5% higher than SVM. Our results fully confirmed the validity of comparing the differences between samples in fMRI data with small quantity. © 2021 Elsevier B.V., All rights reserved.},	keywords  = {Bioinformatics; Brain; Classification (of information); Data reduction; Decoding; Intelligent computing; Learning systems; Magnetic resonance imaging; Support vector machines; Fully connected networks; Functional magnetic resonance imaging; High dimensionality; Human visual cortex; Inherent characteristics; Visual classification; Visual information; Visual information processing; Functional neuroimaging},
	type  = {Conference paper}}





@article{Huang2021Neural,
	author  = {Huang, Wei and Yan, Hongmei and Cheng, Kaiwen and Wang, Chong and Li, Jiyi and Wang, Yuting and Li, Chen and Li, Chaorong and Li, Yunhan and Zuo, Zhentao and Chen, Huafu},
	title  = {A neural decoding algorithm that generates language from visual activity evoked by natural images},
	year  = {2021},
	journal  = {Neural Networks},
	volume  = {144},
	pages  = {90 - 100},
	doi  = {10.1016/j.neunet.2021.08.006},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113990280&doi=10.1016%2Fj.neunet.2021.08.006&partnerID=40&md5=045538c96a65b28da54eaac9b0444645},
	abstract  = {Transforming neural activities into language is revolutionary for human–computer interaction as well as functional restoration of aphasia. Present rapid development of artificial intelligence makes it feasible to decode the neural signals of human visual activities. In this paper, a novel Progressive Transfer Language Decoding Model (PT-LDM) is proposed to decode visual fMRI signals into phrases or sentences when natural images are being watched. The PT-LDM consists of an image-encoder, a fMRI encoder and a language-decoder. The results showed that phrases and sentences were successfully generated from visual activities. Similarity analysis showed that three often-used evaluation indexes BLEU, ROUGE and CIDEr reached 0.182, 0.197 and 0.680 averagely between the generated texts and the corresponding annotated texts in the testing set respectively, significantly higher than the baseline. Moreover, we found that higher visual areas usually had better performance than lower visual areas and the contribution curve of visual response patterns in language decoding varied at successively different time points. Our findings demonstrate that the neural representations elicited in visual cortices when scenes are being viewed have already contained semantic information that can be utilized to generate human language. Our study shows potential application of language-based brain–machine interfaces in the future, especially for assisting aphasics in communicating more efficiently with fMRI signals. © 2021 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Human computer interaction; Semantics; Signal encoding; Visual languages; Computer interaction; Decoding algorithm; Language decoding; Natural images; Neural activity; Neural decoding; Neural signals; Progressive transfer; Visual activity; Visual areas; Decoding; adult; algorithm; Article; electroencephalogram; female; functional magnetic resonance imaging; human; human computer interaction; language; language processing; male; model; semantics; vision; visual cortex; visual evoked potential; artificial intelligence; brain mapping; nuclear magnetic resonance imaging; Algorithms; Artificial Intelligence; Brain Mapping; Humans; Language; Magnetic Resonance Imaging},
	type  = {Article}}





@article{Huang2021Deep,
	author  = {Huang, Wei and Yan, Hongmei and Wang, Chong and Yang, Xiaoqing and Li, Jiyi and Zuo, Zhentao and Zhang, Jiang and Chen, Huafu},
	title  = {Deep Natural Image Reconstruction from Human Brain Activity Based on Conditional Progressively Growing Generative Adversarial Networks},
	year  = {2021},
	journal  = {Neuroscience Bulletin},
	volume  = {37},
	number  = {3},
	pages  = {369 - 379},
	doi  = {10.1007/s12264-020-00613-4},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096395474&doi=10.1007%2Fs12264-020-00613-4&partnerID=40&md5=11fc49b4432eb636b70a84b65ef5cd1a},
	abstract  = {Brain decoding based on functional magnetic resonance imaging has recently enabled the identification of visual perception and mental states. However, due to the limitations of sample size and the lack of an effective reconstruction model, accurate reconstruction of natural images is still a major challenge. The current, rapid development of deep learning models provides the possibility of overcoming these obstacles. Here, we propose a deep learning-based framework that includes a latent feature extractor, a latent feature decoder, and a natural image generator, to achieve the accurate reconstruction of natural images from brain activity. The latent feature extractor is used to extract the latent features of natural images. The latent feature decoder predicts the latent features of natural images based on the response signals from the higher visual cortex. The natural image generator is applied to generate reconstructed images from the predicted latent features of natural images and the response signals from the visual cortex. Quantitative and qualitative evaluations were conducted with test images. The results showed that the reconstructed image achieved comparable, accurate reproduction of the presented image in both high-level semantic category information and low-level pixel information. The framework we propose shows promise for decoding the brain activity. © 2021 Elsevier B.V., All rights reserved.},	keywords  = {article; brain function; deep learning; functional magnetic resonance imaging; human; human experiment; image reconstruction; qualitative analysis; quantitative analysis; reproduction; visual cortex; brain; diagnostic imaging; image processing; nuclear magnetic resonance imaging; Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Visual Cortex},
	type  = {Article}}





@article{Nonaka2021Brain,
	author  = {Nonaka, Soma and Majima, Kei and Aoki, Shuntaro C. and Kamitani, Yukiyasu},
	title  = {Brain hierarchy score: Which deep neural networks are hierarchically brain-like?},
	year  = {2021},
	journal  = {iScience},
	volume  = {24},
	number  = {9},
	pages  = {},
	doi  = {10.1016/j.isci.2021.103013},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122826845&doi=10.1016%2Fj.isci.2021.103013&partnerID=40&md5=cc8ea22a5576d4f8f355de24c3ee9e2d},
	abstract  = {Achievement of human-level image recognition by deep neural networks (DNNs) has spurred interest in whether and how DNNs are brain-like. Both DNNs and the visual cortex perform hierarchical processing, and correspondence has been shown between hierarchical visual areas and DNN layers in representing visual features. Here, we propose the brain hierarchy (BH) score as a metric to quantify the degree of hierarchical correspondence based on neural decoding and encoding analyses where DNN unit activations and human brain activity are predicted from each other. We find that BH scores for 29 pre-trained DNNs with various architectures are negatively correlated with image recognition performance, thus indicating that recently developed high-performance DNNs are not necessarily brain-like. Experimental manipulations of DNN models suggest that single-path sequential feedforward architecture with broad spatial integration is critical to brain-like hierarchy. Our method may provide new ways to design DNNs in light of their representational homology to the brain. © 2022 Elsevier B.V., All rights reserved.},	type  = {Article}}





@article{Rakhimberdina2021Natural,
	author  = {Rakhimberdina, Zarina and Jodelet, Quentin and Liu, Xin and Murata, Tsuyoshi},
	title  = {Natural Image Reconstruction From fMRI Using Deep Learning: A Survey},
	year  = {2021},
	journal  = {Frontiers in Neuroscience},
	volume  = {15},
	pages  = {},
	doi  = {10.3389/fnins.2021.795488},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122311969&doi=10.3389%2Ffnins.2021.795488&partnerID=40&md5=abde4ee9d4039110d10d258e09c66ea1},
	abstract  = {With the advent of brain imaging techniques and machine learning tools, much effort has been devoted to building computational models to capture the encoding of visual information in the human brain. One of the most challenging brain decoding tasks is the accurate reconstruction of the perceived natural images from brain activities measured by functional magnetic resonance imaging (fMRI). In this work, we survey the most recent deep learning methods for natural image reconstruction from fMRI. We examine these methods in terms of architectural design, benchmark datasets, and evaluation metrics and present a fair performance evaluation across standardized evaluation metrics. Finally, we discuss the strengths and limitations of existing studies and present potential future directions. © 2021 Elsevier B.V., All rights reserved.},	keywords  = {brain; deep learning; functional magnetic resonance imaging; human; human experiment; image reconstruction; review},
	type  = {Review}}





@inproceedings{Takada2021Ieee,
  author  = {Takada, Saya and Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
  booktitle  = {2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)}, 
  title  = {Estimating Imagined Images from Brain Activities via Visual Question Answering}, 
  year  = {2021},
  volume  = {},
  number  = {},
  pages  = {35--36},
  abstract  = {Investigating human mental contents has been a topic for many years, but its ambiguous property has made the analysis difficult. We propose a neural decoding method via a machine learning model that predicts the imagined content based on measuring brain activity in this paper. This technique uses brain activity and computer vision models to discover the association between human functional magnetic resonance imaging (fMRI) activity and imagined contents. Decoding models based on neural networks learned using stimulus-induced brain activity in the visual cortex region showed an accurate estimation of the content. We provide a means of revealing subjective mental content by analysis with visual question answering. This result shows that a mental experience relates to brain activity patterns.},
  keywords  = {Visualization;Image color analysis;Computational modeling;Estimation;Machine learning;Functional magnetic resonance imaging;Predictive models},
  doi  = {10.1109/GCCE53005.2021.9622077},
  issn  = {2378-8143},
  month  = {Oct}}



@article{Tripathy2021Decoding,
	author  = {Tripathy, Kalyan and Markow, Zachary E. and Fishell, Andrew K. and Sherafati, Arefeh and Burns-Yocum, Tracy M. and Schroeder, Mariel Lee and Svoboda, Alexandra M. and Eggebrecht, Adam T. and Anastasio, Mark A. and Schlaggar, Bradley L. and Culver, Joseph P.},
	title  = {Decoding visual information from high-density diffuse optical tomography neuroimaging data},
	year  = {2021},
	journal  = {NeuroImage},
	volume  = {226},
	pages  = {},
	doi  = {10.1016/j.neuroimage.2020.117516},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097338427&doi=10.1016%2Fj.neuroimage.2020.117516&partnerID=40&md5=ffaf23af600c0cfeb45496579d3f7a55},
	abstract  = {Background: Neural decoding could be useful in many ways, from serving as a neuroscience research tool to providing a means of augmented communication for patients with neurological conditions. However, applications of decoding are currently constrained by the limitations of traditional neuroimaging modalities. Electrocorticography requires invasive neurosurgery, magnetic resonance imaging (MRI) is too cumbersome for uses like daily communication, and alternatives like functional near-infrared spectroscopy (fNIRS) offer poor image quality. High-density diffuse optical tomography (HD-DOT) is an emerging modality that uses denser optode arrays than fNIRS to combine logistical advantages of optical neuroimaging with enhanced image quality. Despite the resulting promise of HD-DOT for facilitating field applications of neuroimaging, decoding of brain activity as measured by HD-DOT has yet to be evaluated. Objective: To assess the feasibility and performance of decoding with HD-DOT in visual cortex. Methods and Results: To establish the feasibility of decoding at the single-trial level with HD-DOT, a template matching strategy was used to decode visual stimulus position. A receiver operating characteristic (ROC) analysis was used to quantify the sensitivity, specificity, and reproducibility of binary visual decoding. Mean areas under the curve (AUCs) greater than 0.97 across 10 imaging sessions in a highly sampled participant were observed. ROC analyses of decoding across 5 participants established both reproducibility in multiple individuals and the feasibility of inter-individual decoding (mean AUCs > 0.7), although decoding performance varied between individuals. Phase-encoded checkerboard stimuli were used to assess more complex, non-binary decoding with HD-DOT. Across 3 highly sampled participants, the phase of a 60° wide checkerboard wedge rotating 10° per second through 360° was decoded with a within-participant error of 25.8±24.7°. Decoding between participants was also feasible based on permutation-based significance testing. Conclusions: Visual stimulus information can be decoded accurately, reproducibly, and across a range of detail (for both binary and non-binary outcomes) at the single-trial level (without needing to block-average test data) using HD-DOT data. These results lay the foundation for future studies of more complex decoding with HD-DOT and applications in clinical populations. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {adult; area under the curve; article; brain function; clinical article; controlled study; feasibility study; female; functional near-infrared spectroscopy; functional neuroimaging; human; human experiment; human tissue; image quality; male; optical tomography; quantitative analysis; receiver operating characteristic; reproducibility; sensitivity and specificity; visual cortex; visual information; visual stimulation; image processing; middle aged; physiology; procedures; vision; Adult; Female; Functional Neuroimaging; Humans; Image Processing, Computer-Assisted; Male; Middle Aged; Tomography, Optical; Visual Perception},
	type  = {Article}}





@article{Wakita2021Photorealistic,
	author  = {Wakita, Suguru and Orima, Taiki and Motoyoshi, Isamu},
	title  = {Photorealistic Reconstruction of Visual Texture From EEG Signals},
	year  = {2021},
	journal  = {Frontiers in Computational Neuroscience},
	volume  = {15},
	pages  = {},
	doi  = {10.3389/fncom.2021.754587},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120531286&doi=10.3389%2Ffncom.2021.754587&partnerID=40&md5=0ab74dc3e7fe164f9c559c8075732fcf},
	abstract  = {Recent advances in brain decoding have made it possible to classify image categories based on neural activity. Increasing numbers of studies have further attempted to reconstruct the image itself. However, because images of objects and scenes inherently involve spatial layout information, the reconstruction usually requires retinotopically organized neural data with high spatial resolution, such as fMRI signals. In contrast, spatial layout does not matter in the perception of “texture,” which is known to be represented as spatially global image statistics in the visual cortex. This property of “texture” enables us to reconstruct the perceived image from EEG signals, which have a low spatial resolution. Here, we propose an MVAE-based approach for reconstructing texture images from visual evoked potentials measured from observers viewing natural textures such as the textures of various surfaces and object ensembles. This approach allowed us to reconstruct images that perceptually resemble the original textures with a photographic appearance. The present approach can be used as a method for decoding the highly detailed “impression” of sensory stimuli from brain activity. © 2021 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Decoding; Electrophysiology; Image coding; Image reconstruction; Image resolution; Image texture; Network coding; Neurons; Textures; Auto encoders; Brain decoding; Deep neural network; EEG signals; Multi-modal; Multimodal variational auto encoder; Object and scenes; Photo-realistic; Spatial layout; Visual texture; Deep neural networks; article; autoencoder; brain function; deep neural network; electroencephalogram; human; perception; visual cortex; visual evoked potential},
	type  = {Article}}





@inproceedings{Wang2021Ieee,
  author  = {Wang, Sixian and Dai, Jincheng and Yao, Shengshi and Niu, Kai and Zhang, Ping},
  booktitle  = {2021 IEEE Global Communications Conference (GLOBECOM)}, 
  title  = {A Novel Deep Learning Architecture for Wireless Image Transmission}, 
  year  = {2021},
  volume  = {},
  number  = {},
  pages  = {1--6},
  abstract  = {In this paper, the problem of neural compression based image transmission over wireless channels is studied. Since all procedures are considered over wireless links, the quality of training is affected by wireless factors such as packet errors. In the considered model, compressed data given by the neural source encoder (NSE) are fed into an error-control channel encoder and modulated as discrete symbols sent over a memoryless channel. In the receiving end, the channel decoder and the neural source decoder (NSD) forms an iterative structure to reconstruct the original image. Since all neural compressed data are transmitted over wireless channels, the training of NSD is affected by wireless channel factors such as residual bit errors given by the channel decoder. Meanwhile, during outer-loop iterations, the NSD needs to match the variant of information reliability output by the channel decoder so as to build a global optimal receiver. To this end, a refiner neural network is first attached after the NSD to adjust its output as the format of a priori information sent into the channel decoder. Then, the extrinsic information transfer (EXIT) functions of channel decoder and NSD are derived. At each iteration, the reliability of messages sent into the NSD is explicitly predicted by using the EXIT chart. By this means, the NSD can be trained in a residual bit error aware manner, and we realize a joint learning and iterative decoding framework to ensure the quality of neural image transmission over realistic wireless channels.},
  keywords  = {Wireless communication;Training;Image coding;Image communication;Neural networks;Receivers;Reliability engineering},
  doi  = {10.1109/GLOBECOM46510.2021.9685036},
  issn  = {},
  month  = {Dec}}



@article{Wu2021Encoding,
  author  = {Wu, Hao and Zhu, Ziyu and Wang, Jiayi and Zheng, Nanning and Chen, Badong},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems}, 
  title  = {An Encoding Framework With Brain Inner State for Natural Image Identification}, 
  year  = {2021},
  volume  = {13},
  number  = {3},
  pages  = {453--464},
  abstract  = {Neural encoding and decoding, which aim to characterize the relationship between stimuli and brain activities, have emerged as an important area in cognitive neuroscience. Traditional encoding models, which focus on feature extraction and mapping, consider the brain as an input-output mapper without inner states. In this article, inspired by the fact that the human brain acts like a state machine, we proposed a novel encoding framework that combines information from both the external world and the inner state to predict brain activity. The framework comprises two parts: 1) forward encoding model that deals with visual stimuli and 2) inner state model that captures influence from intrinsic connections in the brain. The forward model can be any traditional encoding model, making the framework flexible. The inner state model is a linear model to utilize information in the prediction residuals of the forward model. The proposed encoding framework achieved much better performance on natural image identification than forward-only models, with a maximum identification accuracy of 100%. The identification accuracy decreased slightly with the data set size increasing, but remained relatively stable with different identification methods. The results confirm that the new encoding framework is effective and robust when used for brain decoding.},
  keywords  = {Encoding;Brain modeling;Decoding;Predictive models;Visualization;Feature extraction;Connectivity;decoding;functional magnetic resonance imaging (fMRI);perception;voxelwise encoding},
  doi  = {10.1109/TCDS.2020.2987352},
  issn  = {2379-8939},
  month  = {Sep.}}



@article{Yang2021Revealing,
	author  = {Yang, Qianli and Walker, Edgar Y. and Cotton, Ronald James and Tolias, Andreas Savas and Pitkow, Xaq},
	title  = {Revealing nonlinear neural decoding by analyzing choices},
	year  = {2021},
	journal  = {Nature Communications},
	volume  = {12},
	number  = {1},
	pages  = {},
	doi  = {10.1038/s41467-021-26793-9},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119093419&doi=10.1038%2Fs41467-021-26793-9&partnerID=40&md5=9977d514944407fd931324b10f53526a},
	abstract  = {Sensory data about most natural task-relevant variables are entangled with task-irrelevant nuisance variables. The neurons that encode these relevant signals typically constitute a nonlinear population code. Here we present a theoretical framework for quantifying how the brain uses or decodes its nonlinear information. Our theory obeys fundamental mathematical limitations on information content inherited from the sensory periphery, describing redundant codes when there are many more cortical neurons than primary sensory neurons. The theory predicts that if the brain uses its nonlinear population codes optimally, then more informative patterns should be more correlated with choices. More specifically, the theory predicts a simple, easily computed quantitative relationship between fluctuating neural activity and behavioral choices that reveals the decoding efficiency. This relationship holds for optimal feedforward networks of modest complexity, when experiments are performed under natural nuisance variation. We analyze recordings from primary visual cortex of monkeys discriminating the distribution from which oriented stimuli were drawn, and find these data are consistent with the hypothesis of near-optimal nonlinear decoding. © 2021 Elsevier B.V., All rights reserved.},
	keywords  = {brain; data set; experimental study; sensory system; animal cell; animal experiment; article; brain cell; conceptual framework; Haplorhini; nonhuman; quantitative analysis; sensory nerve cell; striate cortex; algorithm; animal; biological model; metabolism; nerve cell; theoretical model; Algorithms; Animals; Brain; Models, Neurological; Models, Theoretical; Neurons; Primary Visual Cortex},
	type  = {Article}}





@conference{Awangga2020Literature,
	author  = {Awangga, Rolly Maulana and Mengko, Tati Latifah E.R. and Utama, Nugraha Priya},
	title  = {A literature review of brain decoding research},
	year  = {2020},
	journal  = {IOP Conference Series: Materials Science and Engineering},
	volume  = {830},
	number  = {3},
	pages  = {},
	doi  = {10.1088/1757-899X/830/3/032049},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086339669&doi=10.1088%2F1757-899X%2F830%2F3%2F032049&partnerID=40&md5=71d4124a3b70c284072b55b8ee16c6bc},
	abstract  = {Brain Decoding is a popular topic in neuroscience. The purpose is how to reconstruct an object that came from a sensory system using brain activity data. There is three brain area generally use in brain decoding research. The somatosensory area generally using mice and touch they whisker. Auditory area using different sound frequency as stimuli. The visual area using shape, random image, and video. Take one example in the visual cortex. Using the retinotopic mapping concept, the object possible to reconstruct using visual cortex activity recorded by fMRI. Retinotopic mapping focus is to relate fMRI records into visual objects seen by the subject. This brain possibilities of decoding research come to the next level when combining using deep learning. The image seen by the subject can be reconstructed by using visual cortex activity. Make reconstruction come faster and realistic to predict the stimuli. This opportunity is opening the era of the brain-computer interface. Combine a method to analyze brain functionality related to the human sensory. Bring hope and increased human quality of life. This paper reviews research in the field of brain encoding. Divide into three sections, the first section is brain decoding research in somatosensory. The second section is brain decoding in the auditory cortex. For the last section, explain visual cortex reconstruction. Every section includes equipment devices to record brain activity and the source of datasets and methods to get the brain activity data. © 2020 Elsevier B.V., All rights reserved.},
	type  = {Conference paper}}





@article{Huang2020Long,
	author  = {Huang, Wei and Yan, Hongmei and Wang, Chong and Li, Jiyi and Yang, Xiaoqing and Li, Liang and Zuo, Zhentao and Zhang, Jiang and Chen, Huafu},
	title  = {Long short-term memory-based neural decoding of object categories evoked by natural images},
	year  = {2020},
	journal  = {Human Brain Mapping},
	volume  = {41},
	number  = {15},
	pages  = {4442 - 4453},
	doi  = {10.1002/hbm.25136},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087712424&doi=10.1002%2Fhbm.25136&partnerID=40&md5=693d7f7134d0c12c6eccdaefa87b5dc0},
	abstract  = {Visual perceptual decoding is one of the important and challenging topics in cognitive neuroscience. Building a mapping model between visual response signals and visual contents is the key point of decoding. Most previous studies used peak response signals to decode object categories. However, brain activities measured by functional magnetic resonance imaging are a dynamic process with time dependence, so peak signals cannot fully represent the whole process, which may affect the performance of decoding. Here, we propose a decoding model based on long short-term memory (LSTM) network to decode five object categories from multitime response signals evoked by natural images. Experimental results show that the average decoding accuracy using the multitime (2–6 s) response signals is 0.540 from the five subjects, which is significantly higher than that using the peak ones (6 s; accuracy: 0.492; p '.05). In addition, from the perspective of different durations, methods and visual areas, the decoding performances of the five object categories are deeply and comprehensively explored. The analysis of different durations and decoding methods reveals that the LSTM-based decoding model with sequence simulation ability can fit the time dependence of the multitime visual response signals to achieve higher decoding performance. The comparative analysis of different visual areas demonstrates that the higher visual cortex (VC) contains more semantic category information needed for visual perceptual decoding than lower VC. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {adult; article; controlled study; deep learning; female; functional magnetic resonance imaging; human; human experiment; long short term memory network; major clinical study; male; simulation; visual cortex; brain mapping; concept formation; diagnostic imaging; long term memory; nerve cell network; nuclear magnetic resonance imaging; pattern recognition; physiology; short term memory; theoretical model; young adult; Adult; Brain Mapping; Concept Formation; Female; Humans; Magnetic Resonance Imaging; Male; Memory, Long-Term; Memory, Short-Term; Models, Theoretical; Nerve Net; Pattern Recognition, Visual; Visual Cortex; Young Adult},
	type  = {Article}}





@article{Huang2020Perceptiontoimage,
	author  = {Huang, Wei and Yan, Hongmei and Wang, Chong and Li, Jiyi and Zuo, Zhentao and Zhang, Jiang and Shen, Zhan and Chen, Huafu},
	title  = {Perception-to-Image: Reconstructing Natural Images from the Brain Activity of Visual Perception},
	year  = {2020},
	journal  = {Annals of Biomedical Engineering},
	volume  = {48},
	number  = {9},
	pages  = {2323 - 2332},
	doi  = {10.1007/s10439-020-02502-3},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083795890&doi=10.1007%2Fs10439-020-02502-3&partnerID=40&md5=cbc780cb64570a975f6e4e551917b31c},
	abstract  = {The reappearance of human visual perception is a challenging topic in the field of brain decoding. Due to the complexity of visual stimuli and the constraints of fMRI data collection, the present decoding methods can only reconstruct the basic outline or provide similar figures/features of the perceived natural stimuli. To achieve a high-quality and high-resolution reconstruction of natural images from brain activity, this paper presents an end-to-end perception reconstruction model called the similarity-conditions generative adversarial network (SC-GAN), where visually perceptible images are reconstructed based on human visual cortex responses. The SC-GAN extracts the high-level semantic features of natural images and corresponding visual cortical responses and then introduces the semantic features as conditions of generative adversarial networks (GANs) to realize the perceptual reconstruction of visual images. The experimental results show that the semantic features extracted from SC-GAN play a key role in the reconstruction of natural images. The similarity between the presented and reconstructed images obtained by the SC-GAN is significantly higher than that obtained by a condition generative adversarial network (C-GAN). The model we proposed offers a potential perspective for decoding the brain activity of complex natural stimuli. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {Brain; Complex networks; Decoding; Neurophysiology; Semantics; Vision; Adversarial networks; High-level semantic features; High-resolution reconstruction; Human visual cortex; Human visual perception; Reconstructed image; Semantic features; Visual perception; Image reconstruction; article; brain function; deep learning; human; human experiment; retina image; visual cortex; adult; clinical trial; diagnostic imaging; female; image processing; male; nuclear magnetic resonance imaging; physiology; vision; Adult; Female; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Visual Cortex; Visual Perception},
	type  = {Article}}





@article{Kauvar2020Cortical,
	author  = {Kauvar, Isaac V. and MacHado, Timothy A. and Yuen, Elle and Kochalka, John and Choi, Minseung and Allen, William E. and Wetzstein, Gordon and Deisseroth, Karl},
	title  = {Cortical Observation by Synchronous Multifocal Optical Sampling Reveals Widespread Population Encoding of Actions},
	year  = {2020},
	journal  = {Neuron},
	volume  = {107},
	number  = {2},
	pages  = {351 - 367.e19},
	doi  = {10.1016/j.neuron.2020.04.023},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085762879&doi=10.1016%2Fj.neuron.2020.04.023&partnerID=40&md5=2f82c09bd7643398f91a254b0d47d8cf},
	abstract  = {Kauvar, Machado, et al. have developed a new method, COSMOS, to simultaneously record neural dynamics at ∼30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice. With COSMOS, they observe cortex-spanning population encoding of actions during a three-option lick-to-target task.; To advance the measurement of distributed neuronal population representations of targeted motor actions on single trials, we developed an optical method (COSMOS) for tracking neural activity in a largely uncharacterized spatiotemporal regime. COSMOS allowed simultaneous recording of neural dynamics at ∼30 Hz from over a thousand near-cellular resolution neuronal sources spread across the entire dorsal neocortex of awake, behaving mice during a three-option lick-to-target task. We identified spatially distributed neuronal population representations spanning the dorsal cortex that precisely encoded ongoing motor actions on single trials. Neuronal correlations measured at video rate using unaveraged, whole-session data had localized spatial structure, whereas trial-averaged data exhibited widespread correlations. Separable modes of neural activity encoded history-guided motor plans, with similar population dynamics in individual areas throughout cortex. These initial experiments illustrate how COSMOS enables investigation of large-scale cortical dynamics and that information about motor actions is widely shared between areas, potentially underlying distributed computations. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {animal experiment; animal tissue; Article; controlled study; cortical observation by synchronous multifocal optical sampling; female; Go No Go task; licking; male; motor control; motor cortex; motor performance; mouse; neocortex; nonhuman; optogenetics; parietal cortex; priority journal; retrosplenial cortex; signal noise ratio; somatosensory cortex; stimulus response; visual cortex; visual orientation; visual stimulation; algorithm; animal; animal behavior; brain cortex; brain mapping; craniotomy; cytology; devices; instrumental conditioning; nerve cell; neuroimaging; observation; physiology; procedures; psychomotor performance; robot assisted surgery; Algorithms; Animals; Behavior, Animal; Brain Mapping; Cerebral Cortex; Conditioning, Operant; Craniotomy; Mice; Neocortex; Neuroimaging; Neurons; Observation; Optogenetics; Psychomotor Performance; Robotic Surgical Procedures; Signal-To-Noise Ratio},
	type  = {Article}}





@article{MaimonMor2020Artificial,
	author  = {Maimon-Mor, Roni O. and Makin, Tamar R.},
	title  = {Is an artificial limb embodied as a hand? Brain decoding in prosthetic limb users},
	year  = {2020},
	journal  = {PLOS Biology},
	volume  = {18},
	number  = {6},
	pages  = {},
	doi  = {10.1371/journal.pbio.3000729},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086748893&doi=10.1371%2Fjournal.pbio.3000729&partnerID=40&md5=e6246dafd3dc190cd6fee7d0f02d8f67},
	abstract  = {The potential ability of the human brain to represent an artificial limb as a body part (embodiment) has been inspiring engineers, clinicians, and scientists as a means to optimise human-machine interfaces. Using functional MRI (fMRI), we studied whether neural embodiment actually occurs in prosthesis users' occipitotemporal cortex (OTC). Compared with controls, different prostheses types were visually represented more similarly to each other, relative to hands and tools, indicating the emergence of a dissociated prosthesis categorisation. Greater daily life prosthesis usage correlated positively with greater prosthesis categorisation. Moreover, when comparing prosthesis users' representation of their own prosthesis to controls' representation of a similar looking prosthesis, prosthesis users represented their own prosthesis more dissimilarly to hands, challenging current views of visual prosthesis embodiment. Our results reveal a use-dependent neural correlate for wearable technology adoption, demonstrating adaptive use-related plasticity within the OTC. Because these neural correlates were independent of the prostheses' appearance and control, our findings offer new opportunities for prosthesis design by lifting restrictions imposed by the embodiment theory for artificial limbs. © 2020 Elsevier B.V., All rights reserved.},
	keywords  = {Article; body image; brain cortex; brain decoding; brain function; functional magnetic resonance imaging; human; mental representation; nerve cell plasticity; occipitotemporal cortex; perception; prosthesis design; adult; brain; cluster analysis; female; hand; limb prosthesis; male; middle aged; photostimulation; physiology; visual cortex; young adult; Adult; Artificial Limbs; Brain; Cluster Analysis; Female; Hand; Humans; Male; Middle Aged; Photic Stimulation; Visual Cortex; Young Adult},
	type  = {Article}}





@inproceedings{Mali2020Data,
  author  = {Mali, Ankur and Ororbia, Alexander G. and Giles, C Lee},
  booktitle  = {2020 Data Compression Conference (DCC)}, 
  title  = {The Sibling Neural Estimator: Improving Iterative Image Decoding with Gradient Communication}, 
  year  = {2020},
  volume  = {},
  number  = {},
  pages  = {23--32},
  abstract  = {For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that "help" each other reconstruct the same target image patches using complementary portions of the spatial context, communicating with each other via gradient signals. This dual agent system builds upon prior work that proposed an iterative refinement algorithm for recurrent neural network (RNN) based decoding. Our approach works with any neural or non-neural encoder. Our system progressively reduces image patch reconstruction error over a fixed number of steps. Experiments with variations of RNN memory cells show that our system consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe gains of 1:64 decibel (dB) over JPEG, a 1:46 dB over JPEG2000, a 1:34 dB over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder.},
  keywords  = {Measurement;Visualization;Image coding;Recurrent neural networks;Nonlinear distortion;Transform coding;Rate-distortion;Decoding;Iterative decoding;Image reconstruction;Compression;decoder;RNN;Neural Networks},
  doi  = {10.1109/DCC47342.2020.00010},
  issn  = {2375-0359},
  month  = {March}}



@inproceedings{Nategh2020Asilomar,
  author  = {Nategh, Neda},
  booktitle  = {2020 54th Asilomar Conference on Signals, Systems, and Computers}, 
  title  = {Decoding Neural Activity to Anticipate Eye Movements}, 
  year  = {2020},
  volume  = {},
  number  = {},
  pages  = {375--378},
  abstract  = {Neural interfaces for motor control read out motor planning activity in the brain and use it to control a computer or physical device in order to restore or replace motor functions of the brain. Applications involving the readout of intended eye movements however remain comparatively undeveloped. In this study, we aim to develop decoding algorithms that can predict eye movement signals from neural responses during the planning period, before the animal makes an eye movement. The single-trial local field potentials of neurons in Frontal Eye Field (FEF), a cortical area that contributes to the control of eye movements, as well as, in area V4 of nonhuman primates are used to train the eye movement decoders. The algorithms identified and optimized in this study can facilitate the development of brain machine interface systems for eye movements, with an ultimate goal of providing assistive technologies to correct for the impaired gaze control in patients with eye movement disorders.},
  keywords  = {Motor drives;Animals;Assistive technology;Neurons;Neural activity;Prediction algorithms;Brain-computer interfaces;Neural decoding;eye movement;microsaccade;local field potential;visual cortex},
  doi  = {10.1109/IEEECONF51394.2020.9443415},
  issn  = {2576-2303},
  month  = {Nov}}



@article{Nestor2020Face,
	author  = {Nestor, Adrian and Lee, Andy C.H. and Plaut, David C. and Behrmann, Marlene Behrmann},
	title  = {The Face of Image Reconstruction: Progress, Pitfalls, Prospects},
	year  = {2020},
	journal  = {Trends in Cognitive Sciences},
	volume  = {24},
	number  = {9},
	pages  = {747 - 759},
	doi  = {10.1016/j.tics.2020.06.006},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087940586&doi=10.1016%2Fj.tics.2020.06.006&partnerID=40&md5=9fcf699627ba8a7ce99109d98f655ef6},
	abstract  = {Recent research has demonstrated that neural and behavioral data acquired in response to viewing face images can be used to reconstruct the images themselves. However, the theoretical implications, promises, and challenges of this direction of research remain unclear. We evaluate the potential of this research for elucidating the visual representations underlying face recognition. Specifically, we outline complementary and converging accounts of the visual content, the representational structure, and the neural dynamics of face processing. We illustrate how this research addresses fundamental questions in the study of normal and impaired face recognition, and how image reconstruction provides a powerful framework for uncovering face representations, for unifying multiple types of empirical data, and for facilitating both theoretical and methodological progress. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {Face recognition; Behavioral data; Empirical data; Face processing; Face representations; Neural dynamics; Recent researches; Visual content; Visual representations; Image reconstruction; facial recognition; image reconstruction; review; theoretical study; brain mapping; human; image processing; nuclear magnetic resonance imaging; pattern recognition; Brain Mapping; Facial Recognition; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Pattern Recognition, Visual},
	type  = {Review}}





@article{Sharon2020Neural,
  author  = {Sharon, Rini A. and Narayanan, Shrikanth S. and Sur, Mriganka and Murthy, A. Hema},
  journal  = {IEEE Access}, 
  title  = {Neural Speech Decoding During Audition, Imagination and Production}, 
  year  = {2020},
  volume  = {8},
  number  = {},
  pages  = {149714--149729},
  abstract  = {Interpretation of neural signals to a form that is as intelligible as speech facilitates the development of communication mediums for the otherwise speech/motor-impaired individuals. Speech perception, production, and imagination often constitute phases of human communication. The primary goal of this article is to analyze the similarity between these three phases by studying electroencephalogram(EEG) patterns across these modalities, in order to establish their usefulness for brain computer interfaces. Neural decoding of speech using such non-invasive techniques necessitates the optimal choice of signal analysis and translation protocols. By employing selection-by-exclusion based temporal modeling algorithms, we discover fundamental syllable-like units that reveal similar set of signal signatures across all the three phases. Significantly higher than chance accuracies are recorded for single trial multi-unit EEG classification using machine learning approaches over three datasets across 30 subjects. Repeatability and subject independence tests performed at every step of the analysis further strengthens the findings and holds promise for translating brain signals to speech non-invasively.},
  keywords  = {Electroencephalography;Production;Protocols;Image segmentation;Brain modeling;Correlation;Image reconstruction;Assistive technology;brain computer interface;EEG;imagined speech;speech-EEG correlation;unit classification},
  doi  = {10.1109/ACCESS.2020.3016756},
  issn  = {2169-3536},
  month  = {}}



@article{Vetter2020Decoding,
	author  = {Vetter, Philipp and Bola, Łukasz and Reich, Lior and Bennett, Matthew A. and Muckli, Lars F. and Amedi, Amir},
	title  = {Decoding Natural Sounds in Early “Visual” Cortex of Congenitally Blind Individuals},
	year  = {2020},
	journal  = {Current Biology},
	volume  = {30},
	number  = {15},
	pages  = {3039 - 3044.e2},
	doi  = {10.1016/j.cub.2020.05.071},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088361291&doi=10.1016%2Fj.cub.2020.05.071&partnerID=40&md5=46fcdc07af48bbc3e02ba1034be35fd7},
	abstract  = {Complex natural sounds, such as bird singing, people talking, or traffic noise, induce decodable fMRI activation patterns in early visual cortex of sighted blindfolded participants [1]. That is, early visual cortex receives non-visual and potentially predictive information from audition. However, it is unclear whether the transfer of auditory information to early visual areas is an epiphenomenon of visual imagery or, alternatively, whether it is driven by mechanisms independent from visual experience. Here, we show that we can decode natural sounds from activity patterns in early “visual” areas of congenitally blind individuals who lack visual imagery. Thus, visual imagery is not a prerequisite of auditory feedback to early visual cortex. Furthermore, the spatial pattern of sound decoding accuracy in early visual cortex was remarkably similar in blind and sighted individuals, with an increasing decoding accuracy gradient from foveal to peripheral regions. This suggests that the typical organization by eccentricity of early visual cortex develops for auditory feedback, even in the lifelong absence of vision. The same feedback to early visual cortex might support visual perception in the sighted [1] and drive the recruitment of this area for non-visual functions in blind individuals [2, 3].; Natural sounds can be distinguished based on early visual cortex activity in sighted people. Is this effect driven by visual imagery? Vetter et al. report successful sound decoding, increasing from fovea to periphery, in people blind from birth, proving that visual imagery is not necessary for sound representation in these early visual areas. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {auditory stimulation; blindness; diagnostic imaging; human; nuclear magnetic resonance imaging; pathophysiology; physiology; sensory feedback; sound; visual cortex; Acoustic Stimulation; Blindness; Feedback, Sensory; Humans; Magnetic Resonance Imaging; Sound; Visual Cortex},
	type  = {Article}}





@article{Zhang2020Reconstruction,
	author  = {Zhang, Yichen and Jia, Shanshan and Zheng, Yajing and Yu, Zhaofei and Tian, Yonghong and Ma, Siwei and Huang, Tie Jun and Liu, Jian K.},
	title  = {Reconstruction of natural visual scenes from neural spikes with deep neural networks},
	year  = {2020},
	journal  = {Neural Networks},
	volume  = {125},
	pages  = {19 - 30},
	doi  = {10.1016/j.neunet.2020.01.033},
	url  = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079328270&doi=10.1016%2Fj.neunet.2020.01.033&partnerID=40&md5=f96edc7f6152742fb50abe7d06f9c3a0},
	abstract  = {Neural coding is one of the central questions in systems neuroscience for understanding how the brain processes stimulus from the environment, moreover, it is also a cornerstone for designing algorithms of brain–machine interface, where decoding incoming stimulus is highly demanded for better performance of physical devices. Traditionally researchers have focused on functional magnetic resonance imaging (fMRI) data as the neural signals of interest for decoding visual scenes. However, our visual perception operates in a fast time scale of millisecond in terms of an event termed neural spike. There are few studies of decoding by using spikes. Here we fulfill this aim by developing a novel decoding framework based on deep neural networks, named spike-image decoder (SID), for reconstructing natural visual scenes, including static images and dynamic videos, from experimentally recorded spikes of a population of retinal ganglion cells. The SID is an end-to-end decoder with one end as neural spikes and the other end as images, which can be trained directly such that visual scenes are reconstructed from spikes in a highly accurate fashion. Our SID also outperforms on the reconstruction of visual stimulus compared to existing fMRI decoding models. In addition, with the aid of a spike encoder, we show that SID can be generalized to arbitrary visual scenes by using the image datasets of MNIST, CIFAR10, and CIFAR100. Furthermore, with a pre-trained SID, one can decode any dynamic videos to achieve real-time encoding and decoding of visual scenes by spikes. Altogether, our results shed new light on neuromorphic computing for artificial visual systems, such as event-based visual cameras and visual neuroprostheses. © 2020 Elsevier B.V., All rights reserved.},	keywords  = {Aldehydes; Decoding; Deep learning; Deep neural networks; Image reconstruction; Magnetic resonance imaging; Neural networks; Signal encoding; Video signal processing; Vision; Artificial retinas; Functional magnetic resonance imaging; Natural scenes; Neural decoding; Neural spike; Neuromorphic computing; Real-time encoding; Retinal ganglion cells; Functional neuroimaging; Article; deep learning; deep neural network; functional magnetic resonance imaging; human; longitudinal study; mathematical computing; priority journal; retina; retina ganglion cell; spike image decoder; vision; visual system; animal; biological model; brain computer interface; diagnostic imaging; nuclear magnetic resonance imaging; physiology; visual cortex; visual evoked potential; Animals; Brain-Computer Interfaces; Evoked Potentials, Visual; Magnetic Resonance Imaging; Models, Neurological; Neural Networks, Computer; Retinal Ganglion Cells; Visual Cortex; Visual Perception},
	type  = {Article}}





@inproceedings{Zhou2020International,
  author  = {Zhou, Qiongyi and Du, Changde and Li, Dan and Wang, Haibao and Liu, Jian K. and He, Huiguang},
  booktitle  = {2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title  = {Simultaneous Neural Spike Encoding and Decoding Based on Cross-modal Dual Deep Generative Model}, 
  year  = {2020},
  volume  = {},
  number  = {},
  pages  = {1--8},
  abstract  = {Neural encoding and decoding of retinal ganglion cells (RGCs) have been attached great importance in the research work of brain-machine interfaces. Much effort has been invested to mimic RGC and get insight into RGC signals to reconstruct stimuli. However, there remain two challenges. On the one hand, complex nonlinear processes in retinal neural circuits hinder encoding models from enhancing their ability to fit the natural stimuli and modelling RGCs accurately. On the other hand, current research of the decoding process is separate from that of the encoding process, in which the liaison of mutual promotion between them is neglected. In order to alleviate the above problems, we propose a cross-modal dual deep generative model (CDDG) in this paper. CDDG treats the RGC spike signals and the stimuli as two modalities, which learns a shared latent representation for the concatenated modality and two modal-specific latent representations. Then, it imposes distribution consistency restriction on different latent space, cross-consistency and cycle-consistency constraints on the generated variables. Thus, our model ensures cross-modal generation from RGC spike signals to stimuli and vice versa. In our framework, the generation from stimuli to RGC spike signals is equivalent to neural encoding while the inverse process is equivalent to neural decoding. Hence, the proposed method integrates neural encoding and decoding and exploits the reciprocity between them. The experimental results demonstrate that our proposed method can achieve excellent encoding and decoding performance compared with the state-of-the-art methods on three salamander RGC spike datasets with natural stimuli.},
  keywords  = {Decoding;Encoding;Retina;Visualization;Brain modeling;Image reconstruction;Bidirectional control;dual learning;cross-modal generation;retinal ganglion cells;neural encoding;neural decoding},
  doi  = {10.1109/IJCNN48605.2020.9207466},
  issn  = {2161-4407},
  month  = {July}}



@inproceedings{Baluja2019Ieee,
  author  = {Baluja, Shumeet and Marwood, David and Johnston, Nick and Covell, Michele},
  booktitle  = {2019 IEEE International Conference on Image Processing (ICIP)}, 
  title  = {Learning to Render Better Image Previews}, 
  year  = {2019},
  volume  = {},
  number  = {},
  pages  = {1700--1704},
  abstract  = {A rapidly increasing portion of Internet traffic is dominated by requests from mobile devices with limited and metered bandwidth constraints. To satisfy these requests, it has become standard practice for websites to transmit small and extremely compressed image previews as part of the initial page-load process. Recent work, based on an adaptive triangulation of the target image, has performed well at extreme compression rates: 200 bytes or less. Gains have been shown, in terms of PSNR and SSIM, over both JPEG and WebP standards. However, qualitative assessments and preservation of semantic content can be less favorable. We present a novel method to significantly improve the reconstruction quality of the original image that requires no changes to the encoded information. Our neural-based decoding triples the amount of semantic-level content preservation while also improving both SSIM and PSNR scores. In addition, by keeping the same encoding stream, our solution is completely inter-operable with the original, and remains suitable for small-device deployment.},
  keywords  = {Decoding;Image coding;Image color analysis;Transform coding;Standards;Image reconstruction;Image edge detection;Compression;Semantic Quality Measurement;Image Triangulation;Deep Neural Networks},
  doi  = {10.1109/ICIP.2019.8803147},
  issn  = {2381-8549},
  month  = {Sep.}}



@article{Du2019Reconstructing,
  author  = {Du, Changde and Du, Changying and Huang, Lijie and He, Huiguang},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems}, 
  title  = {Reconstructing Perceived Images From Human Brain Activities With Bayesian Deep Multiview Learning}, 
  year  = {2019},
  volume  = {30},
  number  = {8},
  pages  = {2310--2323},
  abstract  = {Neural decoding, which aims to predict external visual stimuli information from evoked brain activities, plays an important role in understanding human visual system. Many existing methods are based on linear models, and most of them only focus on either the brain activity pattern classification or visual stimuli identification. Accurate reconstruction of the perceived images from the measured human brain activities still remains challenging. In this paper, we propose a novel deep generative multiview model for the accurate visual image reconstruction from the human brain activities measured by functional magnetic resonance imaging (fMRI). Specifically, we model the statistical relationships between the two views (i.e., the visual stimuli and the evoked fMRI) by using two view-specific generators with a shared latent space. On the one hand, we adopt a deep neural network architecture for visual image generation, which mimics the stages of human visual processing. On the other hand, we design a sparse Bayesian linear model for fMRI activity generation, which can effectively capture voxel correlations, suppress data noise, and avoid overfitting. Furthermore, we devise an efficient mean-field variational inference method to train the proposed model. The proposed method can accurately reconstruct visual images via Bayesian inference. In particular, we exploit a posterior regularization technique in the Bayesian inference to regularize the model posterior. The quantitative and qualitative evaluations conducted on multiple fMRI data sets demonstrate the proposed method can reconstruct visual images more accurately than the state of the art.},
  keywords  = {Visualization;Brain modeling;Functional magnetic resonance imaging;Image reconstruction;Decoding;Bayes methods;Deep neural network (DNN);image reconstruction;multiview learning;neural decoding;variational Bayesian inference},
  doi  = {10.1109/TNNLS.2018.2882456},
  issn  = {2162-2388},
  month  = {Aug}}



@inproceedings{Lee2019Ieee,
  author  = {Lee, Seo-Hyun and Lee, Minji and Jeong, Ji-Hoon and Lee, Seong-Whan},
  booktitle  = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)}, 
  title  = {Towards an EEG-based Intuitive BCI Communication System Using Imagined Speech and Visual Imagery}, 
  year  = {2019},
  volume  = {},
  number  = {},
  pages  = {4409--4414},
  abstract  = {Communication using brain-computer interface (BCI) has developed in attempts toward an intuitive system by decoding the imagined speech or visual imagery. However, discrimination between the two paradigms may be ambiguous because the user intention contains their original meaning. A clear distinction between the two paradigms may facilitate the active use of them leading to an intuitive BCI conversation system. In this study, we compared imagined speech and visual imagery in the perspective of its presence, spatial features, and classification performance based on electroencephalography. Seven subjects performed both imagined speech and visual imagery of twelve words/phrases. We showed the presence of the two paradigms, having distinct brain region from each other. The maximum thirteen-class classification accuracy including rest class was 34.2 % for imagined speech and 26.7 % for visual imagery. Therefore, we investigated the possibility of multiclass classification of more than ten classes in both paradigms, showing the potential of them to be used in the real world communication system. These findings could further be utilized in the intuitive communication for locked-in patients sending commands to the external world simply by thinking of `the very thing' that the user wants to deliver.},
  keywords  = {Visualization;Task analysis;Electroencephalography;Brain;Decoding;Radio frequency},
  doi  = {10.1109/SMC.2019.8914645},
  issn  = {2577-1655},
  month  = {Oct}}



@inproceedings{Ororbia2019Data,
  author  = {Ororbia, Alexander G. and Mali, Ankur and Wu, Jian and O'Connell, Scott and Dreese, William and Miller, David and Giles, C. Lee},
  booktitle  = {2019 Data Compression Conference (DCC)}, 
  title  = {Learned Neural Iterative Decoding for Lossy Image Compression Systems}, 
  year  = {2019},
  volume  = {},
  number  = {},
  pages  = {3--12},
  abstract  = {For lossy image compression systems, we develop an algorithm, iterative refinement, to improve the decoder's reconstruction compared to standard decoding techniques. Specifically, we propose a recurrent neural network approach for nonlinear, iterative decoding. Our decoder, which works with any encoder, employs self-connected memory units that make use of causal and non-causal spatial context information to progressively reduce reconstruction error over a fixed number of steps. We experiment with variants of our estimator and find that iterative refinement consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a 0.971 dB gain over a competitive neural model.},
  keywords  = {Decoding;Iterative decoding;Image reconstruction;Image coding;Transform coding;Computational modeling;Iterative algorithms;iterative refinement;compression;recurrent neural networks},
  doi  = {10.1109/DCC.2019.00008},
  issn  = {2375-0359},
  month  = {March}}



@article{Yu2019Brain,
  author  = {Yu, Siyu and Zheng, Nanning and Ma, Yongqiang and Wu, Hao and Chen, Badong},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems}, 
  title  = {A Novel Brain Decoding Method: A Correlation Network Framework for Revealing Brain Connections}, 
  year  = {2019},
  volume  = {11},
  number  = {1},
  pages  = {95--106},
  abstract  = {Brain decoding is a hot spot in cognitive science, which focuses on reconstructing perceptual images from brain activities. Analyzing the correlations of collected data from human brain activities and representing activity patterns are two key problems in brain decoding based on functional magnetic resonance imaging signals. However, existing correlation analysis methods mainly focus on the strength information of voxel, which reveals functional connectivity in the cerebral cortex. They tend to neglect the structural information that implies the intracortical or intrinsic connections; that is, structural connectivity. Hence, the effective connectivity inferred by these methods is relatively unilateral. Therefore, we propose in this paper a correlation network (CorrNet) framework that could be flexibly combined with diverse pattern representation models. In the CorrNet framework, the topological correlation is introduced to reveal structural information. Rich correlations can be obtained, which contribute to specifying the underlying effective connectivity. We also combine the CorrNet framework with a linear support vector machine and a dynamic evolving spike neuron network for pattern representation separately, thus provide a novel method for decoding cognitive activity patterns. Experimental results verify the reliability and robustness of our CorrNet framework, and demonstrate that the new method can achieve significant improvement in brain decoding over comparable methods.},
  keywords  = {Correlation;Brain modeling;Functional magnetic resonance imaging;Decoding;Image reconstruction;Visualization;Brain decoding;connection;correlation network (CorrNet) framework;functional magnetic resonance imaging (fMRI);pattern representation},
  doi  = {10.1109/TCDS.2018.2854274},
  issn  = {2379-8939},
  month  = {March}}



