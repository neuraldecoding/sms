@INPROCEEDINGS{10759955,
  author={Ishizaki, Fumi and Kobayashi, Ichiro},
  booktitle={2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCIS&ISIS)}, 
  title={A Study on Brain Decoding of Image Stimuli Using a Diffusion Model}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Humans recognize the external world by processing information received by the eyes and other sensory organs in the brain. Understanding how the human brain processes complex information from the outside world is expected to improve the performance of image and speech recognition technologies, which have made remarkable progress in recent years. In this study, we focus on brain activity decoding of visual experience, aim to read what humans are looking at by predicting image features from brain activity data, and further attempt to develop a method to output high-definition and semantically valid images by generating images from predicted features using a diffusion model. As a result, similarity to the stimulus image was confirmed in the image generated by the training data, but the evaluation data confirmed that there is still room for further study.},
  keywords={Visualization;Noise reduction;Training data;Speech recognition;Predictive models;Diffusion models;Brain modeling;Data models;Decoding;Image reconstruction;Brain decoding;diffusion models;image stimu-lation},
  doi={10.1109/SCISISIS61014.2024.10759955},
  ISSN={},
  month={Nov},}@ARTICLE{10473144,
  author={Meng, Lu and Yang, Chuanhao},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Semantics-Guided Hierarchical Feature Encoding Generative Adversarial Network for Visual Image Reconstruction From Brain Activity}, 
  year={2024},
  volume={32},
  number={},
  pages={1267-1283},
  abstract={The utilization of deep learning techniques for decoding visual perception images from brain activity recorded by functional magnetic resonance imaging (fMRI) has garnered considerable attention in recent research. However, reconstructed images from previous studies still suffer from low quality or unreliability. Moreover, the complexity inherent to fMRI data, characterized by high dimensionality and low signal-to-noise ratio, poses significant challenges in extracting meaningful visual information for perceptual reconstruction. In this regard, we proposes a novel neural decoding model, named the hierarchical semantic generative adversarial network (HS-GAN), inspired by the hierarchical encoding of the visual cortex and the homology theory of convolutional neural networks (CNNs), which is capable of reconstructing perceptual images from fMRI data by leveraging the hierarchical and semantic representations. The experimental results demonstrate that HS-GAN achieved the best performance on Horikawa2017 dataset (histogram similarity: 0.447, SSIM-Acc: 78.9%, Peceptual-Acc: 95.38%, AlexNet(2): 96.24% and AlexNet(5): 94.82%) over existing advanced methods, indicating improved naturalness and fidelity of the reconstructed image. The versatility of the HS-GAN was also highlighted, as it demonstrated promising generalization capabilities in reconstructing handwritten digits, achieving the highest SSIM (0.783±0.038), thus extending its application beyond training solely on natural images.},
  keywords={Visualization;Functional magnetic resonance imaging;Image reconstruction;Decoding;Feature extraction;Semantics;Training;Visual decoding;image reconstruction;generative adversarial network;fMRI},
  doi={10.1109/TNSRE.2024.3377698},
  ISSN={1558-0210},
  month={},}@ARTICLE{9229132,
  author={Du, Changde and Du, Changying and Huang, Lijie and Wang, Haibao and He, Huiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Structured Neural Decoding With Multitask Transfer Learning of Deep Neural Network Representations}, 
  year={2022},
  volume={33},
  number={2},
  pages={600-614},
  abstract={The reconstruction of visual information from human brain activity is a very important research topic in brain decoding. Existing methods ignore the structural information underlying the brain activities and the visual features, which severely limits their performance and interpretability. Here, we propose a hierarchically structured neural decoding framework by using multitask transfer learning of deep neural network (DNN) representations and a matrix-variate Gaussian prior. Our framework consists of two stages, Voxel2Unit and Unit2Pixel. In Voxel2Unit, we decode the functional magnetic resonance imaging (fMRI) data to the intermediate features of a pretrained convolutional neural network (CNN). In Unit2Pixel, we further invert the predicted CNN features back to the visual images. Matrix-variate Gaussian prior allows us to take into account the structures between feature dimensions and between regression tasks, which are useful for improving decoding effectiveness and interpretability. This is in contrast with the existing single-output regression models that usually ignore these structures. We conduct extensive experiments on two real-world fMRI data sets, and the results show that our method can predict CNN features more accurately and reconstruct the perceived natural images and faces with higher quality.},
  keywords={Decoding;Image reconstruction;Functional magnetic resonance imaging;Visualization;Task analysis;Brain;Correlation;Deep neural network (DNN);functional magnetic resonance imaging (fMRI);image reconstruction;multioutput regression;neural decoding},
  doi={10.1109/TNNLS.2020.3028167},
  ISSN={2162-2388},
  month={Feb},}@ARTICLE{8409319,
  author={Yu, Siyu and Zheng, Nanning and Ma, Yongqiang and Wu, Hao and Chen, Badong},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={A Novel Brain Decoding Method: A Correlation Network Framework for Revealing Brain Connections}, 
  year={2019},
  volume={11},
  number={1},
  pages={95-106},
  abstract={Brain decoding is a hot spot in cognitive science, which focuses on reconstructing perceptual images from brain activities. Analyzing the correlations of collected data from human brain activities and representing activity patterns are two key problems in brain decoding based on functional magnetic resonance imaging signals. However, existing correlation analysis methods mainly focus on the strength information of voxel, which reveals functional connectivity in the cerebral cortex. They tend to neglect the structural information that implies the intracortical or intrinsic connections; that is, structural connectivity. Hence, the effective connectivity inferred by these methods is relatively unilateral. Therefore, we propose in this paper a correlation network (CorrNet) framework that could be flexibly combined with diverse pattern representation models. In the CorrNet framework, the topological correlation is introduced to reveal structural information. Rich correlations can be obtained, which contribute to specifying the underlying effective connectivity. We also combine the CorrNet framework with a linear support vector machine and a dynamic evolving spike neuron network for pattern representation separately, thus provide a novel method for decoding cognitive activity patterns. Experimental results verify the reliability and robustness of our CorrNet framework, and demonstrate that the new method can achieve significant improvement in brain decoding over comparable methods.},
  keywords={Correlation;Brain modeling;Functional magnetic resonance imaging;Decoding;Image reconstruction;Visualization;Brain decoding;connection;correlation network (CorrNet) framework;functional magnetic resonance imaging (fMRI);pattern representation},
  doi={10.1109/TCDS.2018.2854274},
  ISSN={2379-8939},
  month={March},}@INPROCEEDINGS{10191903,
  author={Meng, Lu and Yang, Chuanhao},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Semantics-guided hierarchical feature encoding generative adversarial network for natural image reconstruction from brain activities}, 
  year={2023},
  volume={},
  number={},
  pages={1-9},
  abstract={The use of deep learning methods to decode visual perception images from brain activity recorded by fMRI has received a lot of attention. However, limited fMRI data make the task of visual reconstruction challenging. Inspired by hierarchical encoding of the visual cortex and the theory of brain homology with convolutional neural networks (CNNs), we propose a novel neural decoding model called hierarchical semantic generative adversarial network (HS-GAN). Specifically, we use CNN-based image encoder to extract hierarchical and semantic features of visually stimulus images. Then a neural decoder is used to decode hierarchical and semantic features from fMRI. In order to take full advantage of the information from different visual cortexes, we construct a generator with self-attention modules and skip connections to fuse the image features of different layers. In model training, adversarial learning is introduced to realize more natural image reconstruction. Compared to existing advanced methods, our method significantly improves the naturalness and fidelity of reconstructed images.},
  keywords={Visualization;Image coding;Semantics;Functional magnetic resonance imaging;Generative adversarial networks;Feature extraction;Brain modeling;brain decoding;fMRI;GAN;image reconstruction;deep learning},
  doi={10.1109/IJCNN54540.2023.10191903},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10655620,
  author={Wang, Shizun and Liu, Songhua and Tan, Zhenxiong and Wang, Xinchao},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={MindBridge: A Cross-Subject Brain Decoding Framework}, 
  year={2024},
  volume={},
  number={},
  pages={11333-11342},
  abstract={Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI). Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained. This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited data availability for new subjects in real-world scenarios hampers the performance of decoding models. In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model. Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant representation learning. Notably, by cycle re-construction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo data augmentation. Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject. Experimental results demonstrate MindBridge's ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models. Fur-thermore, with limited data for a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models. This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI data in real-world scenarios. Project page: https://littlepure2333.github.io/MindBridge},
  keywords={Representation learning;Adaptation models;Neuroscience;Accuracy;Biological system modeling;Functional magnetic resonance imaging;Brain modeling;Brain decoding;Cross-subject;diffusion model},
  doi={10.1109/CVPR52733.2024.01077},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9892276,
  author={Chen, Kai and Ma, Yongqiang and Sheng, Mingyang and Zheng, Nanning},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Foreground-attention in neural decoding: Guiding Loop-Enc-Dec to reconstruct visual stimulus images from fMRI}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={The reconstruction of visual stimulus images from functional Magnetic Resonance Imaging (fMRI) has received extensive attention in recent years, which provides a possibility to interpret the human brain. Due to the high-dimensional and high-noise characteristics of fMRI data, how to extract stable, reliable and useful information from fMRI data for image reconstruction has become a challenging problem. Inspired by the mechanism of human visual attention, in this paper, we propose a novel method of reconstructing visual stimulus images, which first decodes human visual salient region from fMRI, we define human visual salient region as foreground attention (F-attention), and then reconstructs the visual images guided by F-attention. Because the human brain is strongly wound into sulci and gyri, some spatially adjacent voxels are not connected in practice. Therefore, it is necessary to consider the global information when decoding fMRI, so we introduce the self-attention module for capturing global information into the process of decoding F-attention. In addition, in order to obtain more loss constraints in the training process of encoder-decoder, we also propose a new training strategy called Loop-Enc-Dec. The experimental results show that the F-attention decoder decodes the visual attention from fMRI successfully, and the Loop-Enc-Dec guided by F-attention can also well reconstruct the visual stimulus images.},
  keywords={Training;Visualization;Neuroscience;Shape;Functional magnetic resonance imaging;Decoding;Reliability;visual reconstruction;Foreground-attention;fMRI;neural decoding},
  doi={10.1109/IJCNN55064.2022.9892276},
  ISSN={2161-4407},
  month={July},}@ARTICLE{10292854,
  author={Li, Ziyu and Li, Qing and Zhu, Zhiyuan and Hu, Zhongyi and Wu, Xia},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Multi-Scale Spatio-Temporal Fusion With Adaptive Brain Topology Learning for fMRI Based Neural Decoding}, 
  year={2024},
  volume={28},
  number={1},
  pages={262-272},
  abstract={Neural decoding aims to extract information from neurons' activities to reveal how the brain functions. Due to the inherent spatial and temporal characteristics of brain signals, spatio-temporal computing has become a hot topic for neural decoding. However, the extant spatio-temporal decoding methods usually use static brain topology, ignoring the dynamic patterns of the interaction between brain regions. Further, they do not identify the hierarchical organization of brain topology, leading to only superficial insight into brain spatio-temporal interactions. Therefore, here we propose a novel framework, the Multi-Scale Spatio-Temporal framework with Adaptive Brain Topology Learning (MSST-ABTL), for neural decoding. It includes two new capabilities to enhance spatio-temporal decoding: i) ABTL module, which learns dynamic brain topology while updating specific patterns of brain regions, ii) MSST module, which captures the association of spatial pattern and temporal evolution, and further enhances the interpretability of the learned dynamic topology from multi-scale perspective. We evaluated the framework on the public Human Connectome Project (HCP) dataset (resting-state and task-related fMRI data). The extensive experiments show that the proposed MSST-ABTL outperforms state-of-the-art methods on four evaluation metrics, and also can renew the neuroscientific discoveries in the brain's hierarchical patterns.},
  keywords={Topology;Decoding;Network topology;Correlation;Brain modeling;Functional magnetic resonance imaging;Task analysis;Neural decoding;adaptive;brain topology;multi-scale;spatio-temporal},
  doi={10.1109/JBHI.2023.3327023},
  ISSN={2168-2208},
  month={Jan},}@ARTICLE{10949627,
  author={Zhu, Chunzheng and Shao, Jialin and Lin, Jianxin and Wang, Yijun and Wang, Jing and Tang, Jinhui and Li, Kenli},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={fMRI2GES: Co-Speech Gesture Reconstruction From fMRI Signal With Dual Brain Decoding Alignment}, 
  year={2025},
  volume={35},
  number={9},
  pages={9017-9029},
  abstract={Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired {brain, speech, gesture} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, fMRI2GES, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using Dual Brain Decoding Alignment. This method relies on two key components: 1) observed texts that elicit brain responses, and 2) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.},
  keywords={Decoding;Functional magnetic resonance imaging;Brain modeling;Image reconstruction;Training;Data models;Recording;Neuroscience;Linguistics;Legged locomotion;fMRI signal;diffusoin models;neurosciences},
  doi={10.1109/TCSVT.2025.3558125},
  ISSN={1558-2205},
  month={Sep.},}@ARTICLE{11018227,
  author={Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang},
  journal={IEEE Transactions on Image Processing}, 
  title={MindGPT: Interpreting What You See With Non-Invasive Brain Recordings}, 
  year={2025},
  volume={34},
  number={},
  pages={3281-3293},
  abstract={Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed MindGPT, which interprets perceived visual stimuli into natural languages from functional Magnetic Resonance Imaging (fMRI) signals in an end-to-end manner. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism. By the collaborative use of data augmentation techniques, this architecture permits us to guide latent neural representations towards a desired language semantic direction in a self-supervised fashion. Through doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. The source code for the MindGPT model is publicly available at https://github.com/JxuanC/MindGPT.},
  keywords={Visualization;Semantics;Functional magnetic resonance imaging;Image reconstruction;Decoding;Brain modeling;Recording;Training;Data augmentation;Predictive models;Neural decoding;functional magnetic resonance imaging;text reconstruction;self-supervised learning;multimodal representation learning},
  doi={10.1109/TIP.2025.3572784},
  ISSN={1941-0042},
  month={},}@ARTICLE{10683806,
  author={Pan, Hongguang and Li, Zhuoyi and Fu, Yunpeng and Qin, Xuebin and Hu, Jianchen},
  journal={IEEE Transactions on Human-Machine Systems}, 
  title={Reconstructing Visual Stimulus Representation From EEG Signals Based on Deep Visual Representation Model}, 
  year={2024},
  volume={54},
  number={6},
  pages={711-722},
  abstract={Reconstructing visual stimulus representation is a significant task in neural decoding. Until now, most studies have considered functional magnetic resonance imaging (fMRI) as the signal source. However, fMRI-based image reconstruction methods are challenging to apply widely due to the complexity and high cost of acquisition equipment. Taking into account the advantages of the low cost and easy portability of electroencephalogram (EEG) acquisition equipment, we propose a novel image reconstruction method based on EEG signals in this article. First, to meet the high recognizability of visual stimulus images in a fast-switching manner, we construct a visual stimuli image dataset and obtain the corresponding EEG dataset through EEG signals collection experiment. Second, we introduce the deep visual representation model (DVRM), comprising a primary encoder and a subordinate decoder, to reconstruct visual stimuli representation. The encoder is designed based on residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images. Meanwhile, the decoder is designed using a deep neural network to reconstruct the visual stimulus representation from the learned deep visual representation. The DVRM can accommodate the deep and multiview visual features of the human natural state, resulting in more precise reconstructed images. Finally, we evaluate the DVRM based on the quality of the generated images using our EEG dataset. The results demonstrate that the DVRM exhibits an excellent performance in learning deep visual representation from EEG signals, generating reconstructed representation of images that are realistic and highly resemble the original images.},
  keywords={Visualization;Electroencephalography;Image reconstruction;Feature extraction;Brain modeling;Functional magnetic resonance imaging;Decoding;Deep visual representation model (DVRM);electroencephalogram (EEG) dataset;image reconstruction;neural decoding},
  doi={10.1109/THMS.2024.3407875},
  ISSN={2168-2305},
  month={Dec},}@ARTICLE{9931072,
  author={Zhang, Yijun and Bu, Tong and Zhang, Jiyuan and Tang, Shiming and Yu, Zhaofei and Liu, Jian K. and Huang, Tiejun},
  journal={Neural Computation}, 
  title={Decoding Pixel-Level Image Features From Two-Photon Calcium Signals of Macaque Visual Cortex}, 
  year={2022},
  volume={34},
  number={6},
  pages={1369-1397},
  abstract={Images of visual scenes comprise essential features important for visual cognition of the brain. The complexity of visual features lies at different levels, from simple artificial patterns to natural images with different scenes. It has been a focus of using stimulus images to predict neural responses. However, it remains unclear how to extract features from neuronal responses. Here we address this question by leveraging two-photon calcium neural data recorded from the visual cortex of awake macaque monkeys. With stimuli including various categories of artificial patterns and diverse scenes of natural images, we employed a deep neural network decoder inspired by image segmentation technique. Consistent with the notation of sparse coding for natural images, a few neurons with stronger responses dominated the decoding performance, whereas decoding of ar tificial patterns needs a large number of neurons. When natural images using the model pretrained on artificial patterns are decoded, salient features of natural scenes can be extracted, as well as the conventional category information. Altogether, our results give a new perspective on studying neural encoding principles using reverse-engineering decoding strategies.},
  keywords={},
  doi={10.1162/neco_a_01498},
  ISSN={0899-7667},
  month={May},}@ARTICLE{10336717,
  author={De Luca, Daniela and Moccia, Sara and Lupori, Leonardo and Mazziotti, Raffaele and Pizzorusso, T. and Micera, Silvestro},
  journal={IEEE Sensors Journal}, 
  title={Predicting Visual Stimuli From Cortical Response Recorded With Wide-Field Imaging in a Mouse}, 
  year={2024},
  volume={24},
  number={6},
  pages={7299-7307},
  abstract={Neural decoding of the visual system is a subject of research interest, both to understand how the visual system works and to be able to use this knowledge in areas, such as computer vision or brain–computer interfaces. Spike-based decoding is often used, but it is difficult to record data from the whole visual cortex, and it requires proper preprocessing. We here propose a decoding method that combines wide-field calcium brain imaging, which allows us to obtain large-scale visualization of cortical activity with a high signal-to-noise ratio (SNR), and convolutional neural networks (CNNs). A mouse was presented with ten different visual stimuli, and the activity from its primary visual cortex (V1) was recorded. A CNN we designed was then compared with other existing commonly used CNNs, that were trained to classify the visual stimuli from wide-field calcium imaging images, obtaining a weighted  $F1$  score of more than 0.70 on the test set, showing it is possible to automatically detect what is present in the visual field of the animal.},
  keywords={Visualization;Convolutional neural networks;Decoding;Calcium;Imaging;Fluorescence;Deep learning;Transfer learning;Prosthetics;Neural activity;Deep learning;transfer learning;visual cortex;visual prostheses;wide-field imaging},
  doi={10.1109/JSEN.2023.3335613},
  ISSN={1558-1748},
  month={March},}@ARTICLE{10413505,
  author={Yu, Zhaofei and Bu, Tong and Zhang, Yijun and Jia, Shanshan and Huang, Tiejun and Liu, Jian K.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Robust Decoding of Rich Dynamical Visual Scenes With Retinal Spikes}, 
  year={2025},
  volume={36},
  number={2},
  pages={3396-3409},
  abstract={Sensory information transmitted to the brain activates neurons to create a series of coping behaviors. Understanding the mechanisms of neural computation and reverse engineering the brain to build intelligent machines requires establishing a robust relationship between stimuli and neural responses. Neural decoding aims to reconstruct the original stimuli that trigger neural responses. With the recent upsurge of artificial intelligence, neural decoding provides an insightful perspective for designing novel algorithms of brain–machine interface. For humans, vision is the dominant contributor to the interaction between the external environment and the brain. In this study, utilizing the retinal neural spike data collected over multi trials with visual stimuli of two movies with different levels of scene complexity, we used a neural network decoder to quantify the decoded visual stimuli with six different metrics for image quality assessment establishing comprehensive inspection of decoding. With the detailed and systematical study of the effect and single and multiple trials of data, different noise in spikes, and blurred images, our results provide an in-depth investigation of decoding dynamical visual scenes using retinal spikes. These results provide insights into the neural coding of visual scenes and services as a guideline for designing next-generation decoding algorithms of neuroprosthesis and other devices of brain–machine interface.},
  keywords={Decoding;Visualization;Retina;Image reconstruction;Neurons;Convolution;Training;Deep learning;image reconstruction;neural decoding;neural spikes;video;visual scenes},
  doi={10.1109/TNNLS.2024.3351120},
  ISSN={2162-2388},
  month={Feb},}@ARTICLE{9790856,
  author={Luo, Jie and Cui, Weigang and Liu, Jingyu and Li, Yang and Guo, Yuzhu and Xu, Song and Wang, Lina},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={Visual Image Decoding of Brain Activities Using a Dual Attention Hierarchical Latent Generative Network With Multiscale Feature Fusion}, 
  year={2023},
  volume={15},
  number={2},
  pages={761-773},
  abstract={Reconstructing visual stimulus from human brain activity measured with functional magnetic resonance imaging (fMRI) is a challenging decoding task for revealing the visual system. Recent deep learning approaches commonly neglect the relationship between hierarchical image features and different regions of the visual cortex, and fail to use global and local image features in reconstructing visual stimulus. To address these issues, in this article, a novel neural decoding framework is proposed by using a dual attention (DA) hierarchical latent generative network with multiscale feature fusion (DA-HLGN-MSFF) method. Specifically, the fMRI data are first encoded to hierarchical features of our image encoder network, which employs a multikernel convolution block to extract the multiscale spatial information of images. In order to reconstruct the perceived images and further improve the performance of our generator network, a DA block based on the channel-spatial attention mechanism is then proposed to exploit the interchannel relationships and spatial long-range dependencies of features. Moreover, a multiscale feature fusion block is finally adopted to aggregate the global and local information of features at different scales and synthesize the final reconstructed images in the generator network. Competitive experimental results on two public fMRI data sets demonstrate that our method is able to achieve promising reconstructing performance compared with the state-of-the-art methods. The codes of our proposed DA-HLGN-MSFF method will be open access on https://github.com/ljbuaa/HLDAGN.},
  keywords={Image reconstruction;Functional magnetic resonance imaging;Visualization;Generators;Decoding;Generative adversarial networks;Feature extraction;Deep neural network (DNN);functional magnetic resonance imaging (fMRI) decoding;generative adversarial network (GAN);image reconstruction;visual cortex},
  doi={10.1109/TCDS.2022.3181469},
  ISSN={2379-8939},
  month={June},}@INPROCEEDINGS{9892673,
  author={Ozcelik, Furkan and Choksi, Bhavin and Mozafari, Milad and Reddy, Leila and VanRullen, Rufin},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Reconstructing perceived natural images from fMRI signals is one of the most engaging topics of neural decoding research. Prior studies had success in reconstructing either the low-level image features or the semantic/high-level aspects, but rarely both. In this study, we utilized an Instance-Conditioned GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate semantic attributes and preserved low-level details. The IC-GAN model takes as input a 119-dim noise vector and a 2048-dim instance feature vector extracted from a target image via a self-supervised learning model (SwAV ResNet-50); these instance features act as a conditioning for IC-GAN image generation, while the noise vector introduces variability between samples. We trained ridge regression models to predict instance features, noise vectors, and dense vectors (the output of the first dense layer of the IC-GAN generator) of stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to reconstruct novel test images based on these fMRI-predicted variables. The generated images presented state-of-the-art results in terms of capturing the semantic attributes of the original test images while remaining relatively faithful to low-level image details. Finally, we use the learned regression model and the IC-GAN generator to systematically explore and visualize the semantic features that maximally drive each of several regions-of-interest in the human brain.},
  keywords={Visualization;Image synthesis;Semantics;Self-supervised learning;Functional magnetic resonance imaging;Predictive models;Feature extraction;Natural Image Reconstruction;fMRI Decoding;IC-GAN;Brain-Computer Interface},
  doi={10.1109/IJCNN55064.2022.9892673},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10667256,
  author={Sugimoto, Yuma and Pongthanisorn, Goragod and Capi, Genci},
  booktitle={2024 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
  title={Image Generation using EEG data: A Contrastive Learning based Approach}, 
  year={2024},
  volume={},
  number={},
  pages={794-798},
  abstract={In recent years, there has been active research on Neural Decoding aims to decrypt perceptual cognitive content, recall content, and motor information directly from brain signals. Simultaneously, deep learning has been widely implemented especially in generative models. Diverse architectures and learning methods have been formulated and applied across numerous fields. In this work, we focus on generating perceptual and cognitive contents using brain electroencephalography (EEG) data. We propose and demonstrate the efficacy of contrastive learning to generate images only from EEG data. We compare the performance of two electrode settings 1) visual cortex and 2) motor cortex.},
  keywords={Learning systems;Electrodes;Deep learning;Visualization;Image synthesis;Contrastive learning;Motors;Visual Imagery;EEG signals;Contrastive Learning},
  doi={10.1109/CCECE59415.2024.10667256},
  ISSN={2576-7046},
  month={Aug},}@ARTICLE{8574054,
  author={Du, Changde and Du, Changying and Huang, Lijie and He, Huiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Reconstructing Perceived Images From Human Brain Activities With Bayesian Deep Multiview Learning}, 
  year={2019},
  volume={30},
  number={8},
  pages={2310-2323},
  abstract={Neural decoding, which aims to predict external visual stimuli information from evoked brain activities, plays an important role in understanding human visual system. Many existing methods are based on linear models, and most of them only focus on either the brain activity pattern classification or visual stimuli identification. Accurate reconstruction of the perceived images from the measured human brain activities still remains challenging. In this paper, we propose a novel deep generative multiview model for the accurate visual image reconstruction from the human brain activities measured by functional magnetic resonance imaging (fMRI). Specifically, we model the statistical relationships between the two views (i.e., the visual stimuli and the evoked fMRI) by using two view-specific generators with a shared latent space. On the one hand, we adopt a deep neural network architecture for visual image generation, which mimics the stages of human visual processing. On the other hand, we design a sparse Bayesian linear model for fMRI activity generation, which can effectively capture voxel correlations, suppress data noise, and avoid overfitting. Furthermore, we devise an efficient mean-field variational inference method to train the proposed model. The proposed method can accurately reconstruct visual images via Bayesian inference. In particular, we exploit a posterior regularization technique in the Bayesian inference to regularize the model posterior. The quantitative and qualitative evaluations conducted on multiple fMRI data sets demonstrate the proposed method can reconstruct visual images more accurately than the state of the art.},
  keywords={Visualization;Brain modeling;Functional magnetic resonance imaging;Image reconstruction;Decoding;Bayes methods;Deep neural network (DNN);image reconstruction;multiview learning;neural decoding;variational Bayesian inference},
  doi={10.1109/TNNLS.2018.2882456},
  ISSN={2162-2388},
  month={Aug},}@ARTICLE{10993346,
  author={Li, Wei and Zhao, Penglu and Xu, Cheng and Hou, Yingting and Jiang, Wenhao and Song, Aiguo},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Deep Learning for EEG-Based Visual Classification and Reconstruction: Panorama, Trends, Challenges and Opportunities}, 
  year={2025},
  volume={},
  number={},
  pages={1-17},
  abstract={Deep learning has significantly enhanced the research on the emerging issue of Electroencephalogram (EEG)-based visual classification and reconstruction, which has gained a growth of attention and concern recently. To promote the research progress, at this critical moment, a review work on the deep learning methodology for the issue becomes necessary and important. However, such a work seems absent in the literature. This paper provides the first review on EEG-based visual classification and reconstruction, whose contents can be categorized into the following four main parts: 1) comprehensively summarizing and systematically analyzing the representative deep learning methods from both feature encoding and decoding perspectives; 2) introducing the available benchmark datasets, describing the experimental paradigms, and displaying the method performances; 3) proposing the methodological essences and neuroscientific insights as well as the dynamic closed-loop interaction and promotion between them, which are potentially beneficial for technological innovations and academic progress; 4) discussing the potential challenges of current research and the prospective opportunities in future trends. We expect that this work can shed light on the technological directions and also enlighten the academic breakthroughs for the issue in the not-so-far future.},
  keywords={Visualization;Electroencephalography;Reviews;Image reconstruction;Deep learning;Decoding;Functional magnetic resonance imaging;Convolutional neural networks;Feature extraction;Transformers;Electroencephalogram;Deep Learning;Visual Classification;Visual Reconstruction},
  doi={10.1109/TBME.2025.3568282},
  ISSN={1558-2531},
  month={},}@ARTICLE{10858771,
  author={Ma, Yongqiang and Liu, Yulong and Chen, Liangjun and Zhu, Guibo and Chen, Badong and Zheng, Nanning},
  journal={IEEE Transactions on Medical Imaging}, 
  title={BrainCLIP: Brain Representation via CLIP for Generic Natural Visual Stimulus Decoding}, 
  year={2025},
  volume={},
  number={},
  pages={1-1},
  abstract={Functional Magnetic Resonance Imaging (fMRI) presents challenges due to limited paired samples and low signal-to-noise ratios, particularly in tasks involving reconstructing natural images or decoding their semantic content. To address these challenges, we introduce BrainCLIP, an innovative fMRI-based brain decoding model. BrainCLIP leverages Contrastive Language-Image Pre-training’s (CLIP) cross-modal generalization abilities to bridge brain activity, images, and text for the first time. Our experiments demonstrate CLIP’s effectiveness in diverse brain decoding tasks, including zero-shot visual category decoding, fMRI-image/text alignment, and fMRI-to-image generation. The core objective of BrainCLIP is to train a mapping network that translates fMRI patterns into a unified CLIP embedding space, achieved through visual and textual supervision integration. Our experiments highlight that this approach significantly enhances performance in tasks such as fMRI-text alignment and fMRI-based image generation. Notably, BrainCLIP surpasses BraVL, a recent multi-modal method, in zero-shot visual category decoding. Moreover, BrainCLIP demonstrates strong capability in reconstructing visual stimuli with high semantic fidelity, competing favorably with state-of-the-art methods in capturing high-level semantic features during fMRI-based natural image reconstruction.},
  keywords={Visualization;Decoding;Functional magnetic resonance imaging;Brain modeling;Semantics;Image reconstruction;Training;Brain;Feature extraction;Contrastive learning;Brain decoding;CLIP;Visual-Linguistic representation;Cross-modal},
  doi={10.1109/TMI.2025.3537287},
  ISSN={1558-254X},
  month={},}@INPROCEEDINGS{10656844,
  author={Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Mind Artist: Creating Artistic Snapshots with Human Thought}, 
  year={2024},
  volume={},
  number={},
  pages={27197-27207},
  abstract={We introduce Mind Artist (MindArt), a novel and efficient neural decoding architecture to snap artistic photographs from our mind in a controllable manner. Recently, progress has been made in image reconstruction with non-invasive brain recordings, but it's still difficult to generate realistic images with high semantic fidelity due to the scarcity of data annotations. Unlike previous methods, this work casts the neural decoding into optimal transport (OT) and representation decoupling problems. Specifically, under discrete OT theory, we design a graph matching-guided neural representation learning framework to seek the underlying correspondences between conceptual semantics and neural signals, which yields a natural and meaningful self-supervisory task. Moreover, the proposed MindArt, structured with multiple stand-alone modal branches, enables the seamless incorporation of semantic representation into any visual style information, thus leaving it to have multi-modal reconstruction and training-free semantic editing capabilities. By doing so, the reconstructed images of MindArt have phenomenal realism both in terms of semantics and appearance. We compare our MindArt with leading alternatives, and achieve SOTA performance in different decoding tasks. Importantly, our approach can directly generate a series of stylized “mind snapshots” w/o extra optimizations, which may open up more potential applications. Code is available at https://github.com/JxuanC/MindArt.},
  keywords={Visualization;Solid modeling;Three-dimensional displays;Computational modeling;Semantics;Linguistics;Decoding;Neural decoding;Representation learning;Multimodal learning},
  doi={10.1109/CVPR52733.2024.02569},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10001915,
  author={Taguchi, Haruka and Nishida, Satoshi and Nishimoto, Shinji and Kobayashi, Ichiro},
  booktitle={2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems (SCIS&ISIS)}, 
  title={Validation of the Role of Attention Mechanism in Predicting Brain Activity}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={In this study, we estimate the state of the human brain from visual stimuli by regressing the brain activity state from image features extracted from an image identification deep learning model. We introduce Attention Branch Network, which enhances to capture the features of the identified target by attention when extracting image features, into the image identification deep learning model and estimate the brain activity state from the image features weighted or unweighted by attention. Through experiments, we aim to verify the role of attention mechanism in estimating brain activity state from visual stimuli. As a result, we confirmed that the introduction of Attention did not have a significant effect on the estimation accuracy, but that differences were observed in the areas where the estimation accuracy was higher.},
  keywords={Deep learning;Visualization;Decision making;Estimation;Information representation;Predictive models;Functional magnetic resonance imaging;brain decoding;attention branch network;fMRI;ResNet},
  doi={10.1109/SCISISIS55246.2022.10001915},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10051903,
  author={Weng, Geyu and Akbarian, Amir and Noudoost, Behrad and Nategh, Neda},
  booktitle={2022 56th Asilomar Conference on Signals, Systems, and Computers}, 
  title={Modeling the Relationship between Perisaccadic Neural Responses and Location Information}, 
  year={2022},
  volume={},
  number={},
  pages={451-454},
  abstract={Eye movements are essential for the brain to collect visual information from the environment. Visual images projected on the retina change abruptly during rapid ballistic eye movements (saccades), but our perception of the visual world is continuous. To generate a stable visual perception, the spatiotemporal sensitivity of visual neurons needs to change quickly prior to and during saccades. This study uses a modeling framework to characterize the fast dynamics of neuronal responses across saccades, thereby quantifying the contribution of perisaccadic response dynamics to the readout of location information during saccades. We apply this approach to neuronal responses recorded from the visual cortex of nonhuman primates during a visually-guided saccade task with visual stimulations. Using the model-predicted responses and a classification method, we measure the spatial discriminability performance of neurons at pre-saccadic and post-saccadic receptive field locations. Characterizing the readout of perisaccadic spatial information and its precise time course can provide insights into how neurons integrate spatial information across saccades to generate a continuous visual experience.},
  keywords={Visualization;Sensitivity;Neurons;Electrophysiology;Brain modeling;Retina;Time measurement;computational models;neural decoding;eye movement;visual cortex},
  doi={10.1109/IEEECONF56349.2022.10051903},
  ISSN={2576-2303},
  month={Oct},}@INPROCEEDINGS{10798169,
  author={Song, Yahao and Liu, Tianyi and Sun, Chao and Zhang, Yuwei and Zheng, Minqian and Zhang, Milin},
  booktitle={2024 IEEE Biomedical Circuits and Systems Conference (BioCAS)}, 
  title={A Low-Power Wireless 128-Channel Neural Interface Circuit for Multiregional Brain Recording}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={This paper proposes a low-power, wireless 128channel neural interface circuit, comprising a 40 nm recorder circuit and a WiFi module. The proposed design enables realtime monitoring and neural signal decoding during behavioral studies, facilitating high-density, long-term tracking of neural signals in freely moving subjects. The recorder circuit integrates 16 -bit resolution at a maximum rate of 32 k samples per second (sps), with a power consumption of 3.43 mW and an area of $6.27 \mathrm{~mm}^{2}$,achieving the lowest normalized power consumption of $0.84 \mu \mathrm{~W} / \mathrm{ch} / \mathrm{ksps}$ among state-of-the-art works. The VoltageControlled Oscillator (VCO) Analog Front Ends (AFEs) feature an input-referred noise of $0.66 \mu \mathrm{~V}_{\text {rms}}$ within the $0.5-60 \mathrm{~Hz}$ range. The digital backend allows flexible channel configuration and includes an on-chip CIC filter to reduce out-of-band noise. The WiFi module, connected via QSPI, facilitates wireless control and data transmission. The proposed circuit was validated through electrocorticogram (ECoG) measurements in rodent subjects, demonstrating its reliability and flexibility.},
  keywords={Wireless communication;Power demand;Voltage-controlled oscillators;Noise;Recording;Decoding;System-on-chip;Data communication;Integrated circuit reliability;Wireless fidelity;Neural Signal Recording;Biomedical Signal Processing;ECoG;Wireless},
  doi={10.1109/BioCAS61083.2024.10798169},
  ISSN={2766-4465},
  month={Oct},}@INPROCEEDINGS{10981123,
  author={Posso-Murillo, Santiago and Sanchez-Giraldo, Luis G. and Bae, Jihye},
  booktitle={2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)}, 
  title={Semantic Reconstruction from Fnirs Using Recurrent Neural Networks}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Semantic reconstruction of language aims to decode the meaning of words or sentences from neural activity. Previous studies have demonstrated that functional near-infrared spectroscopy (fNIRS) contains information suitable for language decoding. However, most of the existing work on fNIRS-based neural decoding relies on traditional machine learning algorithms such as linear models and support vector machines, and it has been limited to classification of limited set of words. To address these shortcomings, we examine 4 recurrent neural networks (RNNs) that learn features to decode semantic representations from fNIRS: the Elman recurrent neural network (ERNN), long short-term memory (LSTM), and bidirectional version of them (BiERNN and BiLSTM). Using a publicly available fNIRS dataset, we performed within-category, between-category, and leave-two-out tests. The decoding performance was measured by computing the matching score, a pairwise metric that assesses the model's ability to distinguish between two concepts. The results show that ERNN and BiLSTM models consistently outperform linear decoder models. Specifically, ERNN shows better performance for 4 out of 7 subjects in the between-category test, and BiLSTM performs better for 6 out of 7 subjects in the within-category test and 4 out of 7 subjects in the leave-two-out test. Notably, in between-category experiment, the BiLSTM scored 61 % matching score for subject 3, representing a 9% improvement, and ERNN achieved an 80% matching score for subject 2, marking a significant 33% improvement. These promising results encourage the use of advanced machine learning models for semantic reconstruction from fNIRS. Code is available at https://github.com/sposso/Semantic-Reconstruction-using-fNIRS-signal.},
  keywords={Support vector machines;Neuroimaging;Machine learning algorithms;Computational modeling;Semantics;Neural activity;Bidirectional long short term memory;Decoding;Functional near-infrared spectroscopy;Image reconstruction;fNIRS;RNN;Semantic Reconstruction},
  doi={10.1109/ISBI60581.2025.10981123},
  ISSN={1945-8452},
  month={April},}@INPROCEEDINGS{9207466,
  author={Zhou, Qiongyi and Du, Changde and Li, Dan and Wang, Haibao and Liu, Jian K. and He, Huiguang},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Simultaneous Neural Spike Encoding and Decoding Based on Cross-modal Dual Deep Generative Model}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Neural encoding and decoding of retinal ganglion cells (RGCs) have been attached great importance in the research work of brain-machine interfaces. Much effort has been invested to mimic RGC and get insight into RGC signals to reconstruct stimuli. However, there remain two challenges. On the one hand, complex nonlinear processes in retinal neural circuits hinder encoding models from enhancing their ability to fit the natural stimuli and modelling RGCs accurately. On the other hand, current research of the decoding process is separate from that of the encoding process, in which the liaison of mutual promotion between them is neglected. In order to alleviate the above problems, we propose a cross-modal dual deep generative model (CDDG) in this paper. CDDG treats the RGC spike signals and the stimuli as two modalities, which learns a shared latent representation for the concatenated modality and two modal-specific latent representations. Then, it imposes distribution consistency restriction on different latent space, cross-consistency and cycle-consistency constraints on the generated variables. Thus, our model ensures cross-modal generation from RGC spike signals to stimuli and vice versa. In our framework, the generation from stimuli to RGC spike signals is equivalent to neural encoding while the inverse process is equivalent to neural decoding. Hence, the proposed method integrates neural encoding and decoding and exploits the reciprocity between them. The experimental results demonstrate that our proposed method can achieve excellent encoding and decoding performance compared with the state-of-the-art methods on three salamander RGC spike datasets with natural stimuli.},
  keywords={Decoding;Encoding;Retina;Visualization;Brain modeling;Image reconstruction;Bidirectional control;dual learning;cross-modal generation;retinal ganglion cells;neural encoding;neural decoding},
  doi={10.1109/IJCNN48605.2020.9207466},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10931751,
  author={Choi, Yebin and Kim, Jun-Mo and Choi, WooHyeok and Ji, Chang-Hoon and Oh, Ji-Hye and Kam, Tae-Eui},
  booktitle={2025 13th International Conference on Brain-Computer Interface (BCI)}, 
  title={Visual Decoding Using a Learnable Wavelet-Based Spatial-Spectral-Temporal EEG Embedding}, 
  year={2025},
  volume={},
  number={},
  pages={1-5},
  abstract={Visual decoding seeks to identify or reconstruct visual stimuli perceived by individuals based on neural activity. Although functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have achieved remarkable success in visual decoding, their high costs, limited portability, and real-time processing challenges necessitate alternative approaches. Electroencephalography (EEG) provides a promising solution due to its cost-effectiveness, high temporal resolution, and suitability for real-time applications. However, conventional EEG-based encoders often rely on simplistic architectures, which limits their ability to fully capture the spatial, spectral, and temporal (SST) features of EEG signals, leading to suboptimal performance. In this study, we propose a novel brain decoding framework utilizing a state-of-the-art EEG encoder specifically designed to capture the SST characteristics of EEG signals. The proposed framework was evaluated on the THINGS-EEG dataset. It achieved a mean top-l accuracy of 19.7% and a top-5 accuracy of 50.7% in zero-shot retrieval tasks, outperforming conventional EEG encoders. These results demonstrate the potential of our method in advancing EEG-based visual decoding task.},
  keywords={Visualization;Accuracy;Functional magnetic resonance imaging;Feature extraction;Electroencephalography;Real-time systems;Decoding;Spatial resolution;Image reconstruction;Visual perception;Brain Decoding;Visual Decoding;Elec-troencephalogram;Learnable Wavelet Kernel;Spectral-spatial-temporal Representation},
  doi={10.1109/BCI65088.2025.10931751},
  ISSN={2572-7672},
  month={Feb},}@INPROCEEDINGS{9443415,
  author={Nategh, Neda},
  booktitle={2020 54th Asilomar Conference on Signals, Systems, and Computers}, 
  title={Decoding Neural Activity to Anticipate Eye Movements}, 
  year={2020},
  volume={},
  number={},
  pages={375-378},
  abstract={Neural interfaces for motor control read out motor planning activity in the brain and use it to control a computer or physical device in order to restore or replace motor functions of the brain. Applications involving the readout of intended eye movements however remain comparatively undeveloped. In this study, we aim to develop decoding algorithms that can predict eye movement signals from neural responses during the planning period, before the animal makes an eye movement. The single-trial local field potentials of neurons in Frontal Eye Field (FEF), a cortical area that contributes to the control of eye movements, as well as, in area V4 of nonhuman primates are used to train the eye movement decoders. The algorithms identified and optimized in this study can facilitate the development of brain machine interface systems for eye movements, with an ultimate goal of providing assistive technologies to correct for the impaired gaze control in patients with eye movement disorders.},
  keywords={Motor drives;Animals;Assistive technology;Neurons;Neural activity;Prediction algorithms;Brain-computer interfaces;Neural decoding;eye movement;microsaccade;local field potential;visual cortex},
  doi={10.1109/IEEECONF51394.2020.9443415},
  ISSN={2576-2303},
  month={Nov},}@INPROCEEDINGS{9781770,
  author={Karam, Andrew and Boles, Kirollos and Raouf, Mario and Yousef, Mina Atef and Khoriba, Ghada},
  booktitle={2022 2nd International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)}, 
  title={Deep learning approaches for analysing visual stimuli from human fMRI}, 
  year={2022},
  volume={},
  number={},
  pages={449-458},
  abstract={Decoding brain activities corresponding to an external stimulus is an excellent challenge because of the complexity of brain activities data and the understood of the activity of the brain is not yet complete. This research paper focuses on the functional magnetic resonance imaging (fMRI) data of different people corresponding to external visual stimulus images. The images consist of three main types: letters, artificial shapes, and natural images. The proposed model provides an analysis and classification of the three main types of images using Support Vector Machine and Convolutional Neural Networks by proving that the brain is affected differently by each type. In addition, the identification of 200 natural image categories was analyzed by calculating the similarity between the features for each category by using the features of visual images using CNN and fMRI using the regression model. Also, we used deep convolution generative adversarial networks (DCGANs) for image reconstruction.},
  keywords={Support vector machines;Visualization;Analytical models;Functional magnetic resonance imaging;Predictive models;Brain modeling;Data models;Brain Decoding;Visual Image Reconstruction;Deep Neural Networks;GAN;fMRI},
  doi={10.1109/MIUCC55081.2022.9781770},
  ISSN={},
  month={May},}@INPROCEEDINGS{10029097,
  author={K, Sowmya and S, Sushitha},
  booktitle={2022 International Conference on Automation, Computing and Renewable Systems (ICACRS)}, 
  title={An Interpretation on Brain Gate System Network and Technology- A Study}, 
  year={2022},
  volume={},
  number={},
  pages={868-873},
  abstract={The process of thoughts-to-movement is a scientific discovery that makes a severely disabled human to manipulate a device using thoughts. These were achieved, in a great portion through the brain gate network technology. The network system uses a different kind of Brain neural signal which regulates the language of the neuron's sensing, transmission, perception, and implementation. The working idea of the brain gate system is that brainwave patterns are produced with brain function and not transmitted to the body. It is a brain implant device developed at Brown University in partnership with the biotech firm‚ Cyber-Kinetics Neuroscience. Even though effective BCI research has been arrived in the past 25 years, this paper aims to study the working, core principles and research aspects of brain gate technology in depth.},
  keywords={Digital control;Computer interfaces;Renewable energy sources;Neuroscience;Epilepsy;Neural implants;Logic gates;Neurotechnologies;Brain Computer Interface (BCI);Electro-magnetic Neurons;Motor Cortex;Neural Decoding;Central Nervus System(CNS)},
  doi={10.1109/ICACRS55517.2022.10029097},
  ISSN={},
  month={Dec},}@ARTICLE{10979442,
  author={Letafati, Mehdi and Amirhossein Ameli Kalkhoran, Seyyed and Erdemir, Ecenaz and Hossein Khalaj, Babak and Behroozi, Hamid and Gündüz, Deniz},
  journal={IEEE Transactions on Machine Learning in Communications and Networking}, 
  title={Deep Joint Source Channel Coding for Privacy-Aware End-to-End Image Transmission}, 
  year={2025},
  volume={3},
  number={},
  pages={568-584},
  abstract={Deep neural network (DNN)-based joint source and channel coding is proposed for privacy-aware end-to-end image transmission against multiple eavesdroppers. Both scenarios of colluding and non-colluding eavesdroppers are considered. Unlike prior works that assume perfectly known and independent identically distributed (i.i.d.) source and channel statistics, the proposed scheme operates under unknown and non-i.i.d. conditions, making it more applicable to real-world scenarios. The goal is to transmit images with minimum distortion, while simultaneously preventing eavesdroppers from inferring certain private attributes of images. Simultaneously generalizing the ideas of privacy funnel and wiretap coding, a multi-objective optimization framework is expressed that characterizes the trade-off between image reconstruction quality and information leakage to eavesdroppers, taking into account the structural similarity index (SSIM) for improving the perceptual quality of image reconstruction. Extensive experiments on the CIFAR-10 and CelebA, along with ablation studies, demonstrate significant performance improvements in terms of SSIM, adversarial accuracy, and the mutual information leakage compared to benchmarks. Experiments show that the proposed scheme restrains the adversarially-trained eavesdroppers from intercepting privatized data for both cases of eavesdropping a common secret, as well as the case in which eavesdroppers are interested in different secrets. Furthermore, useful insights on the privacy-utility trade-off are also provided.},
  keywords={Communication system security;Security;Image communication;Wireless sensor networks;Image reconstruction;Eavesdropping;Channel coding;Autoencoders;Wireless networks;Training;DeepJSCC;secure image transmission;end-to-end learning;privacy-utility trade-off;adversarial neural networks;deep learning},
  doi={10.1109/TMLCN.2025.3564907},
  ISSN={2831-316X},
  month={},}@INPROCEEDINGS{8803147,
  author={Baluja, Shumeet and Marwood, David and Johnston, Nick and Covell, Michele},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, 
  title={Learning to Render Better Image Previews}, 
  year={2019},
  volume={},
  number={},
  pages={1700-1704},
  abstract={A rapidly increasing portion of Internet traffic is dominated by requests from mobile devices with limited and metered bandwidth constraints. To satisfy these requests, it has become standard practice for websites to transmit small and extremely compressed image previews as part of the initial page-load process. Recent work, based on an adaptive triangulation of the target image, has performed well at extreme compression rates: 200 bytes or less. Gains have been shown, in terms of PSNR and SSIM, over both JPEG and WebP standards. However, qualitative assessments and preservation of semantic content can be less favorable. We present a novel method to significantly improve the reconstruction quality of the original image that requires no changes to the encoded information. Our neural-based decoding triples the amount of semantic-level content preservation while also improving both SSIM and PSNR scores. In addition, by keeping the same encoding stream, our solution is completely inter-operable with the original, and remains suitable for small-device deployment.},
  keywords={Decoding;Image coding;Image color analysis;Transform coding;Standards;Image reconstruction;Image edge detection;Compression;Semantic Quality Measurement;Image Triangulation;Deep Neural Networks},
  doi={10.1109/ICIP.2019.8803147},
  ISSN={2381-8549},
  month={Sep.},}@INPROCEEDINGS{10618584,
  author={Wang, Yaqi and Gui, Renzhou and Zhu, Wenbo and Yin, Yumiao and Tong, Meisong},
  booktitle={2024 Photonics & Electromagnetics Research Symposium (PIERS)}, 
  title={VTVBrain: A Two-stage Brain Encoding Model for Decoding Key Neural Responses in Multimodal Contexts}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={In the field of cognitive neuroscience, understanding how the brain processes multimodal complex stimuli is a long-standing and complex challenge. In this study, we propose a novel two-stage brain coding model called "VTVBrain" that focuses on decoding key neural responses in multimodal environments. In the first stage, we built a high-dimensional multimodal latent space pre-training model using an attentional mechanism-based variational autoencoder, aiming to capture and encode the main perceptual processes of observers when faced with multimodal stimuli integrating textual and visual information. In the second stage, the Versatile Diffusion model is utilized for image reconstruction of the first stage primary representational images for high-level perception. The proposed model further refines the latent space representation approach, explores the relationship between the brain’s neural responses and complex natural scenes, and provides insight into how the brain integrates and processes information from different senses at a higher level. The introduction of the VTVBrain model not only brings a new perspective to the field of neural decoding, but also has a great potential for application in the development of brain-like artificial intelligence and future cognitive neuroscience research. The introduction of the VTVBrain model not only brings a new perspective to the field of neural decoding, but also has great potential for the development of brain-like artificial intelligence and future cognitive neuroscience research.},
  keywords={Cognitive neuroscience;Semantics;Brain modeling;Diffusion models;Encoding;Decoding;Space exploration},
  doi={10.1109/PIERS62282.2024.10618584},
  ISSN={2831-5804},
  month={April},}@ARTICLE{10535091,
  author={Greenidge, C. Daniel and Scholl, Benjamin and Yates, Jacob L. and Pillow, Jonathan W.},
  journal={Neural Computation}, 
  title={Efficient Decoding of Large-Scale Neural Population Responses With Gaussian-Process Multiclass Regression}, 
  year={2024},
  volume={36},
  number={2},
  pages={175-226},
  abstract={Neural decoding methods provide a powerful tool for quantifying the information content of neural population codes and the limits imposed by correlations in neural activity. However, standard decoding methods are prone to overfitting and scale poorly to high-dimensional settings. Here, we introduce a novel decoding method to overcome these limitations. Our approach, the gaussian process multiclass decoder (GPMD), is well suited to decoding a continuous low-dimensional variable from high-dimensional population activity and provides a platform for assessing the importance of correlations in neural population codes. The GPMD is a multinomial logistic regression model with a gaussian process prior over the decoding weights. The prior includes hyperparameters that govern the smoothness of each neuron's decoding weights, allowing automatic pruning of uninformative neurons during inference. We provide a variational inference method for fitting the GPMD to data, which scales to hundreds or thousands of neurons and performs well even in data sets with more neurons than trials. We apply the GPMD to recordings from primary visual cortex in three species: monkey, ferret, and mouse. Our decoder achieves state-of-the-art accuracy on all three data sets and substantially outperforms independent Bayesian decoding, showing that knowledge of the correlation structure is essential for optimal decoding in all three species.},
  keywords={},
  doi={10.1162/neco_a_01630},
  ISSN={0899-7667},
  month={Jan},}@INPROCEEDINGS{8914645,
  author={Lee, Seo-Hyun and Lee, Minji and Jeong, Ji-Hoon and Lee, Seong-Whan},
  booktitle={2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)}, 
  title={Towards an EEG-based Intuitive BCI Communication System Using Imagined Speech and Visual Imagery}, 
  year={2019},
  volume={},
  number={},
  pages={4409-4414},
  abstract={Communication using brain-computer interface (BCI) has developed in attempts toward an intuitive system by decoding the imagined speech or visual imagery. However, discrimination between the two paradigms may be ambiguous because the user intention contains their original meaning. A clear distinction between the two paradigms may facilitate the active use of them leading to an intuitive BCI conversation system. In this study, we compared imagined speech and visual imagery in the perspective of its presence, spatial features, and classification performance based on electroencephalography. Seven subjects performed both imagined speech and visual imagery of twelve words/phrases. We showed the presence of the two paradigms, having distinct brain region from each other. The maximum thirteen-class classification accuracy including rest class was 34.2 % for imagined speech and 26.7 % for visual imagery. Therefore, we investigated the possibility of multiclass classification of more than ten classes in both paradigms, showing the potential of them to be used in the real world communication system. These findings could further be utilized in the intuitive communication for locked-in patients sending commands to the external world simply by thinking of `the very thing' that the user wants to deliver.},
  keywords={Visualization;Task analysis;Electroencephalography;Brain;Decoding;Radio frequency},
  doi={10.1109/SMC.2019.8914645},
  ISSN={2577-1655},
  month={Oct},}@INPROCEEDINGS{9622077,
  author={Takada, Saya and Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
  booktitle={2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)}, 
  title={Estimating Imagined Images from Brain Activities via Visual Question Answering}, 
  year={2021},
  volume={},
  number={},
  pages={35-36},
  abstract={Investigating human mental contents has been a topic for many years, but its ambiguous property has made the analysis difficult. We propose a neural decoding method via a machine learning model that predicts the imagined content based on measuring brain activity in this paper. This technique uses brain activity and computer vision models to discover the association between human functional magnetic resonance imaging (fMRI) activity and imagined contents. Decoding models based on neural networks learned using stimulus-induced brain activity in the visual cortex region showed an accurate estimation of the content. We provide a means of revealing subjective mental content by analysis with visual question answering. This result shows that a mental experience relates to brain activity patterns.},
  keywords={Visualization;Image color analysis;Computational modeling;Estimation;Machine learning;Functional magnetic resonance imaging;Predictive models},
  doi={10.1109/GCCE53005.2021.9622077},
  ISSN={2378-8143},
  month={Oct},}@ARTICLE{10798967,
  author={Mai, Weijian and Zhang, Jian and Fang, Pengfei and Zhang, Zhijun},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy}, 
  year={2025},
  volume={6},
  number={5},
  pages={1080-1099},
  abstract={In the era of artificial intelligence generated content (AIGC), conditional multimodal synthesis technologies (e.g., text-to-image) are dynamically reshaping the natural content. Brain signals, serving as potential reflections of how the brain interprets external information, exhibit a distinctive one-to-many correspondence with various external modalities. This correspondence makes brain signals emerge as a promising guiding condition for multimodal synthesis (e.g., image, text, and audio), which is crucial for developing practical brain–computer interface systems and unraveling complex mechanisms underlying human perception. This survey comprehensively examines the emerging field of brain-conditional multimodal synthesis, termed AIGC-brain, to delineate the current landscape and future directions. To begin, related neuroimaging datasets and generative models are introduced as the foundation of AIGC-brain decoding and analysis. Next, we present a comprehensive taxonomy according to AIGC-brain methodologies, followed by task-specific representative work and implementation details to facilitate in-depth comparison and analysis. Quality assessments are then introduced for both qualitative and quantitative evaluation. Finally, this survey explores insights gained, outlining current challenges and prospects of AIGC-brain. As a pioneering survey, this article paves the way for future advances in AIGC-brain research.},
  keywords={Functional magnetic resonance imaging;Decoding;Surveys;Neuroimaging;Electroencephalography;Brain modeling;Artificial intelligence;Taxonomy;Music;Faces;Artificial intelligence generated content;brain–computer interface;brain decoding;multimodal synthesis},
  doi={10.1109/TAI.2024.3516698},
  ISSN={2691-4581},
  month={May},}@ARTICLE{9363315,
  author={Wu, Hao and Zheng, Nanning and Chen, Badong},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={Feature-Specific Denoising of Neural Activity for Natural Image Identification}, 
  year={2022},
  volume={14},
  number={2},
  pages={629-638},
  abstract={Decoding the content in neural activity through voxelwise encoding plays an important role in investigating cognitive functions of the human brain. However, unlike multivoxel pattern analysis (MVPA), voxelwise encoding builds a model for each individual voxel; therefore, ignores the interactions between voxels and is sensitive to noise. In this work, we propose the feature-specific denoise (FSdenoise), a noise reduction method for encoding-based models to improve their decoding performance. FSdenoise considers the response of a voxel to a stimulus as a combination of two components: 1) feature-relevant component, which can be predicted from stimulus features and 2) feature-irrelevant component, which shows no direct relation to the concerned features. Exploiting the correlations between voxels, FSdenoise reduces the feature-irrelevant component in voxels that exhibit more feature-relevant component, enhancing their predictive power from stimulus features. Decoding performance with the denoised voxels would be improved in consequence. We validate the FSdenoise on two functional magnetic resonance imaging data sets and the results demonstrate that FSdenoise can efficiently improve the decoding accuracy for encoding-based approaches. Moreover, the encoding-based approaches combined with FSdenoise can even outperform the MVPA-based approach in brain decoding.},
  keywords={Decoding;Predictive models;Image coding;Feature extraction;Noise reduction;Encoding;Brain modeling;Brain decoding;denoising;functional magnetic resonance imaging (fMRI);visual cognition;voxelwise encoding},
  doi={10.1109/TCDS.2021.3062067},
  ISSN={2379-8939},
  month={June},}@ARTICLE{11104249,
  author={Xiong, Daowen and Hu, Liangliang and Jin, Jiahao and Ding, Yikang and Tan, Congming and Zhang, Jing and Tian, Yin},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Interpretable Cross-Modal Alignment Network for EEG Visual Decoding With Algorithm Unrolling}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Accurate decoding in electroencephalography (EEG) technology, particularly for rapid visual stimuli, remains challenging due to the low signal-to-noise ratio (SNR). Additionally, existing neural networks struggle with issues related to generalization and interpretability. This article proposes a cross-modal aligned network, E2IVAE, which leverages shared information from multiple modalities for self-supervised alignment of EEG to images for extracting visual perceptual information and features a novel EEG encoder, ISTANet, based on algorithm unrolling. This network framework significantly enhances the accuracy and stability of EEG decoding for object recognition in novel classes while reducing the extensive neural data typically required for training neural decoders. The proposed ISTANet employs algorithm unrolling to transform the multilayer sparse coding algorithm into an end-to-end format, extracting features from noisy EEG signals while incorporating the interpretability of traditional machine learning. The experimental results demonstrate that our method achieves SOTA top-1 accuracy of 62.39% and top-5 accuracy of 88.98% on a comprehensive rapid serial visual presentation (RSVP) dataset for public comparison in a 200-class zero-shot neural decoding task. Additionally, ISTANet enables visualization and analysis of multiscale atom features and overall reconstruction features, exploring biological plausibility across temporal, spatial, and spectral dimensions. On another more challenging RSVP large-scale dataset, the proposed framework also achieves significantly above chance-level performance, proving its robustness and generalization. This research provides critical insights into neural decoding and brain–computer interfaces (BCIs) within the fields of cognitive science and artificial intelligence.},
  keywords={Electroencephalography;Decoding;Feature extraction;Visualization;Brain modeling;Training;Encoding;Image reconstruction;Object recognition;Data mining;Algorithm unrolling;brain–computer interfaces (BCIs);electroencephalography (EEG) decoding;interpretability},
  doi={10.1109/TNNLS.2025.3592646},
  ISSN={2162-2388},
  month={},}@ARTICLE{11130422,
  author={Gao, Jianxiong and Fu, Yanwei and Fu, Yuqian and Wang, Yun and Qian, Xuelin and Feng, Jianfeng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={MinD-3D++: Advancing fMRI-Based 3D Reconstruction With High-Quality Textured Mesh Generation and a Comprehensive Dataset}, 
  year={2025},
  volume={},
  number={},
  pages={1-15},
  abstract={Reconstructing 3D visuals from functional Magnetic Resonance Imaging (fMRI) data, introduced as Recon3DMind, is of significant interest to both cognitive neuroscience and computer vision. To advance this task, we present the fMRI-3D dataset, which includes data from 15 participants and showcases a total of 4,768 3D objects. The dataset consists of two components: fMRI-Shape, previously introduced and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Shape , and fMRI-Objaverse, proposed in this paper and available at https://huggingface.co/datasets/Fudan-fMRI/fMRI-Objaverse. fMRI-Objaverse includes data from 5 subjects, 4 of whom are also part of the core set in fMRI-Shape. Each subject views 3,142 3D objects across 117 categories, all accompanied by text captions. This significantly enhances the diversity and potential applications of the dataset. Moreover, we propose MinD-3D++, a novel framework for decoding textured 3D visual information from fMRI signals. The framework evaluates the feasibility of not only reconstructing 3D objects from the human mind but also generating, for the first time, 3D textured meshes with detailed textures from fMRI data. We establish new benchmarks by designing metrics at the semantic, structural, and textured levels to evaluate model performance. Furthermore, we assess the model's effectiveness in out-of-distribution settings and analyze the attribution of the proposed 3D pari fMRI dataset in visual regions of interest (ROIs) in fMRI signals. Our experiments demonstrate that MinD-3D++ not only reconstructs 3D objects with high semantic and spatial accuracy but also provides deeper insights into how the human brain processes 3D visual information. Project page: https://jianxgao.github.io/MinD-3D.},
  keywords={Three-dimensional displays;Functional magnetic resonance imaging;Feature extraction;Visualization;Image reconstruction;Diffusion models;Decoding;Solid modeling;Semantics;Brain modeling;3D vision;dataset;diffusion model;FMRI decoding},
  doi={10.1109/TPAMI.2025.3599860},
  ISSN={1939-3539},
  month={},}@ARTICLE{9167421,
  author={Sharon, Rini A. and Narayanan, Shrikanth S. and Sur, Mriganka and Murthy, A. Hema},
  journal={IEEE Access}, 
  title={Neural Speech Decoding During Audition, Imagination and Production}, 
  year={2020},
  volume={8},
  number={},
  pages={149714-149729},
  abstract={Interpretation of neural signals to a form that is as intelligible as speech facilitates the development of communication mediums for the otherwise speech/motor-impaired individuals. Speech perception, production, and imagination often constitute phases of human communication. The primary goal of this article is to analyze the similarity between these three phases by studying electroencephalogram(EEG) patterns across these modalities, in order to establish their usefulness for brain computer interfaces. Neural decoding of speech using such non-invasive techniques necessitates the optimal choice of signal analysis and translation protocols. By employing selection-by-exclusion based temporal modeling algorithms, we discover fundamental syllable-like units that reveal similar set of signal signatures across all the three phases. Significantly higher than chance accuracies are recorded for single trial multi-unit EEG classification using machine learning approaches over three datasets across 30 subjects. Repeatability and subject independence tests performed at every step of the analysis further strengthens the findings and holds promise for translating brain signals to speech non-invasively.},
  keywords={Electroencephalography;Production;Protocols;Image segmentation;Brain modeling;Correlation;Image reconstruction;Assistive technology;brain computer interface;EEG;imagined speech;speech-EEG correlation;unit classification},
  doi={10.1109/ACCESS.2020.3016756},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9685036,
  author={Wang, Sixian and Dai, Jincheng and Yao, Shengshi and Niu, Kai and Zhang, Ping},
  booktitle={2021 IEEE Global Communications Conference (GLOBECOM)}, 
  title={A Novel Deep Learning Architecture for Wireless Image Transmission}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper, the problem of neural compression based image transmission over wireless channels is studied. Since all procedures are considered over wireless links, the quality of training is affected by wireless factors such as packet errors. In the considered model, compressed data given by the neural source encoder (NSE) are fed into an error-control channel encoder and modulated as discrete symbols sent over a memoryless channel. In the receiving end, the channel decoder and the neural source decoder (NSD) forms an iterative structure to reconstruct the original image. Since all neural compressed data are transmitted over wireless channels, the training of NSD is affected by wireless channel factors such as residual bit errors given by the channel decoder. Meanwhile, during outer-loop iterations, the NSD needs to match the variant of information reliability output by the channel decoder so as to build a global optimal receiver. To this end, a refiner neural network is first attached after the NSD to adjust its output as the format of a priori information sent into the channel decoder. Then, the extrinsic information transfer (EXIT) functions of channel decoder and NSD are derived. At each iteration, the reliability of messages sent into the NSD is explicitly predicted by using the EXIT chart. By this means, the NSD can be trained in a residual bit error aware manner, and we realize a joint learning and iterative decoding framework to ensure the quality of neural image transmission over realistic wireless channels.},
  keywords={Wireless communication;Training;Image coding;Image communication;Neural networks;Receivers;Reliability engineering},
  doi={10.1109/GLOBECOM46510.2021.9685036},
  ISSN={},
  month={Dec},}@ARTICLE{9580757,
  author={Xu, Qi and Shen, Jiangrong and Ran, Xuming and Tang, Huajin and Pan, Gang and Liu, Jian K.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Robust Transcoding Sensory Information With Neural Spikes}, 
  year={2022},
  volume={33},
  number={5},
  pages={1935-1946},
  abstract={Neural coding, including encoding and decoding, is one of the key problems in neuroscience for understanding how the brain uses neural signals to relate sensory perception and motor behaviors with neural systems. However, most of the existed studies only aim at dealing with the continuous signal of neural systems, while lacking a unique feature of biological neurons, termed spike, which is the fundamental information unit for neural computation as well as a building block for brain–machine interface. Aiming at these limitations, we propose a transcoding framework to encode multi-modal sensory information into neural spikes and then reconstruct stimuli from spikes. Sensory information can be compressed into 10% in terms of neural spikes, yet re-extract 100% of information by reconstruction. Our framework can not only feasibly and accurately reconstruct dynamical visual and auditory scenes, but also rebuild the stimulus patterns from functional magnetic resonance imaging (fMRI) brain activities. More importantly, it has a superb ability of noise immunity for various types of artificial noises and background signals. The proposed framework provides efficient ways to perform multimodal feature representation and reconstruction in a high-throughput fashion, with potential usage for efficient neuromorphic computing in a noisy environment.},
  keywords={Decoding;Neurons;Image reconstruction;Biological information theory;Transcoding;Visualization;Computational modeling;Cross-multimodal;decoding;denoising;neural spikes;reconstruction;spatio-temporal representations},
  doi={10.1109/TNNLS.2021.3107449},
  ISSN={2162-2388},
  month={May},}@INPROCEEDINGS{8712688,
  author={Ororbia, Alexander G. and Mali, Ankur and Wu, Jian and O'Connell, Scott and Dreese, William and Miller, David and Giles, C. Lee},
  booktitle={2019 Data Compression Conference (DCC)}, 
  title={Learned Neural Iterative Decoding for Lossy Image Compression Systems}, 
  year={2019},
  volume={},
  number={},
  pages={3-12},
  abstract={For lossy image compression systems, we develop an algorithm, iterative refinement, to improve the decoder's reconstruction compared to standard decoding techniques. Specifically, we propose a recurrent neural network approach for nonlinear, iterative decoding. Our decoder, which works with any encoder, employs self-connected memory units that make use of causal and non-causal spatial context information to progressively reduce reconstruction error over a fixed number of steps. We experiment with variants of our estimator and find that iterative refinement consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a 0.971 dB gain over a competitive neural model.},
  keywords={Decoding;Iterative decoding;Image reconstruction;Image coding;Transform coding;Computational modeling;Iterative algorithms;iterative refinement;compression;recurrent neural networks},
  doi={10.1109/DCC.2019.00008},
  ISSN={2375-0359},
  month={March},}@INPROCEEDINGS{9105697,
  author={Mali, Ankur and Ororbia, Alexander G. and Giles, C Lee},
  booktitle={2020 Data Compression Conference (DCC)}, 
  title={The Sibling Neural Estimator: Improving Iterative Image Decoding with Gradient Communication}, 
  year={2020},
  volume={},
  number={},
  pages={23-32},
  abstract={For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that "help" each other reconstruct the same target image patches using complementary portions of the spatial context, communicating with each other via gradient signals. This dual agent system builds upon prior work that proposed an iterative refinement algorithm for recurrent neural network (RNN) based decoding. Our approach works with any neural or non-neural encoder. Our system progressively reduces image patch reconstruction error over a fixed number of steps. Experiments with variations of RNN memory cells show that our system consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe gains of 1:64 decibel (dB) over JPEG, a 1:46 dB over JPEG2000, a 1:34 dB over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder.},
  keywords={Measurement;Visualization;Image coding;Recurrent neural networks;Nonlinear distortion;Transform coding;Rate-distortion;Decoding;Iterative decoding;Image reconstruction;Compression;decoder;RNN;Neural Networks},
  doi={10.1109/DCC47342.2020.00010},
  ISSN={2375-0359},
  month={March},}@ARTICLE{10745614,
  author={Yang, Xiaomeng and Xiong, Xinzhu and Li, Xufei and Lian, Qi and Zhu, Junming and Zhang, Jianmin and Qi, Yu and Wang, Yueming},
  journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
  title={Reconstructing Multi-Stroke Characters From Brain Signals Toward Generalizable Handwriting Brain–Computer Interfaces}, 
  year={2024},
  volume={32},
  number={},
  pages={4230-4239},
  abstract={Handwriting Brain-Computer Interfaces (BCIs) provides a promising communication avenue for individuals with paralysis. While English-based handwriting BCIs have achieved rapid typewriting with 26 lowercase letters (mostly containing one stroke each), it is difficult to extend to complex characters, especially those with multiple strokes and large character sets. The Chinese characters, including over 3500 commonly used characters with 10.3 strokes per character on average, represent a highly complex writing system. This paper proposes a Chinese handwriting BCI system, which reconstructs multi-stroke handwriting trajectories from brain signals. Through the recording of cortical neural signals from the motor cortex, we reveal distinct neural representations for stroke-writing and pen-lift phases. Leveraging this finding, we propose a stroke-aware approach to decode stroke-writing trajectories and pen-lift movements individually, which can reconstruct recognizable characters (accuracy of 86% with 400 characters). Our approach demonstrates high stability over 5 months, shedding light on generalized and adaptable handwriting BCIs.},
  keywords={Writing;Decoding;Trajectory;Character recognition;Long short term memory;Stroke (medical condition);Recording;Image reconstruction;Monitoring;Micromechanical devices;Brain-computer interfaces (BCIs);neural signal decoding;handwriting reconstruction;multi-stroke characters},
  doi={10.1109/TNSRE.2024.3492191},
  ISSN={1558-0210},
  month={},}@ARTICLE{10839023,
  author={Sharon, Rini and Sur, Mriganka and Murthy, Hema},
  journal={IEEE Open Journal of Signal Processing}, 
  title={Harnessing the Multi-Phasal Nature of Speech-EEG for Enhancing Imagined Speech Recognition}, 
  year={2025},
  volume={6},
  number={},
  pages={78-88},
  abstract={Analyzing speech-electroencephalogram (EEG) is pivotal for developing non-invasive and naturalistic brain-computer interfaces. Recognizing that the nature of human communication involves multiple phases like audition, imagination, articulation, and production, this study uncovers the shared cognitive imprints that represent speech cognition across these phases. Regression analysis, using correlation metrics reveal pronounced inter-phasal congruence. This insight promotes a shift from single-phase-centric recognition models to harnessing integrated phase data, thereby enhancing recognition of cognitive speech. Having established the presence of inter-phase associations, a common representation learning feature extractor is introduced, adept at capturing the correlations and replicability across phases. The features so extracted are observed to provide superior discrimination of cognitive speech units. Notably, the proposed approach proves resilient even in the absence of comprehensive multi-phasal data. Through thorough control checks and illustrative topographical visualizations, our observations are substantiated. The findings indicate that the proposed multi-phase approach significantly enhances EEG-based speech recognition, achieving an accuracy gain of 18.2% for 25 cognitive units in continuous speech EEG over models reliant solely on single-phase data.},
  keywords={Correlation;Electroencephalography;Speech recognition;Feature extraction;Accuracy;Image reconstruction;Training;Speech enhancement;Production;Loss measurement;Audition;articulation;BCI;imagination;regression;speech-EEG correlation;neural decoding of speech},
  doi={10.1109/OJSP.2025.3528368},
  ISSN={2644-1322},
  month={},}@INPROCEEDINGS{10221339,
  author={Gui, Renzhou and Zhang, Aobo and Liu, Shuai and Tong, Mei Song},
  booktitle={2023 Photonics & Electromagnetics Research Symposium (PIERS)}, 
  title={Analysis of Functional Areas of Human Brain Based on Reconstructed Images of DMFG-generated Countermeasure Network}, 
  year={2023},
  volume={},
  number={},
  pages={1131-1138},
  abstract={The structure of human brain is complex, and fMRI data can be used to reveal the working mechanism of human brain. We construct a generative confrontation deep learning network based on DMFG-loss function. Using this network, we can not only reconstruct the simple scene images perceived and imagined by human brain with high precision, but also achieve good results for the restoration and reconstruction of complex natural images. In addition, we propose to set the detection threshold based on the constant false alarm algorithm. Further, we explore the distribution of brain sensitive areas, and make a deep analysis of the impact of different regions on image reconstruction. The contribution ratio of specific brain regions to the image reconstruction of human brain is gived. This will help to explore the unknown areas of human brain and reveal the mechanism of human brain operation. It has broad application prospects in brain computer interaction and human brain decoding.},
  keywords={Training;Deep learning;Support vector machines;Visualization;Image color analysis;Fitting;Functional magnetic resonance imaging},
  doi={10.1109/PIERS59004.2023.10221339},
  ISSN={2831-5804},
  month={July},}@ARTICLE{9064701,
  author={Wu, Hao and Zhu, Ziyu and Wang, Jiayi and Zheng, Nanning and Chen, Badong},
  journal={IEEE Transactions on Cognitive and Developmental Systems}, 
  title={An Encoding Framework With Brain Inner State for Natural Image Identification}, 
  year={2021},
  volume={13},
  number={3},
  pages={453-464},
  abstract={Neural encoding and decoding, which aim to characterize the relationship between stimuli and brain activities, have emerged as an important area in cognitive neuroscience. Traditional encoding models, which focus on feature extraction and mapping, consider the brain as an input-output mapper without inner states. In this article, inspired by the fact that the human brain acts like a state machine, we proposed a novel encoding framework that combines information from both the external world and the inner state to predict brain activity. The framework comprises two parts: 1) forward encoding model that deals with visual stimuli and 2) inner state model that captures influence from intrinsic connections in the brain. The forward model can be any traditional encoding model, making the framework flexible. The inner state model is a linear model to utilize information in the prediction residuals of the forward model. The proposed encoding framework achieved much better performance on natural image identification than forward-only models, with a maximum identification accuracy of 100%. The identification accuracy decreased slightly with the data set size increasing, but remained relatively stable with different identification methods. The results confirm that the new encoding framework is effective and robust when used for brain decoding.},
  keywords={Encoding;Brain modeling;Decoding;Predictive models;Visualization;Feature extraction;Connectivity;decoding;functional magnetic resonance imaging (fMRI);perception;voxelwise encoding},
  doi={10.1109/TCDS.2020.2987352},
  ISSN={2379-8939},
  month={Sep.},}@INPROCEEDINGS{10004161,
  author={Jin, Yingxin and Shang, Shaohua and Tang, Liwei and He, Lianzhua and Zhou, MengChu},
  booktitle={2022 IEEE International Conference on Networking, Sensing and Control (ICNSC)}, 
  title={EEG channel selection algorithm based on Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={Multichannel EEG is generally used to collect brain activities from various locations across the brain. However, BCIs using lesser channels will be more convenient for subjects. What's more, information acquired from adjacent channels is usually inter-correlated or irrelevant to the task. And some channels are noisy. This paper proposes a novel channel selection algorithm based on reinforcement learning. It can adaptively transform the full-channel EEG data to the optimal-channel-number EEG format conditioned on different input trials to make a trade-off between brain decoding accuracy and efficiency. Experimen-tal results showed that the proposed model can improve the classification accuracy by 2% ~ 6% compared to channel set $\{C3,C4,Cz\}$.},
  keywords={Knowledge engineering;Reinforcement learning;Transforms;Feature extraction;Electroencephalography;Decoding;Classification algorithms;EEG;channel selection;reinforcement learning},
  doi={10.1109/ICNSC55942.2022.10004161},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10394227,
  author={Luo, Ying and Kobayashi, Ichiro},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={BrainLM: Estimation of Brain Activity Evoked Linguistic Stimuli Utilizing Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={1904-1909},
  abstract={In recent years, with the recent remarkable development of large-scale language models in natural language processing research, there has been an increasing number of studies employing large-scale language models to investigate the information processing mechanisms of encoding and decoding in the brain. In this study, we developed a new pre-trained language model, BrainLM, which incorporates paired data of brain activity induced by text and stimuli, and verified the accuracy of estimating brain states from natural language in multiple NLP tasks. In essence, our research has achieved several noteworthy accomplishments. Firstly, we successfully developed a multimodal model that incorporates both brain and text. Subsequently, we conducted bi-directional experiments to validate the model and ensure the reliability of both brain encoding and decoding processes. Furthermore, we performed meticulous comparative experiments, wherein we introduced 20 state-of-the-art (SOTA) language models as a control group. Our findings reveal that our proposed model outperforms superior brain encoding ability compared to the control group. Lastly, we designed a discrete Autoencoder module that extracts brain features. This module can be utilized independently to extract brain features in a wider range of brain decoding studies beyond fMRI.},
  keywords={Brain modeling;Feature extraction;Encoding;Data models;Natural language processing;Decoding;Task analysis},
  doi={10.1109/SMC53992.2023.10394227},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{9964307,
  author={Wang, Pengpai and Gong, Peiliang and Zhou, Yueying and Wen, Xuyun and Zhang, Daoqiang},
  journal={IEEE Transactions on Instrumentation and Measurement}, 
  title={Decoding the Continuous Motion Imagery Trajectories of Upper Limb Skeleton Points for EEG-Based Brain–Computer Interface}, 
  year={2023},
  volume={72},
  number={},
  pages={1-12},
  abstract={In the field of brain–computer interface (BCI), brain decoding using electroencephalography (EEG) is an essential direction, and motion imagery EEG-based BCI can not only help rehabilitation of patients with physical disabilities, but also enhance the endurance and power of people. Most of the existing MI-based BCI studies are limited to discrete EEG classification or 3-D directional limb trajectory reconstruction. To suitable for the requirements of BCI systems in practical applications, here, we explored the decoding of trajectories of continuous nondirectional motion imagination in 3-D space based on Chinese sign language. We propose a motion imagery trajectory reconstruction Transformer (MITRT) model to decode the EEG signals of the subjects performing motion imagery, and obtain the positional changes in the 3-D space of the shoulder, elbow, and wrist skeleton points in the neural activity. We add the geometric constraint features of upper limb skeleton points to the model, and the MITRT decoding model can obtain prior knowledge to improve the reconstruction accuracy of spatial positions. To verify the decoding performance of our proposed model, we collected motor imagery (MI) EEG signals of 20 subjects based on Chinese sign language for experiments. The experimental results show that the average Pearson correlation coefficient of the six skeleton points was 0.975, which was significantly higher than the contrast models. This study is the first attempt to reconstruct multidirectional continuous nondirectional upper limb MI trajectories based on Chinese sign language. The experimental results show that it is feasible to decode and reconstruct imagined 3-D trajectories of human upper limb skeleton points from scalp EEG.},
  keywords={Electroencephalography;Trajectory;Image reconstruction;Decoding;Gesture recognition;Assistive technologies;Skeleton;Brain–computer interface (BCI);electroencephalography (EEG);limb motion decoding;motor imagery (MI);trajectory reconstruction},
  doi={10.1109/TIM.2022.3224991},
  ISSN={1557-9662},
  month={},}
