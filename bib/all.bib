% BibTeX References - Merged and Deduplicated
% Generated: 2025-11-26 02:41:14
% Tool: BibTeX Merger + Deduplicator v2.0
%
% Statistics:
% Files processed: 2
% Total imported: 281
% Duplicates removed: 15
% Final unique entries: 266
%
% Duplicate detection method:
% Priority 1: DOI matching
% Priority 2: Title + Year matching
%
@article{AlRadhi2026Prompting,
author = {Al-Radhi, Mohammed Salah and Shurid, Sadi Mahmud and Németh, Géza},
title = {Prompting the Mind: EEG-to-Text Translation with Multimodal LLMs and Semantic Control},
year = {2026},
journal = {Lecture Notes in Computer Science},
volume = {16187 LNCS},
pages = {52 - 66},
doi = {10.1007/978-3-032-07956-5_4},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020236218&doi=10.1007%2F978-3-032-07956-5_4&partnerID=40&md5=c23d3435ce95bc801c6b7c0131dbcf8d},
affiliations = {},
abstract = {We present Prompting the Mind (PTM), an extended EEG-to-text translation framework that combines large language models (LLMs) with multimodal alignment to decode human brain signals into natural language. Our system follows a multi-stage pipeline: an EEG encoder first transforms raw neural activity into discriminative embeddings; these are then mapped into a shared vision-language semantic space using CLIP-based cross-modal alignment. Finally, a general-purpose base LLM, DeepSeek-7B-Base, generates descriptive text conditioned on the EEG-derived representations through structured prompting. We evaluate the framework on a publicly available EEG-image dataset, comparing its performance with chance-level and alignment-only baselines as well as an instruction-tuned LLM (Mistral-7B). Results on BLEU, METEOR, ROUGE-L, and BERTScore show that while instruction-tuned models yield higher token overlap, our prompt-conditioned base LLM produces shorter, more semantically faithful outputs that better align with the original brain signals. Qualitative examples highlight this trade-off and the practical value of structured prompting for non-invasive neural decoding. All code, prompt templates, and configuration files are shared (https://github.com/Sadi-Mahmud-Shurid/PTM) to promote reproducibility and future extensions of open-weight frameworks for brain-to-text communication. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2026.}, keywords = {Alignment; Biomedical signal processing; Brain; Brain computer interface; Computational linguistics; Electroencephalography; Interactive computer systems; Interfaces (computer); Natural language processing systems; Neural networks; Semantics; Signal encoding; Speech communication; Brain signals; EEG-to-text; Human brain; Language model; Language semantics; Large language model; Multi-modal; Multimodal alignment; Neural speech decoding; Speech decoding; Economic and social effects},
correspondence_address = {M.S. Al-Radhi; Department of Telecommunications and Artificial Intelligence, Budapest, Hungary; email: malradhi@tmit.bme.hu},
editor = {Karpov, A. and Gosztolya, G.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {16113349; 03029743},
isbn = {9789819698936; 9789819698042; 9789819698110; 9789819698905; 9783032004949; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141},
language = {English},
abbrev_source_title = {Lect. Notes Comput. Sci.},
type = {Conference paper}}
@article{Ban2026Advances,
author = {Ban, Seunghyeb and Chong, David and Kwon, Junwoo and Lee, Sangyoon and Huang, Yunuo and Yoo, Seungwon and Yeo, Woonhong},
title = {Advances in flexible high-density microelectrode arrays for brain-computer interfaces},
year = {2026},
journal = {Biosensors and Bioelectronics},
volume = {292},
pages = {},
doi = {10.1016/j.bios.2025.118102},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020972258&doi=10.1016%2Fj.bios.2025.118102&partnerID=40&md5=af95e495ea7a7478158942c611eec04f},
affiliations = {The George W. Woodruff School of Mechanical Engineering, Atlanta, GA, United States; Georgia Institute of Technology, Wearable Intelligent Systems and Healthcare Center (WISH Center), Atlanta, GA, United States; Cornell University College of Engineering, Ithaca, NY, United States; Pennsylvania State University, College of Engineering, University Park, PA, United States; Georgia Institute of Technology, School of Industrial Design, Atlanta, GA, United States; Wallace H. Coulter Department of Biomedical Engineering, Atlanta, GA, United States; Georgia Institute of Technology, Atlanta, GA, United States; Georgia Institute of Technology, Atlanta, GA, United States},
abstract = {Recent advances in flexible high-density microelectrode arrays (FHD-MEA) have revolutionized brain-computer interfaces (BCIs) by providing high spatial resolution, mechanical compliance, and long-term biocompatibility. This technology enables stable neural recording and precise stimulation, addressing the shortcomings of conventional rigid BCI arrays. In this review, we outline the challenges of signal acquisition and stimulation of conventional low-density, rigid BCI systems. These include poor spatial resolution, micro-motor-induced instability, electrochemical degradation, wiring bottlenecks, off-target activation, and charge injection hazards. We then describe how these barriers are addressed through advanced materials, device designs, and system-level integration. We summarize representative applications of clinical therapy for sensory enhancement, human-machine interfaces, and neurological diseases, highlighting translational potential. Collectively, this review article presents recent progress and emerging trends in establishing FHD-MEAs as a crucial foundation for next-generation, clinically viable BCIs. © 2025 The Authors.}, keywords = {Article; biocompatibility; brain mapping; decoding; degenerative disease; degradation; electric activity; electric potential; electrochemical degradation; electrocorticography; electroencephalogram; feature extraction; functional near-infrared spectroscopy; human; mechanical stress; nerve cell; neural decoding; neurologic disease; neurorehabilitation; somatosensory cortex; visual evoked potential; animal; brain; brain computer interface; devices; electroencephalography; equipment design; genetic procedures; microelectrode; physiology; procedures; Animals; Biosensing Techniques; Brain; Brain-Computer Interfaces; Electroencephalography; Equipment Design; Humans; Microelectrodes},
correspondence_address = {W.-H. Yeo; George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, 30332, United States; email: whyeo@gatech.edu},
publisher = {Elsevier Ltd},
issn = {09565663; 18734235},
isbn = {9780128031018; 9780128031001},
coden = {BBIOE},
pmid = {41100980},
language = {English},
abbrev_source_title = {Biosens. Bioelectron.},
type = {Article}}
@article{Cao2026Eegclip,
author = {Cao, Xuhao and Gong, Peiliang and Zhang, Liying and Zhang, Daoqiang},
title = {EEG-CLIP: A transformer-based framework for EEG-guided image generation},
year = {2026},
journal = {Neural Networks},
volume = {194},
pages = {},
doi = {10.1016/j.neunet.2025.108167},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018169304&doi=10.1016%2Fj.neunet.2025.108167&partnerID=40&md5=e3591c868009a1ed98057f731f8e64c6},
affiliations = {Nanjing University of Aeronautics and Astronautics, MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, Jiangsu, China},
abstract = {Decoding visual perception from neural signals represents a fundamental step toward advanced brain-computer interfaces (BCIs), where functional magnetic resonance imaging (fMRI) has shown promising results despite practical constraints in deployment and costs. Electroencephalography (EEG), with its superior temporal resolution, portability, and cost-effectiveness, emerges as a promising alternative for real-time brain-computer interface (BCI) applications. While existing EEG-based approaches have advanced neural decoding capabilities, they remain constrained by inadequate architectural designs, limited reconstruction fidelity, and inconsistent evaluation protocols. To address these challenges, we present EEG-CLIP, a novel Transformer-based framework that systematically addresses each limitation: (1) We introduce a specialized EEG-ViT encoder that adeptly captures the spatial and temporal characteristics of EEG signals to augment model capacity, along with a Diffusion Prior Transformer architecture to approximate the image feature distribution. (2) We employ a dual-stage reconstruction pipeline that integrates class contrastive learning and pretrained diffusion models to enhance visual reconstruction quality. (3) We establish comprehensive evaluation protocols across multiple datasets. Our framework operates through two stages: first projecting EEG signals into CLIP image space via class contrastive learning and refining them into image priors, then reconstructing perceived images through a pretrained conditional diffusion model. Comprehensive empirical analysis, including temporal window sensitivity studies and regional brain activation visualization, demonstrates the framework's robustness. We demonstrate through ablations that EEG-CLIP's performance improvements over previous methods result from specialized architecture for EEG encoding and improved training techniques. Quantitative and qualitative evaluations on ThingsEEG and Brain2Image datasets establish EEG-CLIP's state-of-the-art performance in both classification and reconstruction tasks, advancing neural signal-based visual decoding capabilities. © 2025 Elsevier Ltd}, keywords = {Activation analysis; Architecture; Biomedical signal processing; Brain; Brain computer interface; Brain mapping; Cost effectiveness; Decoding; Diffusion; Electrophysiology; Image coding; Image reconstruction; Interfaces (computer); Magnetic resonance imaging; Signal encoding; Brain decoding; Diffusion model; Evaluation protocol; Functional magnetic resonance imaging; Guided images; Image generations; Neural signals; Temporal resolution; Transformer; Visual perception; Electroencephalography; Article; brain analysis; brain region; computer model; cost effectiveness analysis; data visualization; diffusion; EEG CLIP; electroencephalogram; electroencephalography; functional magnetic resonance imaging; human; image analysis; image reconstruction; image segmentation; learning; mathematical analysis; neuroimaging; spatial analysis; supervised machine learning; temporal cortex; time series analysis; transformer based framework; visual stimulation},
correspondence_address = {P. Gong; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China; email: plgong@nuaa.edu.cn},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
pmid = {41072287},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{Ferrante2026Towards,
author = {Ferrante, Matteo and Boccato, Tommaso and Rashkov, Grigorii and Toschi, Nicola},
title = {Towards neural foundation models for vision: Aligning EEG, MEG, and fMRI representations for decoding, encoding, and modality conversion},
year = {2026},
journal = {Information Fusion},
volume = {126},
pages = {},
doi = {10.1016/j.inffus.2025.103650},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014517393&doi=10.1016%2Fj.inffus.2025.103650&partnerID=40&md5=3a85ce582045b38986515b01621d37c7},
affiliations = {Università degli Studi di Roma "Tor Vergata", Department of Biomedicine and Prevention, Rome, RM, Italy; Harvard Medical School, Boston, MA, United States},
abstract = {This paper presents a novel approach towards creating a foundational model for aligning neural data and visual stimuli across cross-modal representations of brain activity by leveraging contrastive learning. We leverage electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI) data. The framework is validated through three key experiments: decoding visual information from neural data, encoding images into neural representations, and converting between neural modalities. The results highlight the model's ability to accurately capture semantic information across different brain imaging techniques, underscoring its potential in decoding, encoding, and modality conversion tasks. Our results reveal that EEG, MEG, and fMRI signals – despite being collected from different subjects and datasets – can be aligned in a shared vision-grounded space that preserves semantic information. This finding suggests the existence of modality-invariant neural codes and offers a new framework for comparing and decoding brain activity across heterogeneous non-invasive recordings. © 2025 The Authors}, keywords = {Biomedical signal processing; Brain; Brain mapping; Electroencephalography; Electrophysiology; Encoding (symbols); Image coding; Magnetic resonance imaging; Magnetoencephalography; Neural networks; Neurons; Semantics; Signal encoding; Brain activity; Brain decoding; Brain encoding; Encodings; Functional magnetic resonance imaging; Modality conversion; Neural data; Neural modality conversion; Neuroscience; Representation alignment; Vision},
correspondence_address = {M. Ferrante; University of Rome, Tor Vergata, Department of Biomedicine and Prevention, Rome, Italy; email: matteo.ferrante@uniroma2.it},
publisher = {Elsevier B.V.},
issn = {15662535; 18726305},
language = {English},
abbrev_source_title = {Inf. Fusion},
type = {Article}}
@article{Li2026Adaptive,
author = {Li, Pengrui and Peng, Maoqin and Zhang, Haokai and Liu, Shihong and Gao, Dongrui and Qin, Yun and Wu, Dingming and Liu, Tiejun},
title = {An adaptive decoupling learning system informed by the brain functional structure for EEG decoding},
year = {2026},
journal = {Neural Networks},
volume = {195},
pages = {},
doi = {10.1016/j.neunet.2025.108228},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019393504&doi=10.1016%2Fj.neunet.2025.108228&partnerID=40&md5=8cd0813aa6ebf02cb0b3dcf449612077},
affiliations = {University of Electronic Science and Technology of China, School of Life Science and Technology, Chengdu, Sichuan, China; Chengdu University of Information Technology, School of Computer Science, Chengdu, Sichuan, China},
abstract = {Neuroscientific investigations have revealed the presence of regional pathway connections among functional brain areas, as well as the asymmetrical structural characteristics of the left and right hemispheres. These connections, along with their potential coupling relationships and strengths, significantly influence the representation of neuronal signals as recorded by electroencephalography (EEG) within the cerebral cortex. Therefore, there is an urgent necessity to develop data-driven approaches that can effectively decode latent feature representations from EEG data. In this regard, the current study presents a functional-structural adaptive decoupling learning framework (FS-AD), which is informed by cognitive insights into the functional structure of the brain and integrates local-global spatial representations to decode EEG patterns across various states. To accomplish this, we initially implemented a one-dimensional separable convolution module and designed a local-domain attention interaction layer to extract inter-channel interaction information for each region, thereby enabling the capture of fully connected regional pathways. Following this, we developed a global-local kernel-level fusion decoder (GKFD) to amalgamate multiple local-domain features and decode them through a global-domain connection layer. Furthermore, a cross-domain adaptive fusion decoder (CAFD) was meticulously crafted to dynamically identify the fully connected optimal cross-domain pathway and decode it via a local-domain connection layer. The primary aim of FS-AD is to excavate the connectivity patterns of different brain states to enhance the efficiency of EEG decoding. The results indicate that the proposed FS-AD learning system significantly surpasses existing competitive methods in EEG decoding tasks related to various brain states, including fatigue, emotion, and motor imagery. Importantly, this study elucidates the variations in coupling strength among brain regional pathway connections and their representation of brain activity, while also investigating the optimal regional pathways under distinct brain states. This study contributes to the advancement of universal brain decoding methodologies. © 2025 Elsevier Ltd}, keywords = {Artificial intelligence; Biomedical signal processing; Brain; Decoding; Electrophysiology; Functional neuroimaging; Learning systems; Neurons; Adaptive decoupling; Brain decoding; Brain decoding technique; Brain state; Data-driven approach; Decoding techniques; Functional structure; Functionals; Learning frameworks; Regional pathway; Electroencephalography; Article; artificial intelligence; brain cortex; brain function; brain functional structure; brain region; cognition; convolutional neural network; electroencephalogram; electroencephalography; emotion; fatigue; graph neural network; hemisphere; human; independent component analysis; knowledge; learning; methodology; nerve cell network; power spectrum; short time Fourier transform},
correspondence_address = {D. Wu; School of Life Sciences and Technology, University of Electronic Science and Technology of China, Chengdu, 611731, China; email: dmw@uestc.edu.cn},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{Sun2026Decoding,
author = {Sun, Kaili and Miao, Xingyu and Zhai, Bing and Duan, Haoran and Long, Yang},
title = {Decoding visual neural representations by multimodal with dynamic balancing},
year = {2026},
journal = {Expert Systems with Applications},
volume = {297},
pages = {},
doi = {10.1016/j.eswa.2025.129382},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013958067&doi=10.1016%2Fj.eswa.2025.129382&partnerID=40&md5=fb9e127a873b0ae5eb6e3ee3b1d536b0},
affiliations = {Durham University, Department of Computer Science, Durham, County Durham, United Kingdom; University of Northumbria, Department of Computer and Information Science, Newcastle, Tyne and Wear, United Kingdom; Tsinghua University, Department of Automation, Beijing, China},
abstract = {In this work, we propose an innovative framework that integrates EEG, image, and text data, aiming to decode visual neural representations from low signal-to-noise ratio EEG signals. Specifically, we introduce text modality to enhance the semantic correspondence between EEG signals and visual content. With the explicit semantic labels provided by text, image and EEG features of the same category can be more closely aligned with the corresponding text representations in a shared multimodal space. To fully utilize pre-trained visual and textual representations, we propose an adapter module that alleviates the instability of high-dimensional representation while facilitating the alignment and fusion of cross-modal features. Additionally, to alleviate the imbalance in multimodal feature contributions introduced by the textual representations, we propose a Modal Consistency Dynamic Balance (MCDB) strategy that dynamically adjusts the contribution weights of each modality. We further propose a stochastic perturbation regularization (SPR) term to enhance the generalization ability of semantic perturbation-based models by introducing dynamic Gaussian noise in the modality optimization process. The evaluation results on the ThingsEEG dataset show that our method surpasses previous state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by 2.0 % and 4.7 % respectively. © 2025 Elsevier Ltd}, keywords = {Balancing; Balancing machines; Biomedical signal processing; Brain; Dynamics; Electroencephalography; Gaussian noise (electronic); Neural networks; Semantics; Signal to noise ratio; Stochastic systems; Brain neural decoding; Dynamic balancing; EEG signals; Gradient modulation; Modal balance; Multi-modal; Multimodal contrastive learning; Neural decoding; Neural representations; Textual representation; Stochastic models},
correspondence_address = {Y. Long; Department of Computer Science, Durham University, United Kingdom; email: yang.long@durham.ac.uk},
publisher = {Elsevier Ltd},
issn = {09574174},
coden = {ESAPE},
language = {English},
abbrev_source_title = {Expert Sys Appl},
type = {Article}}
@article{Akama2025Predicting,
author = {Akama, Taketo and Zhang, Zhuohao and Li, Pengcheng and Hongo, Kotaro and Minamikawa, Shun and Polouliakh, Natalia},
title = {Predicting artificial neural network representations to learn recognition model for music identification from brain recordings},
year = {2025},
journal = {Scientific Reports},
volume = {15},
number = {1},
pages = {},
doi = {10.1038/s41598-025-02790-6},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006921300&doi=10.1038%2Fs41598-025-02790-6&partnerID=40&md5=0a09bfef60b913745aafc222ec92821e},
affiliations = {Sony Computer Science Laboratories, Inc., Tokyo, Japan},
abstract = {Recent studies have demonstrated that the representations of artificial neural networks (ANNs) can exhibit notable similarities to cortical representations when subjected to identical auditory sensory inputs. In these studies, the ability to predict cortical representations is probed by regressing from ANN representations to cortical representations. Building upon this concept, our approach reverses the direction of prediction: we utilize ANN representations as a supervisory signal to train recognition models using noisy brain recordings obtained through non-invasive measurements. Specifically, we focus on constructing a recognition model for music identification, where electroencephalography (EEG) brain recordings collected during music listening serve as input. By training an EEG recognition model to predict ANN representations-representations associated with music identification-we observed a significant improvement in classification accuracy. This study introduces a novel approach to developing recognition models for brain recordings in response to external auditory stimuli. It holds promise for advancing brain-computer interfaces (BCI), neural decoding techniques, and our understanding of music cognition. Furthermore, it provides new insights into the relationship between auditory brain activity and ANN representations. © The Author(s) 2025.},
keywords = {adult; artificial neural network; auditory stimulation; brain; brain computer interface; electroencephalography; female; hearing; human; male; music; physiology; young adult; Acoustic Stimulation; Adult; Auditory Perception; Brain; Brain-Computer Interfaces; Electroencephalography; Female; Humans; Male; Music; Neural Networks, Computer; Young Adult},
correspondence_address = {T. Akama; Sony Computer Science Laboratories, Inc, Tokyo, Japan; email: taketo.akama@sony.com},
publisher = {Nature Research},
issn = {20452322},
pmid = {40442206},
language = {English},
abbrev_source_title = {Sci. Rep.},
type = {Article}}
@article{Alcolea2025Less,
author = {Alcolea, Pedro I. and Ma, Xuan and Bodkin, Kevin L. and Miller, Lee E. and Danziger, Zachary C.},
title = {Less is more: selection from a small set of options improves BCI velocity control},
year = {2025},
journal = {Journal of Neural Engineering},
volume = {22},
number = {2},
pages = {},
doi = {10.1088/1741-2552/adbcd9},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000297308&doi=10.1088%2F1741-2552%2Fadbcd9&partnerID=40&md5=8ab5cd501432388330196d5593fbb0b0},
affiliations = {FIU College of Engineering and Computing, Miami, FL, United States; Northwestern University Feinberg School of Medicine, Chicago, IL, United States; Northwestern University Feinberg School of Medicine, Chicago, IL, United States; Robert R. McCormick School of Engineering and Applied Science, Evanston, IL, United States; Shirley Ryan AbilityLab, Chicago, IL, United States; Emory University School of Medicine, Division of Physical Therapy, Atlanta, GA, United States; Wallace H. Coulter Department of Biomedical Engineering, Atlanta, GA, United States},
abstract = {Objective. Decoding algorithms used in invasive brain-computer interfaces (iBCIs) typically convert neural activity into continuously varying velocity commands. We hypothesized that putting constraints on which decoded velocity commands are permissible could improve user performance. To test this hypothesis, we designed the discrete direction selection (DDS) decoder, which uses neural activity to select among a small menu of preset cursor velocities. Approach. We tested DDS in a closed-loop cursor control task against many common continuous velocity decoders in both a human-operated real-time iBCI simulator (the jaBCI) and in a monkey using an iBCI. In the jaBCI, we compared performance across four visits by each of 48 naïve, able-bodied human subjects using either DDS, direct regression with assist (an affine map from neural activity to cursor velocity, DR-A), ReFIT, or the velocity Kalman Filter (vKF). In a follow up study to verify the jaBCI results, we compared a monkey’s performance using an iBCI with either DDS or the Wiener filter decoder (a direct regression decoder that includes time history, WF). Main Result. In the jaBCI, DDS substantially outperformed all other decoders with 93% mean targets hit per visit compared to DR-A, ReFIT, and vKF with 56%, 39%, and 26% mean targets hit, respectively. With the iBCI, the monkey achieved a 61% success rate with DDS and a 37% success rate with WF. Significance. Discretizing the decoded velocity with DDS effectively traded high resolution velocity commands for less tortuous and lower noise trajectories, highlighting the potential benefits of discretization in simplifying online BCI control. © 2025 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Brain mapping; Closed loop control systems; Kalman filters; Neurons; Wiener filtering; Center-out; Cursor control; Direction selection; Discrete velocity control; Interface modeling; Invasive brain-computer interface model (jaBCI); Monkey invasive brain-computer interface; Motor-cortex; Neural activity; Neural decoding; Brain computer interface; adult; affine transform; aged; animal experiment; Article; artificial neural network; cell activity; comparative study; controlled study; discretization; feedback system; finger joint; firing rate; follow up; Haplorhini; human; kalman filter; kinematics; measurement accuracy; motor cortex; nerve cell; noise; nonhuman; regression analysis; velocity; algorithm; animal; brain computer interface; electroencephalography; female; male; physiology; procedures; psychomotor performance; rhesus monkey; young adult; Adult; Algorithms; Animals; Brain-Computer Interfaces; Electroencephalography; Female; Humans; Macaca mulatta; Male; Psychomotor Performance; Young Adult},
correspondence_address = {Z.C. Danziger; Department of Biomedical Engineering, Florida International University, Miami, 33199, United States; email: zachary.danziger@emory.edu},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {40043320},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{An2025Effects,
author = {An, Hyunjung and Lee, Jee-won and Park, Youngjin and Suh, Myung-Whan and Lim, Yoonseob},
title = {Effects of Age on the Neural Tracking of Speech in Noise},
year = {2025},
journal = {Brain Sciences},
volume = {15},
number = {8},
pages = {},
doi = {10.3390/brainsci15080874},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014466737&doi=10.3390%2Fbrainsci15080874&partnerID=40&md5=f0ef62ca91161f1c506739623f9d1e69},
affiliations = {Korea Institute of Science and Technology, Center for Intelligent and Interactive Robotics, Seoul, South Korea; Hallym University, Division of Speech Pathology and Audiology, Chuncheon, Gangwon-do, South Korea; Ewha Womans University, Department of Electrical and Electronic Engineering, Seoul, Seoul, South Korea; Korea Electrotechnology Research Institute, Electro-Medicine Device Research Division, Changwon, Gyeongsangnam-do, South Korea; Seoul National University Hospital, Department of Otolaryngology-Head and Neck Surgery, Seoul, South Korea; Hanyang University, Seoul, South Korea},
abstract = {Background: Older adults often struggle to comprehend speech in noisy environments, a challenge influenced by declines in both auditory processing and cognitive functions. This study aimed to investigate how differences in speech-in-noise perception among individual with clinically normal hearing thresholds (ranging from normal to mild hearing loss in older adults) are related to neural speech tracking and cognitive function, particularly working memory. Method: Specifically, we examined delta (1–4 Hz) and theta (4–8 Hz) EEG oscillations during speech recognition tasks to determine their association with cognitive performance in older adults. EEG data were collected from 23 young adults (20–35 years) and 23 older adults (65–80 years). Cognitive assessments were administered to older adults, and both groups completed an EEG task involving speech recognition in Speech-Shaped Noise (SSN) at individualized noise levels based on their Sentence Recognition Scores (SRS). Results: The results showed that age significantly impacted hit rates and reaction times in noisy speech recognition tasks. Theta-band neural tracking was notably stronger in older adults, while delta-band tracking showed no age-related difference. Pearson’s correlations indicated significant associations between age-related cognitive decline, reduced hearing sensitivity, and Mini-Mental State Examination (MMSE) scores. Regression analyses showed that theta-band neural tracking at specific SRS levels significantly predicted word list recognition in the higher SRT group, while constructional recall was strongly predicted in the lower SRT group. Conclusions: These findings suggest that older adults may rely on theta-band neural tracking as a compensatory mechanism. However, regression results alone were not sufficient to fully explain how working memory affects neural tracking, and additional cognitive and linguistic factors should be considered in future studies. Furthermore, cognitive assessments were administered only to older adults, which limits the ability to determine whether group differences are driven by age, hearing, or cognitive status—a major limitation that should be addressed in future research. © 2025 by the authors.}, keywords = {adult; aged; Article; auditory threshold; central nervous system function; cognition; cognitive defect; cognitive impairment assessment; dose response; electroencephalogram; electroencephalography; environmental noise; female; hearing; hearing acuity; hearing impairment; human; human experiment; male; mental performance; Mini Mental State Examination; neural decoding; Neural Tracking; noise; oscillation; reaction time; reliability; semantic memory; sensitivity and specificity; speech; speech discrimination; speech perception; Speech Recognition Score; Speech-Shaped Noise; verbal production; working memory},
correspondence_address = {Y. Lim; Center for Intelligent & Interactive Robotics, Korea Institute of Science and Technology, Seoul, 5, Hwarang-ro 14-gil, Seongbuk-gu, 02792, South Korea; email: yslim@kist.re.kr},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {20763425},
language = {English},
abbrev_source_title = {Brain Sci.},
type = {Article}}
@article{An2025Role,
author = {An, Hyunjung},
title = {The Role of Neurofeedback in Audiology: A Review of Current Evidence and Future Directions},
year = {2025},
journal = {Audiology and Speech Research},
volume = {21},
number = {3},
pages = {139 - 147},
doi = {10.21848/asr.250187},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012846841&doi=10.21848%2Fasr.250187&partnerID=40&md5=2c591f76045db696eedc9ce543b74fc3},
affiliations = {Hallym University, Division of Speech Pathology and Audiology, Chuncheon, Gangwon-do, South Korea},
abstract = {This study reviews the neurophysiological principles of neurofeedback (NFB) and its diverse clinical applications, with a particular focus on its emerging role in auditory rehabilitation and tinnitus treatment. By examining recent key studies, we explore how NFB extends beyond traditional psychological or behavioral modulation and offers potential for enhancing central auditory processing and cognitive regulation. Specifically, NFB has shown promise in modulating brain activity associated with auditory perception, attentional control, and neuroplasticity in both clinical and subclinical populations. Based on these findings, we propose future directions for sustained research on auditory-focused neurofeedback, emphasizing the need for individualized protocols, advanced neural decoding techniques, and integration with existing auditory assistive technologies. This review highlights NFB as a promising neurocognitive intervention within audiology and auditory neuroscience. © © 2025 Korean Academy of Audiology.},
keywords = {Auditory rehabilitation; Brain-computer interface (BCI); Electroencephalography (EEG); Neurofeedback (NF)},
correspondence_address = {H. An; Division of Speech Pathology and Audiology, Hallym University, Chuncheon, South Korea; email: hyunjung.an@hallym.ac.kr},
publisher = {Korean Academy of Audiology},
issn = {26355027; 26355019},
language = {Korean},
abbrev_source_title = {Audio. Speech Res.},
type = {Review}}
@inproceedings{Balasubramanian2025International,
author = {Balasubramanian, Arun and Pandey, Kartik and Veer, Gautam and Samanta, Debasis},
booktitle = {2025 13th International Conference on Brain-Computer Interface (BCI)},
title = {Activity Prediction for Localizing the Events in Imagined Speech EEG Signals},
year = {2025},
volume = {},
number = {},
pages = {1--5},
abstract = {Imagined speech electroencephalogram (EEG) signals are often collected for longer durations than necessary, leading to a difficulty in understanding the generation of EEG during the task as it is likely that most of the data collected is that of resting state. Developing a filter to identify the segments of the trials with actual information and remove those that contain the EEG from resting states could significantly advance our understanding of EEG signals in neuroscience and biomedical engineering. This work uses derivatives-based features to form the feature vectors used to train the classifiers. Based on feature importance analysis, it has been found that the derivative-based features contributed more to the classification than the traditionally preferred feature, band power in the alpha frequency band. Using the proposed features, the classification performance experienced a significant enhancement, surpassing the results reported in previous studies. The greater accuracy of the classifiers with the proposed features implies that they are effective at filtering out the resting state segments from imagined speech EEG signals.},
keywords = {Location awareness;Image segmentation;Accuracy;Neuroscience;Signal processing;Information filters;Feature extraction;Electroencephalography;Vectors;Speech processing;Imagined speech;EEG;resting state;signal processing;frequency analysis;feature importance;neural decoding},
doi = {10.1109/BCI65088.2025.10931413},
issn = {2572-7672},
month = {Feb}}
@article{Bigand2025Dancing,
author = {Bigand, Félix and Bianco, Roberta and Abalde, Sara F. and Nguyen, Trinh and Novembre, Giacomo},
title = {EEG of the Dancing Brain: Decoding Sensory, Motor, and Social Processes during Dyadic Dance},
year = {2025},
journal = {Journal of Neuroscience},
volume = {45},
number = {21},
pages = {},
doi = {10.1523/JNEUROSCI.2372-24.2025},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005648108&doi=10.1523%2FJNEUROSCI.2372-24.2025&partnerID=40&md5=788541134f92935ff3c73c1c51224268},
affiliations = {Istituto Italiano di Tecnologia, Neuroscience of Perception and Action Lab, Genoa, GE, Italy; Istituto Italiano di Tecnologia, Genoa, GE, Italy},
abstract = {Real-world social cognition requires processing and adapting to multiple dynamic information streams. Interpreting neural activity in such ecological conditions remains a key challenge for neuroscience. This study leverages advancements in denoising techniques and multivariate modeling to extract interpretable EEG signals from pairs of (male and/or female) participants engaged in spontaneous dyadic dance. Using multivariate temporal response functions (mTRFs), we investigated how music acoustics, self-generated kinematics, other-generated kinematics, and social coordination uniquely contributed to EEG activity. Electromyogram recordings from ocular, face, and neck muscles were also modeled to control for artifacts. The mTRFs effectively disentangled neural signals associated with four processes: (I) auditory tracking of music, (II) control of self-generated movements, (III) visual monitoring of partner movements, and (IV) visual tracking of social coordination. We show that the first three neural signals are driven by event-related potentials: the P50-N100-P200 triggered by acoustic events, the central lateralized movement-related cortical potentials triggered by movement initiation, and the occipital N170 triggered by movement observation. Notably, the (previously unknown) neural marker of social coordination encodes the spatiotemporal alignment between dancers, surpassing the encoding of self- or partner-related kinematics taken alone. This marker emerges when partners can see each other, exhibits a topographical distribution over occipital areas, and is specifically driven by movement observation rather than initiation. Using data-driven kinematic decomposition, we further show that vertical bounce movements best drive observers’ EEG activity. These findings highlight the potential of real-world neuroimaging, combined with multivariate modeling, to uncover the mechanisms underlying complex yet natural social behaviors. © © 2025 Bigand et al.}, keywords = {adult; algorithm; Article; brain function; electroencephalogram; electroencephalography; electromyogram; electrooculogram; event related potential; female; functional connectivity; hearing; human; human experiment; kinematics; Likert scale; male; muscle contraction; music; neuroimaging; nuclear magnetic resonance imaging; perception; principal component analysis; sensorimotor function; social cognition; social interaction; task performance; biomechanics; brain; dancing; electromyography; evoked response; physiology; psychology; psychomotor performance; young adult; Adult; Biomechanical Phenomena; Brain; Dancing; Electroencephalography; Electromyography; Evoked Potentials; Female; Humans; Male; Psychomotor Performance; Young Adult},
correspondence_address = {F. Bigand; Neuroscience of Perception & Action Lab, Italian Institute of Technology, Rome, 00161, Italy; email: felix.bigand@iit.it; G. Novembre; Neuroscience of Perception & Action Lab, Italian Institute of Technology, Rome, 00161, Italy; email: giacomo.novembre@iit.it},
publisher = {Society for Neuroscience},
issn = {02706474; 15292401},
coden = {JNRSD},
pmid = {40228893},
language = {English},
abbrev_source_title = {J. Neurosci.},
type = {Article}}
@article{Borges2025Speech,
author = {Borges, Heidi B. and Zaar, Johannes and Aličković, Emina and Christensen, Christian Bech and Kidmose, Preben},
title = {The speech reception threshold can be estimated using EEG electrodes in and around the ear},
year = {2025},
journal = {Journal of Neural Engineering},
volume = {22},
number = {5},
pages = {},
doi = {10.1088/1741-2552/ae00f3},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015768521&doi=10.1088%2F1741-2552%2Fae00f3&partnerID=40&md5=15ffbc889dae9d3ca7e8fc18e37df87b},
affiliations = {Oticon A/S, Denmark, Smorum, Denmark; Aarhus Universitet, Department of Electrical & Computer Engineering, Aarhus, Midtjylland, Denmark; Technical University of Denmark, Department of Health Technology, Lyngby, Hovedstaden, Denmark; Linköpings Universitet, Department of Electrical Engineering, Linkoping, Östergötland, Sweden},
abstract = {Objective. Previous studies have demonstrated that the speech reception threshold (SRT) can be estimated using scalp electroencephalography (EEG), referred to as SRTneuro. The present study assesses the feasibility of using ear-EEG, which allows for discreet measurement of neural activity from in and around the ear, to estimate the SRTneuro. Approach. Twenty young normal-hearing participants listened to audiobook excerpts at varying signal-to-noise ratios (SNRs) whilst wearing a 66-channel EEG cap and 12 ear-EEG electrodes. A linear decoder was trained on different electrode configurations to estimate the envelope of the audio excerpts from the EEG recordings. The reconstruction accuracy was determined by calculating the Pearson’s correlation between the actual and the estimated envelope. A sigmoid function was then fitted to the reconstruction-accuracy-vs-SNR data points, with the midpoint of the sigmoid serving as the SRTneuro estimate for each participant. Main results. Using only in-ear electrodes, the estimated SRTneuro was within 3 dB of the behaviorally measured SRT (SRTbeh) for 6 out of 20 participants (30%). With electrodes placed both in and around the ear, the SRTneuro was within 3 dB of the SRTbeh for 19 out of 20 participants (95%) and thus on par with the reference estimate obtained from full-scalp EEG. Using only electrodes in and around the ear from the right side of the head, the SRTneuro remained within 3 dB of the SRTbeh for 19 out of 20 participants. Significance. These findings suggest that the SRTneuro can be reliably estimated using ear-EEG, especially when combining in-ear electrodes and around-the-ear electrodes. Such an estimate can be highly useful e.g. for continuously adjusting noise-reduction algorithms in hearing aids or for logging the SRT in the user’s natural environment. © 2025 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Acoustic noise; Audio acoustics; Audio recordings; Audition; Biomedical signal processing; Correlation methods; Decoding; Electrodes; Electrophysiology; Noise abatement; Signal to noise ratio; Speech communication; Discreet measurements; Ear-electroencephalography; Measurements of; Neural activity; Neural decoding; Noise ratio; Normal hearing; Reconstruction accuracy; Signal to noise; Speech reception threshold; Electroencephalography; Speech intelligibility; accuracy; adult; Article; clinical article; comparative study; controlled study; correlation coefficient; ear electroencephalography; electroencephalography; feasibility study; female; hearing impairment; hemisphere; Hilbert transform; human; male; mastoid; measurement; normal human; scalp electroencephalography; sigmoid sinus; signal noise ratio; speech audiometry; speech intelligibility; auditory stimulation; auditory threshold; devices; ear; electrode; physiology; procedures; speech perception; young adult; Acoustic Stimulation; Adult; Auditory Threshold; Ear; Female; Humans; Male; Signal-To-Noise Ratio; Speech Perception; Speech Reception Threshold Test; Young Adult},
correspondence_address = {P. Kidmose; Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark; email: pki@ece.au.dk},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {40882673},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Borges2025Speech2,
author = {Borges, Heidi B. and Zaar, Johannes and Aličković, Emina and Christensen, Christian Bech and Kidmose, Preben},
title = {Speech Reception Threshold Estimation via EEG-Based Continuous Speech Envelope Reconstruction},
year = {2025},
journal = {European Journal of Neuroscience},
volume = {61},
number = {6},
pages = {},
doi = {10.1111/ejn.70083},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001975457&doi=10.1111%2Fejn.70083&partnerID=40&md5=4be594c01142acc260c13748a6e4a977},
affiliations = {Oticon A/S, Denmark, Smorum, Denmark; Aarhus Universitet, Department of Electrical & Computer Engineering, Aarhus, Midtjylland, Denmark; Technical University of Denmark, Department of Health Technology, Lyngby, Hovedstaden, Denmark; Linköpings Universitet, Department of Electrical Engineering, Linkoping, Östergötland, Sweden},
abstract = {This study investigates the potential of speech reception threshold (SRT) estimation through electroencephalography (EEG) based envelope reconstruction techniques with continuous speech. Additionally, we investigate the influence of the stimuli's signal-to-noise ratio (SNR) on the temporal response function (TRF). Twenty young normal-hearing participants listened to audiobook excerpts with varying background noise levels while EEG was recorded. A linear decoder was trained to reconstruct the speech envelope from the EEG data. The reconstruction accuracy was calculated as the Pearson's correlation between the reconstructed and actual speech envelopes. An EEG SRT estimate (SRTneuro) was obtained as the midpoint of a sigmoid function fitted to the reconstruction accuracy versus SNR data points. Additionally, the TRF was estimated at each SNR level, followed by a statistical analysis to reveal significant effects of SNR levels on the latencies and amplitudes of the most prominent components. The SRTneuro was within 3 dB of the behavioral SRT for all participants. The TRF analysis showed a significant latency decrease for N1 and P2 and a significant amplitude magnitude increase for N1 and P2 with increasing SNR. The results suggest that both envelope reconstruction accuracy and the TRF components are influenced by changes in SNR, indicating they may be linked to the same underlying neural process. © 2025 The Author(s). European Journal of Neuroscience published by Federation of European Neuroscience Societies and John Wiley & Sons Ltd.}, keywords = {accuracy; adult; Article; auditory threshold; continuous speech envelope reconstruction; diagnostic procedure; edinburgh handedness inventory test; electric potential; electroencephalogram; electroencephalography; female; hearing; human; human experiment; latent period; male; normal human; parameters; participation; reading span test; signal noise ratio; speech audiometry; speech test; temporal response function; auditory evoked potential; auditory stimulation; physiology; procedures; speech perception; young adult; Acoustic Stimulation; Adult; Electroencephalography; Evoked Potentials, Auditory; Female; Humans; Male; Signal-To-Noise Ratio; Speech Perception; Speech Reception Threshold Test; Young Adult},
correspondence_address = {P. Kidmose; Department of Electrical and Computer Engineering, Aarhus University, Aarhus, Denmark; email: pki@ece.au.dk},
publisher = {John Wiley and Sons Inc},
issn = {0953816X; 14609568},
coden = {EJONE},
pmid = {40145625},
language = {English},
abbrev_source_title = {Eur. J. Neurosci.},
type = {Article}}
@article{Borges2025Agerelated,
author = {Borges, Heidi B. and Aličković, Emina and Christensen, Christian Bech and Kidmose, Preben and Zaar, Johannes},
title = {Age-Related Differences in EEG-Based Speech Reception Threshold Estimation Using Scalp and Ear-EEG},
year = {2025},
journal = {Trends in Hearing},
volume = {29},
pages = {},
doi = {10.1177/23312165251372462},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014277504&doi=10.1177%2F23312165251372462&partnerID=40&md5=d5b8ba02f910510f8dd6773005fd32a2},
affiliations = {Oticon A/S, Denmark, Smorum, Denmark; Aarhus Universitet, Department of Electrical & Computer Engineering, Aarhus, Midtjylland, Denmark; Linköpings Universitet, Department of Electrical Engineering, Linkoping, Östergötland, Sweden; Technical University of Denmark, Department of Health Technology, Lyngby, Hovedstaden, Denmark},
abstract = {Previous studies have demonstrated the feasibility of estimating the speech reception threshold (SRT) based on electroencephalography (EEG), termed SRTneuro, in younger normal-hearing (YNH) participants. This method may support speech perception in hearing-aid users through continuous adaptation of noise-reduction algorithms. The prevalence of hearing impairment and thereby hearing-aid use increases with age. The SRTneuroestimation is based on envelope reconstruction accuracy, which has also been shown to increase with age, possibly due to excitatory/inhibitory imbalance or recruitment of additional cortical regions. This could affect the estimated SRTneuro. This study investigated the age-related changes in the temporal response function (TRF) and the feasibility of SRTneuroestimation across age. Twenty YNH and 22 older normal-hearing (ONH) participants listened to audiobook excerpts at various signal-to-noise ratios (SNRs) while EEG was recorded using 66 scalp electrodes and 12 in-ear-EEG electrodes. A linear decoder reconstructed the speech envelope, and the Pearson's correlation was calculated between the reconstructed and speech-stimulus envelopes. A sigmoid function was fitted to the reconstruction-accuracy-versus-SNR data points, and the midpoint was used as the estimated SRTneuro. The results show that the SRTneurocan be estimated with similar precision in both age groups, whether using all scalp electrodes or only those in and around the ear. This consistency across age groups was observed despite physiological differences, with the ONH participants showing higher reconstruction accuracies and greater TRF amplitudes. Overall, these findings demonstrate the robustness of the SRTneuromethod in older individuals and highlight its potential for applications in age-related hearing loss and hearing-aid technology. © The Author(s) 2025. This article is distributed under the terms of the Creative Commons Attribution-NonCommercial 4.0 License (https://creativecommons.org/licenses/by-nc/4.0/) which permits non-commercial use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access page (https://us.sagepub.com/en-us/nam/open-access-at-sage).}, keywords = {adult; adverse event; age; aged; aging; auditory stimulation; auditory threshold; case control study; comparative study; electroencephalography; feasibility study; female; hearing aid; human; male; middle aged; noise; procedures; psychology; scalp; signal noise ratio; speech audiometry; speech perception; very elderly; young adult; Acoustic Stimulation; Adult; Age Factors; Aged; Aged, 80 and over; Aging; Auditory Threshold; Case-Control Studies; Electroencephalography; Feasibility Studies; Female; Hearing Aids; Humans; Male; Middle Aged; Noise; Scalp; Signal-To-Noise Ratio; Speech Perception; Speech Reception Threshold Test; Young Adult},
correspondence_address = {J. Zaar; Eriksholm Research Centre, Snekkersten, Denmark; email: jozr@eriksholm.com},
publisher = {SAGE Publications Inc.},
issn = {23312165},
pmid = {40853325},
language = {English},
abbrev_source_title = {Trends Hear.},
type = {Article}}
@article{Borra2025Editorial,
author = {Borra, Davide and Ma, Ming and Martinez-Martin, Ester and Xia, Likun},
title = {Editorial: Methods in brain-computer interfaces: 2023},
year = {2025},
journal = {Frontiers in Human Neuroscience},
volume = {19},
pages = {},
doi = {10.3389/fnhum.2025.1647584},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011358717&doi=10.3389%2Ffnhum.2025.1647584&partnerID=40&md5=1e0069fff8cb639a75d0fb445b19605a},
affiliations = {Alma Mater Studiorum Università di Bologna, Cesena, Department of Electrical, Cesena, FC, Italy; Yeshiva University, Department of Graduate Computer Science and Engineering, New York, NY, United States; Universitat d'Alacant, Department of Computer Science and Artificial Intelligence, Alicante, Alicante, Spain; Capital Normal University, College of Information Engineering, Beijing, Beijing, China}, keywords = {cerebrovascular accident; decoding; Editorial; electroencephalography; functional near-infrared spectroscopy; human; large language model; machine learning; virtual reality},
correspondence_address = {D. Borra; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi” (DEI), University of Bologna, Cesena, Italy; email: davide.borra2@unibo.it; M. Ma; Department of Graduate Computer Science and Engineering, Katz School of Science and Health, Yeshiva University, New York City, United States; email: ming.ma@yu.edu; E. Martinez-Martin; Department of Computer Science and Artificial Intelligence, University of Alicante, Alicante, Spain; email: ester@ua.es; L. Xia; College of Information Engineering, Capital Normal University, Beijing, China; email: xlk@cnu.edu.cn},
publisher = {Frontiers Media SA},
issn = {16625161},
language = {English},
abbrev_source_title = {Front. Human Neurosci.},
type = {Editorial}}
@article{Borra2025Compact,
author = {Borra, Davide and Diciotti, Stefano and Magosso, Elisa},
title = {A Compact Convolutional Neural Network for Decoding EEG Functional Connectivity: Application to Motor Imagery},
year = {2025},
journal = {Lecture Notes in Computer Science},
volume = {15510 LNCS},
pages = {102 - 115},
doi = {10.1007/978-3-031-82487-6_8},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001048510&doi=10.1007%2F978-3-031-82487-6_8&partnerID=40&md5=95bda36132827c0ffbefef4c87e2fa24},
affiliations = {Alma Mater Studiorum Università di Bologna, Cesena, Cesena Campus, Cesena, FC, Italy},
abstract = {The design of neural decoders exploiting brain functional connectivity is growing interest in the neuroscience community, mainly for brain-computer interface applications. By exploiting also/exclusively the interactions among different brain regions as input features, neural decoding improved. However, the adoption of connectivity estimates for neural decoding is still in its infancy. Indeed, it is mainly adopted with non-directed connectivity measures, and it is mainly exploited in classic machine learning pipelines, limiting the analysis only on few interactions or frequency ranges extracted from the connectivity estimate, without fully exploiting the totality of the information contained in the measured functional connectivity (i.e., spatial and frequency domains characterizing connectivity matrices). To overcome this limitation, we designed a convolutional neural network for handling directed connectivity measures estimated via spectral Granger causality. The network automatically learned features in the frequency and spatial domains, separately resuming connectivity inflow and outflow. Our approach was applied to motor imagery decoding, and achieved state-of-the-art performance compared to existing decoders. Moreover, the features learned by the network matched the directional interaction known occurring when imagining movements, confirming that the network feature learning was able to capture the most relevant brain network characteristics. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.}, keywords = {Brain mapping; Image enhancement; Neurons; Brain regions; Brain-computer interface applications; Convolutional neural network; Functional connectivity; Granger Causality; Input features; Machine-learning; Motor imagery; Neural decoding; Spectral granger causality; Convolutional neural networks},
correspondence_address = {D. Borra; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi” (DEI), University of Bologna, Cesena Campus, Cesena, Italy; email: davide.borra2@unibo.it},
editor = {Nicosia, G. and Ojha, V. and Giesselbach, S. and Pardalos, M.P. and Umeton, R.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {16113349; 03029743},
isbn = {9789819698936; 9789819698042; 9789819698110; 9789819698905; 9783032004949; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141},
language = {English},
abbrev_source_title = {Lect. Notes Comput. Sci.},
type = {Conference paper}}
@article{Burns2025Neural,
author = {Burns, Joseph D. and Lerner, David P.},
title = {Neural Decoding, Disorders of Consciousness, and the Hard Consciousness Problem},
year = {2025},
journal = {Neurology},
volume = {104},
number = {4},
pages = {},
doi = {10.1212/WNL.0000000000213354},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216769651&doi=10.1212%2FWNL.0000000000213354&partnerID=40&md5=cb9326e15fdd85361f040f031e5f4106},
affiliations = {Lahey Hospital & Medical Center, Department of Neurology, Burlington, MA, United States; University of Massachusetts Chan Medical School, Worcester, MA, United States; College of Medicine, Division of Neurocritical Care, New York, NY, United States},
keywords = {awareness; brain function; case fatality rate; cognition; consciousness; consciousness disorder; diagnostic accuracy; Editorial; electroencephalogram; functional magnetic resonance imaging; human; neural decoding; neuroscience; quality of life; editorial},
correspondence_address = {J.D. Burns; Division of Neurocritical Care, Department of Neurology, Lahey Hospital and Medical Center, Burlington, United States; email: joseph.d.burns@gmail.com},
publisher = {Lippincott Williams and Wilkins},
issn = {00283878; 1526632X},
isbn = {9781437736113},
coden = {NEURA},
pmid = {39883907},
language = {English},
abbrev_source_title = {Neurology},
type = {Editorial}}
@article{Cai2025Cgnet,
author = {Cai, Guoqing and Chen, Yiyi and Yang, Bolun and Yang, Yanwu and Ma, Ting and Wang, Yilong},
title = {CGNet: A Complex-valued Graph Network for jointly learning amplitude-phase information in EEG-based brain–computer interfaces},
year = {2025},
journal = {Neural Networks},
volume = {191},
pages = {},
doi = {10.1016/j.neunet.2025.107795},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010124163&doi=10.1016%2Fj.neunet.2025.107795&partnerID=40&md5=df646fb3758032dc6286878d6a6a7eef},
affiliations = {Harbin Institute of Technology, School of Information Science and Technology, Harbin, Heilongjiang, China; Beijing Tiantan Hospital, Capital Medical University, Beijing, China; Harbin Institute of Technology, School of Biomedical Engineering, Harbin, Heilongjiang, China; National Center for Neurological Disorders, Shanghai, Shanghai, China; Beijing Tiantan Hospital, Capital Medical University, Beijing, China; Chinese Institute for Brain Research, Beijing, Beijing, China; Capital Medical University, Advanced Innovation Center for Human Brain Protection, Beijing, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China},
abstract = {The synergy between amplitude and phase in electroencephalogram (EEG)-based brain–computer interfaces (BCIs) provides comprehensive and essential insights into neural oscillatory processes. However, constrained by real-valued computation paradigms, most deep learning methods have to process amplitude and phase independently, neglecting their crucial interaction mechanisms. To address this issue, we construct a Complex-valued Graph Network (CGNet) to capture comprehensive information from EEG signals, where both amplitude and phase information are encoded into the complex-valued representation. Specifically, we design a two-scale complex-valued convolutional network to learn local spatio-temporal information, develop a spatial attention module to enhance spatial information learning, and formulate a dynamic graph convolution to capture global temporal dependencies. Furthermore, we extend CGNet to Filter-Band CGNet (FBCGNet), enhancing the model's adaptability to broadband EEG data. Applied to motor imagery and execution BCI tasks, CGNet achieves state-of-the-art classification performance while maintaining computational efficiency comparable to other advanced algorithms. Notably, FBCGNet further improves CGNet's performance. Visualization results show that CGNet can identify the key spatio-temporal information consistent with paradigm principles. In addition, compared with using amplitude or phase alone, CGNet can capture more comprehensive task-related neural activities, thereby showing higher classification performance. CGNet is a promising tool for mining amplitude-phase information and offering more comprehensive neural decoding in EEG-based BCIs. © 2025}, keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Complex networks; Computational electromagnetics; Convolution; Convolutional neural networks; Deep learning; Electroencephalography; Graph neural networks; Interfaces (computer); Learning systems; Phase interfaces; Amplitude-phase learning; Classification performance; Complex-valued; Complex-valued computation; Convolutional networks; Global dependency; Graph convolutional network; Graph networks; Phase information; Spatiotemporal information; Computational efficiency; algorithm; Article; controlled study; electroencephalogram; human; learning; artificial neural network; brain; brain computer interface; deep learning; electroencephalography; physiology; procedures; Algorithms; Brain-Computer Interfaces; Deep Learning; Humans; Neural Networks, Computer},
correspondence_address = {T. Ma; School of Biomedical Engineering, Harbin Institute of Technology (Shenzhen), Shenzhen, China; email: tma@hit.edu.cn; Y. Wang; Department of Neurology, Beijing Tiantan Hospital, Capital Medical University, Beijing, China; email: yilong528@aliyun.com},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
pmid = {40644990},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{Caria2025Towards,
author = {Caria, Andrea},
title = {Towards Predictive Communication: The Fusion of Large Language Models and Brain–Computer Interface},
year = {2025},
journal = {Sensors},
volume = {25},
number = {13},
pages = {},
doi = {10.3390/s25133987},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010621629&doi=10.3390%2Fs25133987&partnerID=40&md5=e0efa414eb43c447855028e39786eaf1},
affiliations = {Università di Trento, Department of Psychology and Cognitive Science, Trento, TN, Italy},
abstract = {Integration of advanced artificial intelligence with neurotechnology offers transformative potential for assistive communication. This perspective article examines the emerging convergence between non-invasive brain–computer interface (BCI) spellers and large language models (LLMs), with a focus on predictive communication for individuals with motor or language impairments. First, I will review the evolution of language models—from early rule-based systems to contemporary deep learning architectures—and their role in enhancing predictive writing. Second, I will survey existing implementations of BCI spellers that incorporate language modeling and highlight recent pilot studies exploring the integration of LLMs into BCI. Third, I will examine how, despite advancements in typing speed, accuracy, and user adaptability, the fusion of LLMs and BCI spellers still faces key challenges such as real-time processing, robustness to noise, and the integration of neural decoding outputs with probabilistic language generation frameworks. Finally, I will discuss how fully integrating LLMs with BCI technology could substantially improve the speed and usability of BCI-mediated communication, offering a path toward more intuitive, adaptive, and effective neurotechnological solutions for both clinical and non-clinical users. © 2025 by the author.}, keywords = {Brain; Brain computer interface; Brain mapping; Deep learning; Electrophysiology; Integration; Interfaces (computer); Learning systems; Modeling languages; Brain–computer interface speller; Computer interaction; Human machine interaction; Human–computer interaction; Human–machine interaction; Language model; Large language model; Predictive writing; Transformer modeling; Electroencephalography; artificial intelligence; brain; brain computer interface; deep learning; electroencephalography; human; interpersonal communication; language; large language model; physiology; Artificial Intelligence; Brain-Computer Interfaces; Communication; Deep Learning; Humans; Language; Large Language Models},
correspondence_address = {A. Carìa; Department of Psychology and Cognitive Science, University of Trento, Rovereto, 38068, Italy; email: andrea.caria@unitn.it},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {14248220},
pmid = {40648241},
language = {English},
abbrev_source_title = {Sensors},
type = {Article}}
@article{Caria2025Integrating,
author = {Caria, Andrea},
title = {Integrating Large Language Models and Brain Decoding for Augmented Human-Computer Interaction: A Prototype LLM-P3-BCI Speller},
year = {2025},
journal = {Lecture Notes in Networks and Systems},
volume = {1283 LNNS},
pages = {403 - 416},
doi = {10.1007/978-3-031-84457-7_25},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000724174&doi=10.1007%2F978-3-031-84457-7_25&partnerID=40&md5=aa9637b175c52f50be474003bbf0276b},
affiliations = {Università di Trento, Trento, TN, Italy},
abstract = {One of the major advances in Human-Computer Interaction has been the integration of predictive language modeling in text input interfaces. The introduction of intelligent text entry systems, proposing for instance word completion and word suggestions, made written communication faster and more efficient. Currently, intelligent text entry systems depend upon a manual selection of the intended word or phrase. Such a necessary motor act inherently impacts communication and possibly represents a system bottleneck and a source of potential errors. This required task might be even more problematic in individuals with motor disorders, where simple motor acts, if ever possible, might require high cognitive and physical effort. In this regard, Brain–Computer Interfaces (BCI) provide alternative non-muscular channels for efficient human-computer and human-machine interactions. I here present a prototype BCI system that exploits advanced predictive writing and online brain decoding to boost written communication. Specifically, a novel system combining an intelligent predictive writing system based on GPT with a Rapid Serial Visual Presentation-based P3-BCI speller is described. The proposed LLM-P3-BCI speller prototype represents an exemplary system possibly enabling rapid decoding of user’s intended characters and words via effective online brain signals classification, with no manual intervention. Prospectively, the integration of large language models with BCI promises to substantially augment communication and control in patients with motor or language disorders as well as in healthy individuals. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.}, keywords = {Decoding; Electrotherapeutics; Problem oriented languages; Computer interaction; Electroencephalography; Event related potentials; Event-related potential; Generative pre-trained transformer; Human machine interaction; Human-computer interaction; Human-machine interaction; Language model; Large language model; Predictive writing},
correspondence_address = {A. Carìa; University of Trento, Rovereto, 38068, Italy; email: andrea.caria@unitn.it},
editor = {Arai, K.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {23673389; 23673370},
isbn = {9789819652372; 9783031931055; 9789819662968; 9783031999963; 9783031950162; 9783031947698; 9783032004406; 9783031910074; 9783031926105; 9783031877032},
language = {English},
abbrev_source_title = {Lect. Notes Networks Syst.},
type = {Conference paper}}
@article{Cernera2025Master,
author = {Cernera, Stephanie and Gemicioglu, Tan and Berezutskaya, Julia and Csaky, Richard and Verwoert, Maxime and Polyakov, Daniel and Papadopoulos, Sotirios and Spagnolo, Valeria and Astudillo, Juliana Gonzalez and Kumar, Satyam and Alawieh, Hussein and Kelly, Dion and Keough, Joanna R.G. and Minhas, Araz and Dold, Matthias R. and Han, Yiyuan and McClanahan, Alexander and Mustafa, Mousa and González España, Juan José and Garro, Florencia and Vujic, Angela and Kacker, Kriti and Kapeller, Christoph and Geukes, Simon H. and Verbaarschot, Ceci and Wimmer, Michael and Sultana, Mushfika and Ahmadi, Sara and Herff, Christian and Sburlea, Andreea Ioana and Jeunet-Kelway, Camille and Thompson, David E. and Semprini, Marianna and Andersen, Richard A. and Stavisky, Sergey D. and Kinney-Lang, Eli and Lotte, Fabien and Thielen, Jordy and Chen, Xing and Peterson, Victoria and Gunduz, Aysegul and Vaughan, Theresa M. and Valeriani, Davide},
title = {Master classes of the tenth international brain-computer interface meeting: showcasing the research of BCI trainees},
year = {2025},
journal = {Journal of Neural Engineering},
volume = {22},
number = {2},
pages = {},
doi = {10.1088/1741-2552/adb335},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000188492&doi=10.1088%2F1741-2552%2Fadb335&partnerID=40&md5=462641f246e80945d10bdf4dbc570606},
affiliations = {UCSF School of Medicine, San Francisco, CA, United States; Cornell University, Department of Information Science, Ithaca, NY, United States; University Medical Center Utrecht, Department of Neurology and Neurosurgery, Utrecht, Netherlands; University of Oxford Medical Sciences Division, Oxford, Oxfordshire, United Kingdom; Universiteit Maastricht, Department of Neurosurgery, Maastricht, Limburg, Netherlands; Ben-Gurion University of the Negev, Beer-Sheva, Southern District, Israel; Ben-Gurion University of the Negev, Agricultural, Beer-Sheva, Southern District, Israel; Université Claude Bernard Lyon 1, Villeurbanne, Auvergne-Rhone-Alpes, France; Inserm, Paris, Ile-de-France, France; CNRS Centre National de la Recherche Scientifique, Paris, Ile-de-France, France; Instituto de Matemática Aplicada del Litoral, Santa Fe, Santa Fe, Argentina; Hôpital Universitaire Pitié Salpêtrière, Paris, Ile-de-France, France; Cockrell School of Engineering, Austin, TX, United States; Cumming School of Medicine, Departments of Pediatrics and Clinical Neurosciences, Calgary, AB, Canada; Donders Institute for Brain, Cognition and Behaviour, Data-Driven Neurotechnology Lab, Nijmegen, Gelderland, Netherlands; University of Essex, Colchester, Essex, United Kingdom; UAMS College of Medicine, Little Rock, AR, United States; Technische Universität Berlin, Neurotechnology Group, Berlin, Germany; University of Houston, NSF BRAIN Center, Houston, TX, United States; Università degli Studi di Genova, Genoa, Italy; MIT Media Lab, Cambridge, MA, United States; College of Engineering, Pittsburgh, PA, United States; tec medical engineering GmbH, Schiedlberg, Austria; University of Pittsburgh School of Medicine, Rehab Neural Engineering Labs, Pittsburgh, PA, United States; Know-Center, Graz, Graz, Styria, Austria; University of Essex, Colchester, Essex, United Kingdom; Faculty of Health, Medicine and Life Sciences, Department of Neurosurgery, Maastricht, Limburg, Netherlands; Rijksuniversiteit Groningen, Computer Science and Artificial Intelligence, Groningen, Groningen, Netherlands; Université de Bordeaux, Bordeaux, Nouvelle-Aquitaine, France; Carl R. Ice College of Engineering, Manhattan, KS, United States; Istituto Italiano di Tecnologia, Genoa, GE, Italy; Division of Biology and Biological Engineering, Pasadena, CA, United States; UC Davis School of Medicine, Sacramento, CA, United States; Cumming School of Medicine, Calgary, AB, Canada; Laboratoire Bordelais de Recherche en Informatique, Talence, Nouvelle-Aquitaine, France; University of Pittsburgh, Department of Ophthalmology, Pittsburgh, PA, United States; Herbert Wertheim College of Engineering, Fixel Institute for Neurological Disorders, Gainesville, FL, United States; Albany VA Medical Center, National Center for Adaptive Neurotechnologies, Albany, NY, United States; Technogym U.K. Limited, Bracknell, United Kingdom},
abstract = {The Tenth International brain-computer interface (BCI) meeting was held June 6-9, 2023, in the Sonian Forest in Brussels, Belgium. At that meeting, 21 master classes, organized by the BCI Society’s Postdoc & Student Committee, supported the Society’s goal of fostering learning opportunities and meaningful interactions for trainees in BCI-related fields. Master classes provide an informal environment where senior researchers can give constructive feedback to the trainee on their chosen and specific pursuit. The topics of the master classes span the whole gamut of BCI research and techniques. These include data acquisition, neural decoding and analysis, invasive and noninvasive stimulation, and ethical and transitional considerations. Additionally, master classes spotlight innovations in BCI research. Herein, we discuss what was presented within the master classes by highlighting each trainee and expert researcher, providing relevant background information and results from each presentation, and summarizing discussion and references for further study. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.}, keywords = {Adversarial machine learning; Personnel training; Supervised learning; Belgium; Brussels; Constructive feedback; Learning opportunity; Neural decoding; Neurorehabilitation []; Noninvasive stimulations; Sensorimotor cortex; Sensory restoration; Speech decoding; Brain computer interface; brain; brain computer interface; constructive feedback; data mining; decoding; drug therapy; forest; human; information processing; learning; neurorehabilitation; review; sensorimotor cortex; speech; student; therapy; electroencephalography; procedures; Brain-Computer Interfaces; Electroencephalography; Humans},
correspondence_address = {S. Cernera; Department of Neurological Surgery, University of California San Francisco, San Francisco, United States; email: stephcernera@gmail.com},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {39914028},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Review}}
@article{Chae2025Eyeblink,
author = {Chae, Min Seong and Rehman, Abdul and Kim, Yeni and Kim, Jaedeok and Han, Yaeeun and Park, Sangin and Mun, Sungchul},
journal = {IEEE Access},
title = {Eye-Blink and SSVEP-Based Selective Attention Interface for XR User Authentication: An Explainable Neural Decoding and Machine Learning Approach to Reducing Visual Fatigue},
year = {2025},
volume = {13},
number = {},
pages = {176998--177018},
abstract = {The growing demand for secure, immersive authentication in extended reality (XR) environments calls for neural interfaces that are both robust and user-friendly. This study introduces a novel and robust dual-modality EEG-based authentication framework that independently exploits: 1) steady-state visually evoked potentials (SSVEP) and 2) eye-blink-induced EEG responses as covert neural signatures. Both signals are recorded using a 64-channel EEG system seamlessly integrated with the Microsoft HoloLens 2 for immersive XR-based user evaluation. To mitigate visual fatigue while preserving signal fidelity, we replace conventional flicker stimuli with a 10 Hz grow–shrink visual design. We employ a modality-specific classification strategy, modeling SSVEP and eye-blink signals independently to retain their distinct neurophysiological characteristics. A multi-stage feature selection pipeline combines SHAP and Random Forest rankings, followed by logistic regression-based permutation importance to identify the top 10 discriminative features per modality. These features undergo statistical validation via non-parametric tests to ensure physiological plausibility and class separability. Classification is subsequently performed using four machine learning models—Random Forest, XGBoost, Support Vector Machine, and Logistic Regression—with Random Forest and XGBoost consistently yielding the highest performance. Evaluated across 20 participants using user-wise validation, our framework achieves over 99% accuracy and near-perfect ROC-AUC scores for both modalities, confirming strong discriminability between genuine and impostor attempts. Our results demonstrate that interpretable, fatigue-aware EEG features can deliver high authentication performance under XR conditions. The proposed system is lightweight and explicitly engineered for real-time deployment and spoof-resistance, making it well-suited for future XR-based defense, training, and industrial applications.},
keywords = {Electroencephalography;Authentication;Visualization;Protocols;Brain modeling;Biometrics;Resists;Fatigue;Biomedical monitoring;Feature extraction;Biometric security;EEG;eye blink;Explainable AI;neural decoding;SHAP;SSVEP;visual fatigue;XR authentication.},
doi = {10.1109/ACCESS.2025.3613355},
issn = {2169-3536},
month = {}}
@article{Chang2025Human,
author = {Chang, Yanhan and Chen, Hsi An and Tsai, Minjiun and Tseng, Chunlung and Lo, Chinghuei and Huang, Kuanchih and Wei, Chunshu},
title = {A Human EEG Dataset for Multisensory Perception and Mental Imagery},
year = {2025},
journal = {Scientific Data},
volume = {12},
number = {1},
pages = {},
doi = {10.1038/s41597-025-05881-1},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017597465&doi=10.1038%2Fs41597-025-05881-1&partnerID=40&md5=062248b0fa835280ce4d7ccdeef2877d},
affiliations = {National Yang Ming Chiao Tung University, Department of Computer Science, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Interdisciplinary Science Degree Program, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Institute of Mathematical Modeling and Scientific Computing, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Institute of Education, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Brain Science and Technology Center, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Institute of Biomedical Engineering, Hsinchu, Taiwan},
abstract = {The YOTO (You Only Think Once) dataset presents a human electroencephalog- raphy (EEG) resource for exploring multisensory perception and mental imagery. The study enrolled 26 participants who performed tasks involving both unimodal and multimodal stimuli. Researchers collected high-resolution EEG signals at a 1000 Hz sampling rate to capture high-temporal-resolution neural activity related to internal mental representations. The protocol incorporated visual, auditory, and combined cues to investigate the integration of multiple sensory modalities, and participants provided self-reported vividness ratings that indicate subjec- tive perceptual strength. Technical validation involved event-related potentials (ERPs) and power spectral density (PSD) analyses, which demonstrated the reli- ability of the data and confirmed distinct neural responses across stimuli. This dataset aims to foster studies on neural decoding, perception, and cognitive mod- eling, and it is publicly accessible for researchers who seek to advance multimodal mental imagery research and related applications. © The Author(s) 2025.},
keywords = {adult; electroencephalography; evoked response; female; hearing; human; imagination; male; vision; Adult; Auditory Perception; Electroencephalography; Evoked Potentials; Female; Humans; Imagination; Male; Visual Perception},
correspondence_address = {C.-S. Wei; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; email: wei@nycu.edu.tw},
publisher = {Nature Research},
issn = {20524463},
pmid = {41028775},
language = {English},
abbrev_source_title = {Sci. Data},
type = {Data paper}}
@article{Chen2025Decoding,
author = {Chen, Qiupu and Wang, Yimou and Wang, Fenmei and Sun, Duolin and Li, Qiankun},
title = {Decoding text from electroencephalography signals: A novel Hierarchical Gated Recurrent Unit with Masked Residual Attention Mechanism},
year = {2025},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {},
doi = {10.1016/j.engappai.2024.109615},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209147076&doi=10.1016%2Fj.engappai.2024.109615&partnerID=40&md5=51fd426475aa1828d4caea5fd30692bf},
affiliations = {University of Science and Technology of China, Science Island Branch of Graduate School, Hefei, Anhui, China; Institute of Intelligent Machines Chinese Academy of Sciences, Hefei, Anhui, China; Army Academy of Artillery and Air Defense, Hefei, Anhui, China; University of Science and Technology of China, Department of Automation, Hefei, Anhui, China},
abstract = {Progress in both neuroscience and natural language processing has opened doors for investigating brain to text techniques to reconstruct what individuals see, perceive, or focus on from human brain activity patterns. Non-invasive decoding, utilizing electroencephalography (EEG) signals, is preferred due to its comfort, cost-effectiveness, and portability. In brain-to-text applications, a pressing need has arisen to develop effective models that can accurately capture the intricate details of EEG signals, such as global and local contextual information and long-term dependencies. In response to this need, we propose the Hierarchical Gated Recurrent Unit with Masked Residual Attention Mechanism (HGRU-MRAM) model, which ingeniously combines the hierarchical structure and the masked residual attention mechanism to deliver a robust brain-to-text decoding system. Our experimental results on the ZuCo dataset demonstrate that this model significantly outperforms existing baselines, achieving state-of-the-art performance with Bilingual Evaluation Understudy Score (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), US National Institute of Standards and Technology Metric (NIST), Metric for Evaluation of Translation with Explicit Ordering (METEOR), Translation Edit Rate (TER), and BiLingual Evaluation Understudy with Representations from Transformers (BLEURT) scores of 48.29, 34.84, 4.07, 34.57, 21.98, and 40.45, respectively. The code is available at https://github.com/qpuchen/EEG-To-Sentence. © 2024 Elsevier Ltd}, keywords = {Brain mapping; Neurophysiology; Attention mechanisms; Bilinguals; Brain decoding; Brain to text; Brain–machine interface; Deep learning; Language processing; Machine interfaces; Natural languages; Neurolinguistics; Electroencephalography},
correspondence_address = {F. Wang; Science Island Branch of Graduate School, University of Science and Technology of China, Hefei, China; email: wangfenmei205@126.com},
publisher = {Elsevier Ltd},
issn = {09521976},
coden = {EAAIE},
language = {English},
abbrev_source_title = {Eng Appl Artif Intell},
type = {Article}}
@article{Cheng2025Neural,
author = {Cheng, Huan and Wang, Guangnan and Liu, Donghong and Ye, Xingqian},
title = {Neural Decoding of Alcohol Stimulation Perception Based on EEG and Machine Learning and a Comparative Study of Different Brain Partitioning Methods; 基于 EEG 与机器学习的酒精刺激感知神经解码及脑区 分区策略的对比研究},
year = {2025},
journal = {Journal of Chinese Institute of Food Science and Technology},
volume = {25},
number = {6},
pages = {14 - 26},
doi = {10.16429/j.1009-7848.2025.06.002},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013220533&doi=10.16429%2Fj.1009-7848.2025.06.002&partnerID=40&md5=7a4c8bf03744b77d41e5d34f8e3862b0},
affiliations = {National-Local Joint Engineering Laboratory of Intelligent Food Processing Technology and Equipment, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China},
abstract = {Alcohol concentration plays a crucial role in shaping the flavor characteristics and sensory experience of alcoholic beverages. However, its neural perception mechanism remains insufficiently understood, limiting the objective quantification of alcohol-induced sensory responses. Electroencephalography (EEG), with its high temporal resolution, offers a valuable tool for investigating the neural basis of alcohol perception. However, the choice of brain parcellation methods can significantly influence EEG feature extraction and modeling accuracy, ultimately affecting predictive performance. This study systematically compared the applicability of the traditional 10-10 anatomical parcellation and the Yeo-7 functional parcellation in predicting alcohol stimulation scores using EEG. Eight machine learning models were employed to assess their predictive capabilities. The results demonstrated that, compared to the 10-10 anatomical parcellation, the Yeo-7 functional parcellation significantly improved prediction performance, with linear regression (R2 = 0.76) and support vector machines (R2 = 0.74) achieving the best results. Furthermore, feature contribution analysis revealed that the limbic network, frontoparietal network (FPN), and ventral attention network (VAN) played the most significant roles in EEG-based prediction, suggesting that functional parcellation more accurately captures alcohol-related neural signals. This study validates the advantage of functionally guided brain parcellation in EEG-based predictive modeling and provides new insights for applying EEG in flavor perception, neuroscience, and food science. © 2025 Chinese Institute of Food Science and Technology. All rights reserved.}, keywords = {Biomedical signal processing; Brain; Electrophysiology; Forecasting; Learning systems; Prediction models; Sensory perception; Support vector regression; Alcohol concentrations; Comparatives studies; Electroencephalography; Flavor perceptions; Functionals; Machine-learning; Neural decoding; Partitioning methods; Perception-based; Sensory experiences; Alcoholic beverages},
correspondence_address = {X. Ye; College of Biosystems Engineering and Food Science, National-Local Joint Engineering Research Center of Intelligent Food Technology and Equipment, Zhejiang Key Laboratory of Agri-food Resources and High-value Utilization, Hangzhou, 310058, China; email: psu@zju.edu.cn},
publisher = {Chinese Institute of Food Science and Technology},
issn = {10097848},
language = {Chinese},
abbrev_source_title = {J. Chin. Inst. Food Sci. Technol.},
type = {Article}}
@conference{Chikkudu2025Enhanced,
author = {Chikkudu, Sreedevi and Annamalai, Suresh},
title = {Enhanced Motor-Imagery Signal Processing for Neuromuscular Disease Classification Using SMOTE-MRMR},
year = {2025},
pages = {},
doi = {10.1109/OTCON65728.2025.11070330},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012091493&doi=10.1109%2FOTCON65728.2025.11070330&partnerID=40&md5=918ba99ded438cf42defdef899f7a2f6},
affiliations = {SRM Institute of Science and Technology, School of Computing, Kattankulathur, TN, India},
abstract = {Decoding motor imagery (MI) signals with precision remains crucial for brain-computer interface (BCI) systems in healthcare applications. Electroencephalographic (EEG) signals, primarily time-domain recordings, contain complex frequency components representing specific brain activities. In this study, we leveraged the Minimum Redundancy Maximum Relevance (MRMR) feature selection technique to enhance EEG signal classification accuracy. The experimental protocol involved ten healthy participants performing hand movement tasks, specifically "open"and "close"actions, with three electrodes placed on their scalps to capture EEG signals. Addressing the persistent challenge of imbalanced MI-EEG channel data, we implemented the MRMR algorithm to select the most informative and least redundant features from the dataset. The MRMR technique systematically evaluates feature relevance by minimizing re-dundancy between selected features while maximizing their individual correlation with the target classification, differing from traditional feature selection methods by simultaneously considering both feature-to-target and inter-feature relationships. Our methodology applied MRMR feature selection to the preprocessed EEG signals, followed by synthetic minority over-sampling technique (SMOTE) to balance the dataset. The balanced feature set was then utilized to train multiple machine learning classifiers, including K-Nearest Neighbors (KNN), Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), and Support Vector Machine (SVM). Performance evaluation involved comprehensive metrics such as accuracy, F1 score, specificity, precision, and sensitivity. The MRMR-based feature selection demonstrated significant improvements in classification performance, with the SVM classifier achieving an outstanding accuracy of 98.3% on the MI-BCI-EEG dataset, showcasing the potential of the MRMR technique in addressing feature selection challenges in imbalanced EEG signal classification and offering a promising avenue for advancing brain-computer interface technologies. © 2025 IEEE.}, keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Decision trees; Decoding; Electroencephalography; Electrophysiology; Image classification; Interfaces (computer); Learning systems; Logistic regression; Medical computing; Motion compensation; Nearest neighbor search; Neurophysiology; Random forests; Support vector machines; Time domain analysis; Cortical activity; Electroencephalographic; Features selection; Minimum redundancy-maximum relevances; Motor cortex neural decoding; Motor imagery; Motor-cortex; Neural decoding; Neural signal processing; Neuroplasticity; Neuroscience keyword; Signal processing keyword; Signal-processing; Feature extraction},
correspondence_address = {S. Chikkudu; Srm Institute of Science and Technology, School of Computing, Faculty of Engineering and Technology, Department of Networking and Communications, Chennai, SRM Nagar, Kattankulathur, Tamil Nadu, 603203, India; email: sc4708@srmist.edu.in},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9798331535360},
language = {English},
abbrev_source_title = {OPJU Int. Technol. Conf. Smart Comput. Innov. Adv. Ind. 5.0, OTCON},
type = {Conference paper}}
@article{Cho2025Decoding,
author = {Cho, Jeonghwa and Brennan, Jonathan R.},
title = {Decoding of lexical items and grammatical features in EEG: A cross-linguistic study},
year = {2025},
journal = {Neuropsychologia},
volume = {213},
pages = {},
doi = {10.1016/j.neuropsychologia.2025.109150},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003656268&doi=10.1016%2Fj.neuropsychologia.2025.109150&partnerID=40&md5=c78c26fa190b08369c9698b6226398e9},
affiliations = {University of Michigan, Ann Arbor, Department of Linguistics, Ann Arbor, MI, United States; Hongik University, Department of English Language and Literature, Seoul, South Korea},
abstract = {Diverse evidence supports the theory that bilingual language users have language-invariant representations of concepts and grammatical forms such as argument structure. Here we extend that work to test the representation of morphosyntactic features and lexical concepts in typologically different languages. Specifically, we deploy machine learning techniques with EEG data collected from eighteen Korean-English bilinguals while they read singular and plural nouns and present and past tense verbs in English and Korean. Whereas event-related potentials (ERPs) analyses show limited sensitivity to discriminate lexical, number, and tense information, neural decoding revealed robust within-language classification of lexical and morphosyntactic information in both languages. In contrast, between-languages decoding was possible only for number information; decoding of lexical items and tense did not generalize between the two languages, even when accounting for temporal differences. These results indicate stable within-language EEG representations for lexical items and morphosyntactic features but suggest that only the number feature show evidence for shared EEG response patterns between the two languages studied. © 2025 Elsevier Ltd}, keywords = {adult; article; decoding; diagnosis; electroencephalogram; electroencephalography; event related potential; evoked response; human; human experiment; machine learning; male; brain; female; linguistics; multilingualism; physiology; psycholinguistics; reading; semantics; young adult; Adult; Brain; Electroencephalography; Evoked Potentials; Female; Humans; Linguistics; Machine Learning; Male; Multilingualism; Psycholinguistics; Reading; Semantics; Young Adult},
correspondence_address = {J. Cho; Department of Linguistics, University of Michigan, United States; email: jhcho@hongik.ac.kr},
publisher = {Elsevier Ltd},
issn = {00283932; 18733514},
coden = {NUPSA},
pmid = {40280403},
language = {English},
abbrev_source_title = {Neuropsychologia},
type = {Article}}
@inproceedings{Choi2025International,
author = {Choi, Yebin and Kim, Jun-Mo and Choi, WooHyeok and Ji, Chang-Hoon and Oh, Ji-Hye and Kam, Tae-Eui},
booktitle = {2025 13th International Conference on Brain-Computer Interface (BCI)},
title = {Visual Decoding Using a Learnable Wavelet-Based Spatial-Spectral-Temporal EEG Embedding},
year = {2025},
volume = {},
number = {},
pages = {1--5},
abstract = {Visual decoding seeks to identify or reconstruct visual stimuli perceived by individuals based on neural activity. Although functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG) have achieved remarkable success in visual decoding, their high costs, limited portability, and real-time processing challenges necessitate alternative approaches. Electroencephalography (EEG) provides a promising solution due to its cost-effectiveness, high temporal resolution, and suitability for real-time applications. However, conventional EEG-based encoders often rely on simplistic architectures, which limits their ability to fully capture the spatial, spectral, and temporal (SST) features of EEG signals, leading to suboptimal performance. In this study, we propose a novel brain decoding framework utilizing a state-of-the-art EEG encoder specifically designed to capture the SST characteristics of EEG signals. The proposed framework was evaluated on the THINGS-EEG dataset. It achieved a mean top-l accuracy of 19.7% and a top-5 accuracy of 50.7% in zero-shot retrieval tasks, outperforming conventional EEG encoders. These results demonstrate the potential of our method in advancing EEG-based visual decoding task.},
keywords = {Visualization;Accuracy;Functional magnetic resonance imaging;Feature extraction;Electroencephalography;Real-time systems;Decoding;Spatial resolution;Image reconstruction;Visual perception;Brain Decoding;Visual Decoding;Elec-troencephalogram;Learnable Wavelet Kernel;Spectral-spatial-temporal Representation},
doi = {10.1109/BCI65088.2025.10931751},
issn = {2572-7672},
month = {Feb}}
@article{Edelman2025Noninvasive,
author = {Edelman, Bradley Jay and Zhang, Shuailei and Schalk, Gerwin and Brunner, Peter and Müller-Putz, Gernot R. and Guan, Cuntai and He, Bin},
title = {Non-Invasive Brain-Computer Interfaces: State of the Art and Trends},
year = {2025},
journal = {IEEE Reviews in Biomedical Engineering},
volume = {18},
pages = {26 - 49},
doi = {10.1109/RBME.2024.3449790},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202729402&doi=10.1109%2FRBME.2024.3449790&partnerID=40&md5=98717daff008e11da469402794ecd12f},
affiliations = {Max Planck Institute of Psychiatry, Munich, Bayern, Germany; Max Planck Institute for Biological Intelligence, Planegg, Bayern, Germany; Nanyang Technological University, Singapore City, Singapore; Fudan University, Shanghai, China; Washington University in St. Louis, St. Louis, MO, United States; Technische Universitat Graz, Graz, Styria, Austria; Carnegie Mellon University, Pittsburgh, PA, United States},
abstract = {Brain-computer interface (BCI) is a rapidly evolving technology that has the potential to widely influence research, clinical and recreational use. Non-invasive BCI approaches are particularly common as they can impact a large number of participants safely and at a relatively low cost. Where traditional non-invasive BCIs were used for simple computer cursor tasks, it is now increasingly common for these systems to control robotic devices for complex tasks that may be useful in daily life. In this review, we provide an overview of the general BCI framework as well as the various methods that can be used to record neural activity, extract signals of interest, and decode brain states. In this context, we summarize the current state-of-the-art of non-invasive BCI research, focusing on trends in both the application of BCIs for controlling external devices and algorithm development to optimize their use. We also discuss various open-source BCI toolboxes and software, and describe their impact on the field at large. © 2024 The Authors.}, keywords = {Brain computer interface; Brain mapping; Computational neuroscience; Computer keyboards; Deep learning; Job analysis; Neurophysiology; Robot learning; Robotic arms; Transfer learning; Cortical potentials; Manifold classification; Motor imagery; Motor-related cortical potential; Neural decoding; Neurotechnology; Recording; Task analysis; Electroencephalography; algorithm; Article; brain; classification; deep learning; electric potential; electroencephalogram; electroencephalography; evoked response; human; imagery; spatial attention; structural model; transfer of learning; visual evoked potential; brain computer interface; physiology; procedures; signal processing; software; Algorithms; Brain; Brain-Computer Interfaces; Humans; Signal Processing, Computer-Assisted; Software},
correspondence_address = {B. He; The Carnegie Mellon University, Pittsburgh, 15217, United States; email: bhe1@andrew.cmu.edu},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {19411189; 19373333},
pmid = {39186407},
language = {English},
abbrev_source_title = {IEEE Rev. Biomed. Eng.},
type = {Article}}
@article{Elgammal2025Neurotutor,
author = {Elgammal, Shahd and Alsereidi, Shaymaa and Almansoori, Mahra K. and Alblooshi, Shamma and Hireche, Abdelhadi and Belkacem, Abdelkader Nasreddine},
title = {NeuroTutor: Neural Decoding of Student Engagement During Virtual and Robotic Tutoring},
year = {2025},
journal = {IEEE Access},
volume = {13},
pages = {123967 - 123978},
doi = {10.1109/ACCESS.2025.3585826},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010149939&doi=10.1109%2FACCESS.2025.3585826&partnerID=40&md5=1d24505a1533c5d2a2d714c3f4481c96},
affiliations = {United Arab Emirates University, Department of Computer and Network Engineering, Al Ain, Abu Dhabi, United Arab Emirates; United Arab Emirates University, Department of Computer Science and Software Engineering, Al Ain, Abu Dhabi, United Arab Emirates; United Arab Emirates University, Department of Information Systems and Security, Al Ain, Abu Dhabi, United Arab Emirates},
abstract = {The use of electroencephalography (EEG)-based analysis in educational research offers valuable insight into the dynamics of virtual and robot-based tutoring systems. This study aims to classify neural patterns associated with two distinct tutoring modalities (virtual tutor versus robot tutor) to understand how different educational interfaces affect brain activity and engagement biomarkers. This classification enables adaptive learning systems that can optimize educational environments based on real-time neural responses, potentially enhancing learning outcomes by matching tutoring modalities to individual neural preferences. We examine the performance of multiple EEG features such as power spectral density, Fast Fourier Transform (FFT) magnitude coefficients, amplitude and variance, coherence, Hjorth parameters, and wavelet transform coefficients in distinguishing between these two tutoring modalities. A transformer based binary classifier was employed to evaluate the effectiveness of these characteristics in classifying EEG data collected during interactions with both types of tutors. The wavelet transform coefficients demonstrated the highest classification accuracy of 98.75%, precision of 99.17%, recall of 98.18%, F1 score of 98.61%, and Area Under the Curve (AUC) of 1.00, indicating exceptional performance in all metrics. This high accuracy represents a significant advancement in EEG-based educational classification systems. The Hjorth parameters (90.13% accuracy, 89.91% precision, 95.45% recall) and preprocessed EEG (91.73% accuracy, 91.57% precision, 90.91% recall) also showed strong performance. Simpler features, such as amplitude and variance, exhibited limited discriminatory power with only 72.67% accuracy. These findings underscore the importance of feature engineering and robust preprocessing in EEG-based educational studies and demonstrate the feasibility of creating personalized learning environments that can adapt to individual students’ neural preferences. Limitations such as a small sample size and topic-specific focus are noted, paving the way for future research to generalize findings and optimize methodologies. © 2013 IEEE.}, keywords = {Brain; Brain computer interface; Classification (of information); Computer aided instruction; E-learning; Education computing; Educational robots; Electroencephalography; Engineering education; Learning systems; Neurophysiology; Real time systems; Spectral density; Students; Teaching; Wavelet transforms; Hjorth parameters; Learning outcome; Neurotutor; Performance; Robot tutor; Student engagement; Student engagement biomarker; Virtual tutors; Wavelet transform coefficients; Electrophysiology},
correspondence_address = {A.N. Belkacem; United Arab Emirates University, Department of Computer and Network Engineering, Al Ain, Abu Dhabi, United Arab Emirates; email: belkacem@uaeu.ac.ae},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21693536},
language = {English},
abbrev_source_title = {IEEE Access},
type = {Article}}
@article{Faghfouri2025Toward,
author = {Faghfouri, Alireza and Shalchyan, Vahid},
title = {Toward Practical Wrist BCIs: Multi-Class EEG Classification of Actual and Imagined Movements},
year = {2025},
journal = {IEEE Access},
volume = {13},
pages = {191795 - 191808},
doi = {10.1109/ACCESS.2025.3630223},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021107784&doi=10.1109%2FACCESS.2025.3630223&partnerID=40&md5=2f2832ed24b6626c4e55240bc727b506},
affiliations = {Iran University of Science and Technology, Department of Biomedical Engineering, Tehran, Tehran, Iran},
abstract = {Brain-Computer Interfaces (BCIs) offer a promising solution for enabling the control of external systems. However, the traditional inter-limb approach is insufficient to achieve the dexterity required for daily tasks. The intra-limb approach, while more relevant for fine motor control, presents significant challenges in decoding fine intra-joint wrist movements, which are critical for dexterity, from non-invasive electroencephalography (EEG) signals. This study investigates the feasibility of classifying four wrist movements (flexion, extension, radial deviation, and ulnar deviation) from short EEG segments recorded during motor execution (ME) and motor imagery (MI). Data were collected from eight healthy participants using a 16-channel EEG setup. A lightweight machine learning pipeline, combining Common Spatial Pattern (CSP) and Filter Bank Common Spatial Pattern (FBCSP) for feature extraction, along with Support Vector Machine (SVM) and K-nearest neighbor (KNN) for classification, was validated using 10 × 10 -fold cross-validation. The proposed pipeline achieved high performance in binary classification, with accuracies of 88.36% (ME) and 87.82% (MI) for flexion vs. extension. Comparable performance was observed for radial vs. ulnar deviation, with accuracies reaching 88.78% (ME) and 88.36% (MI). For the more complex four-class classification task, accuracies of 51.91% (ME) and 51.59% (MI) were obtained, significantly above chance level. Notably, MI performance was comparable to ME, likely due to the sequential ME–MI task design that enhanced neural priming. These findings demonstrate the feasibility of decoding fine intra-joint wrist kinematics from a lightweight EEG configuration, highlighting the potential for real-time, resource-efficient BCIs for neuroprosthetics, rehabilitation, and assistive technologies requiring fine motor control. © 2013 IEEE.}, keywords = {Artificial limbs; Assistive technology; Biomedical signal processing; Brain; Brain computer interface; Brain mapping; Classification (of information); Image classification; Interfaces (computer); Learning systems; Nearest neighbor search; Neural networks; Neural prostheses; Support vector machines; Common spatial patterns; Daily tasks; External systems; Fine motor control; Machine-learning; Motor execution; Motor imagery; Neural decoding; Performance; Wrist movements; Electroencephalography; Electrophysiology},
correspondence_address = {V. Shalchyan; Iran University of Science and Technology, Neuroscience and Neuroengineering Research Laboratory, Biomedical Engineering Department, School of Electrical Engineering, Tehran, 1684613114, Iran; email: shalchyan@iust.ac.ir},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21693536},
language = {English},
abbrev_source_title = {IEEE Access},
type = {Article}}
@inproceedings{Fan2025Ieee,
author = {Fan, Xiaoya and Xue, Haixiao and Feng, Yufan and Zhao, Qi and Zhao, Zheng and Wang, Zhong},
booktitle = {2025 IEEE International Conference on Multimedia and Expo (ICME)},
title = {Visual Feature Learning from Randomized EEG Trials for Object Recognition},
year = {2025},
volume = {},
number = {},
pages = {1--6},
abstract = {Object recognition from electroencephalography (EEG) responses to visual stimuli has received growing attention but has remained challenging, with prior methods achieving only marginally above-chance accuracy for randomized EEG trials. This paper introduces GVFL-EEG (Guided Visual Feature Learning from EEG), a novel framework that leverages well-trained computer vision models to enhance EEG object decoding. Specifically, a pre-trained image encoder is used to guide EEG feature learning via contrastive learning, aligning EEG embeddings with image representations. We present EEGMambaformer, a custom EEG encoder incorporating a residual Mamba block to capture temporal dynamics, an inverted transformer encoder to extract spatial dependencies, a temporal-spatial convolution block for feature fusion, and a projection layer for dimension alignment with the image encoder. Evaluation on the EEG40000 dataset show that our framework achieves 60.94% accuracy in 40-way classification, outperforming existing state-of-the-art methods. This work advances neural decoding and provides insights into brain-computer interface development. The code is available at https://github.com/xuehaixiao/GVFL_EEG/tree/main/GVFL.},
keywords = {Representation learning;Visualization;Accuracy;Contrastive learning;Transformers;Feature extraction;Electroencephalography;Brain-computer interfaces;Decoding;Object recognition;EEG visual recognition;randomized EEG trials;contrastive learning;state space models;inverted transformer},
doi = {10.1109/ICME59968.2025.11210210},
issn = {1945-788X},
month = {June}}
@article{Fischer2025Reconstructing,
author = {Fischer, David B. and Edlow, Brian L. and Freeman, Holly J. and Alaiev, Daniel and Wu, Qichao and Ware, Jeffrey B. and Detre, John A. and Aguirre, Geoffrey Karl},
title = {Reconstructing Covert Consciousness: Neural Decoding as a Novel Consciousness Assessment},
year = {2025},
journal = {Neurology},
volume = {104},
number = {4},
pages = {},
doi = {10.1212/WNL.0000000000210208},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216761395&doi=10.1212%2FWNL.0000000000210208&partnerID=40&md5=5fc28e3e8f9b51245a4d0af6e5d4e5b9},
affiliations = {University of Pennsylvania Perelman School of Medicine, Department of Neurology, Philadelphia, PA, United States; Harvard Medical School, Center for Neurotechnology and Neurorecovery, Boston, MA, United States; Harvard Medical School, Boston, MA, United States; University of Pennsylvania Perelman School of Medicine, Department of Radiology, Philadelphia, PA, United States},
abstract = {Determining the level of consciousness in patients with brain injury - and more fundamentally, establishing what they can experience - is ethically and clinically impactful. Patient behaviors may unreliably reflect their level of consciousness: a subset of unresponsive patients demonstrate covert consciousness by willfully modulating their brain activity to commands through fMRI or EEG. However, current paradigms for assessing covert consciousness remain fundamentally limited because they are insensitive, rely on imperfect assumptions of functional neuroanatomy, and do not reflect the spectrum of conscious experience. Neural decoding, in which stimuli and concepts are reconstructed from brain activity, offers a novel approach to covert consciousness assessment that overcomes many of these limitations. In this article, we discuss the current state of covert consciousness assessments, their shortcomings, the state of the science in neural decoding, the potential application of neural decoding to disorders of consciousness, and future directions that may help realize this potential. To do so, we searched PubMed and Google Scholar databases for pertinent articles published between January 1990 and September 2024, using the search terms "covert consciousness,""cognitive motor dissociation,""neural decoding,"and "semantic decoding."Redefining covert consciousness with neural decoding may improve sensitivity, enhance granularity, and more directly address the question of what patients can experience after brain injury. © American Academy of Neurology.},
keywords = {brain injury; cognition; consciousness; consciousness disorder; electroencephalogram; electroencephalography; functional magnetic resonance imaging; human; neuroanatomy; review; brain; diagnostic imaging; nuclear magnetic resonance imaging; pathophysiology; physiology; procedures; Brain; Brain Injuries; Consciousness; Consciousness Disorders; Electroencephalography; Humans; Magnetic Resonance Imaging},
correspondence_address = {D. Fischer; Department of Neurology, Perelman School of Medicine, University of Pennsylvania, Philadelphia, United States; email: david.fischer@pennmedicine.upenn.edu},
publisher = {Lippincott Williams and Wilkins},
issn = {00283878; 1526632X},
isbn = {9781437736113},
coden = {NEURA},
pmid = {39883908},
language = {English},
abbrev_source_title = {Neurology},
type = {Review}}
@article{Ghio2025Prediction,
author = {Ghio, Marta and Haegert, Karolin and Seidel, Alexander and Suchan, Boris and Thoma, Patrizia and Bellebaum, Christian},
title = {The prediction of auditory consequences of own and observed actions: a brain decoding multivariate pattern study},
year = {2025},
journal = {Cerebral Cortex},
volume = {35},
number = {4},
pages = {},
doi = {10.1093/cercor/bhaf091},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004256257&doi=10.1093%2Fcercor%2Fbhaf091&partnerID=40&md5=f43a57cf092cd9b0279ca2b5afc7b08b},
affiliations = {Heinrich-Heine-Universität Düsseldorf, Faculty of Mathematics and Natural Sciences, Dusseldorf, Nordrhein-Westfalen, Germany; Ruhr-Universitat Bochum, Neuropsychological Therapy Centre, Bochum, Nordrhein-Westfalen, Germany},
abstract = {Evidence from the auditory domain suggests that sounds generated by self-performed as well as observed actions are processed differently compared to external sounds. This study aimed to investigate which brain regions are involved in the processing of auditory stimuli generated by actions, addressing the question of whether cerebellar forward models, which are supposed to predict the sensory consequences of self-performed actions, similarly underlie predictions for action observation. We measured brain activity with functional magnetic resonance imaging (fMRI) while participants elicited a sound via button press, observed another person performing this action, or listened to external sounds. By applying multivariate pattern analysis (MVPA), we found evidence for altered processing in the right auditory cortex for sounds following both self-performed and observed actions relative to external sounds. Evidence for the prediction of auditory action consequences was found in the bilateral cerebellum and the right supplementary motor area, but only for self-performed actions. Our results suggest that cerebellar forward models contribute to predictions of sensory consequences only for action performance. While predictions are also generated for action observation, the underlying mechanisms remain to be elucidated. © 2025 The Author(s).}, keywords = {adult; article; brain; brain region; cerebellum; decoding; diagnosis; electroencephalogram; etiology; functional magnetic resonance imaging; human; human experiment; male; normal human; prediction; right auditory cortex; supplementary motor area; auditory cortex; auditory stimulation; brain mapping; diagnostic imaging; female; hearing; multivariate analysis; nuclear magnetic resonance imaging; physiology; psychomotor performance; young adult; Acoustic Stimulation; Adult; Auditory Cortex; Auditory Perception; Brain; Brain Mapping; Cerebellum; Female; Humans; Magnetic Resonance Imaging; Male; Multivariate Analysis; Psychomotor Performance; Young Adult},
correspondence_address = {M. Ghio; Faculty of Mathematics and Natural Sciences, Heinrich Heine University Düsseldorf, Düsseldorf, Universitätsstrasse 1, 40225, Germany; email: Marta.Ghio@hhu.de},
publisher = {Oxford University Press},
issn = {14602199; 10473211},
coden = {CECOE},
pmid = {40298443},
language = {English},
abbrev_source_title = {Cereb. Cortex},
type = {Article}}
@article{Haro2025Braincomputer,
author = {Haro, Stephanie and Beauchene, Christine and Quatieri, Thomas F. and Smalt, Christopher J.},
title = {A Brain-Computer Interface for Improving Auditory Attention in Multi-Talker Environments},
year = {2025},
journal = {IEEE Access},
volume = {13},
pages = {189903 - 189914},
doi = {10.1109/ACCESS.2025.3623842},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019673058&doi=10.1109%2FACCESS.2025.3623842&partnerID=40&md5=00a9cb26968d9f8ac43877ac821a9d75},
affiliations = {Lincoln Laboratory, Human Health and Performance Systems, Lexington, MA, United States; Harvard Medical School, Program in Speech and Hearing Bioscience and Technology, Boston, MA, United States; School of Engineering, Providence, RI, United States},
abstract = {There is significant research in accurately determining the focus of a listener’s attention in a multi-talker environment using auditory attention decoding (AAD) algorithms. These algorithms rely on neural signals to identify the intended speaker, assuming that these signals consistently reflect the listener’s focus. However, some listeners struggle with this competing talkers task, leading to suboptimal tracking of the desired speaker due to potential interference from distractors. The goal of this study was to enhance a listener’s attention to the target speaker in real time and investigate the underlying neural bases of this improvement. This paper describes a closed-loop neurofeedback system that decodes the auditory attention of the listener in real time, utilizing data from a non-invasive, wet electroencephalography (EEG) brain-computer interface (BCI). Fluctuations in the listener’s real-time attention decoding accuracy were used to provide acoustic feedback. As accuracy improved, the ignored talker in the two-talker listening scenario was attenuated; making the desired talker easier to attend to due to the improved attended talker signal-to-noise ratio (SNR). A one-hour session was divided into a 10-minute decoder training phase, with the rest of the session allocated to observing changes in neural decoding. In this study, we found evidence of suppression of (i.e., reduction in) net neural tracking and decoding of the unattended talker when comparing the first and second half of the neurofeedback session ( p=0.02 , Cohen’s d = -1.29 , 95% CI [-0.02, -0.01] and p=0.01 , Cohen’s d = -1.56 , 95% CI [-7.25,-3.44] , respectively). We did not find a statistically significant increase in the neural tracking or decoding of the attended talker. These results establish a single session performance benchmark for a time-invariant, non-adaptive attended talker linear decoder utilized to extract attention from a listener integrated within a closed-loop neurofeedback system. This research lays the engineering and scientific foundation for prospective multi-session clinical trials of an auditory attention training paradigm. © 2013 IEEE.}, keywords = {Acoustic noise; Audition; Benchmarking; Biomedical signal processing; Decoding; Electrophysiology; Interfaces (computer); Speech communication; Speech recognition; Auditory attention; Auditory attention decoding; Closed-loop; Decoding algorithm; Neural signals; Neurofeedback; Real- time; Speech perception; Sub-optimal tracking; Brain computer interface; Electroencephalography},
correspondence_address = {C.J. Smalt; MIT Lincoln Laboratory, Human Health and Performance Systems Group, Lexington, 02421, United States; email: Christopher.Smalt@ll.mit.edu},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21693536},
language = {English},
abbrev_source_title = {IEEE Access},
type = {Article}}
@article{He2025Cire,
author = {He, Shengrui and Li, Zhongjie and Dang, Jianwu and Luo, Yingyi and Zhang, Gaoyan},
title = {CIRE: A Chinese EEG Dataset for decoding speech intention modulated by prosodic emotion},
year = {2025},
journal = {Scientific Data},
volume = {12},
number = {1},
pages = {},
doi = {10.1038/s41597-025-05957-y},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019496313&doi=10.1038%2Fs41597-025-05957-y&partnerID=40&md5=4c3f075927bfd3032743ed2db42ac54c},
affiliations = {Tianjin University, College of Intelligence and Computing, Tianjin, China; Northwestern Polytechnical University, Xi'an, Shaanxi, China; Shenzhen Institutes of Advanced Technology, Shenzhen, Guangdong, China; Chinese Academy of Social Sciences, Institute of Linguistics, Beijing, China},
abstract = {Neural decoding of speech intention could advance the development and application of brain-computer interface (BCI) technology. Currently, lack of dataset limited the research on decoding the true speech intention, especially the diverse intentions expressed by the same text when no context is given. This study provides an EEG dataset, CIRE, on spoken language interaction intention featuring aligned textual expressions with divergent intentional meanings due to the differences in prosodic emotion. The dataset comprises preprocessed high-density (128-channel) EEG recordings from 38 participants engaged in comprehension of attitude-conveying speech stimuli, accompanied by Wav2vec2-derived acoustic embeddings of the listening materials. To validate our dataset through cognitive neuroscience studies and binary intent classification, we applied signal processing pipelines, cognitive analysis frameworks, and machine learning (ML) approaches. Our baseline model achieved a cross-subject classification accuracy of 68.2%, with differences exhibiting interpretable neurophysiological correlates. The high-density and high temporal resolution EEG data offer broader application areas, both in cognitive neuroscience and speech BCI, and can also contribute to the brain-inspired algorithms. © The Author(s) 2025.},
keywords = {adult; behavior; brain computer interface; China; East Asian; electroencephalography; emotion; female; human; machine learning; male; speech; speech perception; Adult; Brain-Computer Interfaces; Chinese people; East Asian People; Electroencephalography; Emotions; Female; Humans; Intention; Machine Learning; Male; Speech; Speech Perception},
correspondence_address = {G. Zhang; Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin, Tianjin, 300350, China; email: zhanggaoyan@tju.edu.cn},
publisher = {Nature Research},
issn = {20524463},
pmid = {41120372},
language = {English},
abbrev_source_title = {Sci. Data},
type = {Article}}
@article{Huang2025Ccsumsp,
author = {Huang, Shuai and Wang, Yongxionga Xiong and Luo, Huan},
title = {CCSUMSP: A cross-subject Chinese speech decoding framework with unified topology and multi-modal semantic pre-training},
year = {2025},
journal = {Information Fusion},
volume = {119},
pages = {},
doi = {10.1016/j.inffus.2025.103022},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217939249&doi=10.1016%2Fj.inffus.2025.103022&partnerID=40&md5=ded6d71771b23bd101b92986d21d72d2},
affiliations = {University of Shanghai for Science and Technology, Shanghai, Shanghai, China},
abstract = {Decoding speech from brain signals has been a long-standing challenge in neuroscience and brain–computer interface research. While significant progress has been made in English speech decoding, cross-subject Chinese speech decoding remains understudied, despite its potential applications and unique linguistic characteristics. Chinese, with its logographic writing system and tonal nature, presents unique challenges for neural decoding, including complex visual processing of characters and the need to distinguish subtle tonal differences that can alter word meanings. In this paper, we propose Cross-Subject Chinese Speech Decoding Framework with Unified Topology and Multi-Modal Semantic Pre-training(CCSUMSP), a novel framework for cross-subject Chinese speech decoding from electroencephalogram (EEG) signals. There are three key innovations in our approach: (1) We develop a unified topological representation(UTR) that can accommodate various EEG montages, enabling better generalization across subjects and recording setups; (2) We design a multi-modal semantic pre-training strategy by using both EEG and eye-tracking data to capture richer linguistic information; (3) We introduce a dynamic multi-view decoder(DMD) where the weights of different brain regions can be adaptively adjusted based on input signals. In contrast to the state-of-the-art methods, this article presents significant improvements in cross-subject decoding accuracy and generalization by evaluating our framework on the ChineseEEG dataset. Moreover, through our work, we advance the field of EEG-based speech decoding and provide insights into the neural mechanisms underlying Chinese language processing. Finally, the framework we proposed is potentially employed in assistive communication technologies and neural rehabilitation for Chinese speakers. © 2025 Elsevier B.V.}, keywords = {Assistive technology; Semantics; Speech enhancement; Brain signals; Chinese speech; Cross-subject; Generalisation; Modal semantics; Multi-modal; Neural decoding; Pre-training; Speech decoding; Writing systems; Electroencephalography},
correspondence_address = {Y. Wang; University of Shanghai for Science and Technology, shanghai, 200093, China; email: wyxiong@usst.edu.cn},
publisher = {Elsevier B.V.},
issn = {15662535; 18726305},
language = {English},
abbrev_source_title = {Inf. Fusion},
type = {Article}}
@conference{Khanday2025Neuroincept,
author = {Khanday, Owais Mujtaba and Pérez-Córdoba, José Luis and Mir, Yaqub and Najar, Ashfaq Ahmad and Gonzalez-Lopez, Jose A.},
title = {NeuroIncept Decoder for High-Fidelity Speech Reconstruction from Neural Activity},
year = {2025},
journal = {Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
pages = {},
doi = {10.1109/ICASSP49660.2025.10888547},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003875083&doi=10.1109%2FICASSP49660.2025.10888547&partnerID=40&md5=80599e6553f52708e9b01161457846f2},
affiliations = {Universidad de Granada, Granada, Granada, Spain; Institutionen för Kliniska Vetenskaper, Lund, Epilepsy Centre, Lund, Skane, Sweden; VIT Bhopal University, School of Computing Science and Engineering, Sehore, MP, India},
abstract = {This paper introduces a novel algorithm designed for speech synthesis from neural activity recordings obtained using invasive electroencephalography (EEG) techniques. The proposed system offers a promising communication solution for individuals with severe speech impairments. Central to our approach is the integration of time-frequency features in the high-gamma band computed from EEG recordings with an advanced NeuroIncept Decoder architecture. This neural network architecture combines Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRUs) to reconstruct audio spectrograms from neural patterns. Our model demonstrates robust mean correlation coefficients between predicted and actual spectrograms, though inter-subject variability indicates distinct neural processing mechanisms among participants. Overall, our study highlights the potential of neural decoding techniques to restore communicative abilities in individuals with speech disorders and paves the way for future advancements in brain-computer interface technologies. © 2025 IEEE.},
keywords = {Brain-computer interfaces; deep neural networks; EEG; speech synthesis},
editor = {Rao, B.D. and Trancoso, I. and Sharma, G. and Mehta, N.B.},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {07367791; 15206149},
isbn = {1424407281; 9781467300469; 0780362934; 9781457705397; 9781728163277; 9781424423545; 9798350368741; 0780305329; 9780780309463; 9781509066315},
coden = {IPROD},
language = {English},
abbrev_source_title = {ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
type = {Conference paper}}
@article{Kim2025Resolving,
author = {Kim, Albert E. and Langlois, Valerie J. and Ness, Tal and Wade, Madeleine and Novick, Jared M.},
title = {Resolving conflicting interpretations: Theta band oscillations and the role of cognitive control},
year = {2025},
journal = {Neuropsychologia},
volume = {217},
pages = {},
doi = {10.1016/j.neuropsychologia.2025.109214},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008990380&doi=10.1016%2Fj.neuropsychologia.2025.109214&partnerID=40&md5=e6bad4f2cee3e017ca4a4f12dcfb03b2},
affiliations = {University of Colorado Boulder, Boulder, CO, United States; University of Colorado Boulder, Department of Psychology and Neuroscience, Boulder, CO, United States; University of Maryland, College Park, Department of Hearing and Speech Sciences, College Park, MD, United States; University of Maryland, College Park, Program in Neuroscience and Cognitive Science, College Park, MD, United States; Maryland Language Science Center, United States},
abstract = {While processing language, readers and listeners frequently encounter conflicting cues and must select the most plausible interpretation from incompatible alternatives. We tested the hypothesis that cognitive control aids in resolving representational conflicts by biasing processing toward the correct interpretation when multiple analyses of linguistic input are possible. Participants read temporarily ambiguous sentences alongside semantically and syntactically anomalous sentences. Ambiguous sentences, such as “While Anna dressed the baby spit up on the bed,” require resolving conflicts between competing interpretations, whereas semantic and syntactic anomalies, though they increase processing demands, do not involve such conflicts. Building on evidence from non-linguistic tasks, we used EEG to assess whether neural oscillations in the theta band (4–8 Hz) serve as a real-time index of cognitive control in resolving conflicting interpretations of linguistic input. Our findings revealed increased theta-band activity over right frontal electrodes during the processing of ambiguous sentences, indicating cognitive control engagement. Additionally, a neural decoding analysis showed that theta-band activity reliably distinguished between correctly and incorrectly understood ambiguous sentences, suggesting that theta activity not only reflects cognitive control engagement but also guides comprehenders toward the correct interpretation. In contrast, ERP analyses showed the expected P600 effects for syntactic anomalies and N400 effects for semantic anomalies, confirming the processing complexity associated with these sentences; however, theta power did not increase for these items. The results support the hypothesis that theta-band oscillations specifically reflect cognitive control processes involved in resolving representational conflicts in language comprehension, helping to prevent interpretation errors and providing insights into the temporal dynamics of cognitive control during sentence processing. © 2025 Elsevier Ltd}, keywords = {adult; Article; cognitive appraisal; electroencephalogram; event related potential; executive function; female; human; language ability; language processing; male; semantics; theta rhythm; brain; cognition; comprehension; electroencephalography; evoked response; inner conflict; physiology; reading; young adult; Adult; Brain; Cognition; Comprehension; Conflict, Psychological; Electroencephalography; Evoked Potentials; Executive Function; Female; Humans; Male; Reading; Semantics; Theta Rhythm; Young Adult},
correspondence_address = {A.E. Kim; Institute of Cognitive Science, University of Colorado, Boulder, United States; email: albert.kim@colorado.edu; J.M. Novick; Department of Hearing and Speech Sciences, University of Maryland, College Park, United States; email: jnovick1@umd.edu},
publisher = {Elsevier Ltd},
issn = {00283932; 18733514},
coden = {NUPSA},
pmid = {40544913},
language = {English},
abbrev_source_title = {Neuropsychologia},
type = {Article}}
@article{Kovacs2025Revealing,
author = {Kovacs, Julio A. and Krusienski, Dean J. and Maninder, Minu and Wriggers, Willy},
title = {Revealing spatiotemporal neural activation patterns in electrocorticography recordings of human speech production by mutual information},
year = {2025},
journal = {Neuroscience Informatics},
volume = {5},
number = {4},
pages = {},
doi = {10.1016/j.neuri.2025.100232},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014756223&doi=10.1016%2Fj.neuri.2025.100232&partnerID=40&md5=88f3d4c3f12326115b75cec1016e3a51},
affiliations = {Frank Batten College of Engineering and Technology, Norfolk, VA, United States; VCU College of Engineering, Richmond, VA, United States; Old Dominion University, Department of Chemistry and Biochemistry, Norfolk, VA, United States},
abstract = {Background: Spatiotemporal mapping of neural activity during continuous speech production has been traditionally approached using correlation coefficient (CC) analysis between cortical signals and speech recordings. A prior study employed this approach using electrocorticography (ECoG) data from participants who underwent invasive intracranial monitoring for epilepsy. However, CC cannot detect nonlinear relationships and is dominated by the correspondence between periods of silence and of non-silence. New Method: We introduce the mutual information (MI) measure, which can capture both linear and nonlinear dependencies. We validated CC and MI on the sub-second spatiotemporal brain activity recorded during continuous speech tasks. To refine the results, we also implemented a novel “masked analysis”, which excludes periods of silence, and compared it with the standard (unmasked) analysis. Results: Our findings show that previous results, obtained through more complex statistical methods, can be reproduced using CC with an appropriate threshold cutoff. Moreover, both standard MI and CC are influenced by broad transitions between silence and speech, but masking allows the detection of intrinsic correspondences between the two signals, revealing more localized activity. Comparison with existing methods: Compared to the standard CC, masked MI highlights early prefrontal and premotor activations emerging ∼440 ms before speech onset. It also identifies sharper, anatomically coherent activations in key speech-related areas, demonstrating improved sensitivity to the fine-grained spatiotemporal dynamics of continuous speech production. Conclusion: These findings deepen our understanding of the neural pathways underlying speech and underscore the potential of masked MI for advancing neural decoding in future speech-based brain-computer interface applications. © 2025 The Author(s)}, keywords = {accuracy; Article; cognition; computer assisted tomography; electrocorticography; electroencephalogram; epilepsy; epileptic patient; female; human; human experiment; left hemisphere; male; normal human; primary motor cortex; right hemisphere; signal noise ratio; spatiotemporal neural activation; speech; superior temporal gyrus; supramarginal gyrus},
correspondence_address = {W. Wriggers; Department of Mechanical and Aerospace Engineering, Old Dominion University, Norfolk, United States; email: wriggers@biomachina.org},
publisher = {Elsevier Masson s.r.l.},
issn = {27725286},
language = {English},
abbrev_source_title = {Neurosci. Inform.},
type = {Article}}
@article{Kumar2025Neuroengineering,
author = {Kumar, Prabhat and Chakraborty, Somdatta and Sahai, Nitin},
title = {Neuroengineering and brain-machine interfaces},
year = {2025},
pages = {325 - 357},
doi = {10.1016/B978-0-443-30146-9.00010-1},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006554456&doi=10.1016%2FB978-0-443-30146-9.00010-1&partnerID=40&md5=94a4956f05f5f293b348f540d778ea59},
affiliations = {University of Pécs Medical School, Institute of Physiology, Pecs, Baranya, Hungary; Pécsi Tudományegyetem, Centre for Neuroscience, Pecs, Baranya, Hungary; Pécsi Tudományegyetem, School of Medicine, Pecs, Baranya, Hungary; Università degli Studi della Campania Luigi Vanvitelli, Department of Psychology, Naples, NA, Italy; North-Eastern Hill University, Department of Biomedical Engineering, Shillong, ML, India},
abstract = {Researchers study neuroengineering and brain-machine interfaces (BMIs) in the field of bioengineering, an emerging discipline that combines engineering, neuroscience, and medicine. BMIs aim to create a direct interface connecting the human brain to external technologies for communication and control. Here, we discuss how neuroengineering and BMI could transform the area with significant biomedical engineering advancements. This chapter addresses the main principles of brain signal acquisition and its processing and discusses the different methodologies, emphasizing electroencephalography (EEG), magnetoencephalography (MEG), and functional near-infrared spectroscopy (fNIRS). This chapter delves into the design and implementation of control mechanisms, signal-transmission protocols, and brain decoding algorithms for BMI. In addition to BMIs, this progress in neuroengineering has opened the door to new forms of neurorehabilitation and may offer hope for those subjected to neurological pathologies that cause severe motor, sensory, or communicative deficits. In the early years, researchers discovered a new potential in developing BMI-based systems that could restore lost functions. These systems, when implemented, could enable paralyzed patients to control robotic limbs or even communicate through brain-controlled devices, among other possibilities. The application of neuroscience in computer engineering holds significant potential. Neuroengineering and BMIs provide a novel way to control digital devices seamlessly and instantly (beyond rehabilitation). The last part of the chapter talks about the moral effects that federal research and development (RD) in neuroengineering and brain-machine interfaces (BMIs) has on society. It stresses how important it is to carefully think about safety, privacy, and informed consent. Advances in neuroengineering and brain-machine interfaces (BMIs) could have a huge impact on the lives of people with neurological impairments and on how we interact with the outside world. © 2025 Elsevier Inc. All rights reserved.}, keywords = {Assistive technology; Biomedical materials; Breath controlled devices; Cell engineering; Electroencephalography; Eye controlled devices; Functional neural stimulation; Haptic interfaces; Neurons; Signal reconstruction; Brain signals; Brain-machine interface; Communication and control; Control mechanism; Design and implementations; Functional near infrared spectroscopy; Human brain; Machine interfaces; Neuroengineering; Signal acquisitions; Magnetoencephalography},
publisher = {Elsevier},
isbn = {9780443301476; 9780443301469},
language = {English},
abbrev_source_title = {Innovations in Biomedical Engineering: Trends in Scientific Advances and Applications},
type = {Book chapter}}
@article{Lee2025Neural,
author = {Lee, Eunji and Kim, Ji-hyun and Park, Jaeseok and Kim, Sung-Phil and Shin, Taehoon},
title = {Neural decoding of Aristotle tactile illusion using deep learning-based fMRI classification},
year = {2025},
journal = {Frontiers in Neuroscience},
volume = {19},
pages = {},
doi = {10.3389/fnins.2025.1606801},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009858157&doi=10.3389%2Ffnins.2025.1606801&partnerID=40&md5=5c35a1c9bcd63897f4f088a35c411c52},
affiliations = {Ewha Womans University, Division of Mechanical and Biomedical Engineering, Seoul, Seoul, South Korea; Ewha Womans University, Graduate Program in Smart Factory, Seoul, Seoul, South Korea; Brown University, Department of Cognitive and Psychological Sciences, Providence, RI, United States; Sungkyunkwan University, Department of Biomedical Engineering, Seoul, South Korea; Sungkyunkwan University, Department of Intelligent Precision Healthcare Convergence, Seoul, South Korea; Ulsan National Institute of Science and Technology, Department of Biomedical Engineering, Ulsan, South Korea; Ewha Womans University, Division of Artificial Intelligence and Software, Seoul, Seoul, South Korea},
abstract = {Introduction: Aristotle illusion is a well-known tactile illusion which causes the perception of one object as two. EEG analysis was employed to investigate the neural correlates of Aristotle illusion, yet was limited due to low spatial resolution of EEG. This study aimed to identify brain regions involved in the Aristotle illusion using functional magnetic resonance imaging (fMRI) and deep learning-based analysis of fMRI data. Methods: While three types of tactile stimuli (Aristotle, Reverse, Asynchronous) were applied to thirty participants’ fingers, we collected fMRI data, and recorded the number of stimuli each participant perceived. Four convolutional neural network (CNN) models were trained for perception-based classification tasks (the occurrence of Aristotle illusion vs. Reverse illusion, the occurrence vs. absence of Reverse illusion), and stimulus-based classification tasks (Aristotle vs. Reverse, Reverse vs. Asynchronous, and Aristotle vs. Asynchronous). Results: Simple fully convolution network (SFCN) achieved the highest classification accuracy of 68.4% for the occurrence of Aristotle illusion vs. Reverse illusion, and 80.1% for the occurrence vs. absence of Reverse illusion. For stimulus-based classification tasks, all CNN models yielded accuracies around 50% failing to distinguish among the three types of applied stimuli. Gradient-weighted class activation mapping (Grad-CAM) analysis revealed salient brain regions-of-interest (ROIs) for the perception-based classification tasks, including the somatosensory cortex and parietal regions. Discussion: Our findings demonstrate that perception-driven neural responses are classifiable using fMRI-based CNN models. Saliency analysis of the trained CNNs reveals the involvement of the somatosensory cortex and parietal regions in making classification decisions, consistent with previous research. Other salient ROIs include orbitofrontal cortex, middle temporal pole, supplementary motor area, and middle cingulate cortex. © © 2025 Lee, Kim, Park, Kim and Shin.}, keywords = {accuracy; adult; algorithm; analysis of variance; Aristotle tactile illusion; Article; artificial neural network; brain mapping; brain region; cingulate gyrus; classification; controlled study; convolutional neural network; decoding; deep learning; dorsolateral prefrontal cortex; echo planar imaging; electroencephalogram; female; functional connectivity; functional magnetic resonance imaging; hemodynamics; human; human experiment; illusion; learning; machine learning; male; nerve cell; nerve cell network; neuroimaging; normal human; orbital cortex; somatosensory cortex; supplementary motor area; support vector machine; T2 weighted imaging; task performance; visual stimulation},
correspondence_address = {T. Shin; Department of Mechanical and Biomedical Engineering, Ewha W. University, Seoul, South Korea; email: taehoons@ewha.ac.kr; S.-P. Kim; Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea; email: spkim@unist.ac.kr},
publisher = {Frontiers Media SA},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@conference{Lee2025Enhancing,
author = {Lee, Jihwan and Feng, Tiantian and Kommineni, Aditya and Kadiri, Sudarsana Reddy and Narayanan, Shrikanth Shri S.},
title = {Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction},
year = {2025},
journal = {Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
pages = {},
doi = {10.1109/ICASSP49660.2025.10887915},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003872459&doi=10.1109%2FICASSP49660.2025.10887915&partnerID=40&md5=e9f58221f38d0d2d05998b3be7eb80a2},
affiliations = {USC Viterbi School of Engineering, Los Angeles, CA, United States},
abstract = {Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available. © 2025 IEEE.},
keywords = {brain-computer interfaces; EEG; neural decoding; speech decoding; text decoding},
editor = {Rao, B.D. and Trancoso, I. and Sharma, G. and Mehta, N.B.},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {07367791; 15206149},
isbn = {1424407281; 9781467300469; 0780362934; 9781457705397; 9781728163277; 9781424423545; 9798350368741; 0780305329; 9780780309463; 9781509066315},
coden = {IPROD},
language = {English},
abbrev_source_title = {ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
type = {Conference paper}}
@inproceedings{Lee2025International,
author = {Lee, Jung-Sun and Jo, Ha-Na and Lee, Seo-Hyun},
booktitle = {2025 13th International Conference on Brain-Computer Interface (BCI)},
title = {Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals},
year = {2025},
volume = {},
number = {},
pages = {1--4},
abstract = {Brain signals accompany various information relevant to human actions and mental imagery, making them crucial to interpreting and understanding human intentions. Brain-computer interface technology leverages this brain activity to generate external commands for controlling the environment, offering critical advantages to individuals with paralysis or locked-in syndrome. Within the brain-computer interface domain, brain-to-speech research has gained attention, focusing on the direct synthesis of audible speech from brain signals. Most current studies decode speech from brain activity using invasive techniques and emphasize spoken speech data. However, humans express various speech states, and distinguishing these states through non-invasive approaches remains a significant yet challenging task. This research investigated the effectiveness of deep learning models for non-invasive-based neural signal decoding, with an emphasis on distinguishing between different speech paradigms, including perceived, overt, whispered, and imagined speech, across multiple frequency bands. The model utilizing the spatial conventional neural network module demonstrated superior performance compared to other models, especially in the gamma band. Additionally, imagined speech in the theta frequency band, where deep learning also showed strong effects, exhibited statistically significant differences compared to the other speech paradigms.},
keywords = {Deep learning;Measurement;Statistical analysis;Focusing;Signal processing;Brain modeling;Brain-computer interfaces;Electroencephalography;Decoding;Speech processing;brain-computer interface;electroencephalography;imagined speech;spoken speech;signal processing},
doi = {10.1109/BCI65088.2025.10931366},
issn = {2572-7672},
month = {Feb}}
@article{Lehnen2025Vocal,
author = {Lehnen, Johannes M. and Schweinberger, Stefan Robert and Nussbaum, Christine},
title = {Vocal Emotion Perception and Musicality—Insights from EEG Decoding},
year = {2025},
journal = {Sensors},
volume = {25},
number = {6},
pages = {},
doi = {10.3390/s25061669},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000991366&doi=10.3390%2Fs25061669&partnerID=40&md5=d34eab6ecae63826711a065220a2ec6b},
affiliations = {Friedrich-Schiller-Universität Jena, Department of Clinical Psychology in Childhood and Adolescence, Jena, Thuringen, Germany; Friedrich-Schiller-Universität Jena, Department for General Psychology and Cognitive Neuroscience, Jena, Thuringen, Germany; Friedrich-Schiller-Universität Jena, Voice Research Unit, Jena, Thuringen, Germany; Université de Genève, Geneva, Switzerland},
abstract = {Musicians have an advantage in recognizing vocal emotions compared to non-musicians, a performance advantage often attributed to enhanced early auditory sensitivity to pitch. Yet a previous ERP study only detected group differences from 500 ms onward, suggesting that conventional ERP analyses might not be sensitive enough to detect early neural effects. To address this, we re-analyzed EEG data from 38 musicians and 39 non-musicians engaged in a vocal emotion perception task. Stimuli were generated using parameter-specific voice morphing to preserve emotional cues in either the pitch contour (F0) or timbre. By employing a neural decoding framework with a Linear Discriminant Analysis classifier, we tracked the evolution of emotion representations over time in the EEG signal. Converging with the previous ERP study, our findings reveal that musicians—but not non-musicians—exhibited significant emotion decoding between 500 and 900 ms after stimulus onset, a pattern observed for F0-Morphs only. These results suggest that musicians’ superior vocal emotion recognition arises from more effective integration of pitch information during later processing stages rather than from enhanced early sensory encoding. Our study also demonstrates the potential of neural decoding approaches using EEG brain activity as a biological sensor for unraveling the temporal dynamics of voice perception. © 2025 by the authors.}, keywords = {Gluing; Sensory perception; Speech analysis; Speech coding; Speech communication; Speech enhancement; Auditory sensitivity; Fundamental frequencies; Fundamental frequency (f0); Group differences; Musicality; Neural decoding; Performance; Timbre; Vocal emotion perception; Voice morphing; Signal encoding; adult; auditory stimulation; brain; electroencephalography; emotion; female; hearing; human; male; music; physiology; pitch perception; procedures; voice; young adult; Acoustic Stimulation; Adult; Auditory Perception; Brain; Electroencephalography; Emotions; Female; Humans; Male; Music; Pitch Perception; Voice; Young Adult},
correspondence_address = {J.M. Lehnen; Department of Clinical Psychology in Childhood and Adolescence, Friedrich Schiller University Jena, Jena, 07743, Germany; email: johannes.lehnen@uni-jena.de; S.R. Schweinberger; Department for General Psychology and Cognitive Neuroscience, Friedrich Schiller University Jena, Jena, 07743, Germany; email: stefan.schweinberger@uni-jena.de},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {14248220},
pmid = {40292745},
language = {English},
abbrev_source_title = {Sensors},
type = {Article}}
@article{Leong2025Neural,
author = {Leong, Chantat and Gao, Fei and Yuan, Zhen},
title = {Neural decoding reveals dynamic patterns of visual chunk memory processes},
year = {2025},
journal = {Brain Research Bulletin},
volume = {221},
pages = {},
doi = {10.1016/j.brainresbull.2025.111208},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214651809&doi=10.1016%2Fj.brainresbull.2025.111208&partnerID=40&md5=509e4af77464e5134462e5cc4c3a9600},
affiliations = {University of Macau, Centre for Cognitive and Brain Sciences, Taipa, Macao; Faculty of Health Sciences, Taipa, Macao; Fudan University, Institute of Modern Languages and Linguistics, Shanghai, China},
abstract = {Chunk memory constitutes the basic unit that manages long-term memory and converts it into immediate decision-making processes, it remains unclear how to interpret and organize incoming information to form effective chunk memory. This paper investigates electroencephalography (EEG) patterns from the perspective of time-domain feature extraction using chunk memory in visual statistical learning and combines time-resolved multivariate pattern analysis (MVPA). The GFP and MVPA results revealed that chunk memory processes occurred during specific time windows in the learning phase. These processes included attention modulation (P1), recognition and feature extraction (P2), and segmentation for long-term memory conversion (P6). In the decision-making stage, chunk memory processes were encoded by four ERP components. Scene processing correlated with P1, followed by feature extraction facilitated by P2, encoding process (P4), and segmentation process (P6). This paper identifies the early process of chunk memory through implicit learning and applies univariate and multivariate approaches to establish the neural activity patterns of the early chunk memory process, which provides ideas for subsequent related studies. © 2025}, keywords = {article; attention; controlled study; decision making; electroencephalogram; electroencephalography; feature extraction; human; learning; long term memory; machine learning; male; memory; nerve cell; adult; brain; evoked response; female; photostimulation; physiology; procedures; vision; visual pattern recognition; young adult; Adult; Attention; Brain; Decision Making; Electroencephalography; Evoked Potentials; Female; Humans; Learning; Male; Memory; Memory, Long-Term; Pattern Recognition, Visual; Photic Stimulation; Visual Perception; Young Adult},
correspondence_address = {Z. Yuan; Centre for Cognitive and Brain Sciences, University of Macau, Macao; email: zhenyuan@um.edu.mo},
publisher = {Elsevier Inc.},
issn = {18732747; 03619230},
coden = {BRBUD},
pmid = {39814325},
language = {English},
abbrev_source_title = {Brain Res. Bull.},
type = {Article}}
@article{Li2025Everybrain,
author = {Li, Boang and Cao, Hui and Chen, Badong and Wang, Tao and Zhang, Jie},
title = {EveryBrain: Generate EEG Responses From Images For Specified Individuals},
year = {2025},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
pages = {},
doi = {10.1109/TCSVT.2025.3607971},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015816076&doi=10.1109%2FTCSVT.2025.3607971&partnerID=40&md5=21805628a1c52673cf4e845b2b549472},
affiliations = {Xi'an Jiaotong University, School of Electrical Engineering, Xi'an, Shaanxi, China; Air Force Medical University, Human-computer Intelligence Application Laboratory, Xi'an, Shaanxi, China; Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, Xi'an, Shaanxi, China},
abstract = {This paper presents EveryBrain, a method to generate electroencephalographic (EEG) signals of visual stimuli using images. Given that individuals exhibit distinct EEG responses to the same visual stimulus, EveryBrain is capable of capturing these individual characteristics during signal generation. The framework operates in two stages. By leveraging the temporal properties of EEG signals and the spatial features of images, EveryBrain presents a self-supervised framework that simultaneously reconstruct EEG signals and perform contrastive learning between image and EEG features. Furthermore, through additional training focused on individual EEG differences, Stage2 injects an ID number (representing a specific person) into image features via a cross-modal projector. The resulting personalized EEG latent codes, supervised by the Stage1 encoder, are then decoded into vivid, individualized EEG responses. Experiments validate the accuracy of EveryBrain in generating EEG signals for various individuals in response to visual stimuli. Overall, the proposed method tackles challenges in EEG generation from images, such as cross-modal alignment, individual variability, and waveform stability, yielding promising results. Additionally, the novel approach of of joint learning between images and EEG demonstrates positive effects on decoding visual neural representations. Both quantitative and qualitative evaluations demonstrate the effectiveness of methods, marking a significant step toward portable and cost-effective "image-to-thought". © 2025 IEEE. All rights reserved.}, keywords = {Biomedical signal processing; Contrastive Learning; Electroencephalography; Electrophysiology; Self-supervised learning; Supervised learning; Cross-modal; Cross-modal learning; Electroencephalographic signals; Embeddings; Imagetoeeg generation; Individual characteristics; Neural decoding; Personalized embedding; Visual neural decoding; Visual stimulus; Cost effectiveness},
correspondence_address = {H. Cao; School of Electrical Engineering, Xian Jiaotong University, Xian, 710049, China; email: huicao@mail.xjtu.edu.cn; J. Zhang; Human-computer Intelligence Application Laboratory, Preventive Medicine Institute, Fourth Military Medical University, Xian, 710000, China; email: zhangjie78@fmmu.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {10518215},
coden = {ITCTE},
language = {English},
abbrev_source_title = {IEEE Trans Circuits Syst Video Technol},
type = {Article}}
@article{Li2025Neural,
author = {Li, Na and He, Nan and Li, Mengqi and Xu, Leiqing},
title = {From neural decoding to design intervention: A systematic review of human emotional responses to architectural spaces through neuroimaging techniques},
year = {2025},
journal = {Wellbeing, Space and Society},
volume = {9},
pages = {},
doi = {10.1016/j.wss.2025.100305},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017791113&doi=10.1016%2Fj.wss.2025.100305&partnerID=40&md5=4d90af7d650cc37f83d08490e8db4f03},
affiliations = {Tongji University, College of Architecture and Urban Planning, Shanghai, China; Jiangnan University, School of Design, Wuxi, Jiangsu, China; Tongji University, Urban Renewal Center, Shanghai, China},
abstract = {Architectural spaces profoundly shape human emotional states, yet the neural mechanisms underlying spatial-emotional interactions remain underexplored in design frameworks. This systematic review synthesizes evidence from 17 neuroimaging studies (2009-2023) to establish methodological paradigms and neural correlates of emotion-driven spatial design. Following PRISMA guidelines, we identified three critical findings: (1) Methodological convergence: electroencephalogram dominates neuro-architectural research (83.3 % studies), predominantly paired with virtual reality (66.7 %) for dynamic emotion tracking, while real-world assessments constitute only 8.3 %; (2) Neural mechanisms: Spatial proportions (width-depth ratios), curvilinear forms, and openness significantly modulate relaxation-arousal balance through α/β oscillations and anterior cingulate cortex activation, while aesthetic valuation engages visual-spatial networks (precuneus, middle temporal gyrus) and reward circuits (ventral striatum); (3) Design thresholds: Empirical parameters such as window-to-wall ratios (0.4–0.6) and spatial proportions (1.6:1) emerge as neurophysiological benchmarks for emotion-oriented design. The review highlights an urgent need for multimodal frameworks integrating EEG, fMRI, and behavioral metrics to decode complex spatial interactions. Future research should prioritize longitudinal neural plasticity studies, computational models of design parameter optimization, and neuroevidence-based design guidelines. This work advances a paradigm shift from intuition-driven to neuro-informed spatial design by bridging the gap between neuroscientific insights and architectural practice. © 2025},
keywords = {Design parameter optimization; EEG; Emotional experience; Housing design; Multimodal evaluation; Neuroarchitecture},
correspondence_address = {L. Xu; College of Architecture and Urban Planning, Tongji University, Shanghai, China; email: leiqing@tongji.edu.cn},
publisher = {Elsevier B.V.},
issn = {26665581},
language = {English},
abbrev_source_title = {Wellbeing, Space Soc.},
type = {Review}}
@article{Li2025Survey,
author = {Li, Suchen and Tang, Zhuo and Li, Mengmeng and Yang, Lifang and Shang, Zhigang},
title = {A survey of neural signal decoding based on domain adaptation},
year = {2025},
journal = {Neurocomputing},
volume = {657},
pages = {},
doi = {10.1016/j.neucom.2025.131653},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017123872&doi=10.1016%2Fj.neucom.2025.131653&partnerID=40&md5=33fd1f941c75a3a0c1e85a468ee60175},
affiliations = {Zhengzhou University, School of Electrical and Information Engineering, Zhengzhou, Henan, China; Henan Key Laboratory of Brain Science and Brain-Computer Interface Technology, Zhengzhou, Henan, China; Zhengzhou University, Zhengzhou, Henan, China},
abstract = {An important objective in brain-computer interfaces (BCIs) is to develop robust and reliable neural signal decoders. However, the decoders will encounter challenges under cross-subject or cross-session conditions due to the randomness, non-stationarity, and individual variability of brain electrical activity. Reducing distributional differences is an exceptionally intuitive way to eliminate inter-subject/session differences and enhance decoder generalizability. In this context, domain adaptation (DA) emerges as a valuable technique, enabling the rapid transfer of knowledge acquired from large datasets with labeled data to new subjects or sessions. This paper provides a comprehensive survey of DA research in neural decoding from 2014 to the present. We categorize neural decoding methods related to DA by considering instance-based, feature-based, and model-based, which is motivated by three fundamental challenges in DA: How can one effectively select suitable source domains or samples for transfer? How can inter-domain distributional differences be minimized through feature space transformation? And how can decoder parameters be optimally shared? Additionally, several decoding methods that combine deep learning with DA are highlighted, given the significant advantages of deep learning over traditional feature extraction techniques. Furthermore, our paper explores the application of DA in complex scenarios, such as multiple source domains and low-resource settings. In summary, we have reviewed domain-adaptive decoding algorithms and their application considerations, while identifying various challenges that need to be addressed in future research. © 2025 Elsevier B.V.}, keywords = {Brain; Decoding; Deep learning; Interfaces (computer); Knowledge management; Knowledge transfer; Labeled data; Large datasets; Learning systems; Neural networks; Signal processing; Condition; Cross-session; Cross-subject; Decoding methods; Domain adaptation; Individual variability; Neural decoding; Neural signals; Non-stationarities; Signal decoders; Brain computer interface; adaptation; algorithm; brain computer interface; decoding; deep learning; electroencephalogram; feature extraction; human; nerve cell; overlearning; review},
correspondence_address = {L. Yang; School of Electrical and Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; email: lifang_yang1014@zzu.edu.cn},
publisher = {Elsevier B.V.},
issn = {18728286; 09252312},
coden = {NRCGE},
language = {English},
abbrev_source_title = {Neurocomputing},
type = {Review}}
@article{Lian2025Multidimensional,
author = {Lian, Yongxiang and Pan, Shihao and Shi, Li},
title = {Multidimensional Representation Dynamics for Abstract Visual Objects in Encoded Tangram Paradigms},
year = {2025},
journal = {Brain Sciences},
volume = {15},
number = {9},
pages = {},
doi = {10.3390/brainsci15090941},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017377866&doi=10.3390%2Fbrainsci15090941&partnerID=40&md5=a358331936c9bffe2e54e6ae65452356},
affiliations = {Tsinghua University, Beijing, China},
abstract = {Background: The human visual system is capable of processing large quantities of visual objects with varying levels of abstraction. The brain also exhibits hierarchical integration and learning capabilities that combine various attributes of visual objects (e.g., color, shape, local features, and categories) into coherent representations. However, prevailing theories in visual neuroscience employ simple stimuli or natural images with uncontrolled feature correlations, which constrains the systematic investigation of multidimensional representation dynamics. Methods: In this study, we aimed to bridge this methodological gap by developing a novel large tangram paradigm in visual cognition research and proposing cognitive-associative encoding as a mathematical basis. Critical representation dimensions—including animacy, abstraction level, and local feature density—were computed across a public dataset of over 900 tangrams, enabling the construction of a hierarchical model of visual representation. Results: Neural responses to 85 representative images were recorded using Electroencephalography (n = 24), and subsequent behavioral analyses and neural decoding revealed that distinct representational dimensions are independently encoded and dynamically expressed at different stages of cognitive processing. Furthermore, representational similarity analysis and temporal generalization analysis indicated that higher-order cognitive processes, such as “change of mind,” reflect the selective activation or suppression of local feature processing. Conclusions: These findings demonstrate that tangram stimuli, structured through cognitive-associative encoding, provide a generalizable computational framework for investigating the dynamic stages of human visual object cognition. © 2025 by the authors.}, keywords = {adult; article; cognition; color; controlled study; dynamics; electroencephalogram; electroencephalography; female; human; human experiment; learning; male; methodological gap; nerve potential; normal human; visual system},
correspondence_address = {L. Shi; Department of Automation, Tsinghua University, Beijing, 100084, China; email: shilits@mail.tsinghua.edu.cn},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {20763425},
language = {English},
abbrev_source_title = {Brain Sci.},
type = {Article}}
@article{Lotey2025Eegbased,
author = {Lotey, Taveena and Verma, Aman and Roy, Partha Pratim},
title = {EEG-Based Mental Imagery Task Adaptation via Ensemble of Weight-Decomposed Low-Rank Adapters},
year = {2025},
journal = {Lecture Notes in Computer Science},
volume = {15311 LNCS},
pages = {309 - 324},
doi = {10.1007/978-3-031-78195-7_21},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211963733&doi=10.1007%2F978-3-031-78195-7_21&partnerID=40&md5=7bd691071a1a2856dea639b3b0043761},
affiliations = {Indian Institute of Technology Roorkee, Department of CSE, Roorkee, UK, India; Indian Institute of Technology Roorkee, Department of ME, Roorkee, UK, India},
abstract = {Electroencephalography (EEG) is widely researched for neural decoding in Brain Computer Interfaces (BCIs) as it is non-invasive, portable, and economical. However, EEG signals suffer from inter- and intra-subject variability, leading to poor performance. Recent technological advancements have led to deep learning (DL) models that have achieved high performance in various fields. However, such large models are compute- and resource-intensive and are a bottleneck for real-time neural decoding. Data distribution shift can be handled with the help of domain adaptation techniques of transfer learning (fine-tuning) and adversarial training that requires model parameter updates according to the target domain. One such recent technique is Parameter-efficient fine-tuning (PEFT), which requires only a small fraction of the total trainable parameters compared to fine-tuning the whole model. Therefore, we explored PEFT methods for adapting EEG-based mental imagery tasks. We considered two mental imagery tasks: speech imagery and motor imagery, as both of these tasks are instrumental in post-stroke neuro-rehabilitation. We proposed a novel ensemble of weight-decomposed low-rank adaptation methods, EDoRA, for parameter-efficient mental imagery task adaptation through EEG signal classification. The performance of the proposed PEFT method is validated on two publicly available datasets, one speech imagery, and the other motor imagery dataset. In extensive experiments and analysis, the proposed method has performed better than full fine-tune and state-of-the-art PEFT methods for mental imagery EEG classification. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.}, keywords = {Adversarial machine learning; Brain mapping; Deep learning; Image analysis; Image reconstruction; Learning to rank; Neurons; Transfer learning; Fine tuning; Fine-tuning methods; Imagery task; Low-rank adaptation.; Mental imagery; Neural decoding; Performance; Task adaptation; Brain computer interface},
correspondence_address = {T. Lotey; Department of CSE, Indian Institute of Technology Roorkee, Roorkee, India; email: taveena@cs.iitr.ac.in},
editor = {Antonacopoulos, A. and Chaudhuri, S. and Chellappa, R. and Liu, C.-L. and Bhattacharya, S. and Pal, U.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {16113349; 03029743},
isbn = {9789819698936; 9789819698042; 9789819698110; 9789819698905; 9783032004949; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141},
language = {English},
abbrev_source_title = {Lect. Notes Comput. Sci.},
type = {Conference paper}}
@article{Ma2025Brainclip,
author = {Ma, Yongqiang and Liu, Yulong and Chen, Liangjun and Zhu, Guibo and Chen, Badong and Zheng, Nanning},
title = {BrainCLIP: Brain Representation via CLIP for Generic Natural Visual Stimulus Decoding},
year = {2025},
journal = {IEEE Transactions on Medical Imaging},
volume = {44},
number = {10},
pages = {3962 - 3972},
doi = {10.1109/TMI.2025.3537287},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216862048&doi=10.1109%2FTMI.2025.3537287&partnerID=40&md5=1438c97c1aafed9b95b48a36b0a97d4a},
affiliations = {Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, Xi'an, Shaanxi, China; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, China},
abstract = {Functional Magnetic Resonance Imaging (fMRI) presents challenges due to limited paired samples and low signal-to-noise ratios, particularly in tasks involving reconstructing natural images or decoding their semantic content. To address these challenges, we introduce BrainCLIP, an innovative fMRI-based brain decoding model. BrainCLIP leverages Contrastive Language-Image Pre-training’s (CLIP) cross-modal generalization abilities to bridge brain activity, images, and text for the first time. Our experiments demonstrate CLIP’s effectiveness in diverse brain decoding tasks, including zero-shot visual category decoding, fMRI-image/text alignment, and fMRI-to-image generation. The core objective of BrainCLIP is to train a mapping network that translates fMRI patterns into a unified CLIP embedding space, achieved through visual and textual supervision integration. Our experiments highlight that this approach significantly enhances performance in tasks such as fMRI-text alignment and fMRI-based image generation. Notably, BrainCLIP surpasses BraVL, a recent multi-modal method, in zero-shot visual category decoding. Moreover, BrainCLIP demonstrates strong capability in reconstructing visual stimuli with high semantic fidelity, competing favorably with state-of-the-art methods in capturing high-level semantic features during fMRI-based natural image reconstruction. © 1982-2012 IEEE.}, keywords = {Alignment; Brain mapping; Image enhancement; Image reconstruction; Magnetic resonance imaging; Semantics; Vision; Brain decoding; Contrastive language-image pre-training; Cross-modal; Functional magnetic resonance imaging; Linguistic representations; Natural images; Pre-training; Text alignments; Visual stimulus; Visual-linguistic representation; Signal to noise ratio; adult; article; brain; electroencephalogram; female; functional magnetic resonance imaging; human; image reconstruction; male; normal human; signal noise ratio; visual stimulation; algorithm; brain mapping; diagnostic imaging; image processing; nuclear magnetic resonance imaging; photostimulation; physiology; procedures; Adult; Algorithms; Brain; Brain Mapping; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Photic Stimulation},
correspondence_address = {B. Chen; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center of Visual Information and Applications, Xi’an, Shaanxi, 710049, China; email: chenbd@xjtu.edu.cn; N. Zheng; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center of Visual Information and Applications, Xi’an, Shaanxi, 710049, China; email: nnzheng@xjtu.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {02780062; 1558254X},
coden = {ITMID},
pmid = {40031248},
language = {English},
abbrev_source_title = {IEEE Trans. Med. Imaging},
type = {Article}}
@article{Makarova2025Stereoeeg,
author = {Makarova, Anna and Soghoyan, Gurgen A. and Sajfutdinov, Timur and Dragoi, Olga and Zuev, Andrey Aleksandrovich and Karpov, O. E. and Lebedev, Mikhail A.},
title = {StereoEEG Dynamics During Visual-Motor Interaction with a Robotic Hand},
year = {2025},
pages = {55 - 57},
doi = {10.1109/CNN67635.2025.11177531},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018910495&doi=10.1109%2FCNN67635.2025.11177531&partnerID=40&md5=661848a9391e7eb3a4f9cc3db5ab7f0d},
affiliations = {Institute for Information Transmission Problems of the Russian Academy of Sciences, Moscow, Moscow Oblast, Russian Federation; Institute for Information Transmission Problems of the Russian Academy of Sciences, Vladimir Zelman Center for Neurobiology and Brain Rehabilitation, Moscow, Moscow Oblast, Russian Federation; MaxBionic, Moscow, Russian Federation; HSE University, Center for Language and Brain, Moscow, Russian Federation; Pirogov Russian National Research Medical University (RNRMU), Moscow, Russian Federation; Sechenov Institute of Evolutionary Physiology and Biochemistry of the Russian Academy of Sciences, Faculty of Mechanics and Mathematics, Saint Petersburg, Russian Federation},
abstract = {We developed a method for decoding hand gestures from stereo-electroencephalographic (sEEG) signals using a transformer-based model. Seven patients with sEEG implants reproduced gestures performed by a prosthetic hand. Neural decoding was conducted offline on these data. Time-frequency features of the sEEG were processed using the transformer architecture with spatial and temporal attention. Participant-specific models were trained, achieving up to 0.83 accuracy for individual finger gestures. These results highlight the potential of combining sEEG with artificial intelligence (AI) for intuitive control of prosthetic limbs with brain activity. © 2025 IEEE.}, keywords = {Artificial limbs; Brain; Electroencephalography; Electrophysiology; Intelligent robots; Interfaces (computer); Neural prostheses; Neurophysiology; Robotic arms; Stereo image processing; Visual servoing; Electroencephalographic signals; Finger gestures; Hand gesture; Hand prosthetic; Intuitive controls; Neural decoding; Offline; Prosthetic hands; Stereo-electroencephalographic; Time frequency features; Brain computer interface},
correspondence_address = {A. Makarova; Institute for Information Transmission Problems, Russian Academy of Sciences, MSU Institute for Artificial Intelligence, Moscow, Russian Federation; email: annmakarova28@yandex.ru},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9798331568955},
language = {English},
abbrev_source_title = {Proc. - Int. Conf. Neurotechnologies Neurointerfaces, CNN},
type = {Conference paper}}
@article{Meng2025Eegbased,
author = {Meng, Qiang and Tian, Lan and Liu, Guoyang and Zhang, Xue},
title = {EEG-based cross-subject passive music pitch perception using deep learning models},
year = {2025},
journal = {Cognitive Neurodynamics},
volume = {19},
number = {1},
pages = {},
doi = {10.1007/s11571-024-10196-9},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214087439&doi=10.1007%2Fs11571-024-10196-9&partnerID=40&md5=02dab89a1166d6b67bc30100228dcc63},
affiliations = {Shandong University, School of Integrated Circuits, Jinan, Shandong, China},
abstract = {Pitch plays an essential role in music perception and forms the fundamental component of melodic interpretation. However, objectively detecting and decoding brain responses to musical pitch perception across subjects remains to be explored. In this study, we employed electroencephalography (EEG) as an objective measure to obtain the neural responses of musical pitch perception. The EEG signals from 34 subjects under hearing violin sounds at pitches G3 and B6 were collected with an efficient passive Go/No-Go paradigm. The lightweight modified EEGNet model was proposed for EEG-based pitch classification. Specifically, within-subject modeling with the modified EEGNet model was performed to construct individually optimized models. Subsequently, based on the within-subject model pool, a classifier ensemble (CE) method was adopted to construct the cross-subject model. Additionally, we analyzed the optimal time window of brain decoding for pitch perception in the EEG data and discussed the interpretability of these models. The experiment results show that the modified EEGNet model achieved an average classification accuracy of 77% for within-subject modeling, significantly outperforming other compared methods. Meanwhile, the proposed CE method achieved an average accuracy of 74% for cross-subject modeling, significantly exceeding the chance-level accuracy of 50%. Furthermore, we found that the optimal EEG data window for the pitch perception lies 0.4 to 0.9 s onset. These promising results demonstrate that the proposed methods can be effectively used in the objective assessment of pitch perception and have generalization ability in cross-subject modeling. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.}, keywords = {adult; article; classifier; controlled study; deep learning; electroencephalogram; electroencephalography; female; hearing; human; human experiment; male; music; nerve potential; normal human; pitch; pitch perception},
correspondence_address = {L. Tian; School of Integrated Circuits, Shandong University, Jinan, 1500 Shunhua Road, Shandong, 250101, China; email: tianlan65@sdu.edu.cn; G. Liu; School of Integrated Circuits, Shandong University, Jinan, 1500 Shunhua Road, Shandong, 250101, China; email: gyliu@sdu.edu.cn},
publisher = {Springer Science and Business Media B.V.},
issn = {18714080; 18714099},
language = {English},
abbrev_source_title = {Cogn. Neurodynamics},
type = {Article}}
@article{Mueller2025Happy,
author = {Mueller, Calla and Durston, Amie J. and Itier, Roxane J.},
title = {Happy and angry facial expressions are processed independently of task demands and semantic context congruency in the first stages of vision – A mass univariate ERP analysis},
year = {2025},
journal = {Brain Research},
volume = {1851},
pages = {},
doi = {10.1016/j.brainres.2025.149481},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216729047&doi=10.1016%2Fj.brainres.2025.149481&partnerID=40&md5=bbbd70bc4643be94fe5162cb73a12936},
affiliations = {University of Waterloo, Department of Psychology, Waterloo, ON, Canada},
abstract = {Neural decoding of others’ facial expressions is critical in social interactions and has been investigated using scalp event related potentials (ERPs). However, the impact of task and emotional context congruency on this neural decoding is unclear. Previous ERP studies employed classic statistical analyses that only focused on specific electrodes and time points, which inflates type I and type II errors. The present study re-analyzed the study by Aguado et al. (2019) using robust data-driven Mass Univariate Statistics across every time point and electrode and rejected trials with early reaction times to rule out motor-related activity on neural recordings. Participants viewed neutral faces paired with negative or positive situational sentences (e.g. “She catches her partner cheating on her with her best friend”), followed by the same individuals’ faces expressing happiness or anger, such that the facial expressions were congruent or incongruent with the situation. Participants engaged in two tasks: an emotion discrimination task, and a situation-expression congruency discrimination task. We found significant effects of expression largest during the N170-P2 interval, and effects of congruency and task around an LPP-like component. However, the effect of congruency was significant only in the congruency task, suggesting a limited and task-dependant influence of semantic context. Importantly, emotion did not interact with any factor neurally, suggesting facial expressions were decoded automatically during the first 400 ms of vision, regardless of context congruency or task demands. The results and their discrepancies with the original findings are discussed in the context of ERP statistics and the replication crisis. © 2025 The Author(s)}, keywords = {adult; anger; Article; controlled study; emotion; event related potential; evoked response; facial expression; female; happiness; human; human experiment; male; motor activity; normal human; reaction time; semantics; social interaction; task performance; type I error; type II error; young adult; brain; electroencephalography; facial recognition; physiology; procedures; Adult; Anger; Brain; Electroencephalography; Emotions; Evoked Potentials; Facial Expression; Facial Recognition; Female; Happiness; Humans; Male; Reaction Time; Semantics; Young Adult},
correspondence_address = {R.J. Itier; University of Waterloo, Department of Psychology, Waterloo, 200 University Ave West, N2L 3G1, Canada; email: ritier@uwaterloo.ca},
publisher = {Elsevier B.V.},
issn = {18726240; 00068993},
coden = {BRREA},
pmid = {39889942},
language = {English},
abbrev_source_title = {Brain Res.},
type = {Article}}
@article{Nigam2025Eegdepth,
author = {Nigam, Jyoti and Prakash, Aditya and Uthamkumar, M. and Salgotra, Samvaidan and Bhavsar, Arnav V.},
title = {EEG-Depth: Learning Structural Information from Visual Brain Decoding via Depth Estimation},
year = {2025},
journal = {Lecture Notes in Computer Science},
volume = {15614 LNCS},
pages = {253 - 264},
doi = {10.1007/978-3-031-87657-8_18},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005571568&doi=10.1007%2F978-3-031-87657-8_18&partnerID=40&md5=3095c110122497f20029f3dd27e9e5e6},
affiliations = {Indian Institute of Technology Mandi, Mandi, HP, India},
abstract = {Electroencephalography (EEG) is a crucial tool for recording the brain’s electrical activity, providing insights into neural processes. However, extracting meaningful information of a complex visual stimulus (e.g. a scene image) from EEG data is challenging due to a large domain shift and the signal’s complexity. Recent research efforts focus on decoding visual information from EEG using advanced models and techniques. In our work, we address structure and locality estimation from EEG signals with a particular focus on depth perception, which is a fundamental aspect of the visual processing pathway and could serve as a key intermediate ground for visual decoding models. Thus, we focus on the task of reconstruction of depth maps from EEG data, corresponding to the images shown to subjects. Our work involves a contrastive learning framework in an attempt to align GNN based EEG embeddings to that of and depth map embeddings. We also perform experiments to draw some insights about the importance of EEG channels in such a EEG to Depth map reconstruction. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.}, keywords = {Brain decoding; Depth Estimation; Depthmap; Electrical activities; Electroencephalography-imagenet dataset; Embeddings; Images reconstruction; Neural process; Structural information; Visual brain decoding; Image analysis},
correspondence_address = {J. Nigam; Indian Institute of Technology-Mandi, Mandi, Himachal Pradesh, India; email: jyoti_nigam@projects.iitmandi.ac.in},
editor = {Palaiahnakote, S. and Schuckers, S. and Ogier, J.-M. and Bhattacharya, P. and Pal, U. and Bhattacharya, S.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {16113349; 03029743},
isbn = {9789819698936; 9789819698042; 9789819698110; 9789819698905; 9783032004949; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141},
language = {English},
abbrev_source_title = {Lect. Notes Comput. Sci.},
type = {Conference paper}}
@article{Olza2025Domain,
author = {Olza, Alexander and Soto, David and Santana, Roberto},
title = {Domain Adaptation-enhanced searchlight: enabling classification of brain states from visual perception to mental imagery},
year = {2025},
journal = {Brain Informatics},
volume = {12},
number = {1},
pages = {},
doi = {10.1186/s40708-025-00263-0},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009369560&doi=10.1186%2Fs40708-025-00263-0&partnerID=40&md5=50f9c4637a30abcde8bf4dd9eb4dab48},
affiliations = {Universidad del Pais Vasco, Intelligent Systems Group, Leioa, Biscay, Spain; BCBL – Basque Center on Cognition, Brain and Language, Consciousness group, Donostia-San Sebastian, Guipuzcoa, Spain; Ikerbasque, Basque Foundation for Science, Bilbao, Basque Country, Spain},
abstract = {In cognitive neuroscience and brain-computer interface research, accurately predicting imagined stimuli is crucial. This study investigates the effectiveness of Domain Adaptation (DA) in enhancing imagery prediction using primarily visual data from fMRI scans of 18 subjects. Initially, we train a baseline model on visual stimuli to predict imagined stimuli, utilizing data from 14 brain regions. We then develop several models to improve imagery prediction, comparing different DA methods. Our results demonstrate that DA significantly enhances imagery prediction in binary classification on our dataset, as well as in multiclass classification on a publicly available dataset. We then conduct a DA-enhanced searchlight analysis, followed by permutation-based statistical tests to identify brain regions where imagery decoding is consistently above chance across subjects. Our DA-enhanced searchlight predicts imagery contents in a highly distributed set of brain regions, including the visual cortex and the frontoparietal cortex, thereby outperforming standard cross-domain classification methods. The complete code and data for this paper have been made openly available for the use of the scientific community. © The Author(s) 2025.}, keywords = {Brain; Classification (of information); Forecasting; Interfaces (computer); Tunneling (excavation); Baseline models; Brain decoding; Brain regions; Brain state; Cognitive neurosciences; Domain adaptation; FMRI; Mental imagery; Visual data; Visual perception; Brain computer interface; adaptation; Article; binary classification; brain; brain region; cognitive neuroscience; deep learning; Domain Adaptation; electroencephalogram; frontoparietal cortex; functional magnetic resonance imaging; fusiform gyrus; hemodynamics; human; image segmentation; imagery; logistic regression analysis; machine learning; mental imagery; neuroimaging; support vector machine; vision; visual cortex},
correspondence_address = {A. Olza; Intelligent Systems Group, University of the Basque Country (UPV/EHU), Donostia-San Sebastián, Spain; email: alexander.olza@ehu.eus},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {21984018; 21984026},
language = {English},
abbrev_source_title = {Brain Informatics},
type = {Article}}
@article{Oyarzo2025Decoding,
author = {Oyarzo, Pablo and Cichy, Radoslaw Martin and Vidaurre, Diego},
title = {ADA: A decoding algorithm for temporally-variable brain responses},
year = {2025},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {4943 - 4951},
doi = {10.1016/j.csbj.2025.10.044},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021083698&doi=10.1016%2Fj.csbj.2025.10.044&partnerID=40&md5=2ff4e8c8cd20635945659a0517d6d42e},
affiliations = {Freie Universität Berlin, Department of Education and Psychology, Berlin, Berlin, Germany; Aarhus Universitet, Department of Clinical Medicine, Aarhus, Midtjylland, Denmark; Centre de Recerca Matemàtica, Cerdanyola del Valles, Barcelona, Spain; University of Oxford Medical Sciences Division, Oxford, Oxfordshire, United Kingdom},
abstract = {Decoding mental contents from brain activity is a long-standing goal in theoretical neuroscience and neural engineering. While current methods perform well in tasks with externally timed events, such as perception or motor execution, decoding covert cognitive processes like imagery or memory recall remains challenging due to uncertainty in the timing of underlying neural dynamics. In these settings, neurophysiological responses are not reliably linked to observable behaviour and likely vary in latency across trials. This complicates the use of time-locked analysis techniques, which perform decoding time point by time point across trials, thus assuming consistent signal timing. This problem corresponds to an understudied class of supervised learning where input features may be effectively mislabelled and need to be aligned across cases. To address this, we present the Adaptive Decoding Algorithm (ADA), a nonparametric method based on a two-level prediction. First, we estimate, for each trial, the temporal window most likely to reflect task-relevant signals; second, we decode the test trials based on the selection of informative windows. Using controlled simulations as well as a model of memory recall based on real perception data, we show that ADA outperforms alternative methods that assume fixed temporal structure. These results provide evidence that explicitly accounting for trial-specific timing can substantially improve decoding performance when the timing of relevant neural activity is unknown. © 2025}, keywords = {Brain; Brain models; Cognitive systems; Computational neuroscience; Decoding; Neural networks; Neurons; Signal processing; Supervised learning; Adaptive decoding algorithms; Brain activity; Brain decoding; Brain response; Cognitive neurosciences; Decoding algorithm; Machine-learning; MEG; Temporal variability; Time points; Learning systems; adult; algorithm; article; brain; cognition; cognitive neuroscience; decoding; electroencephalogram; female; human; human experiment; imagery; latent period; learning; machine learning; male; memory; prediction; simulation; theoretical neuroscience},
correspondence_address = {D. Vidaurre; Department of Clinical Medicine, Aarhus University, Aarhus, Denmark; email: dvidaurre@cfin.au.dk},
publisher = {Elsevier B.V.},
issn = {20010370},
language = {English},
abbrev_source_title = {Comput. Struct. Biotechnol. J.},
type = {Article}}
@article{Park2025Natural,
author = {Park, Jong-yun and Tsukamoto, Mitsuaki and Tanaka, Misato and Kamitani, Yukiyasu},
title = {Natural sounds can be reconstructed from human neuroimaging data using deep neural network representation},
year = {2025},
journal = {PLOS Biology},
volume = {23},
number = {7 July},
pages = {},
doi = {10.1371/journal.pbio.3003293},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011400440&doi=10.1371%2Fjournal.pbio.3003293&partnerID=40&md5=99e3c11b4317fbfb7d3f29df5e0770f3},
affiliations = {Graduate School of Informatics, Department of Intelligence Science and Technology, Kyoto, Kyoto, Japan; Advanced Telecommunications Research Institute International (ATR), Department of Neuroinformatics, Kyoto, Kyoto, Japan},
abstract = {Reconstruction of perceptual experiences from brain activity offers a unique window into how population neural responses represent sensory information. Although decoding visual content from functional MRI (fMRI) has seen significant success, reconstructing arbitrary sounds remains challenging due to the fine temporal structure of auditory signals and the coarse temporal resolution of fMRI. Drawing on the hierarchical auditory features of deep neural networks (DNNs) with progressively larger time windows and their neural activity correspondence, we introduce a method for sound reconstruction that integrates brain decoding of DNN features and an audio-generative model. DNN features decoded from auditory cortical activity outperformed spectrotemporal and modulation-based features, enabling perceptually plausible reconstructions across diverse sound categories. Behavioral evaluations and objective measures confirmed that these reconstructions preserved short-term spectral and perceptual properties, capturing the characteristic timbre of speech, animal calls, and musical instruments, while the reconstructed sounds did not reproduce longer temporal sequences with fidelity. Leave-category-out analyses indicated that the method generalizes across sound categories. Reconstructions at higher DNN layers and from early auditory regions revealed distinct contributions to decoding performance. Applying the model to a selective auditory attention (“cocktail party”) task further showed that reconstructions reflected the attended sound more strongly than the unattended one in some of the subjects. Despite its inability to reconstruct exact temporal sequences, which may reflect the limited temporal resolution of fMRI, our framework demonstrates the feasibility of mapping brain activity to auditory experiences—a step toward more comprehensive understanding and reconstruction of internal auditory representations. © 2025 Park et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords = {adult; algorithm; Article; deep neural network; electroencephalogram; female; functional magnetic resonance imaging; generative model; human; machine learning; musical instrument; neuroimaging; quantitative analysis; signal noise ratio; vocalization; artificial neural network; auditory cortex; auditory stimulation; brain; brain mapping; diagnostic imaging; hearing; male; nuclear magnetic resonance imaging; physiology; procedures; sound; young adult; Acoustic Stimulation; Adult; Auditory Cortex; Auditory Perception; Brain; Brain Mapping; Female; Humans; Magnetic Resonance Imaging; Male; Neural Networks, Computer; Neuroimaging; Sound; Young Adult},
correspondence_address = {J.-Y. Park; Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Kyoto, Japan; email: park_jy.psyc@tmd.ac.jp; Y. Kamitani; Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Kyoto, Japan; email: kamitani@i.kyoto-u.ac.jp},
publisher = {Public Library of Science},
issn = {15449173; 15457885},
coden = {PBLIB},
pmid = {40700446},
language = {English},
abbrev_source_title = {Plos Biol.},
type = {Article}}
@article{Pfeffer2025Trends,
author = {Pfeffer, Maximilian Achim and Wong, Johnny Kwok Wai and Ling, Sai Ho},
title = {Trends and Limitations in Transformer-Based BCI Research},
year = {2025},
journal = {Applied Sciences (Switzerland)},
volume = {15},
number = {20},
pages = {},
doi = {10.3390/app152011150},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020236496&doi=10.3390%2Fapp152011150&partnerID=40&md5=0492e3e15ea25f3aa3dd81f0d2c63fd7},
affiliations = {University of Technology Sydney, Faculty of Engineering and Information Technology, Sydney, NSW, Australia; University of Technology Sydney, Architecture and Building, Sydney, NSW, Australia},
abstract = {Transformer-based models have accelerated EEG motor imagery (MI) decoding by using self-attention to capture long-range temporal structures while complementing spatial inductive biases. This systematic survey of Scopus-indexed works from 2020 to 2025 indicates that reported advances are concentrated in offline, protocol-heterogeneous settings; inconsistent preprocessing, non-standard data splits, and sparse efficiency frequently reporting cloud claims of generalization and real-time suitability. Under session- and subject-aware evaluation on the BCIC IV 2a/2b dataset, typical performance clusters are in the high-80% range for binary MI and the mid-70% range for multi-class tasks with gains of roughly 5–10 percentage points achieved by strong hybrids (CNN/TCN–Transformer; hierarchical attention) rather than by extreme figures often driven by leakage-prone protocols. In parallel, transformer-driven denoising—particularly diffusion–transformer hybrids—yields strong signal-level metrics but remains weakly linked to task benefit; denoise → decode validation is rarely standardized despite being the most relevant proxy when artifact-free ground truth is unavailable. Three priorities emerge for translation: protocol discipline (fixed train/test partitions, transparent preprocessing, mandatory reporting of parameters, FLOPs, per-trial latency, and acquisition-to-feedback delay); task relevance (shared denoise → decode benchmarks for MI and related paradigms); and adaptivity at scale (self-supervised pretraining on heterogeneous EEG corpora and resource-aware co-optimization of preprocessing and hybrid transformer topologies). Evidence from subject-adjusting evolutionary pipelines that jointly tune preprocessing, attention depth, and CNN–Transformer fusion demonstrates reproducible inter-subject gains over established baselines under controlled protocols. Implementing these practices positions transformer-driven BCIs to move beyond inflated offline estimates toward reliable, real-time neurointerfaces with concrete clinical and assistive relevance. © 2025 by the authors.}, keywords = {Biomedical signal processing; Decoding; Deep neural networks; Diffusion; Electric transformer testing; Electric transformers; Electroencephalography; Interfaces (computer); Learning systems; Signal denoising; De-Noise; Deep learning; Motor imagery; Neural decoding; Noises removal; Offline; Real- time; Self-attention; Signal-processing; Transformer; Brain computer interface},
correspondence_address = {S.H. Ling; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, 2007, Australia; email: steve.ling@uts.edu.au},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {20763417},
language = {English},
abbrev_source_title = {Appl. Sci.},
type = {Review}}
@article{Pfeffer2025Evolving,
author = {Pfeffer, Maximilian Achim and Nguyen, Anh Hoang Phuc and Kim, Kyunghun and Wong, Johnny Kwok Wai and Ling, Sai Ho},
title = {Evolving optimized transformer-hybrid systems for robust BCI signal processing using genetic algorithms},
year = {2025},
journal = {Biomedical Signal Processing and Control},
volume = {108},
pages = {},
doi = {10.1016/j.bspc.2025.107883},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004260461&doi=10.1016%2Fj.bspc.2025.107883&partnerID=40&md5=cc99736e47018816e45a68fda2063ba0},
affiliations = {University of Technology Sydney, Faculty of Engineering and Information Technology, Sydney, NSW, Australia; The University of Texas at Austin, Department of Neuroscience, Austin, TX, United States; University of Technology Sydney, Architecture and Building, Sydney, NSW, Australia},
abstract = {Integrating transformer-based architectures in brain-computer interface (BCI) systems has demonstrated significant potential in addressing challenges such as noisy electroencephalography (EEG) signals, inter-subject variability, and low signal-to-noise ratios. This study introduces a Genetic Algorithm (GA)-optimized framework to evolve high-performing transformer-hybrid architectures for EEG-based motor imagery (MI) classification. Leveraging a real-valued genome encoding scheme, the GA dynamically explored a diverse architectural search space comprising convolutional, transformer, and noise-infusion layers. Key findings demonstrate the efficacy of the proposed framework. The GA-derived architectures achieved a validation accuracy of 89.26%±6.1% on Dataset I, significantly surpassing traditional models such as EEGNet (70.00%) and t-CTrans (78.98%). On Dataset II, the GA-heuristic models achieved a validation accuracy of 84.52%±9.62 and a kappa score of 79.37%±12.82%, outperforming state-of-the-art models such as CTNet (82.52%±9.61%). Statistical analysis of the genetic algorithm revealed that genome complexity (genome length) significantly influenced model performance (F = 34.10, p < 0.00001), with larger genomes enabling richer feature extraction capabilities. Furthermore, the prevalence of transformer layers (n_trans) emerged as the most critical architectural component, significantly impacting not only validation accuracy (F = 12.10, p = 0.0019) but also kappa scores (F = 10.97, p = 0.0028). The proposed framework demonstrated strong generalization across datasets, maintaining well-separated clusters in t-SNE visualizations, particularly in test data, highlighting its adaptability to unseen conditions. By achieving state-of-the-art performance and validating key design parameters, this study sets a new benchmark for EEG-based BCI systems, showcasing the potential of GA-optimized transformer-hybrid architectures for more adaptive and generalizable solutions. © 2025 The Authors}, keywords = {Deep learning; Electric transformer testing; Frequency modulation; Image analysis; Image coding; Image compression; Pulse modulation; Signal modulation; Hybrid architectures; Interface system; Low signal-to-noise ratio; Motor imagery; Motor imagery classification; Network optimization; Neural decoding; Signal-processing; Transformer; Network coding; Article; classifier; comparative study; controlled study; convolutional neural network; deep learning; electroencephalography; feature extraction; genetic algorithm; genome; human; imagery; prevalence; signal processing; transformer hybrid system; validation study},
correspondence_address = {S.H. Ling; Faculty of Engineering and Information Technology, University of Technology Sydney, Broadway, Ultimo, 2007, NSW, CB11 81-113, Australia; email: steve.ling@uts.edu.au},
publisher = {Elsevier Ltd},
issn = {17468108; 17468094},
language = {English},
abbrev_source_title = {Biomed. Signal Process. Control},
type = {Article}}
@inproceedings{Postolache2025Icassp,
author = {Postolache, Emilian and Polouliakh, Natalia and Kitano, Hiroaki and Connelly, Akima and Rodolà, Emanuele and Cosmo, Luca and Akama, Taketo},
booktitle = {ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title = {Naturalistic Music Decoding from EEG Data via Latent Diffusion Models},
year = {2025},
volume = {},
number = {},
pages = {1--5},
abstract = {In this article, we explore the potential of using latent diffusion models, a family of powerful generative models, for the task of reconstructing naturalistic music from electroencephalogram (EEG) recordings. Unlike simpler music with limited timbres, such as MIDI-generated tunes or monophonic pieces, the focus here is on intricate music featuring a diverse array of instruments, voices, and effects, rich in harmonics and timbre. This study represents an initial foray into achieving general music reconstruction of high-quality using non-invasive EEG data, employing an end-to-end training approach directly on raw data without the need for manual pre-processing and channel selection. We train our models on the public NMED- T dataset and perform quantitative evaluation proposing neural embedding-based metrics. Our work contributes to the ongoing research in neural decoding and brain-computer interfaces, offering insights into the feasibility of using EEG data for complex auditory information reconstruction.},
keywords = {Training;Music;Signal processing algorithms;Diffusion models;Brain modeling;Electroencephalography;Decoding;Recording;Timbre;Speech processing;Generative AI;music generation;electroencephalography;diffusion models},
doi = {10.1109/ICASSP49660.2025.10887735},
issn = {2379-190X},
month = {April}}
@article{Qian2025Realtime,
author = {Qian, Youkun and Liu, Changjiang and null, null and Ran, Xingchen and Li, Shurui and Yang, Qinrong and null, null and null, null and Wang, Yijie and null, null and Zhou, Erda and Lu, Junfeng and Li, Yuanning and Tao, Tiger H. and Zhou, Zhitao and Wu, Jinsong},
title = {Real-time decoding of full-spectrum Chinese using brain-computer interface},
year = {2025},
journal = {Science Advances},
volume = {11},
number = {45},
pages = {eadz9968},
doi = {10.1126/sciadv.adz9968},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020993241&doi=10.1126%2Fsciadv.adz9968&partnerID=40&md5=19f276f9bdd66f70bff7a90c0b813d77},
affiliations = {Fudan University, Department of Neurosurgery, Shanghai, China; Shanghai Key Laboratory of Clinical and Translational Brain-Computer Interface Research, Shanghai, Shanghai, China; Fudan University, National Center for Neurological Disorders, Shanghai, China; Shanghai Institute of Microsystem and Information Technology Chinese Academy of Sciences, Shanghai, Shanghai, China; ShanghaiTech University, School of Biomedical Engineering, Shanghai, China; ShanghaiTech University, State Key Laboratory of Advanced Medical Materials and Devices, Shanghai, China; Neuroxess Co. Ltd, Shanghai, Shanghai, China; University of Chinese Academy of Sciences, School of Graduate Study, Beijing, China; Fudan University, MOE Frontiers Center for Brain Science, Shanghai, China; Shanghai Clinical Research and Trial Center, Shanghai, Shanghai, China; Lingang Laboratory, Shanghai, Shanghai, China; Guangdong Institute of Intelligence Science and Technology, Zhuhai, Guangdong 519031, China; Tianqiao and Chrissy Chen Institute for Translational Research, Shanghai, Shanghai, China; Shanghai Institute of Microsystem and Information Technology Chinese Academy of Sciences, Shanghai, Shanghai, China; University of Chinese Academy of Sciences, School of Integrated Circuits, Beijing, China},
abstract = {Speech brain-computer interfaces (BCIs) offer a promising means to provide functional communication capacity for patients with anarthria caused by neurological conditions such as amyotrophic lateral sclerosis (ALS) or brainstem stroke. Current speech decoding research has predominantly focused on English using phoneme-driven architectures, whereas real-time decoding of tonal monosyllabic languages such as Mandarin Chinese remains a major challenge. This study demonstrates a real-time Mandarin speech BCI that decodes monosyllabic units directly from neural signals. Using the 256-channel microelectrocorticographic BCI, we achieved robust decoding of a comprehensive set of 394 distinct syllables based purely on neural signals, yielding median syllable identification accuracy of 71.2% in a single-character reading task. Leveraging this high-performing syllable decoder, we further demonstrated real-time sentence decoding. Our findings demonstrate the efficacy of a tonally integrated, direct syllable neural decoding approach for Mandarin Chinese, paving the way for full-coverage systems in tonal monosyllabic languages.},
keywords = {adult; brain computer interface; China; East Asian; electroencephalography; female; human; language; male; physiology; speech; Adult; Brain-Computer Interfaces; Chinese people; East Asian People; Electroencephalography; Female; Humans; Language; Male; Speech},
issn = {23752548},
pmid = {41191764},
language = {English},
abbrev_source_title = {Sci Adv},
type = {Article}}
@article{Quek2025Timecourse,
author = {Quek, Genevieve L. and Theodorou, Alexandra and Peelen, Marius V.},
title = {The timecourse of inter-object contextual facilitation},
year = {2025},
journal = {Cortex},
volume = {190},
pages = {38 - 53},
doi = {10.1016/j.cortex.2025.05.020},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009695449&doi=10.1016%2Fj.cortex.2025.05.020&partnerID=40&md5=c388fd296a541efc99b7b05e4ec1e3f5},
affiliations = {The MARCS Institute for Brain, Behaviour and Development, Bankstown, NSW, Australia; Donders Institute for Brain, Cognition and Behaviour, Nijmegen, Gelderland, Netherlands; University of California, Davis, Davis, CA, United States},
abstract = {High-level vision is frequently studied at the level of either individual objects or whole scenes. An intermediate level of visual organisation that has received less attention is the “object constellation” – a familiar configuration of contextually-associated objects (e.g., plate + spoon). Recent behavioural studies have shown that information from multiple objects can be integrated to support observers' high-level understanding of a “scene” and its constituent objects. Here we used EEG in human participants (both sexes) to test when the visual system integrates information across objects to support recognition, using representations of objects' real-world size as a proxy for recognition. We briefly presented masked object constellations consisting of object silhouettes of either large (e.g., chair + table) or small (e.g., plate + spoon) real-world size, while independently varying retinal size. As a control, observers also viewed each silhouette in isolation. If object context facilitates object recognition, real-world size should be inferred more effectively when the objects appear in their contextually-associated pairs than in isolation, leading to the emergence of real-world size information in multivariate EEG patterns. Representational similarity analysis revealed that neural activity patterns captured information about the real-world size of object constellations from ∼200 msec after stimulus onset. This representation was stronger for, and specific to, object pairs as compared to single objects, and remained significant after regressing out visual similarity models derived from computational models. These results provide evidence for inter-object facilitation of visual processing, leading to a qualitatively different high-level representation of object pairs than single objects. © 2025 The Authors}, keywords = {adult; Article; computer model; controlled study; decoding; electroencephalography; facilitation; female; human; human experiment; male; normal human; recognition; vision; visual information; visual system; attention; photostimulation; physiology; procedures; visual pattern recognition; young adult; Adult; Attention; Electroencephalography; Female; Humans; Male; Pattern Recognition, Visual; Photic Stimulation; Recognition, Psychology; Visual Perception; Young Adult},
correspondence_address = {G.L. Quek; The MARCS Institute for Brain, Behaviour and Development, Western Sydney University, Sydney, Australia; email: g.quek@westernsydney.edu.au},
publisher = {Masson SpA},
issn = {19738102; 00109452},
coden = {CRTXA},
pmid = {40627986},
language = {English},
abbrev_source_title = {Cortex},
type = {Article}}
@inproceedings{Reddy2025World,
author = {Reddy, Nareddy Koushik and Shiva Mani, D. and Zaid, Mohammed and Girish Kumar, N S S S and Jabbar, M. A.},
booktitle = {2025 3rd World Conference on Communication & Computing (WCONF)},
title = {An Electroencephalography-Driven Language Interface Using BI-LSTM and Seq2Seq Decoder},
year = {2025},
volume = {},
number = {},
pages = {1--6},
abstract = {This paper presents a brain-computer interface (BCI) system that translates EEG (Electroencephalogram) signals into natural language using deep learning techniques. In recent time,s Brain-Computer Interfaces(BCI) have become the emerging solutions that provide the ability to communicate with individuals who are suffering with severe motor disabilities. The previous research on this topic lags in enabling real-time verbal communication. EEG signals are collected using noninvasive methods and preprocessed to remove artifacts and noise using standard tools from the MNE Python library. Feature extraction is performed to convert raw signals into structured input suitable for training. The model architecture employs a Bi-directional Long Short-Term Memory (Bi-LSTM) network to capture temporal dependencies in the EEG data, followed by a Sequence-to-Sequence (Seq2Seq) decoder that maps these patterns to corresponding linguistic outputs. Training and evaluation are conducted using publicly available EEG datasets from OpenNeuro and MNE repositories, which provide annotated recordings suitable for thought-to-text translation tasks. The system demonstrates promising results in generating coherent text from EEG patterns, showing potential for real-time communication support for individuals with speech or motor impairments. This paper highlights the effectiveness of combining Bi-LSTM with Seq2Seq models in neural decoding, opening new avenues for assistive technologies and neuro-linguistic research. At the character level, our model has achieved an accuracy of 54.32% and at the word level, our model has achieved 20.18% of accuracy. These results are achieved on the system with basic computational capabilities and our model is suitable for live prediction and output generation.},
keywords = {Training;Translation;Accuracy;Computational modeling;Brain modeling;Motors;Electroencephalography;Brain-computer interfaces;Real-time systems;Decoding;EEG Signal Processing;Brain-Computer Interface (BCI);Bi-LSTM;Seq2Seq Decoder;Neural Decoding},
doi = {10.1109/WCONF64849.2025.11233393},
issn = {},
month = {July}}
@article{Rehman2025Measuring,
author = {Rehman, Asad Ur and Shi, Xiaochuan and Ullah, Farhan and Wang, Zepeng and Ma, Chao},
title = {Measuring student attention based on EEG brain signals using deep reinforcement learning},
year = {2025},
journal = {Expert Systems with Applications},
volume = {269},
pages = {},
doi = {10.1016/j.eswa.2025.126426},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214891508&doi=10.1016%2Fj.eswa.2025.126426&partnerID=40&md5=59aaca9f74da2714f5f29cbb378b58c8},
affiliations = {Wuhan University, School of Cyber Science and Engineering, Wuhan, Hubei, China; Prince Mohammad Bin Fahd University, Cybersecurity Center, Al-Khobar, Ash Sharqiyah, Saudi Arabia},
abstract = {Quantifying attention during learning is essential yet extremely challenging. Conventional methods like surveys and observations are subjective and inaccurate. While electroencephalography (EEG) mirrors attention shifts, analysis is obstructed by artifacts and variability. This research proposes a double deep Q-network (DDQN), a specialized deep reinforcement learning algorithm well-suited for handling complex multidimensional data like EEG signals. Raw multi-channel EEG recordings undergo preprocessing by wavelet transformations, a technique that provides frequency-based signal representations robust to noise and fluctuations. Key features are extracted from these cleaned EEG traces to capture attention-related neural signatures. These feature vectors serve as inputs to the DDQN agent, which uses its learned action-value function to choose one of three discrete attention state labels (attentive, non-attentive, drowsy) at each timestep based on the input EEG features. Correct classifications result in +10 rewards to reinforce the agent predictions, while incorrect labels yield -1 penalties, enabling the DDQN to iteratively improve its classification accuracy over sequential iterations. A dual-network architecture with separate but interlinked target and online Q-networks enhances stability during this reinforcement learning process for attention quantification. Results showed 98.2% test accuracy in classifying attentive versus non-attentive versus drowsy states, significantly improving on the 92% benchmark. The reduced 0.65 loss value evidenced convergence. This methodology enables granular quantification of fluctuating attention from EEGs. The DDQN's neural decoding capability and substantial improvements establish a new state-of-the-art of adaptive education systems to track engagement by leveraging brain signals. © 2025 Elsevier Ltd}, keywords = {Contrastive Learning; Deep learning; Discrete wavelet transforms; Federated learning; Quantization (signal); Reinforcement learning; Self-supervised learning; Adaptive education; Attention classification; Brain signals; Conventional methods; Double deep Q-network; Electroencephalography; Neural correlate; Online learning; Reinforcement learnings; Student engagement; Deep reinforcement learning},
correspondence_address = {X. Shi; School of Cyber Science & Engineering, Wuhan University, Wuhan, Hubei, 430072, China; email: shixiaochuan@whu.edu.cn},
publisher = {Elsevier Ltd},
issn = {09574174},
coden = {ESAPE},
language = {English},
abbrev_source_title = {Expert Sys Appl},
type = {Article}}
@article{Rudroff2025Decoding,
author = {Rudroff, Thorsten},
title = {Decoding thoughts, encoding ethics: A narrative review of the BCI-AI revolution},
year = {2025},
journal = {Brain Research},
volume = {1850},
pages = {},
doi = {10.1016/j.brainres.2024.149423},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213244050&doi=10.1016%2Fj.brainres.2024.149423&partnerID=40&md5=4967054e5c2006a98b4b61b02e52b0f7},
affiliations = {Turun yliopisto, Turku, Southwest Finland, Finland},
abstract = {Objectives: This narrative review aims to analyze mechanisms underlying Brain-Computer Interface (BCI) and Artificial Intelligence (AI) integration, evaluate recent advances in signal acquisition and processing techniques, and assess AI-enhanced neural decoding strategies. The review identifies critical research gaps and examines emerging solutions across multiple domains of BCI-AI integration. Methods: A narrative review was conducted using major biomedical and scientific databases including PubMed, Web of Science, IEEE Xplore, and Scopus (2014–2024). Literature was analyzed to identify key developments in BCI-AI integration, with particular emphasis on recent advances (2019–2024). The review process involved thematic analysis of selected publications focusing on practical applications, technical innovations, and emerging challenges. Results: Recent advances demonstrate significant improvements in BCI-AI systems: 1) High-density electrode arrays achieve spatial resolution up to 5 mm, with stable recordings over 15 months; 2) Deep learning decoders show 40 % improvement in information transfer rates compared to traditional methods; 3) Adaptive algorithms maintain >90 % success rates in motor control tasks over 200-day periods without recalibration; 4) Novel closed-loop optimization frameworks reduce user training time by 55 % while improving accuracy. Latest developments in flexible neural interfaces and self-supervised learning approaches show promise in addressing long-term stability and cross-user generalization challenges. Conclusions: BCI-AI integration shows remarkable progress in improving signal quality, decoding accuracy, and user adaptation. While challenges remain in long-term stability and user training, advances in adaptive algorithms and feedback mechanisms demonstrate the technology's growing viability for clinical applications. Recent innovations in electrode technology, AI architectures, and closed-loop systems, combined with emerging standardization frameworks, suggest accelerating progress toward widespread therapeutic use and human augmentation applications. © 2024 The Author(s)}, keywords = {artificial intelligence; artificial neural network; assistive technology; autoencoder; basic research; cognitive rehabilitation; convolutional neural network; deep learning; dimensionality reduction; downstream processing; electrocorticography; evidence based practice; feedback system; human; human computer interaction; machine learning; Medline; motor control; principal component analysis; research gap; Review; Scopus; signal noise ratio; signal processing; sparse autoencoder; stroke rehabilitation; task performance; thematic analysis; transfer of learning; Web of Science; brain; brain computer interface; electroencephalography; ethics; physiology; procedures; Artificial Intelligence; Brain; Brain-Computer Interfaces; Electroencephalography; Humans},
publisher = {Elsevier B.V.},
issn = {18726240; 00068993},
coden = {BRREA},
pmid = {39719191},
language = {English},
abbrev_source_title = {Brain Res.},
type = {Retracted}}
@article{Ryb2025Simultaneous,
author = {Rybář, Milan and Poli, Riccardo and Daly, Ian},
title = {Simultaneous EEG and fNIRS recordings for semantic decoding of imagined animals and tools},
year = {2025},
journal = {Scientific Data},
volume = {12},
number = {1},
pages = {},
doi = {10.1038/s41597-025-04967-0},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003513173&doi=10.1038%2Fs41597-025-04967-0&partnerID=40&md5=0a1d364923b4bead81af4ca9332695d2},
affiliations = {University of Essex, Brain-Computer Interfaces and Neural Engineering Laboratory, Colchester, Essex, United Kingdom},
abstract = {Semantic neural decoding aims to identify which semantic concepts an individual focuses on at a given moment based on recordings of their brain activity. We investigated the feasibility of semantic neural decoding to develop a new type of brain-computer interface (BCI) that allows direct communication of semantic concepts, bypassing the character-by-character spelling used in current BCI systems. We provide data from our study to differentiate between two semantic categories of animals and tools during a silent naming task and three intuitive sensory-based imagery tasks using visual, auditory, and tactile perception. Participants were instructed to visualize an object (animal or tool) in their minds, imagine the sounds produced by the object, and imagine the feeling of touching the object. Simultaneous electroencephalography (EEG) and near-infrared spectroscopy (fNIRS) signals were recorded from 12 participants. Additionally, EEG signals were recorded from 7 other participants in a follow-up experiment focusing solely on the auditory imagery task. These datasets can serve as a valuable resource for researchers investigating semantic neural decoding, brain-computer interfaces, and mental imagery. © The Author(s) 2025.},
keywords = {adult; animal; brain; brain computer interface; electroencephalography; female; human; imagination; male; near infrared spectroscopy; physiology; semantics; Adult; Animals; Brain; Brain-Computer Interfaces; Electroencephalography; Female; Humans; Imagination; Male; Semantics; Spectroscopy, Near-Infrared},
correspondence_address = {M. Rybář; Brain-Computer Interfaces and Neural Engineering Laboratory, School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, United Kingdom; email: contact@milanrybar.cz; I. Daly; Brain-Computer Interfaces and Neural Engineering Laboratory, School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, United Kingdom; email: i.daly@essex.ac.uk},
publisher = {Nature Research},
issn = {20524463},
pmid = {40221457},
language = {English},
abbrev_source_title = {Sci. Data},
type = {Article}}
@article{Shah2025Pseudolinear,
author = {Shah, Nishal P. and Avansino, Donald T. and Kamdar, Foram B. and Nicolas, Claire and Kapitonava, Anastasia and Vargas-Irwin, Carlos E. and Hochberg, Leigh R. and Pandarinath, Chethan and Shenoy, Krishna V. and Willett, Francis R. and Henderson, Jaimie M.},
title = {Pseudo-linear summation explains neural geometry of multi-finger movements in human premotor cortex},
year = {2025},
journal = {Nature Communications},
volume = {16},
number = {1},
pages = {},
doi = {10.1038/s41467-025-59039-z},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006933783&doi=10.1038%2Fs41467-025-59039-z&partnerID=40&md5=fa130be96165bbd38c2d9611a204b1a0},
affiliations = {Stanford University, Department of Neurosurgery, Stanford, CA, United States; George R. Brown School of Engineering and Computing, Houston, TX, United States; George R. Brown School of Engineering and Computing, Houston, TX, United States; Rice University, Neuroengineering Initiative, Houston, TX, United States; Stanford University, Stanford, CA, United States; Harvard Medical School, Center for Neurotechnology and Neurorecovery, Boston, MA, United States; Providence VA Medical Center, Rehabilitation R&D Service, Providence, RI, United States; Division of Biology and Medicine, Providence, RI, United States; Brown University, Providence, RI, United States; School of Engineering, Providence, RI, United States; Georgia Institute of Technology, Atlanta, GA, United States; Emory University School of Medicine, Atlanta, GA, United States; Stanford University, Stanford, CA, United States; Stanford BIO-X, Stanford, CA, United States; Stanford Engineering, Stanford, CA, United States; Department of Bioengineering, Stanford, CA, United States; Stanford University, Stanford, CA, United States},
abstract = {How does the motor cortex combine simple movements (such as single finger flexion/extension) into complex movements (such as hand gestures, or playing the piano)? To address this question, motor cortical activity was recorded using intracortical multi-electrode arrays in two male people with tetraplegia as they attempted single, pairwise and higher-order finger movements. Neural activity for simultaneous movements was largely aligned with linear summation of corresponding single finger movement activities, with two violations. First, the neural activity exhibited normalization, preventing a large magnitude with an increasing number of moving fingers. Second, the neural tuning direction of weakly represented fingers changed significantly as a result of the movement of more strongly represented fingers. These deviations from linearity resulted in non-linear methods outperforming linear methods for neural decoding. Simultaneous finger movements are thus represented by the combination of individual finger movements by pseudo-linear summation. © The Author(s) 2025.},
keywords = {geometry; linearity; movement; neurology; physical activity; adult; aged; Article; clinical article; electroencephalogram; finger; gesture; hand movement; human; kinematics; male; mathematical model; premotor cortex; pseudo-linear summation model; quadriplegia; motor cortex; movement (physiology); pathophysiology; physiology; Adult; Fingers; Humans; Male; Motor Cortex; Movement; Quadriplegia},
correspondence_address = {N.P. Shah; Department of Neurosurgery, Stanford University, Stanford, United States; email: bhaishahster@gmail.com},
publisher = {Nature Research},
issn = {20411723},
pmid = {40442062},
language = {English},
abbrev_source_title = {Nat. Commun.},
type = {Article}}
@article{Shakeripour2025Object,
author = {Shakeripour, Alireza and Bahmani, Zahra and Aghaomidi, Poorya and Seyed-Allaei, Shima},
title = {A Novel Object Categorization Decoder from fMRI Signals Using Deep Neural Networks},
year = {2025},
journal = {Frontiers in Biomedical Technologies},
volume = {12},
number = {3},
pages = {617 - 626},
doi = {10.18502/fbt.v12i3.19186},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014260606&doi=10.18502%2Ffbt.v12i3.19186&partnerID=40&md5=bb502e5f650887dc34e61e5d3eff281b},
affiliations = {Tarbiat Modares University, Department of Biomedical Engineering, Tehran, Tehran, Iran; Tarbiat Modares University, Department of Cognitive Neuroscience, Tehran, Tehran, Iran; Iranian Research Institute for Fundamental Sciences, School of Cognitive Sciences, Tehran, Tehran, Iran},
abstract = {Purpose: Understanding neural mechanisms is critical for discerning the nature of brain disorders and enhancing treatment methodologies. Functional Magnetic Resonance Imaging (fMRI) plays a vital role in gaining this knowledge by recording various brain regions. In this study, our primary aim was to categorize visual objects based on fMRI data during a natural scene viewing task. We intend to elucidate the challenges and limitations of previous models in order to produce a generalizable model across different subjects using advanced deep-learning methods. Materials and Methods: We’ve designed a new deep-learning model based on transformers for processing fMRI data. The model includes two blocks, the first block receives fMRI data as input and transforms the input data to a set of features called fMRI space. Simultaneously a visual space is extracted from visual images using a pre-trained inceptionv3 network. The model tries to construct the fMRI space similar to the extracted visual space. The other block is a Fully Connected (FC) network for object recognition based on fMRI space. Using transformer capabilities and an overlapping method, the proposed architecture accounts for structural changes across different voxel sizes of the subjects’ brains. Results: A unique model was trained for all subjects with different brain sizes. The results demonstrated that the proposed network achieves an impressive similarity correlation between visual space and fMRI space around 0.86 for train and 0.86 for test dataset. Furthermore, the classification accuracy was about 70.3%. These outcomes underscored the effectiveness of our fMRI transformer network in extracting features from fMRI data. Conclusion: The results indicated the potential of our model for decoding images from the brain activities of new subjects. This unveils a novel direction in image reconstruction from neural activities, an area that has remained relatively uncharted due to its inherent intricacies. © 2025 Tehran University of Medical Sciences. This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International license (https://creativecommons.org/licenses/by-nc/4.0/). Noncommercial uses of the work are permitted, provided the original work is properly cited.}, keywords = {Article; brain disease; brain region; brain size; convolutional neural network; deep learning; deep neural network; electric potential; electroencephalogram; functional magnetic resonance imaging; human; human experiment; normal human; retina image; visual field},
correspondence_address = {Z. Bahmani; Department of Biomedical Engineering, Faculty of Electrical & Computer Engineering, Tarbiat Modares University, Tehran, Iran; email: z.bahmani@modares.ac.ir},
publisher = {Tehran University of Medical Sciences},
issn = {23455837; 23455829},
language = {English},
abbrev_source_title = {Front. Biomed. Technol.},
type = {Article}}
@conference{Shams2025Neurosemantic,
author = {Shams, Siavash and Antonello, Richard J. and Mischler, Gavin and Bickel, Stephan and Mehta, Ashesh D. and Mesgarani, Nima},
title = {Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG},
year = {2025},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
pages = {2920 - 2924},
doi = {10.21437/Interspeech.2025-1010},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020076023&doi=10.21437%2FInterspeech.2025-1010&partnerID=40&md5=eedb666bdb100959693f29e3b451ea13},
affiliations = {The Fu Foundation School of Engineering and Applied Science, New York, NY, United States; The Feinstein Institutes for Medical Research, Northwell, Manhasset, NY, United States},
abstract = {Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves strong performance with as little as 30 minutes of neural data, outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies. © 2025 International Speech Communication Association. All rights reserved.}, keywords = {Biomedical signal processing; Brain; Computational linguistics; Computational neuroscience; Decoding; Electroencephalography; Embeddings; Interfaces (computer); Long short-term memory; Natural language processing systems; Semantics; Speech communication; Brain decoding; Intracranial EEG; Language model; Language processing; Large language model; Natural language processing; Natural languages; Neural signals; Semantic reconstruction; Transfer learning; Brain computer interface},
publisher = {International Speech Communication Association},
issn = {29581796; 19909772; 2308457X; 2958-1796},
isbn = {9781713836902; 9781713820697; 9781605603162; 9781617821233; 9781604234497},
language = {English},
abbrev_source_title = {Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
type = {Conference paper}}
@article{Sharon2025Harnessing,
author = {Sharon, Rini A. and Sur, Mriganka and Murthy, Hema A.},
title = {Harnessing the Multi-Phasal Nature of Speech-EEG for Enhancing Imagined Speech Recognition},
year = {2025},
journal = {IEEE Open Journal of Signal Processing},
volume = {6},
pages = {78 - 88},
doi = {10.1109/OJSP.2025.3528368},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215364624&doi=10.1109%2FOJSP.2025.3528368&partnerID=40&md5=24a57c6dcb3fe2fe9ba3713bdac23f44},
affiliations = {Indian Institute of Technology Madras, Chennai, TN, India; Massachusetts Institute of Technology, Cambridge, MA, United States},
abstract = {Analyzing speech-electroencephalogram (EEG) is pivotal for developing non-invasive and naturalistic brain-computer interfaces. Recognizing that the nature of human communication involves multiple phases like audition, imagination, articulation, and production, this study uncovers the shared cognitive imprints that represent speech cognition across these phases. Regression analysis, using correlation metrics reveal pronounced inter-phasal congruence. This insight promotes a shift from single-phase-centric recognition models to harnessing integrated phase data, thereby enhancing recognition of cognitive speech. Having established the presence of inter-phase associations, a common representation learning feature extractor is introduced, adept at capturing the correlations and replicability across phases. The features so extracted are observed to provide superior discrimination of cognitive speech units. Notably, the proposed approach proves resilient even in the absence of comprehensive multi-phasal data. Through thorough control checks and illustrative topographical visualizations, our observations are substantiated. The findings indicate that the proposed multi-phase approach significantly enhances EEG-based speech recognition, achieving an accuracy gain of 18.2% for 25 cognitive units in continuous speech EEG over models reliant solely on single-phase data. © 2020 IEEE.}, keywords = {Audition; Speech enhancement; Speech recognition; Articulation; BCI; Human communications; Imagination; Phase association; Phase data; Recognition models; Regression; Single phasis; Speech-electroencephalogram correlation; Regression analysis},
correspondence_address = {R. Sharon; Indian Institute of Technology Madras, Chennai, 600036, India; email: evanjilina@gmail.com},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {26441322},
language = {English},
abbrev_source_title = {IEEE Open J. Signal Processing},
type = {Article}}
@conference{Shevchenko2025Comparative,
author = {Shevchenko, Olena and Yeremeieva, Sofiia and Laschowski, Brokoslaw},
title = {Comparative Analysis of Neural Decoding Algorithms for Brain-Machine Interfaces},
year = {2025},
journal = {IEEE International Conference on Rehabilitation Robotics},
pages = {222 - 227},
doi = {10.1109/ICORR66766.2025.11063033},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011136190&doi=10.1109%2FICORR66766.2025.11063033&partnerID=40&md5=f82675d92d21bc27a3b07b4ccae10792},
affiliations = {Ukrainian Catholic University, Department of Computer Science, Lviv, Lviv Oblast, Ukraine; National University of Kyiv-Mohyla Academy, Department of Computer Science, Kyiv, Ukraine; University of Toronto, Toronto, ON, Canada; KITE Research Institute, Toronto, ON, Canada},
abstract = {Accurate neural decoding of brain dynamics remains an open challenge in brain-machine interfaces. While various signal processing, feature extraction, and classification algorithms have been proposed, a systematic comparison of these is lacking. Accordingly, here we conducted one of the largest comparative studies to evaluate different combinations of state-of-the-art algorithms for motor neural decoding in order to find the optimal combination. We studied three signal processing methods (i.e., artifact subspace reconstruction, surface Laplacian filtering, and data normalization), four feature extractors (i.e., common spatial patterns, independent component analysis, short-time Fourier transform, and no feature extraction), and four machine learning classifiers (i.e., support vector machine, linear discriminant analysis, convolutional neural networks, and long short-term memory networks). Using a large-scale EEG dataset, we optimized each combination for individual subjects (i.e., resulting in 672 total experiments) and evaluated performance based on classification accuracy. We also compared the computational and memory storage requirements, which are important for real-time embedded computing. Our comparative analysis provides novel insights that can help inform the design of next-generation neural decoding algorithms for brain-machine interfaces. © 2025 IEEE.},
keywords = {Brain; Brain computer interface; Convolutional neural networks; Decoding; Digital storage; Embedded systems; Extraction; Feature extraction; Image processing; Independent component analysis; Interfaces (materials); Large datasets; Learning systems; Long short-term memory; Support vector machines; Brain dynamics; Classification algorithm; Comparative analyzes; Comparatives studies; Decoding algorithm; Feature extraction algorithms; Feature extraction and classification; Machine interfaces; Neural decoding; Signal-processing; Discriminant analysis; algorithm; artificial neural network; brain; brain computer interface; comparative study; electroencephalography; human; machine learning; physiology; procedures; signal processing; Algorithms; Brain-Computer Interfaces; Electroencephalography; Humans; Machine Learning; Neural Networks, Computer; Signal Processing, Computer-Assisted},
publisher = {IEEE Computer Society},
issn = {19457898; 19457901},
isbn = {9781665488297; 9798350342758; 9781467360241; 9781728127552; 9781538622964; 9781479918072; 9781424498628; 9798350380682},
pmid = {40644105},
language = {English},
abbrev_source_title = {IEEE Int. Conf. Rehabil. Rob.},
type = {Conference paper}}
@article{Shi2025Neuroanatomyinformed,
author = {Shi, Jianting and Wang, Jiaqi and Fei, Weijie and Feleke, A. Genetu and Bi, Luzheng},
title = {Neuroanatomy-Informed Brain–Machine Hybrid Intelligence for Robust Acoustic Target Detection},
year = {2025},
journal = {Cyborg and Bionic Systems},
volume = {6},
pages = {},
doi = {10.34133/cbsystems.0438},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018949192&doi=10.34133%2Fcbsystems.0438&partnerID=40&md5=ffa0fa8b50d200eb3539186aaac96869},
affiliations = {School of Mechanical Engineering, Beijing Institute of Technology, Beijing, China},
abstract = {Sound target detection (STD) plays a critical role in modern acoustic sensing systems. However, existing automated STD methods show poor robustness and limited generalization, especially under low signal-to-noise ratio (SNR) conditions or when processing previously unencountered sound categories. To overcome these limitations, we first propose a brain–computer interface (BCI)-based STD method that utilizes neural responses to auditory stimuli. Our approach features the Triple-Region Spatiotemporal Dynamics Attention Network (Tri-SDANet), an electroencephalogram (EEG) decoding model incorporating neuroanatomical priors derived from EEG source analysis to enhance decoding accuracy and provide interpretability in complex auditory scenes. Recognizing the inherent limitations of stand-alone BCI systems (notably their high false alarm rates), we further develop an adaptive confidence-based brain–machine fusion strategy that intelligently combines decisions from both the BCI and conventional acoustic detection models. This hybrid approach effectively merges the complementary strengths of neural perception and acoustic feature learning. We validate the proposed method through experiments with 16 participants. Experimental results demonstrate that the Tri-SDANet achieves state-of-the-art performance in neural decoding under complex acoustic conditions. Moreover, the hybrid system maintains reliable detection performance at low SNR levels while exhibiting remarkable generalization to unseen target classes. In addition, source-level EEG analysis reveals distinct brain activation patterns associated with target perception, offering neuroscientific validation for our model design. This work pioneers a neuro-acoustic fusion paradigm for robust STD, offering a generalizable solution for real-world applications through the integration of noninvasive neural signals with artificial intelligence. © © 2025 Jianting Shi et al.},
keywords = {Acoustic measuring instruments; Acoustic noise; Acoustics; Activation analysis; Biomedical signal processing; Brain; Brain computer interface; Decoding; Electroencephalography; Error detection; Hybrid systems; Interfaces (computer); Neural networks; Neurophysiology; Acoustic sensing; Acoustic target detection; Detection methods; Generalisation; Hybrid intelligence; Low signal-to-noise ratio; Sensing systems; Sound target; Spatio-temporal dynamics; Targets detection; Complex networks},
correspondence_address = {L. Bi; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, 100081, China; email: bhxblz@bit.edu.cn},
publisher = {American Association for the Advancement of Science},
issn = {26927632; 20971087},
language = {English},
abbrev_source_title = {Cyborg. Bionic. Syst.},
type = {Article}}
@conference{Shimizu2025Reviving,
author = {Shimizu, Shinya and Ota, Airi and Nakane, Ai},
title = {Reviving Intentional Facial Expressions: an Interface for ALS Patients using Brain Decoding and Image-Generative AI},
year = {2025},
pages = {},
doi = {10.1145/3706599.3719775},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005775410&doi=10.1145%2F3706599.3719775&partnerID=40&md5=b4c5a507f519aa6f95e3bab3df552c06},
affiliations = {Nippon Telegraph and Telephone Corporation, Tokyo, Japan},
abstract = {This study proposes an interface to enable non-verbal emotional communication for amyotrophic lateral sclerosis (ALS) patients with impaired face movement. By integrating EEG decoding and image-generative AI, the interface generates facial expressions based on decoding voluntary intentions rather than emotions, addressing privacy concerns. To enhance training data acquisition for decoding 17 action units (AUs), we developed a personalized approach using latent facial expression spaces and six transition animations for imitation. Experiments with participants without ALS validated the proposed method, and subsequent tests with one ALS patient showed an average AU decoding accuracy of 0.20, comparable to participants without ALS (0.23). In real-time generation tests, the ALS patient achieved a correlation of 0.40 between intended and decoded facial expressions, outperforming participants without ALS (0.30). While less accurate than invasive methods, this non-invasive, task-driven approach offers an effective solution for Augmentative and Alternative Communication in patients with severe neurological conditions. © 2025 Copyright held by the owner/author(s).}, keywords = {Neurons; User interfaces; Action Unit; Amyotrophic lateral sclerose; Amyotrophic lateral sclerosis; Brain computer interface; Brain decoding; Brain images; Electroencephalogram; Emotional communications; Facial Expressions; Privacy concerns; Noninvasive medical procedures},
correspondence_address = {S. Shimizu; NTT Corporation, Yokosuka, Kanagawa, Japan; email: shinya.shimizu@ntt.com},
publisher = {Association for Computing Machinery},
isbn = {9798400713958; 9798400713941},
language = {English},
abbrev_source_title = {Conf Hum Fact Comput Syst Proc},
type = {Conference paper}}
@article{Shirakawa2025Spurious,
author = {Shirakawa, Ken and Nagano, Yoshihiro and Tanaka, Misato and Aoki, Shuntaro C. and Muraki, Yusuke and Majima, Kei and Kamitani, Yukiyasu},
title = {Spurious reconstruction from brain activity},
year = {2025},
journal = {Neural Networks},
volume = {190},
pages = {},
doi = {10.1016/j.neunet.2025.107515},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007598357&doi=10.1016%2Fj.neunet.2025.107515&partnerID=40&md5=cd83086d39c85fb2eb7f840c8f0e20c3},
affiliations = {Kyoto University, Department of Informatics, Kyoto, Kyoto, Japan; Advanced Telecommunications Research Institute International (ATR), Kyoto, Kyoto, Japan; Institute for Quantum Life Science, Chiba, Chiba, Japan; Riken, Wako, Saitama, Japan},
abstract = {Advances in brain decoding, particularly in visual image reconstruction, have sparked discussions about the societal implications and ethical considerations of neurotechnology. As reconstruction methods aim to recover visual experiences from brain activity and achieve prediction beyond training samples (zero-shot prediction), it is crucial to assess their capabilities and limitations to inform public expectations and regulations. Our case study of recent text-guided reconstruction methods, which leverage a large-scale dataset (Natural Scenes Dataset, NSD) and text-to-image diffusion models, reveals critical limitations in their generalizability, demonstrated by poor reconstructions on a different dataset. UMAP visualization of the text features from NSD images shows limited diversity with overlapping semantic and visual clusters between training and test sets. We identify that clustered training samples can lead to “output dimension collapse,” restricting predictable output feature dimensions. While diverse training data improves generalization over the entire feature space without requiring exponential scaling, text features alone prove insufficient for mapping to the visual space. Our findings suggest that the apparent realism in current text-guided reconstructions stems from a combination of classification into trained categories and inauthentic image generation (hallucination) through diffusion models, rather than genuine visual reconstruction. We argue that careful selection of datasets and target features, coupled with rigorous evaluation methods, is essential for achieving authentic visual image reconstruction. These insights underscore the importance of grounding interdisciplinary discussions in a thorough understanding of the technology's current capabilities and limitations to ensure responsible development. © 2025 The Authors}, keywords = {Photointerpretation; Brain activity; Brain decoding; Images reconstruction; Natural scenes; Naturalistic approach; Neuroai; Reconstruction method; Training sample; Visual image; Visual image reconstruction; Large datasets; Article; artificial intelligence; brain decoding; electroencephalogram; human; image reconstruction; naturalistic approach; visual field; visual image reconstruction; brain; diagnostic imaging; image processing; physiology; procedures; Brain; Humans; Image Processing, Computer-Assisted},
correspondence_address = {K. Shirakawa; of Informatics, Kyoto University, Kyoto, Sakyo-ku, 606-8501, Japan; email: ken.shirakawa@atr.jp},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
pmid = {40499302},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{Shoura2025Revealing,
author = {Shoura, Moaz and Liang, Yong Z. and Sama, Marco Agazio and De, Arijit and Nestor, Adrian},
title = {Revealing the neural representations underlying other-race face perception},
year = {2025},
journal = {Frontiers in Human Neuroscience},
volume = {19},
pages = {},
doi = {10.3389/fnhum.2025.1543840},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000475153&doi=10.3389%2Ffnhum.2025.1543840&partnerID=40&md5=e05220bd193c9a276a5c1500455b3498},
affiliations = {University of Toronto, Department of Psychology (Scarborough), Toronto, ON, Canada},
abstract = {The other-race effect (ORE) refers to poorer recognition for faces of other races than one’s own. This study investigates the neural and representational basis of ORE in East Asian and White participants using behavioral measures, neural decoding, and image reconstruction based on electroencephalography (EEG) data. Our investigation identifies a reliable neural counterpart of ORE, with reduced decoding accuracy for other-race faces, and it relates this result to higher density of other-race face representations in face space. Then, we characterize the temporal dynamics and the prominence of ORE for individual variability at the neural level. Importantly, we use a data-driven image reconstruction approach to reveal visual biases underlying other-race face perception, including a tendency to perceive other-race faces as more typical, younger, and more expressive. These findings provide neural evidence for a classical account of ORE invoking face space compression for other-race faces. Further, they indicate that ORE involves not only reduced identity information but also broader, systematic distortions in visual representation with considerable cognitive and social implications. © © 2025 Shoura, Liang, Sama, De and Nestor.}, keywords = {adult; article; Caucasian; cognition; compression; controlled study; decoding; diagnosis; East Asian; electroencephalogram; electroencephalography; facial recognition; female; human; human experiment; image reconstruction; male; normal human; race},
correspondence_address = {A. Nestor; Department of Psychology at Scarborough, University of Toronto, Toronto, Canada; email: adrian.nestor@utoronto.ca},
publisher = {Frontiers Media SA},
issn = {16625161},
language = {English},
abbrev_source_title = {Front. Human Neurosci.},
type = {Article}}
@article{Song2025Exploring,
author = {Song, Yanhui and Yu, Ye},
title = {Exploring the EEG representation of English listening comprehension under hypoxic conditions},
year = {2025},
journal = {Frontiers in Neuroscience},
volume = {19},
pages = {},
doi = {10.3389/fnins.2025.1540539},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009622764&doi=10.3389%2Ffnins.2025.1540539&partnerID=40&md5=bc96d52c8a126ccbf99d48faa1ed7201},
affiliations = {Pingdingshan University, Pingdingshan, Henan, China; Dazhou Vocational and Technical College, Dazhou, Sichuan, China},
abstract = {Introduction: Understanding the impact of hypoxic conditions on cognitive functions, including English listening comprehension, has garnered increasing attention due to its implications for high-altitude education and cognitive resilience. Traditional research in this domain has often relied on behavioral assessments or simple physiological metrics, which lack the granularity to capture the neural underpinnings of cognitive performance. Methods: This study proposes a novel framework combining electroencephalography (EEG)-based neural decoding with the Dynamic Linguistic Enhancement Model (DLEM) to investigate English listening comprehension in hypoxic environments. DLEM integrates adaptive vocabulary acquisition, grammar contextualization, and cultural embedding, leveraging EEG to provide real-time, personalized insights into linguistic processing. Results: The experimental results demonstrate significant improvements in comprehension accuracy and cognitive load management, particularly under adaptive curriculum strategies outlined by the Contextual Augmented Learning Strategy (CALS). Discussion: By bridging physiological responses with advanced educational methodologies, this work contributes a scalable and flexible approach to enhancing cognitive performance under hypoxia, aligning with the goals of understanding both physiological and pathological responses to high-altitude conditions. © © 2025 Song and Yu.}, keywords = {Article; cognitive load; comprehension; Contextual Augmented Learning Strategy; curriculum; Dynamic Linguistic Enhancement Model; electroencephalography; english listening comprehension; grammar; human; hypoxia; language development; learning; linguistics; mathematical analysis; stochastic model; vocabulary},
correspondence_address = {Y. Song; Pingdingshan University, Pingdingshan, China; email: isbo1s@163.com},
publisher = {Frontiers Media SA},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@article{Song2025Recognizing,
author = {Song, Yonghao and Wang, Yijun and He, Huiguang and Gao, Xiaorong},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
title = {Recognizing Natural Images From EEG With Language-Guided Contrastive Learning},
year = {2025},
volume = {36},
number = {9},
pages = {15896--15910},
abstract = {Electroencephalography (EEG), known for its convenient noninvasive acquisition but moderate signal-to-noise ratio, has recently gained much attention due to the potential to decode image information. However, previous works have not delivered sufficient evidence of this task, primarily limited by performance and biological plausibility. In this work, we first introduce a self-supervised framework to demonstrate the feasibility of recognizing images from EEG signals. Contrastive learning is leveraged to align the representations of EEG responses with image stimuli. Then, language descriptions of the stimuli generated by large language models (LLMs) help guide learning core semantic information. With the framework, we attain significantly above-chance results on the THINGS-EEG2 dataset, achieving a top-1 accuracy of 19.7% and a top-5 accuracy of 51.5% in challenging 200-way zero-shot tasks. Furthermore, we conduct thorough experiments to resolve the human visual responses with EEG from temporal, spatial, spectral, and semantic perspectives. These results provide evidence of feasibility and plausibility regarding EEG-based image recognition, substantiated by comparative studies with the THINGS-Magnetoencephalography (MEG) dataset. The findings offer valuable insights for neural decoding and real-world applications of brain-computer interfaces (BCIs), such as health care and robot control. The code is available at https://github.com/eeyhsong/NICE-LLM.},
keywords = {Electroencephalography;Visualization;Decoding;Contrastive learning;Image recognition;Semantics;Functional magnetic resonance imaging;Brain modeling;Training;Biological information theory;Brain-computer interface (BCI);contrastive learning;electroencephalography (EEG);image recognition;large language model;magnetoencephalography (MEG)},
doi = {10.1109/TNNLS.2025.3562743},
issn = {2162-2388},
month = {Sep.}}
@article{Su2025Systematic,
author = {Su, Ke and Tian, Liang},
title = {Systematic review: progress in EEG-based speech imagery brain-computer interface decoding and encoding research},
year = {2025},
journal = {PeerJ Computer Science},
volume = {11},
pages = {},
doi = {10.7717/peerj-cs.2938},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009251978&doi=10.7717%2Fpeerj-cs.2938&partnerID=40&md5=b7f74f78036429ef6bd3e04d2bd9b66c},
affiliations = {Qilu University of Technology, School of Arts and Design, Jinan, Shandong, China; Qilu University of Technology, Department of Computer Science and Technology, Jinan, Shandong, China},
abstract = {This article systematically reviews the latest developments in electroencephalogram (EEG)-based speech imagery brain-computer interface (SI-BCI). It explores the brain connectivity of SI-BCI and reveals its key role in neural encoding and decoding. It analyzes the research progress on vowel-vowel and vowel-consonant combinations, as well as Chinese characters, words, and long-words speech imagery paradigms. In the neural encoding section, the preprocessing and feature extraction techniques for EEG signals are discussed in detail. The neural decoding section offers an in-depth analysis of the applications and performance of machine learning and deep learning algorithms. Finally, the challenges faced by current research are summarized, and future directions are outlined. The review highlights that future research should focus on brain region mechanisms, paradigms innovation, and the optimization of decoding algorithms to promote the practical application of SI-BCI technology. © 2025 Su and Tian}, keywords = {Biomedical signal processing; Brain; Decoding; Deep neural networks; Electroencephalography; Encoding (symbols); Learning algorithms; Learning systems; Linguistics; Signal encoding; Speech communication; Speech recognition; Brain regions; Connectivity of brain region; Decoding algorithm; Deep learning; Encoding technology; Machine-learning; Neural decoding; Neural decoding algorithm; Neural encoding; Neural encoding technology; Speech imagery; Speech imagery brain-computer interface paradigm; Brain computer interface},
correspondence_address = {K. Su; School of Art and Design, Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; email: coco_su0716@163.com},
publisher = {PeerJ Inc.},
issn = {23765992},
language = {English},
abbrev_source_title = {PeerJ Comput. Sci.},
type = {Review}}
@article{Tian2025Decoding,
author = {Tian, Bohao and Zhang, Shijun and Xue, Dinghao and Chen, Sirui and Zhang, Yuru and Peng, Kaiping and Wang, Dangxiao},
title = {Decoding Intrinsic Fluctuations of Engagement From EEG Signals During Fingertip Motor Tasks},
year = {2025},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {33},
pages = {1271 - 1283},
doi = {10.1109/TNSRE.2025.3551819},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000636073&doi=10.1109%2FTNSRE.2025.3551819&partnerID=40&md5=bdf428defaaf16b91cacd751a1a9ae60},
affiliations = {Beihang University, Beijing, China; Tsinghua University, Department of Psychology, Beijing, China; Beihang University, School of Mechanical Engineering and Automation, Beijing, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China},
abstract = {Maintaining a high mental engagement is critical for motor rehabilitation interventions. Achieving a flow experience, often conceptualized as a highly engaged mental state, is an ideal goal for motor rehabilitation tasks. This paper proposes a virtual reality-based fine fingertip motor task in which the difficulty is maintained to match individual abilities. The aim of this study is to decode the intrinsic fluctuations of flow experience from electroencephalogram (EEG) signals during the execution of a motor task, addressing a gap in flow research that overlooks these fluctuations. To resolve the conflict between sparse self-reported flow sampling and the high dimensionality of neural signals, we use motor behavioral measures to represent flow and label the EEG data, thereby increasing the number of samples. A machine learning-based neural decoder is then established to classify each trial into high-flow or low-flow using spectral power and coherence features extracted from the EEG signals. Cross-validation reveals that the classification accuracy of the neural decoder can exceed 80%. Notably, we highlight the contributions of high-frequency bands in EEG activities to flow decoding. Additionally, EEG feature analyses reveal significant increases in the power of parietal-occipital electrodes and global coherence values, specifically in the alpha and beta bands, during high-flow durations. This study validates the feasibility of decoding the intrinsic flow fluctuations during fine motor task execution with a high accuracy. The methodology and findings in this work lay a foundation for future applications in manipulating flow experience and enhancing engagement levels in motor rehabilitation practice. © 2025 The Authors.}, keywords = {Adversarial machine learning; Amplitude modulation; Arthroplasty; Breath controlled devices; Electrotherapeutics; Neurons; Virtualization; Electroencephalogram signals; Engagement; Flow experience; Flow researches; High flow; Intrinsic fluctuations; Machine-learning; Mental state; Motor rehabilitation; Motor tasks; Electroencephalography; adult; area under the curve; Article; behavior; cross validation; data accuracy; data processing; decoding Intrinsic fluctuation; electroencephalogram; female; fingertip motor task; Fourier transform; human; human experiment; Likert scale; machine learning; male; measurement precision; medical parameters; motor performance; neural decoding; recall; receiver operating characteristic; self concept; spatial analysis; algorithm; brain computer interface; electroencephalography; finger; movement (physiology); physiology; procedures; psychomotor performance; reproducibility; signal processing; virtual reality; young adult; Adult; Algorithms; Brain-Computer Interfaces; Female; Fingers; Humans; Machine Learning; Male; Motor Skills; Movement; Psychomotor Performance; Reproducibility of Results; Signal Processing, Computer-Assisted; Virtual Reality; Young Adult},
correspondence_address = {D. Wang; Beihang University, State Key Laboratory of Virtual Reality Technology and Systems, the School of Mechanical Engineering and Automation, Beijing, 100191, China; email: hapticwang@buaa.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {40095842},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Veronese2025Optimized,
author = {Veronese, Lorenzo and Moglia, Andrea and Pecco, Nicolò and Anthony Della Rosa, Pasquale and Scifo, Paola and Mainardi, Luca Tommaso and Cerveri, Pietro},
title = {Optimized AI-based neural decoding from BOLD fMRI signal for analyzing visual and semantic ROIs in the human visual system},
year = {2025},
journal = {Journal of Neural Engineering},
volume = {22},
number = {4},
pages = {},
doi = {10.1088/1741-2552/adfbc2},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014157960&doi=10.1088%2F1741-2552%2Fadfbc2&partnerID=40&md5=a49fd5b7d4d17a7c654076edcea2475b},
affiliations = {Politecnico di Milano, Information and Bioengineering, Milan, MI, Italy; IRCCS Ospedale San Raffaele, Department of Neuroradiology, Milan, MI, Italy; IRCCS Ospedale San Raffaele, Department of Nuclear Medicine, Milan, MI, Italy; Università degli Studi di Pavia, Department of Industrial and Information Engineering, Pavia, PV, Italy},
abstract = {Objective. AI-based neural decoding reconstructs visual perception by leveraging generative models to map brain activity measured through functional magnetic resonance imaging (fMRI) into the observed visual stimulus. Approach. Traditionally, ridge linear models transform fMRI into a latent space, which is then decoded using variational autoencoders (VAE) or LDMs. Owing to the complexity and noisiness of fMRI data, newer approaches split the reconstruction into two sequential stages, the first one providing a rough visual approximation using a VAE, the second one incorporating semantic information through the adoption of LDM guided by contrastive language-image pre-training (CLIP) embeddings. This work addressed some key scientific and technical gaps of the two-stage neural decoding by: (1) implementing a gated recurrent unit-based architecture to establish a non-linear mapping between the fMRI signal and the VAE latent space, (2) optimizing the dimensionality of the VAE latent space, (3) systematically evaluating the contribution of the first reconstruction stage, and (4) analyzing the impact of different brain regions of interest (ROIs) on reconstruction quality. Main results. Experiments on the NSD, containing 73 000 unique natural images, along with fMRI of eight subjects, demonstrated that the proposed architecture maintained competitive performance while reducing the complexity of its first stage by 85%. The sensitivity analysis showcased that the first reconstruction stage is essential for preserving high structural similarity in the final reconstructions. Restricting analysis to semantic ROIs, while excluding early visual areas, diminished visual coherence, preserving semantics though. The inter-subject repeatability across ROIs was about 92% and 98% for visual and sematic metrics, respectively. Significance. This study represents a key step toward optimized neural decoding architectures leveraging non-linear models for stimulus prediction. Sensitivity analysis highlighted the interplay between the two reconstruction stages, while ROI-based analysis provided strong evidence that the two-stage AI model reflects the brain’s hierarchical processing of visual information. © 2025 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Brain; Brain mapping; Contrastive Learning; Decoding; Image enhancement; Image reconstruction; Latent semantic analysis; Neurons; Semantics; Vision; Auto encoders; Functional magnetic resonance imaging; Functional magnetic resonance imaging imagery; Generative artificial intelligence; Human Visual System; Neural decoding; Region-of-interest; Regions of interest; Sensitivity analyzes; Visual stimuls; Sensitivity analysis; Visual languages; adult; Article; artificial intelligence; BOLD signal; brain blood flow; brain region; deep neural network; electroencephalogram; functional magnetic resonance imaging; Gaussian noise; human; human experiment; image reconstruction; mean absolute error; mean squared error; normal human; ridge regression; semantics; statistical model; visual information; visual stimulation; visual system; brain; brain mapping; diagnostic imaging; female; male; nuclear magnetic resonance imaging; photostimulation; physiology; procedures; vision; visual cortex; young adult; Adult; Artificial Intelligence; Brain Mapping; Female; Humans; Magnetic Resonance Imaging; Male; Photic Stimulation; Visual Cortex; Visual Perception; Young Adult},
correspondence_address = {L. Veronese; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; email: lorenzo.veronese@polimi.it; P. Cerveri; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; email: pietro.cerveri@unipv.it},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {40812356},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@conference{Xiao2025Data,
author = {Xiao, Ruoxuan and Guo, Yuzhu},
title = {Data Augmentation for EEG-based SSVEP Decoding},
year = {2025},
pages = {1581 - 1586},
doi = {10.1109/ISCAIT64916.2025.11010622},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010196379&doi=10.1109%2FISCAIT64916.2025.11010622&partnerID=40&md5=0b2860871f7a636b504a4ae3d0fa344b},
affiliations = {Chang'an University, Xi'an, Shaanxi, China; Beihang University, School of Automation Science and Electrical Engineering, Beijing, China},
abstract = {Steady-state visual evoked potential (SSVEP)based neural decoding (BCIs) play a crucial role in brain-computer interfaces (BCIs). However, traditional machine learning algorithms often face SSVEP decoding stability challenges due to the inherent multi-class small-sample pattern recognition, along with external factors such as noise interference or subject fatigue, resulting in unsatisfied performance. To solve this limitation, a data augmented decoding algorithm is proposed by applying Gaussian noise to SSVEP signals to enhance the robustness of traditional machine learning-based SSVEP decoding algorithms. This method was evaluated on selected subjects with initially low decoding accuracy in the public SSVEP dataset, showing significant performance improvement, with an average accuracy increase of 8.33%. The proposed approach effectively improves the robustness of SSVEP decoding with a straightforward manner and facilitates the broader application of SSVEP in practical BCIs. © 2025 IEEE.}, keywords = {Bioelectric potentials; Decoding; Electroencephalography; Electrophysiology; Gaussian noise (electronic); Interface states; Interfaces (computer); Learning algorithms; Learning systems; Machine learning; Pattern recognition; Signal processing; Brain-computer interface; Data augmentation; Decoding algorithm; Discriminant component analysis; Neural decoding; Performance; Steady-state visual evoked potential; Steady-state visual evoked potentials; Task-discriminant component analyze; Brain computer interface},
correspondence_address = {Y. Guo; Beihang University, School of Automation Science and Electrical Engineering, Beijing, 100191, China; email: yuzhuguo@buaa.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9798331542856},
language = {English},
abbrev_source_title = {Int. Symp. Comput. Appl. Inf. Technol., ISCAIT},
type = {Conference paper}}
@article{Xiong2025Interpretable,
author = {Xiong, Daowen and Hu, Liangliang and Jin, Jiahao and Ding, Yikang and Tan, Congming and Zhang, Jing and Tian, Yin},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
title = {Interpretable Cross-Modal Alignment Network for EEG Visual Decoding With Algorithm Unrolling},
year = {2025},
volume = {36},
number = {11},
pages = {19894--19908},
abstract = {Accurate decoding in electroencephalography (EEG) technology, particularly for rapid visual stimuli, remains challenging due to the low signal-to-noise ratio (SNR). Additionally, existing neural networks struggle with issues related to generalization and interpretability. This article proposes a cross-modal aligned network, E2IVAE, which leverages shared information from multiple modalities for self-supervised alignment of EEG to images for extracting visual perceptual information and features a novel EEG encoder, ISTANet, based on algorithm unrolling. This network framework significantly enhances the accuracy and stability of EEG decoding for object recognition in novel classes while reducing the extensive neural data typically required for training neural decoders. The proposed ISTANet employs algorithm unrolling to transform the multilayer sparse coding algorithm into an end-to-end format, extracting features from noisy EEG signals while incorporating the interpretability of traditional machine learning. The experimental results demonstrate that our method achieves SOTA top-1 accuracy of 62.39% and top-5 accuracy of 88.98% on a comprehensive rapid serial visual presentation (RSVP) dataset for public comparison in a 200-class zero-shot neural decoding task. Additionally, ISTANet enables visualization and analysis of multiscale atom features and overall reconstruction features, exploring biological plausibility across temporal, spatial, and spectral dimensions. On another more challenging RSVP large-scale dataset, the proposed framework also achieves significantly above chance-level performance, proving its robustness and generalization. This research provides critical insights into neural decoding and brain–computer interfaces (BCIs) within the fields of cognitive science and artificial intelligence.},
keywords = {Electroencephalography;Decoding;Feature extraction;Visualization;Brain modeling;Training;Encoding;Image reconstruction;Object recognition;Data mining;Algorithm unrolling;brain–computer interfaces (BCIs);electroencephalography (EEG) decoding;interpretability},
doi = {10.1109/TNNLS.2025.3592646},
issn = {2162-2388},
month = {Nov}}
@article{Xu2025Targeting,
author = {Xu, Jiahua and Liu, Yunzhe},
title = {Targeting replay and default mode network dynamics during rest in psychiatric disorders},
year = {2025},
journal = {Current Opinion in Behavioral Sciences},
volume = {65},
pages = {},
doi = {10.1016/j.cobeha.2025.101582},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012390121&doi=10.1016%2Fj.cobeha.2025.101582&partnerID=40&md5=263005063c613106fa0f0475fafd42b5},
affiliations = {Peking University, Psychiatry Research Center, Beijing, China; Beijing Normal University, Beijing, China; Chinese Institute for Brain Research, Beijing, Beijing, China},
abstract = {The default mode network (DMN), a hallmark of spontaneous brain activity during rest, is altered in various psychiatric disorders and linked to cognitive deficits. However, the underlying mechanisms remain unclear, and the development of targeted treatments has been challenging. Historically, resting-state research in psychiatry and task-based studies in cognitive neuroscience have progressed independently, limiting the translation of mechanistic insights into clinical practice. Recent advances in neural decoding methods, specifically those linking hippocampal replay to DMN activity during rest, have begun to bridge this gap. By probing the representations of resting brain activity in psychopathology research, we can more precisely characterise DMN-related dysfunction and uncover symptom-relevant mechanisms. These developments also lay the groundwork for targeted interventions: noninvasive neuromodulation techniques, such as transcranial focused ultrasound, can be used to test causal hypotheses and optimise therapeutic strategies. Together, these innovations offer a more integrated framework for understanding the DMN and improving psychiatric outcomes. © 2025 Elsevier Ltd},
keywords = {adult; clinical practice; cognition; cognitive defect; default mode network; drug therapy; electroencephalogram; focused ultrasound therapy; hippocampus; human; male; mental disease; neuromodulation; rest; review},
publisher = {Elsevier Ltd},
issn = {23521546; 23521554},
language = {English},
abbrev_source_title = {Curr. Opin. Behav. Sci.},
type = {Review}}
@article{Xu2025Brainvision,
author = {Xu, Ting and Yu, Lianzhi and Zheng, Yongwei and Huang, Shuai},
title = {BrainVision: Cross-domain EEG decoding for visual content retrieval and reconstruction},
year = {2025},
journal = {Neuroscience},
volume = {584},
pages = {190 - 205},
doi = {10.1016/j.neuroscience.2025.07.047},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013792683&doi=10.1016%2Fj.neuroscience.2025.07.047&partnerID=40&md5=93e752eb9da3c8e30876380bf3397356},
affiliations = {University of Shanghai for Science and Technology, Shanghai, Shanghai, China},
abstract = {Understanding human visual intent through brain signals remains a fundamental challenge in neuroscience and artificial intelligence. Despite recent advances in brain decoding, existing approaches typically operate within isolated datasets and modalities, limiting their generalization capabilities. This paper introduces BrainVision, a novel framework that bridges visual recognition and emotional EEG datasets to enable comprehensive visual content generation through cross-domain learning. BrainVision addresses the critical challenge of leveraging complementary information across heterogeneous EEG sources by implementing a unified cross-domain alignment strategy. Our framework maps neural patterns from the THINGS-EEG visual recognition dataset and the DEAP emotional response dataset into a shared representation space, enabling three distinct visual output capabilities: (1) accurate content retrieval and classification, (2) detailed linguistic descriptions through adapter-enhanced large language models, and (3) high-fidelity image reconstruction via stable diffusion models. Experimental results demonstrate that BrainVision significantly outperforms single-domain approaches, achieving a 15.3% increase in retrieval accuracy and a 12.7% improvement in structural similarity for reconstructed images compared to state-of-the-art methods. Furthermore, our framework demonstrates robust zero-shot generalization, maintaining 82% of its performance when applied to novel stimuli not seen during training. The multi-modal outputs provide complementary interpretations of neural activity, offering a more comprehensive understanding of visual intent. Our findings establish that integrating diverse neural datasets substantially enhances the capabilities of brain decoding systems, providing a promising direction for developing more intuitive and versatile brain-computer interfaces. BrainVision represents an important step toward bridging the gap between neural activity and rich visual experiences across different cognitive domains. © 2025 International Brain Research Organization (IBRO)}, keywords = {Article; artificial intelligence; brain signal processing; brain vision; Brain2Image; caption generation quality; cognition; cross domain knowledge transfer; cross domain learning; cross domain transfer analysis; data analysis; data processing; dataset configuration analysis; decoding; electroencephalogram; emotion; event related potential; human; image reconstruction; information processing; information retrieval; inter subject variability analysis; knowledge; large language model; learning; learning algorithm; multi modal visual output generation; multi-task learning; nerve cell network; neuroimaging; signal processing; THINGS-EEG dataset; transfer of learning; visual adaptation; visual content retrieval; visual memory; adult; brain; electroencephalography; female; male; physiology; procedures; vision; Adult; Brain; Electroencephalography; Emotions; Female; Humans; Male; Visual Perception},
correspondence_address = {T. Xu; University of Shanghai for Science and Technology, Shanghai, 200093, China; email: 233370909@st.usst.edu.cn},
publisher = {Elsevier Ltd},
issn = {18737544; 03064522},
coden = {NRSCD},
pmid = {40816597},
language = {English},
abbrev_source_title = {Neuroscience},
type = {Article}}
@article{Yang2025Reactivating,
author = {Yang, Can and He, Xianhui and Cai, Ying},
title = {Reactivating and reorganizing activity-silent working memory: two distinct mechanisms underlying pinging the brain},
year = {2025},
journal = {Cerebral Cortex},
volume = {35},
number = {2},
pages = {},
doi = {10.1093/cercor/bhae494},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217844323&doi=10.1093%2Fcercor%2Fbhae494&partnerID=40&md5=db50fe663be7ed0d102081ef7333a85b},
affiliations = {Department of Psychology and Behavioral Sciences, Zhejiang University, Hangzhou, Zhejiang, China},
abstract = {Recent studies have proposed that visual information in working memory (WM) can be maintained in an activity-silent state and reactivated by task-irrelevant high-contrast visual impulses (“ping”). Although pinging the brain has become a popular tool for exploring activity-silent WM, its underlying mechanisms remain unclear. In the current study, we directly compared the neural reactivation effects and behavioral consequences of spatial-nonmatching and spatial-matching pings to distinguish the noise-reduction and target-interaction hypotheses of pinging the brain. Initially, in an electroencephalogram study, our neural decoding results showed that spatial-nonmatching pings reactivated activity-silent WM transiently without changing the original WM representations or recall performance. Conversely, spatial-matching pings reactivated activity-silent WM more durably and further reorganized WM information by decreasing neural representations’ dynamics. Notably, only the reactivation strength of spatial-matching pings correlated with recall performance and was modulated by the location of memorized items, with neural reactivation occurring only when both items and pings were presented horizontally. Consistently, in a follow-up behavioral study, we found that only spatial-matching, horizontal pings impaired recall performance compared to no ping. Together, our results demonstrated two distinct mechanisms underlying pinging the brain, highlighting the critical role of the ping’s context (i.e. spatial information) in reactivating and reorganizing activity-silent WM. © The Author(s) 2025. Published by Oxford University Press. All rights reserved.}, keywords = {adult; article; brain; controlled study; electroencephalogram; electroencephalography; follow up; human; human experiment; middle aged; noise reduction; normal human; recall; visual information; working memory; young adult; female; male; photostimulation; physiology; procedures; short term memory; vision; Adult; Brain; Electroencephalography; Female; Humans; Male; Memory, Short-Term; Mental Recall; Photic Stimulation; Visual Perception; Young Adult},
correspondence_address = {Y. Cai; Department of Psychology and Behavioral Sciences, Zhejiang University, Hangzhou, No. 388 Yuhangtang Road, Zhejiang, 310058, China; email: yingcai@zju.edu.cn},
publisher = {Oxford University Press},
issn = {14602199; 10473211},
coden = {CECOE},
pmid = {39756434},
language = {English},
abbrev_source_title = {Cereb. Cortex},
type = {Article}}
@inproceedings{Yang2025Ieee,
author = {Yang, Junkai and Onken, Arno},
booktitle = {2025 IEEE 35th International Workshop on Machine Learning for Signal Processing (MLSP)},
title = {On the Role of Low-Level Visual Features in EEG-Based Image Reconstruction},
year = {2025},
volume = {},
number = {},
pages = {1--6},
abstract = {Decoding visual stimuli from electroencephalography (EEG) has gained increasing attention in recent years due to EEG's affordability, portability, and high temporal resolution. State-of-the-art approaches typically map EEG signals to Contrastive Language Image Pre-training (CLIP) embeddings via contrastive learning, followed by image reconstruction using conditional diffusion models. These methods were originally developed for other imaging techniques and mainly focus on reconstructing high-level semantic information, often neglecting low-level visual features such as color, texture, and posture, especially when applied to EEG signals. In this study, we explore a two-level decoding framework designed to incorporate low-level visual information alongside highlevel representations. Compared to the Adaptive Thinking Mapper (ATM) baseline, the current state-of-the-art method for EEG decoding, our model demonstrates improved recovery of coarse information from EEG but continues to struggle with fine-grained details and semantic completeness. Furthermore, integrating the low-level and high-level signals into a stable diffusion technique revealed improved preservation of the low-level details and degradation of semantic coherence. These findings underscore the need for more effective low-level modeling and suggest that further exploration of the two-level decoding framework is required to better leverage EEG signals for image reconstruction. Code: https://github.com/jyang635/EEG_decoding.git},
keywords = {Measurement;Visualization;Adaptation models;Semantics;Coherence;Brain modeling;Electroencephalography;Decoding;Image reconstruction;Signal resolution;Brain Decoding;Visual Decoding;Electroencephalogram;Low-level visual features},
doi = {10.1109/MLSP62443.2025.11204210},
issn = {2161-0371},
month = {Aug}}
@article{Yu2025Spatiospectral,
author = {Yu, Haitao and Lin, Zaidong and Li, Fan and Liu, Jialin and Liu, Chen and Wang, Jiang},
title = {Spatiospectral Representation and Neural Decoding of Somatic Perception of Acupuncture Stimulations},
year = {2025},
journal = {IEEE Journal of Biomedical and Health Informatics},
pages = {},
doi = {10.1109/JBHI.2025.3601173},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013976538&doi=10.1109%2FJBHI.2025.3601173&partnerID=40&md5=92e2b4bd3cdf2c039815f94348b121bf},
affiliations = {Tianjin University, Tianjin, China; Tianjin University of Traditional Chinese Medicine, Second Teaching Hospital, Tianjin, Tianjin, China; National Clinical Research Center for Chinese Medicine Acupuncture and Moxibustion, Tianjin, Tianjin, China},
abstract = {Characterizing the neural representations underlying somatic perception is crucial for neural decoding of external stimulations. Acupuncture is an important therapeutic method of traditional Chinese medicine and can effectively modulate brain activity for the treatment of neural diseases. In this work, we investigate the neural representations based on the power spectral density (PSD) estimated from electroencephalogram (EEG) across the whole brain with deep learning. Frequency and spatial characteristics of PSD can reliably represent the dynamical brain responses to acupuncture with different manipulations, manifesting enhanced alpha power in parietal lobe. By removing aperiodic components, periodic spatial spectrum shows a higher representation ability of different brain states during acupuncture stimulations, and twiring-rotating (TR) manipulation have a more pronounced modulatory effect than lifting-thrusting (LT) manipulation. Moreover, we further infer the low-dimensional feature-disentangled representations with generative adversarial network (GAN), i.e., w-latents of StyleGAN, which can capture the latent features of periodic spatial spectrum and strike a balance between separability and generalizability. The effectiveness of feature-disentangled representations is evaluated by decoding the acupuncture states, which can achieve a highest accuracy of 95.71% with Transformer classifier. Compared with high-dimensional spatial spectrum, low-dimensional latent features can best characterize different brain states, indicating a precise representation of somatic perception of acupuncture stimulations. Taken together, our results highlight the significant role of spatial spectral representation underlying somatic perception and serve as an important benchmark for the evaluation of acupuncture effect on human brain. © 2013 IEEE.}, keywords = {Brain; Brain models; Classification (of information); Decoding; Deep neural networks; Electroacupuncture; Neurons; Power spectral density; Power spectrum; Acupuncture stimulation; Adversarial networks; Brain state; External stimulation; Low dimensional; Neural decoding; Neural representations; Power spectra density; Power spectral; Spatial spectra; Electroencephalography},
correspondence_address = {J. Wang; Tianjin University, School of Electrical and Information Engineering, Tianjin, 300072, China; email: jiangwang@tju.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21682208; 21682194},
coden = {ITIBF},
language = {English},
abbrev_source_title = {IEEE J. Biomedical Health Informat.},
type = {Article}}
@article{Yu2025Acoustic,
author = {Yu, Luodi and Ban, Lizhi and Yi, Aiwen and Xin, Jing and Li, Suping and Wang, Suiping and Mottron, Laurent},
title = {Acoustic Exaggeration Enhances Speech Discrimination in Young Autistic Children},
year = {2025},
journal = {Autism Research},
volume = {18},
number = {2},
pages = {402 - 414},
doi = {10.1002/aur.3301},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213561721&doi=10.1002%2Faur.3301&partnerID=40&md5=74d066c1c56272749179afae8a2d9e0e},
affiliations = {Guangzhou University, Center for Autism Research, Guangzhou, Guangdong, China; South China Normal University, Philosophy and Social Science Laboratory of Reading and Development in Children and Adolescents, Guangzhou, Guangdong, China; Guangzhou Medical University, Department of Pediatrics, Guangzhou, Guangdong, China; Guangzhou University of Chinese Medicine, Foshan Clinical Medical School, Guangzhou, Guangdong, China; University of Montreal, Department of Psychiatry and Addictology, Montreal, QC, Canada},
abstract = {Child-directed speech (CDS), which amplifies acoustic and social features of speech during interactions with young children, promotes typical phonetic and language development. In autism, both behavioral and brain data indicate reduced sensitivity to human speech, which predicts absent, decreased, or atypical benefits of exaggerated speech signals such as CDS. This study investigates the impact of exaggerated fundamental frequency (F0) and voice-onset time on the neural processing of speech sounds in 22 Chinese-speaking autistic children aged 2–7 years old with a history of speech delays, compared with 25 typically developing (TD) peers. Electroencephalography (EEG) data were collected during passive listening to exaggerated and non-exaggerated syllables. A time-resolved multivariate pattern analysis (MVPA) was used to evaluate the potential effects of acoustic exaggeration on syllable discrimination in terms of neural decoding accuracy. For non-exaggerated syllables, neither the autism nor the TD group achieved above-chance decoding accuracy. In contrast, for exaggerated syllables, both groups achieved above-chance decoding, indicating significant syllable discrimination, with no difference in accuracy between the autism and TD groups. However, the temporal generalization patterns in the MVPA results revealed distinct neural mechanisms supporting syllable discrimination between the groups. Although the TD group demonstrated a left-hemisphere advantage for decoding and generalization, the autism group displayed similar decoding patterns between hemispheres. These findings highlight the potential of selective acoustic exaggeration to support speech learning in autistic children, underscoring the importance of tailored, sensory-based interventions. © 2024 The Author(s). Autism Research published by International Society for Autism Research and Wiley Periodicals LLC.}, keywords = {Acoustic exaggeration; area under the curve; Article; auditory system function; autism; child; Childhood Autism Rating Scale; chromosome aberration; decoding; DSM-5; electroencephalogram; electroencephalography; female; human; language development; learning; left hemisphere; male; prediction; right hemisphere; sensory evoked potential; speech discrimination; univariate analysis; visual evoked potential; voice onset time; waveform; complication; pathophysiology; phonetics; physiology; preschool child; procedures; speech; speech perception; Autistic Disorder; Child; Child, Preschool; Electroencephalography; Female; Humans; Male; Phonetics; Speech; Speech Acoustics; Speech Perception},
correspondence_address = {L. Yu; Center for Autism Research, School of Education, Guangzhou University, Guangzhou, China; email: yuluodi@gzhu.edu.cn; L. Mottron; Psychiatry and Addictology Department, CIUSSS-NIM Research Center, University of Montreal, Montreal, Canada; email: laurent.mottron@umontreal.ca},
publisher = {John Wiley and Sons Inc},
issn = {19393792; 19393806},
pmid = {39731320},
language = {English},
abbrev_source_title = {Autism Res.},
type = {Article}}
@article{Yu2025Trait,
author = {Yu, Qianqian and Luo, Yuejia and Dolan, Ray Joseph and Ou, Jianxin and Huang, Chuwen and Wang, Haiteng and Xiao, Zhibing and Nour, Matthew M. and Liu, Yunzhe},
title = {Trait anxiety is associated with reduced reward-related replay at rest},
year = {2025},
journal = {Nature Communications},
volume = {16},
number = {1},
pages = {},
doi = {10.1038/s41467-025-63281-w},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014160192&doi=10.1038%2Fs41467-025-63281-w&partnerID=40&md5=74b42b1414ba6a0ddd2922756ef027a7},
affiliations = {Beijing Normal University, Beijing, China; Chinese Institute for Brain Research, Beijing, Beijing, China; Shenzhen University, School of Psychology, Shenzhen, Guangdong, China; Shenzhen University of Advanced Technology, Center for Neurocognition and Social Behavior, Shenzhen, Guangdong, China; University of Health and Rehabilitation Sciences, Institute for Neuropsychological Rehabilitation, Qingdao, Shandong, China; Max Planck UCL Centre for Computational Psychiatry and Ageing Research, Berlin, Germany; Charité – Universitätsmedizin Berlin, Department of Psychiatry, Berlin, Berlin, Germany; University of Oxford Medical Sciences Division, Oxford, Oxfordshire, United Kingdom},
abstract = {Understanding how we learn about the value and structure of our environment is central to neurocognitive theories of many psychiatric and neurological disorders. Learning processes have been extensively studied during performance of behavioural tasks (online learning) but less so in relation to resting (offline) states. A candidate mechanism for such offline learning is replay, the sequential neural reactivation of past experiences. Notably, value-based learning is especially tied to replay unfolding in reverse order relative to the original experience (backward replay). Here, we demonstrate the utility of EEG-based neural decoding for investigating offline learning, and relate it to trait anxiety, measured using the Spielberger Trait Anxiety Inventory. Participants were first required to infer sequential relationships among task objects by using a learned rule to reorganise their visual experiences into distinct sequences. Afterwards, they observed that the final object in one of the sequences was associated with a monetary reward and then entered a post-value resting state. During this rest, we find evidence of backward replay for reward-linked object sequences. The strength of such replay is negatively associated with trait anxiety and positively predicts an increased behavioural preference for reward-predictive stimuli. We also find that healthy individuals with high trait anxiety (score ≥ 45) show inefficient credit assignment irrespective of reward magnitude, indicating that this effect does not merely reflect reduced reward sensitivity. Together, these findings suggest a potential aberrant replay mechanism during offline learning in individuals with high trait anxiety. More broadly, our approach illustrates the potential of EEG for measuring structured neural representations in vivo. © The Author(s) 2025.},
keywords = {behavioral response; cognition; environmental conditions; learning; nervous system disorder; adult; anxiety disorder; Article; decoding; diagnostic accuracy; electroencephalography; feedback system; female; human; image artifact; k fold cross validation; leave one out cross validation; male; monetary reward; nerve cell; normal human; rest; sensitivity analysis; sequence analysis; sequence learning; State Trait Anxiety Inventory; visual evoked potential; anxiety; pathophysiology; physiology; psychology; reward; young adult; Adult; Anxiety; Electroencephalography; Female; Humans; Learning; Male; Rest; Reward; Young Adult},
correspondence_address = {Y. Liu; State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China; email: yunzhe.liu@bnu.edu.cn},
publisher = {Nature Research},
issn = {20411723},
pmid = {40858562},
language = {English},
abbrev_source_title = {Nat. Commun.},
type = {Article}}
@article{Yue2025Bgtransform,
author = {Yue, Jin and Xiao, Xiaolin and Zhang, Hao and Xu, Minpeng and Ming, Dong},
title = {BGTransform: a neurophysiologically informed EEG data augmentation framework},
year = {2025},
journal = {Journal of Neural Engineering},
volume = {22},
number = {5},
pages = {},
doi = {10.1088/1741-2552/ae0c3a},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018725244&doi=10.1088%2F1741-2552%2Fae0c3a&partnerID=40&md5=2e48b0e1b38c42a97f2f358df0e15de9},
affiliations = {Tianjin University, Academy of Medical Engineering and Translational Medicine, Tianjin, China; Haihe Laboratory of Brain-Computer Interaction and Human-Machine Integration, Tianjin, Tianjin, China},
abstract = {Objective. Deep learning has emerged as a powerful approach for decoding electroencephalography (EEG)-based brain–computer interface (BCI) signals. However, its effectiveness is often limited by the scarcity and variability of available training data. Existing data augmentation methods often introduce signal distortions or lack physiological validity. This study proposes a novel augmentation strategy designed to improve generalization while preserving the underlying neurophysiological structure of EEG signals. Approach. We propose Background EEG Transform (BGTransform), a principled data augmentation framework that leverages the neurophysiological dissociation between task-related activity and ongoing background EEG. In contrast to existing methods, BGTransform generates new trials by selectively perturbing the background EEG component while preserving the task-related signal, thus enabling controlled variability without compromising class-discriminative features. We applied BGTransform to three publicly available EEG-BCI datasets spanning steady-state visual evoked potential and P300 paradigms. The effectiveness of BGTransform is evaluated using several widely adopted neural decoding models under three training regimes: (1) without augmentation (baseline model), (2) with conventional augmentation methods, and (3) with BGTransform. Main results. Across all datasets and model architectures, BGTransform consistently outperformed both baseline models and conventional augmentation techniques. Compared to models trained without BGTransform, it achieved average classification accuracy improvements of 2.45%–15.52%, 4.36%–17.15% and 7.55%–10.47% across the three datasets, respectively. In addition, BGTransform demonstrated greater robustness across subjects and tasks, maintaining stable performance under varying recording conditions. Significance. BGTransform provides a principled and effective approach to augmenting EEG data, informed by neurophysiological insight. By preserving task-related components and introducing controlled variability, the method addresses the challenge of data sparsity in EEG-BCI training. These findings support the utility of BGTransform for improving the accuracy, robustness, and generalizability of deep learning models in neural engineering applications. © 2025 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.}, keywords = {Bioelectric potentials; Biomedical signal processing; Brain; Brain computer interface; Data mining; Deep learning; Dissociation; Electrophysiology; Interfaces (computer); Neurophysiology; Augmentation methods; Background electroencephalography; Baseline models; Data augmentation; Discriminative features; Generalisation; Neural decoding; Steady-state visual evoked potentials; Task-related activity; Training data; Electroencephalography; Article; data mining; deep learning; electroencephalogram; human; information processing; model; steady state; visual evoked potential; adult; brain; brain computer interface; electroencephalography; event related potential; factual database; physiology; procedures; Adult; Brain-Computer Interfaces; Databases, Factual; Deep Learning; Event-Related Potentials, P300; Evoked Potentials, Visual; Humans},
correspondence_address = {J. Yue; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; email: Yue_J@hotmail.com; X. Xiao; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; email: xiaoxiao0@tju.edu.cn; M. Xu; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; email: xmp52637@tju.edu.cn},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {41005322},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Zhan2025Dcan,
author = {Zhan, Ruichao and Li, Dongyang and Wang, Song and Liu, Quanying},
title = {D2CAN: Domain-Guided Contrastive Adversarial Network for EEG-Based Cross-Subject Cognitive Workload Decoding},
year = {2025},
journal = {Communications in Computer and Information Science},
volume = {2438 CCIS},
pages = {250 - 264},
doi = {10.1007/978-981-96-4001-0_17},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003622674&doi=10.1007%2F978-981-96-4001-0_17&partnerID=40&md5=c73166f13694bea6e7d9f23cc8550a7d},
affiliations = {Southern University of Science and Technology, Department of Biomedical Engineering, Shenzhen, Guangdong, China},
abstract = {Feature engineering and deep learning methods have allowed to decoding cognitive workload with neural data. However, the low efficiency and low cross-subject performance remain challenges. In this study, we propose a domain-guided cross-subject contrastive adversarial learning framework for cognitive workload decoding based on electroencephalogram (EEG). This framework employs contrastive adversarial learning to train an EEG encoder that projects individual EEG data from diverse sources into a low-dimensional invariant subspace, where subjects from both source and target domains share a common representation. The robustness of the model is enhanced by confusing the domain-specific representations of new subjects in the target domain and effectively interfering with the domain discriminator. These shared representations are subsequently used to classify cognitive workload. We conduct extensive experiments to investigate the influence of noise intensity from different sources and the contribution of different brain regions to decoding. Our method achieves state-of-the-art (SOTA) performance in EEG-based workload decoding, addressing the challenges of cross-subject variability and accuracy, and paving the way for more reliable cognitive workload assessment in diverse populations. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.}, keywords = {Adversarial machine learning; Deep learning; Adversarial learning; Adversarial networks; Cognitive workloads; Feature engineerings; Learning frameworks; Learning methods; Neural data; Neural decoding; Performance; Target domain; Contrastive Learning},
correspondence_address = {Q. Liu; Department of Biomedical Engineering, Southern University of Science and Technology, Shenzhen, China; email: liuqy@sustech.edu.cn},
editor = {Liu, Q. and Qu, Y. and Wu, H. and Qi, Y. and Zeng, A. and Pan, D.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {18650937; 18650929},
isbn = {9789819671748; 9789819664610; 9783032026743; 9783032008831; 9783032026712; 9789819671779; 9783031949425; 9789819666874; 9783031936968; 9783031941207},
language = {English},
abbrev_source_title = {Commun. Comput. Info. Sci.},
type = {Conference paper}}
@inproceedings{Zhang2025Chinese,
author = {Zhang, Haochang and Wang, Yipeng and Mao, Zemin},
booktitle = {2025 37th Chinese Control and Decision Conference (CCDC)},
title = {Neural Decoding of Chinese Sign Language with EEG Signals},
year = {2025},
volume = {},
number = {},
pages = {4904--4910},
abstract = {As a basic language for communication in the deaf community, sign language contains rich body movement patterns, expression states and semantic information. The electroencephalogram (EEG) is a reliable physiological signal that can effectively decode motor information and expression states. The objective of this study was to investigate the neural characteristics of decoding Chinese sign language using electroencephalogram (EEG) signals. First, the study collected electroencephalogram (EEG) signals from 15 deaf college students while they watched sign language vocabulary videos and performed sign language actions. The collected EEG signals were then preprocessed, and the differential entropy and power spectral density features of the EEG signals were extracted. Finally, this paper used different classifiers to categorize six sign language EEG features. Experimental results show that the SVM-based classifier obtains the best classification results with an average classification accuracy of 66.27 %. This study shows that electroencephalogram (EEG) signals can effectively decode the neural features of sign language in the brain, providing some reference value for brain-computer interfaces (BCIs) to facilitate communication in the deaf community.},
keywords = {Support vector machines;Sign language;Vocabulary;Accuracy;Semantics;Feature extraction;Electroencephalography;Entropy;Decoding;Videos;deaf;Chinese Sign Language;electroencephalogram (EEG) signals;recognition},
doi = {10.1109/CCDC65474.2025.11090651},
issn = {1948-9447},
month = {May}}
@article{Zhao2025Decoding,
author = {Zhao, Junyuan and Gao, Ruimin and Brennan, Jonathan R.},
title = {Decoding the Neural Dynamics of Headed Syntactic Structure Building},
year = {2025},
journal = {Journal of Neuroscience},
volume = {45},
number = {17},
pages = {},
doi = {10.1523/JNEUROSCI.2126-24.2025},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003304557&doi=10.1523%2FJNEUROSCI.2126-24.2025&partnerID=40&md5=c9e41a75d35117ea37aaf021602dfb86},
affiliations = {University of Michigan, Ann Arbor, Department of Linguistics, Ann Arbor, MI, United States; Georgia Institute of Technology, School of Psychology, Atlanta, GA, United States},
abstract = {The brain builds hierarchical phrases during language comprehension; however, the details and dynamics of the phrase-building process remain underspecified. This study directly probes whether the neural code of verb phrases involves reactivating the syntactic property of a key subcomponent (the “head” verb). To this end, we train a part-of-speech sliding–window verb/adverb decoder on EEG signals recorded while 30 participants read sentences in a controlled experiment. The decoder reaches above-chance performance that is spatiotemporally consistent and generalizes to unseen data across sentence positions. Applying the decoder to held-out data yields predicted activation levels for the verbal “head” of a verb phrase at a distant nonhead word (adverb); the critical adverb appeared either at the end of a verb phrase or at a sequentially and lexically matched position with no verb phrase boundary. There is stronger verb activation beginning at ∼600 milliseconds at the critical adverb when it appears at a verb phrase boundary; this effect is not modulated by the strength of conceptual association nor does it reflect word predictability. Time-locked analyses additionally reveal a negativity waveform component and increased beta-delta inter-trial phase coherence, both previously linked to linguistic composition, in a similar time window. With a novel application of neural decoding, our findings delineate the dynamics by which the brain encodes phrasal representations by, in part, reactivating the representation of key subcomponents. We thus establish a link between cognitive accounts of phrasal representations and electrophysiological dynamics. © © 2025 the authors.}, keywords = {adult; article; cognition; decoding; dynamics; electroencephalogram; electroencephalography; electrophysiology; human; human experiment; male; nerve cell; normal human; speech; waveform; brain; comprehension; female; language; physiology; reading; speech perception; young adult; Adult; Brain; Comprehension; Electroencephalography; Female; Humans; Language; Male; Reading; Speech Perception; Young Adult},
correspondence_address = {J. Zhao; School of Psychology, Georgia Institute of Technology, Atlanta, 30332, United States; email: zhaojy@umich.edu},
publisher = {Society for Neuroscience},
issn = {02706474; 15292401},
coden = {JNRSD},
pmid = {40050114},
language = {English},
abbrev_source_title = {J. Neurosci.},
type = {Article}}
@article{Zhao2025Electroencephalogrambased,
author = {Zhao, Ran and Liu, Hongxing and Zhang, Shuming and Tang, Qi and Yu, Xiaoli and Bai, Yanru and Ni, Guangjian},
title = {An Electroencephalogram-Based Study of Neural Responses to Imagined Speech in Mandarin},
year = {2025},
journal = {Communications in Computer and Information Science},
volume = {2312 CCIS},
pages = {278 - 289},
doi = {10.1007/978-981-96-1045-7_23},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219214303&doi=10.1007%2F978-981-96-1045-7_23&partnerID=40&md5=5beaede32a4cd7bdc201e904f1206fe1},
affiliations = {Tianjin University, Academy of Medical Engineering and Translational Medicine, Tianjin, China; Tianjin University, Tianjin, China},
abstract = {Speech is a basic and natural form of human communication. However, people with speech disorders have always have great difficulties in communicating. Research on speech neural decoding is more conducive to helping patients with speech disorders communicate with the outside world. As an emerging speech method, imagined speech only requires the subjects to make the sound of the corresponding material in their hearts. Since the perception and production of language originate from specific areas of the cerebral cortex, the brain activity between imagined speech and overt speech is similar, but its internal neural mechanism is not yet fully understood. This study designed a specific imagined speech experimental paradigm, collected and analyzed the electroencephalogram data corresponding to imagined speech, and revealed the time domain, rhythm, time-frequency and neural responses of specific brain areas during the production and processing of imagined speech, which provided theoretical support for imagined speech neural decoding, promoted the development of speech-assisted communication brain-computer interface system, helped patients with speech disorders communicate with the outside world, and truly improved the quality of life of patients with speech disorders. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.}, keywords = {Brain; Electrotherapeutics; Neurophysiology; Speech communication; Speech enhancement; Time domain analysis; Brain activity; Cerebral cortex; Human communications; Imagined speech; Mandarin; Neural decoding; Neural mechanisms; Neural response; Specific areas; Speech disorders; Electroencephalography},
correspondence_address = {Y. Bai; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; email: yr56_bai@tju.edu.cn; G. Ni; Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin, China; email: niguangjian@tju.edu.cn},
editor = {Ling, Z. and Chen, X. and Hamdulla, A. and He, L. and Li, Y.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {18650937; 18650929},
isbn = {9789819671748; 9789819664610; 9783032026743; 9783032008831; 9783032026712; 9789819671779; 9783031949425; 9789819666874; 9783031936968; 9783031941207},
language = {English},
abbrev_source_title = {Commun. Comput. Info. Sci.},
type = {Conference paper}}
@article{Zhou2025Metacontrol,
author = {Zhou, Xianzhen and Ghorbani, Foroogh and Roessner, Veit and Hommel, Bernhard and Prochnow, Astrid and Beśte, Christian},
title = {Metacontrol instructions lead to adult-like event segmentation in adolescents},
year = {2025},
journal = {Developmental Cognitive Neuroscience},
volume = {72},
pages = {},
doi = {10.1016/j.dcn.2025.101521},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216460133&doi=10.1016%2Fj.dcn.2025.101521&partnerID=40&md5=c390db252f64abd8fe34f698ea509fdf},
affiliations = {Medizinische Fakultät Carl Gustav Carus, Cognitive Neurophysiology, Dresden, Sachsen, Germany; Partner Site Dresden, Dresden, Germany; Shandong Normal University, School of Psychology, Jinan, Shandong, China},
abstract = {Event segmentation, which involves dividing continuous information into meaningful units, changes as children develop into adolescents. Adolescents tend to segment events more coarsely than adults. This study explores whether adolescents could adjust their segmentation style to resemble that of adults when provided with explicit metacontrol-related instructions. We compared event segmentation in two adolescent groups and one adult group, while simultaneously recording EEG data. One adolescent group was instructed to perform segmentation as finely as possible, whereas the other adolescent group and adults received no specific instructions on segmentation granularity. EEG data were analyzed using multivariate pattern analysis and source reconstruction. The findings revealed that adolescents given fine-grained instructions adjusted their segmentation probability closer to adult levels, although they did not fully match adults in processing multiple simultaneous changes. Neurophysiological results indicated that adolescents with fine-grained instructions exhibited neural decoding performance more similar to adults. Increased activity in the inferior frontal gyrus in these adolescents compared to adults related to this. The results suggest that adolescents with fine-grained instructions demonstrated more persistent cognitive control and enhanced top-down attention than their peers and adults. The study shows that adolescent cognitive processes can be shifted toward adult-like performance through instructions. © 2025 The Authors}, keywords = {adolescent; adult; article; attention; child; cognition; controlled study; electroencephalogram; electroencephalography; executive function; female; human; inferior frontal gyrus; major clinical study; male; probability; brain; physiology; procedures; young adult; Adolescent; Adult; Attention; Brain; Cognition; Electroencephalography; Female; Humans; Male; Young Adult},
correspondence_address = {C. Beste; Cognitive Neurophysiology, Department of Child and Adolescent Psychiatry, Faculty of Medicine, TU Dresden, Dresden, Schubertstrasse 42, 01307, Germany; email: christian.beste@uniklinikum-dresden.de},
publisher = {Elsevier Ltd},
issn = {18789307; 18789293},
pmid = {39892154},
language = {English},
abbrev_source_title = {Dev. Cognitive Neurosci.},
type = {Article}}
@article{Zuo2025Staeegnet,
author = {Zuo, Mingliang and Chen, Xiaoyu and Sui, Li},
title = {A novel STA-EEGNet combined with channel selection for classification of EEG evoked in 2D and 3D virtual reality},
year = {2025},
journal = {Medical Engineering and Physics},
volume = {141},
pages = {},
doi = {10.1016/j.medengphy.2025.104363},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005165406&doi=10.1016%2Fj.medengphy.2025.104363&partnerID=40&md5=05e1b391e9412580ff158467f1ebdf61},
affiliations = {University of Shanghai for Science and Technology, School of Health Science and Engineering, Shanghai, Shanghai, China; Fudan University, Shanghai, China},
abstract = {Virtual reality (VR), particularly through 3D presentations, significantly boosts user engagement and task efficiency in fields such as gaming, education, and healthcare, offering more immersive and interactive experiences than traditional 2D formats. This study investigates EEG classification in response to 2D and 3D VR stimuli to deepen our understanding of the neural mechanisms driving VR interactions, with implications for brain-computer interfaces (BCIs). We introduce STA-EEGNet, an innovative model that enhances EEGNet by incorporating spatial-temporal attention (STA), improving EEG signal classification from VR environments. A one-way analysis of variance (ANOVA) was utilized to optimize channel selection, enhancing model accuracy. Comparative experiments showed that STA-EEGNet surpassed traditional EEGNet, achieving a peak accuracy of 99.78 % with channel selection. These findings highlight the benefits of spatial-temporal attention and optimal channel selection in classifying VR-evoked EEG data. This study underscores the importance of integrating spatial-temporal attention with compact convolutional neural networks like EEGNet, not only improving EEG signal classification but also advancing neural decoding and optimizing BCI applications. © 2025 IPEM}, keywords = {Convolutional neural networks; Deep learning; User interfaces; Virtual reality; Virtualization; 3-D presentations; 3D virtual reality; Attention mechanisms; Channel selection; EEG signals classification; Spatial temporals; Spatial-temporal attention mechanism; Task efficiencies; User engagement; Analysis of variance (ANOVA); adult; analysis of variance; Article; channel selection; classification; convolutional neural network; data accuracy; deep learning; electroencephalography; feature selection; female; human; human experiment; intermethod comparison; male; normal human; signal processing; spatial attention; stimulus response; temporal attention; virtual reality; artificial neural network; attention; brain computer interface; young adult; Adult; Attention; Brain-Computer Interfaces; Electroencephalography; Female; Humans; Male; Neural Networks, Computer; Signal Processing, Computer-Assisted; Virtual Reality; Young Adult},
correspondence_address = {L. Sui; School of Health Science and Engineering, University of Shanghai for Science and Technology, Shanghai, 200093, China; email: lsui@usst.edu.cn},
publisher = {Elsevier Ltd},
issn = {13504533; 18734030},
coden = {MEPHE},
pmid = {40514107},
language = {English},
abbrev_source_title = {Med. Eng. Phys.},
type = {Article}}
@conference{Agostinho2024Fmrinet,
author = {Agostinho, Daniel and Castelo-Branco, Miguel De Sá E.Sousa and Simões, Marco A.},
title = {FMRINet: Repurposing the EEGNet model to identify emotional arousal states in fMRI data},
year = {2024},
journal = {Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
pages = {},
doi = {10.1109/EMBC53108.2024.10782984},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210237850&doi=10.1109%2FEMBC53108.2024.10782984&partnerID=40&md5=21d75d74ce82474d4eae3879e2bec3e9},
affiliations = {University of Coimbra, Centre for Informatics and System, Department of Informatics Engineering, Coimbra, Portugal; Universidade de Coimbra, Instituto de Ciências Nucleares Aplicadas à Saude, Coimbra, Portugal},
abstract = {In recent years, functional magnetic resonance imaging (fMRI) transformed our understanding of the intricate relationship between emotions and the brain. The precise classification of emotional states from fMRI data poses challenges for traditional machine learning methods dealing with high-dimensional data. The limitations of these conventional approaches have spurred a growing interest in exploring the potential of deep learning (DL) models. This study introduces a novel approach to classifying emotional arousal levels using fMRI data, specifically tailored for projects with limited data. The approach involves the adaptation of the EEGNet architecture, originally designed for the classification of electroencephalography (EEG) signals, to fMRI data. By mapping the fMRI signal into brain regions using a brain atlas, fMRINet is applied to the two-dimensional fMRI time series, achieving a promising performance in identifying emotional states in both typical and clinical participants (balanced accuracy between 70% and 72%). Our findings highlight the successful integration of the EEGNet architecture into fMRI data and contribute to the broader field of brain state classification. © 2024 IEEE.}, keywords = {Contrastive Learning; Deep learning; Frequency modulation; Magnetic resonance imaging; Arousal state; Brain decoding; Emotion classification; Emotional state; Functional magnetic resonance imaging; High dimensional data; Machine learning methods; Repurposing; Resonance imaging data; Brain mapping; adult; arousal; brain; diagnostic imaging; electroencephalography; emotion; female; human; male; nuclear magnetic resonance imaging; physiology; procedures; young adult; Adult; Arousal; Brain; Electroencephalography; Emotions; Female; Humans; Magnetic Resonance Imaging; Male; Young Adult},
correspondence_address = {D. Agostinho; University of Coimbra, Cisuc, Department of Informatics Engineering, Coimbra, Portugal; email: danielagostinho@dei.uc.pt},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {05891019; 1557170X},
isbn = {0780356756; 0780387406; 9781509028092; 9781728111797; 1424400325; 9781424407880; 0780307852; 9780780313774; 9798350324471; 9781457702167},
pmid = {40039361},
language = {English},
abbrev_source_title = {Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
type = {Conference paper}}
@article{Ajioka2024Endtoend,
author = {Ajioka, Takehiro and Nakai, Nobuhiro and Yamashita, Okito and Takumi, Toru},
title = {End-to-end deep learning approach to mouse behavior classification from cortex-wide calcium imaging},
year = {2024},
journal = {PLOS Computational Biology},
volume = {20},
number = {3},
pages = {},
doi = {10.1371/journal.pcbi.1011074},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187651758&doi=10.1371%2Fjournal.pcbi.1011074&partnerID=40&md5=787ef4a62b09e68841318023f9da7b96},
affiliations = {Kobe University School of Medicine, Department of Cell Biology and Physiology, Kobe, Hyogo, Japan; Advanced Telecommunications Research Institute International (ATR), Department of Computational Brain Imaging, Kyoto, Kyoto, Japan; RIKEN Center for Biosystems Dynamics Research, Kobe, Hyogo, Japan},
abstract = {Deep learning is a powerful tool for neural decoding, broadly applied to systems neuroscience and clinical studies. Interpretable and transparent models that can explain neural decoding for intended behaviors are crucial to identifying essential features of deep learning decoders in brain activity. In this study, we examine the performance of deep learning to classify mouse behavioral states from mesoscopic cortex-wide calcium imaging data. Our convolutional neural network (CNN)-based end-to-end decoder combined with recurrent neural network (RNN) classifies the behavioral states with high accuracy and robustness to individual differences on temporal scales of sub-seconds. Using the CNN-RNN decoder, we identify that the forelimb and hindlimb areas in the somatosensory cortex significantly contribute to behavioral classification. Our findings imply that the end-to-end approach has the potential to be an interpretable deep learning method with unbiased visualization of critical brain regions. ©: © 2024 Ajioka et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords = {Brain; Calcium; Convolutional neural networks; Decoding; Learning systems; Behavioral state; Behaviour classification; Calcium imaging; Clinical study; Convolutional neural network; Cortexes; End to end; Learning approach; Mouse behaviors; Neural decoding; Recurrent neural networks; animal experiment; animal model; animal tissue; Article; artificial intelligence; behavior; brain region; cerebral cortical tissue; controlled study; convolutional neural network; deep learning; electroencephalogram; electroencephalography; forelimb; hindlimb; hippocampus; learning; learning algorithm; local field potential; locomotion; machine learning; mouse; nerve cell network; nonhuman; recurrent neural network; somatosensory cortex; support vector machine; time series analysis; animal; artificial neural network; brain; brain cortex; diagnostic imaging; calcium; Animals; Cerebral Cortex; Deep Learning; Mice; Neural Networks, Computer},
correspondence_address = {T. Takumi; Department of Physiology and Cell Biology, Kobe University School of Medicine, Kobe, Chuo, Japan; email: takumit@med.kobe-u.ac.jp},
publisher = {Public Library of Science},
issn = {1553734X; 15537358},
pmid = {38478563},
language = {English},
abbrev_source_title = {PLoS Comput. Biol.},
type = {Article}}
@conference{Akbari2024Joint,
author = {Akbari, Ali and Sanjar, Kosar and Yousefnezhad, Muhammad and Mirian, Maryam Sadat and Arasteh, Emad Malekzadeh},
title = {Joint Learning for Visual Reconstruction from the Brain Activity: Hierarchical Representation of Image Perception with EEG-Vision Transformer},
year = {2024},
journal = {Proceedings of Machine Learning Research},
volume = {285},
pages = {},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014736717&partnerID=40&md5=95ef7cdfe2da78435a3bfdd9b45b1b61},
affiliations = {Sharif University of Technology, Department of Electrical Engineering, Tehran, Tehran, Iran; University of Tehran, Department of Electrical Engineering, Tehran, Tehran, Iran; University of Alberta, Department of Computing Science and The Department of Psychiatry, Edmonton, AB, Canada; The University of British Columbia, Department of Medicine, Vancouver, BC, Canada; University Medical Center Utrecht, Department of Neonatology, Utrecht, Netherlands},
abstract = {Reconstructing visual stimuli from brain activity is a challenging problem, particu-larly when using EEG data, which is more affordable and accessible than fMRI but noisier and lower in spatial resolution. In this paper, we present Hierarchical-ViT, a novel framework designed to improve the quality and precision of EEG-based image reconstruction by integrating hierarchical visual feature extraction, vision transformer-based EEG (EEG-ViT) processing, and CLIP-based joint learning. Inspired by the hierarchical nature of the human visual system, our model progressively captures complex visual features—such as edges, textures, and shapes—through a multi-stage processing approach. These features are aligned with EEG signals processed by the EEG-ViT model, allowing for the creation of a shared latent space that enhances contrastive learning. A StyleGAN is then em-ployed to generate high-resolution images from these aligned representations. We evaluated our method on two benchmark datasets, EEGCVPR40 and ThoughtViz, achieving superior results compared to existing approaches in terms of Inception Score (IS), Kernel Inception Distance (KID), and Frechet Inception Distance (FID) for EEGCVPR, and IS and KID for the ThoughtViz dataset. Through an ablation study, we underscored the feasibility of hierarchical feature extraction, while the multivariate analysis of variance (MANOVA) test confirmed the distinctiveness of the learned feature spaces. In conclusion, our results show the feasibility and uniqueness of using hierarchical filtering of perceived images combined with EEG-ViT-based features to improve brain decoding from EEG data. © 2024, ML Research Press. All rights reserved.},
keywords = {Biomedical signal processing; Brain; Computer vision; Electroencephalography; Feature extraction; Image enhancement; Image reconstruction; Neurophysiology; Textures; Brain activity; Hierarchical representation; Human Visual System; Image perception; Images reconstruction; Joint learning; Spatial resolution; Visual feature extraction; Visual reconstruction; Visual stimulus; Extraction; Multivariant analysis},
editor = {Fumero, M. and Domine, C.C.J. and Lahner, Z. and Crisostomi, D. and Moschella, L. and Stachenfeld, K.},
publisher = {ML Research Press},
issn = {26403498},
isbn = {9781713845065},
language = {English},
abbrev_source_title = {Proc. Mach. Learn. Res.},
type = {Conference paper}}
@conference{Beauchene2024Neurophysiologicalauditory,
author = {Beauchene, Christine and Brandstein, Michael S. and Quatieri, Thomas F. and Thompson, Eric R. and Smalt, Christopher J.},
title = {A NEUROPHYSIOLOGICAL-AUDITORY “LISTEN RECEIPT” FOR COMMUNICATION ENHANCEMENT},
year = {2024},
journal = {Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
pages = {2036 - 2040},
doi = {10.1109/ICASSP48485.2024.10448414},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195426408&doi=10.1109%2FICASSP48485.2024.10448414&partnerID=40&md5=87ab3c5ab12e3e7a92eb9ed315edab3a},
affiliations = {Lincoln Laboratory, Lexington, MA, United States; Air Force Research Laboratory, Dayton, OH, United States},
abstract = {Information overload, and specifically auditory overload, is common in critical situations and detrimental to communication. Currently, there is no auditory equivalent of an email read receipt to know if a person has heard a message, other than waiting for a reply. This work hypothesizes that it may be possible to decode whether a person has indeed heard a message, or in other words, create an an auditory “listen receipt,” through use of non-invasive physiological or neural monitoring. We extracted a variety of features derived from Electrodermal activity (EDA), Electroencephalography (EEG), and the correlations between the acoustic envelope of the radio message and EEG to use in the decoder. We were able to classify the cases in which the subject responded correctly to the question in the message, versus the cases where they missed or heard the message incorrectly, with an accuracy of 79% and a receiver operating characteristic (ROC) area under the curve (AUC) of 0.83. This work suggests that the concept of a “listen receipt” may be possible, and future wearable machine-brain interface technologies may be able to automatically determine if an important radio message has been missed for both human-to-human and human-to-machine communication. © 2024 IEEE.},
keywords = {Auditory attention decoding; EEG; neural decoding; speech communication},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {07367791; 15206149},
isbn = {1424407281; 9781467300469; 0780362934; 9781457705397; 9781728163277; 9781424423545; 9798350368741; 0780305329; 9780780309463; 9781509066315},
coden = {IPROD},
language = {English},
abbrev_source_title = {ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
type = {Conference paper}}
@article{Borra2024Speechbrainmoabb,
author = {Borra, Davide and Paissan, Francesco and Ravanelli, Mirco},
title = {SpeechBrain-MOABB: An open-source Python library for benchmarking deep neural networks applied to EEG signals},
year = {2024},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {},
doi = {10.1016/j.compbiomed.2024.109097},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203503172&doi=10.1016%2Fj.compbiomed.2024.109097&partnerID=40&md5=c3ef0ba99b850f8437056008787e39f1},
affiliations = {Alma Mater Studiorum Università di Bologna, Cesena, Department of Electrical, Cesena, FC, Italy; Bruno Kessler Foundation, Trento, TN, Italy; Concordia University, Montreal, QC, Canada; Montreal Institute for Learning Algorithms, Montreal, QC, Canada},
abstract = {Deep learning has revolutionized EEG decoding, showcasing its ability to outperform traditional machine learning models. However, unlike other fields, EEG decoding lacks comprehensive open-source libraries dedicated to neural networks. Existing tools (MOABB and braindecode) prevent the creation of robust and complete decoding pipelines, as they lack support for hyperparameter search across the entire pipeline, and are sensitive to fluctuations in results due to network random initialization. Furthermore, the absence of a standardized experimental protocol exacerbates the reproducibility crisis in the field. To address these limitations, we introduce SpeechBrain-MOABB, a novel open-source toolkit carefully designed to facilitate the development of a comprehensive EEG decoding pipeline based on deep learning. SpeechBrain-MOABB incorporates a complete experimental protocol that standardizes critical phases, such as hyperparameter search and model evaluation. It natively supports multi-step hyperparameter search for finding the optimal hyperparameters in a high-dimensional space defined by the entire pipeline, and multi-seed training and evaluation for obtaining performance estimates robust to the variability caused by random initialization. SpeechBrain-MOABB outperforms other libraries, including MOABB and braindecode, with accuracy improvements of 14.9% and 25.2% (on average), respectively. By enabling easy-to-use and easy-to-share decoding pipelines, our toolkit can be exploited by neuroscientists for decoding EEG with neural networks in a replicable and trustworthy way. © 2024 The Authors}, keywords = {Benchmarking; Decoding; Deep neural networks; Neurophysiology; Pipeline codes; Problem oriented languages; Speech enhancement; Benchmarking toolkit; Deep learning; EEG signals; Experimental protocols; Hyper-parameter; Machine learning models; Neural decoding; Neural-networks; Open-source; Open-source libraries; Electroencephalography; article; benchmarking; deep learning; deep neural network; electroencephalogram; electroencephalography; experimental protocol; human; machine learning; nerve cell network; reproducibility; artificial neural network; brain; physiology; procedures; signal processing; software; Brain; Deep Learning; Humans; Neural Networks, Computer; Signal Processing, Computer-Assisted; Software},
correspondence_address = {D. Borra; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi” (DEI), University of Bologna, Cesena, Forlì-Cesena, Italy; email: davide.borra2@unibo.it},
publisher = {Elsevier Ltd},
issn = {18790534; 00104825},
coden = {CBMDA},
pmid = {39265481},
language = {English},
abbrev_source_title = {Comput. Biol. Med.},
type = {Article}}
@article{Borra2024Explaining,
author = {Borra, Davide and Ravanelli, Mirco},
title = {Explaining Network Decision Provides Insights on the Causal Interaction Between Brain Regions in a Motor Imagery Task},
year = {2024},
journal = {Lecture Notes in Computer Science},
volume = {15154 LNAI},
pages = {156 - 167},
doi = {10.1007/978-3-031-71602-7_14},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205366273&doi=10.1007%2F978-3-031-71602-7_14&partnerID=40&md5=bbde5bf50d86e356ba12033ef0d7e96c},
affiliations = {Alma Mater Studiorum Università di Bologna, Cesena, Department of Electrical, Cesena, FC, Italy; Concordia University, Montreal, QC, Canada; Montreal Institute for Learning Algorithms, Montreal, QC, Canada},
abstract = {Neural decoding widely exploits machine learning for classifying electroencephalographic (EEG) signals for brain-computer interface applications. Recent advancements in neural decoding regards the use of brain functional connectivity estimates as input features and the adoption of convolutional neural networks (CNNs) to realize decoders. Moreover, explainable artificial intelligence (XAI) approaches based on CNNs are growing interest in the neuroscience community, for validating the knowledge learned by networks and for using the decoder not only to classify the EEG but also to analyze it in a data-driven way, without a priori assumptions. However, the adoption of connectivity estimates for neural decoding is still in its infancy, as adopts non-directed connectivity measures, limits the analysis of few interactions/frequency ranges, and exploits classic machine learning approaches without exploring CNNs. Moreover, XAI approaches have never been applied to analyze EEG-based functional connectivity. To overcome these limitations, we design and apply a CNN for processing directed connectivity measures estimated via spectral Granger causality. The CNN automatically learns features in the frequency and spatial domains, and it is coupled with an explanation technique (DeepLIFT) for highlighting the most relevant connectivity inflow and outflow associated to each decoded brain state. Our approach is applied to motor imagery decoding, and achieves state-of-the-art performance compared to existing networks. DeepLIFT relevance representations match the directional interactions known occurring when imagining movements, validating the features related to the brain network, as learned by the CNN. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.}, keywords = {Brain mapping; Deep neural networks; Neurons; Brain regions; Brain-computer interface applications; Convolutional neural network; Electroencephalographic signals; Explainable AI; Functional connectivity; Machine-learning; Motor imagery; Motor imagery tasks; Neural decoding; Convolutional neural networks},
correspondence_address = {D. Borra; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi” (DEI), University of Bologna, Cesena, Cesena Campus, Italy; email: davide.borra2@unibo.it},
editor = {Suen, C.Y. and Krzyzak, A. and Ravanelli, M. and Nobile, N. and Trentin, E. and Subakan, C.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {16113349; 03029743},
isbn = {9789819698936; 9789819698042; 9789819698110; 9789819698905; 9783032004949; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141},
language = {English},
abbrev_source_title = {Lect. Notes Comput. Sci.},
type = {Conference paper}}
@article{Comstock2024Transcranial,
author = {Comstock, Lindy B. and Carvalho, Vinícius Rezende and Lainscsek, Claudia S.M. and Fallah, Aria and Sejnowski, Terrence Joseph},
title = {Transcranial Magnetic Stimulation Facilitates Neural Speech Decoding},
year = {2024},
journal = {Brain Sciences},
volume = {14},
number = {9},
pages = {},
doi = {10.3390/brainsci14090895},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205232895&doi=10.3390%2Fbrainsci14090895&partnerID=40&md5=b5e6d53ad120ff487b77ad8586888fb0},
affiliations = {David Geffen School of Medicine at UCLA, Los Angeles, CA, United States; Jane & Terry Semel Institute for Neuroscience & Human Behavior, Los Angeles, CA, United States; Universidade Federal de Minas Gerais, Postgraduate Program in Electrical Engineering, Belo Horizonte, MG, Brazil; Salk Institute for Biological Studies, Computational Neurobiology Laboratory, San Diego, CA, United States; Institute for Neural Computation, La Jolla, CA, United States; David Geffen School of Medicine at UCLA, Los Angeles, CA, United States; School of Biological Sciences, La Jolla, CA, United States},
abstract = {Transcranial magnetic stimulation (TMS) has been widely used to study the mechanisms that underlie motor output. Yet, the extent to which TMS acts upon the cortical neurons implicated in volitional motor commands and the focal limitations of TMS remain subject to debate. Previous research links TMS to improved subject performance in behavioral tasks, including a bias in phoneme discrimination. Our study replicates this result, which implies a causal relationship between electro-magnetic stimulation and psychomotor activity, and tests whether TMS-facilitated psychomotor activity recorded via electroencephalography (EEG) may thus serve as a superior input for neural decoding. First, we illustrate that site-specific TMS elicits a double dissociation in discrimination ability for two phoneme categories. Next, we perform a classification analysis on the EEG signals recorded during TMS and find a dissociation between the stimulation site and decoding accuracy that parallels the behavioral results. We observe weak to moderate evidence for the alternative hypothesis in a Bayesian analysis of group means, with more robust results upon stimulation to a brain region governing multiple phoneme features. Overall, task accuracy was a significant predictor of decoding accuracy for phoneme categories (F(1,135) = 11.51, p < 0.0009) and individual phonemes (F(1,119) = 13.56, p < 0.0003), providing new evidence for a causal link between TMS, neural function, and behavior. © 2024 by the authors.}, keywords = {adult; alternative hypothesis; article; Bayes theorem; brain cell; brain region; diagnostic test accuracy study; dissociation; double dissociation; electroencephalogram; electroencephalography; female; human; human experiment; magnetic stimulation; male; nerve cell; neuromodulation; phoneme; psychomotor activity; speech; speech perception; transcranial magnetic stimulation; young adult},
correspondence_address = {L. Comstock; Department of Psychiatry & Biobehavioral Sciences, UCLA, Los Angeles, 90095, United States; email: lbcomstock@ucla.edu},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {20763425},
language = {English},
abbrev_source_title = {Brain Sci.},
type = {Article}}
@conference{ElOuahidi2024Unsupervised,
author = {El Ouahidi, Yassine and Lioi, Giulia and Farrugia, Nicolas and Pasdeloup, Bastien and Gripon, Vincent},
title = {UNSUPERVISED ADAPTIVE DEEP LEARNING METHOD FOR BCI MOTOR IMAGERY DECODING},
year = {2024},
journal = {European Signal Processing Conference},
pages = {1626 - 1630},
doi = {10.23919/eusipco63174.2024.10715003},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208427321&doi=10.23919%2Feusipco63174.2024.10715003&partnerID=40&md5=9153fad404ff0ae14a0954d8cd7f3911},
affiliations = {Laboratoire des Sciences et Techniques de l'Information, de la Communication et de la Connaissance (Lab-Sticc), Brest, Brittany, France},
abstract = {In the context of Brain-Computer Interfaces, we propose an adaptive method that reaches offline performance level while being usable online without requiring supervision. Interestingly, our method does not require retraining the model, as it consists in using a frozen efficient deep learning backbone while continuously realigning data, both at input and latent spaces, based on streaming observations. We demonstrate its efficiency for Motor Imagery brain decoding from electroencephalography data, considering challenging cross-subject scenarios. For reproducibility, we share the code of our experiments. © 2024 European Signal Processing Conference, EUSIPCO. All rights reserved.}, keywords = {Brain mapping; Contrastive Learning; Deep learning; Image coding; Interfaces (computer); Self-supervised learning; Unsupervised learning; Adaptive methods; Brain decoding; Its efficiencies; Learning methods; Motor imagery; Off-line performance; Online learning; Performance:level; Space-based; Brain computer interface},
publisher = {European Signal Processing Conference, EUSIPCO},
issn = {22195491},
isbn = {9789082797060; 9789082797039; 9789082797091; 9781467310680; 9783200001657; 9789464593600; 9788392134022; 9780992862602; 9789082797015; 9780992862657},
language = {English},
abbrev_source_title = {European Signal Proces. Conf.},
type = {Conference paper}}
@article{Erbsloh2024Technical,
author = {Erbsloh, Andreas and Buron, Leo and Ur-Rehman, Zia and Musall, Simon and Hrycak, Camilla Patrizia and Löhler, Philipp and Klaes, Christian and Seidl, Karsten and Schiele, Gregor},
title = {Technical survey of end-to-end signal processing in BCIs using invasive MEAs},
year = {2024},
journal = {Journal of Neural Engineering},
volume = {21},
number = {5},
pages = {},
doi = {10.1088/1741-2552/ad8031},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206403401&doi=10.1088%2F1741-2552%2Fad8031&partnerID=40&md5=856e7d37da6e75241c974cadef17314e},
affiliations = {Universität Duisburg-Essen, Duisburg, Nordrhein-Westfalen, Germany; Ruhr-Universitat Bochum, Bochum, Nordrhein-Westfalen, Germany; Forschungszentrum Jülich GmbH, Julich, Germany; Fraunhofer Institute for Microelectronic Circuits and Systems IMS, Duisburg, Nordrhein-Westfalen, Germany},
abstract = {Modern brain-computer interfaces and neural implants allow interaction between the tissue, the user and the environment, where people suffer from neurodegenerative diseases or injuries.This interaction can be achieved by using penetrating/invasive microelectrodes for extracellular recordings and stimulation, such as Utah or Michigan arrays. The application-specific signal processing of the extracellular recording enables the detection of interactions and enables user interaction. For example, it allows to read out movement intentions from recordings of brain signals for controlling a prosthesis or an exoskeleton. To enable this, computationally complex algorithms are used in research that cannot be executed on-chip or on embedded systems. Therefore, an optimization of the end-to-end processing pipeline, from the signal condition on the electrode array over the analog pre-processing to spike-sorting and finally the neural decoding process, is necessary for hardware inference in order to enable a local signal processing in real-time and to enable a compact system for achieving a high comfort level. This paper presents a survey of system architectures and algorithms for end-to-end signal processing pipelines of neural activity on the hardware of such neural devices, including (i) on-chip signal pre-processing, (ii) spike-sorting on-chip or on embedded hardware and (iii) neural decoding on workstations. A particular focus for the hardware implementation is on low-power electronic design and artifact-robust algorithms with low computational effort and very short latency. For this, current challenges and possible solutions with support of novel machine learning techniques are presented in brief. In addition, we describe our future vision for next-generation BCIs. © 2024 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Deep neural networks; Electrotherapeutics; Exoskeleton (Robotics); Image analysis; Image coding; Image compression; Image quality; Image thinning; Integrated circuit design; Neurons; Deep learning; Embedded-system; End to end; Extracellular recording; Low-power electronics; Neural decoder; Neural signal processing; On chips; Signal-processing; Spike-sorting; Neurodegenerative diseases; algorithm; artifact; brain computer interface; deep learning; degenerative disease; electrode; exoskeleton; human; latent period; machine learning; Michigan; microelectrode; neurological implant; nonhuman; prosthesis; review; signal processing; spike; electrode implant; electroencephalography; procedures; Algorithms; Brain-Computer Interfaces; Electrodes, Implanted; Electroencephalography; Humans; Microelectrodes; Signal Processing, Computer-Assisted},
correspondence_address = {A. Erbslöh; University of Duisburg-Essen, Duisburg, Germany; email: andreas.erbsloeh@uni-due.de; L. Buron; University of Duisburg-Essen, Duisburg, Germany; email: leo.buron@uni-due.de; Z. Ur-Rehman; Ruhr University Bochum, Bochum, Germany; email: zia.ur-rehman@ruhr-uni-bochum.de},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {39326451},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Review}}
@article{Falciglia2024Learning,
author = {Falciglia, Salvatore and Betello, Filippo and Russo, Samuele Icaro and Napoli, Christian},
title = {Learning visual stimulus-evoked EEG manifold for neural image classification},
year = {2024},
journal = {Neurocomputing},
volume = {588},
pages = {},
doi = {10.1016/j.neucom.2024.127654},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190506481&doi=10.1016%2Fj.neucom.2024.127654&partnerID=40&md5=44ecaf55b09356104dd1891ed08aadd1},
affiliations = {Control and Management Engineering, Department of Computer Science, Rome, Italy; Sapienza Università di Roma, Department of Psychology, Rome, RM, Italy; Sant'Anna Scuola Universitaria Superiore Pisa, Pisa, PI, Italy; Consiglio Nazionale delle Ricerche, Institute for Systems Analysis and Computer Science, Rome, RM, Italy; Częstochowa University Of Technology, Department of Intelligent Computer Systems, Czestochowa, Silesian, Poland},
abstract = {Visual neural decoding, namely the ability to interpret external visual stimuli from patterns of brain activity, is a challenging task in neuroscience research. Recent studies have focused on characterizing patterns of activity across multiple neurons that can be described in terms of population-level features. In this study, we combine spatial, spectral, and temporal features to achieve neural manifold classification capable to characterize visual perception and to simulate the working memory activity in the human brain. We treat spatio-temporal and spectral information separately by means of custom deep learning architectures based on Riemann manifold and the two-dimensional EEG spectrogram representation. In addition, a CNN-based classification model is used to classify visual stimulus-evoked EEG signals while viewing the 11-class (i.e., all-black plus 0-9 digit images) MindBigData Visual MNIST dataset. The effectiveness of the proposed integration strategy is evaluated on the stimulus-evoked EEG signal classification task, achieving an overall accuracy of 86%, comparable to state-of-the-art benchmarks. © 2024 The Author(s)}, keywords = {Brain; Brain computer interface; Classification (of information); Convolutional neural networks; Decoding; Deep learning; Neurophysiology; Signal encoding; Spectrographs; Auto encoders; Convolutional neural network; Images classification; Manifold learning; Neural decoding; Neural image classification; Neural manifold learning; Riemann Manifold; Spectrogram analyse; Spectrograms; Uniform manifold approximation and projection; Variational auto-encoder; Visual neural decoding; Image classification; Article; benchmarking; brain analysis; clinical effectiveness; convolutional neural network; deep learning; diagnostic accuracy; electroencephalogram; human; image analysis; spatial analysis; spatiotemporal analysis; spectroscopy; temporal analysis; two-dimensional imaging; visual evoked potential; visual stimulation; working memory},
correspondence_address = {C. Napoli; Department of Computer, Control and Management Engineering, Rome, Via Ariosto 25, 00185, Italy; email: cnapoli@diag.uniroma1.it},
publisher = {Elsevier B.V.},
issn = {18728286; 09252312},
coden = {NRCGE},
language = {English},
abbrev_source_title = {Neurocomputing},
type = {Article}}
@article{Ferrante2024Retrieving,
author = {Ferrante, Matteo and Boccato, Tommaso and Passamonti, Luca and Toschi, Nicola},
title = {Retrieving and reconstructing conceptually similar images from fMRI with latent diffusion models and a neuro-inspired brain decoding model},
year = {2024},
journal = {Journal of Neural Engineering},
volume = {21},
number = {4},
pages = {},
doi = {10.1088/1741-2552/ad593c},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197363165&doi=10.1088%2F1741-2552%2Fad593c&partnerID=40&md5=be30a60850031c296abea98f3281e465},
affiliations = {Università degli Studi di Roma "Tor Vergata", Department of Biomedicine and Prevention, Rome, RM, Italy; Istituto di Bioimmagini e Fisiologia Molecolare, Segrate, MI, Italy; Harvard Medical School, Boston, MA, United States},
abstract = {Objective. Brain decoding is a field of computational neuroscience that aims to infer mental states or internal representations of perceptual inputs from measurable brain activity. This study proposes a novel approach to brain decoding that relies on semantic and contextual similarity. Approach. We use several functional magnetic resonance imaging (fMRI) datasets of natural images as stimuli and create a deep learning decoding pipeline inspired by the bottom-up and top-down processes in human vision. Our pipeline includes a linear brain-to-feature model that maps fMRI activity to semantic visual stimuli features. We assume that the brain projects visual information onto a space that is homeomorphic to the latent space of last layer of a pretrained neural network, which summarizes and highlights similarities and differences between concepts. These features are categorized in the latent space using a nearest-neighbor strategy, and the results are used to retrieve images or condition a generative latent diffusion model to create novel images. Main results. We demonstrate semantic classification and image retrieval on three different fMRI datasets: Generic Object Decoding (vision perception and imagination), BOLD5000, and NSD. In all cases, a simple mapping between fMRI and a deep semantic representation of the visual stimulus resulted in meaningful classification and retrieved or generated images. We assessed quality using quantitative metrics and a human evaluation experiment that reproduces the multiplicity of conscious and unconscious criteria that humans use to evaluate image similarity. Our method achieved correct evaluation in over 80% of the test set. Significance. Our study proposes a novel approach to brain decoding that relies on semantic and contextual similarity. The results demonstrate that measurable neural correlates can be linearly mapped onto the latent space of a neural network to synthesize images that match the original content. These findings have implications for both cognitive neuroscience and artificial intelligence. © 2024 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Brain; Classification (of information); Decoding; Deep learning; Diffusion; Image reconstruction; Magnetic resonance imaging; Multilayer neural networks; Pipelines; Brain decoding; Computational neuroscience; Diffusion model; Functional magnetic resonance imaging; Latent; Mental state; Neural-networks; Similar image; State representation; Visual stimulus; Semantics; Article; artificial intelligence; autoencoder; brain decoding model; clustering algorithm; conceptual model; controlled study; convolutional neural network; deep learning; diffusion; electroencephalogram; functional magnetic resonance imaging; hemodynamics; human; image reconstruction; image retrieval; magnetic field; model; neurobiology; neuroscience; visual cortex; visual stimulation; artificial neural network; biological model; brain; brain mapping; diagnostic imaging; image processing; nuclear magnetic resonance imaging; photostimulation; physiology; procedures; semantics; vision; Brain Mapping; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Models, Neurological; Neural Networks, Computer; Photic Stimulation; Visual Perception},
correspondence_address = {M. Ferrante; Department of Biomedicine and Prevention, University of Rome, Rome, Tor Vergata, Italy; email: matteo.ferrante@uniroma2.it},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {38885689},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Ferrante2024Decoding,
author = {Ferrante, Matteo and Boccato, Tommaso and Bargione, Stefano and Toschi, Nicola},
title = {Decoding visual brain representations from electroencephalography through knowledge distillation and latent diffusion models},
year = {2024},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {},
doi = {10.1016/j.compbiomed.2024.108701},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196144814&doi=10.1016%2Fj.compbiomed.2024.108701&partnerID=40&md5=6a50ce04957122140b02e8e8c30bb254},
affiliations = {Università degli Studi di Roma "Tor Vergata", Department of Biomedicine and Prevention, Rome, RM, Italy; Harvard Medical School, Boston, MA, United States},
abstract = {Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain–computer interfaces. Our study presents an innovative method that employs knowledge distillation to train an EEG classifier and reconstruct images from the ImageNet and THINGS-EEG 2 datasets using only electroencephalography (EEG) data from participants who have viewed the images themselves (i.e. “brain decoding”). We analyzed EEG recordings from 6 participants for the ImageNet dataset and 10 for the THINGS-EEG 2 dataset, exposed to images spanning unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 87%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism based on pre-trained latent diffusion models, which allowed us to generate an estimate of the images that had elicited EEG activity. Therefore, our architecture not only decodes images from neural activity but also offers a credible image reconstruction from EEG only, paving the way for, e.g., swift, individualized feedback experiments. © 2024 The Author(s)}, keywords = {Brain; Brain computer interface; Classification (of information); Convolution; Decoding; Distillation; Electroencephalography; Electrophysiology; Image classification; Neurons; Personnel training; Semantics; BCI vision; Brain activity; Brain decoding; Convolutional neural network; Diffusion model; Electroencephalography decoding; Human brain; Images reconstruction; Research domains; Visual representations; Image reconstruction; Article; classification algorithm; classifier; clustering algorithm; comparative study; computer vision; contrastive language image pre training; controlled study; convolutional neural network; deep learning; diffusion; distillation; electroencephalogram; electroencephalography; follow up; human; image reconstruction; k means clustering; knowledge; knowledge distillation; latent period; logistic regression analysis; long short term memory network; measurement accuracy; probability; recurrent neural network; short time Fourier transform; signal processing; time frequency decomposition; time series analysis; visual attention; visual memory; adult; artificial neural network; brain; brain computer interface; female; image processing; male; physiology; procedures; Adult; Brain-Computer Interfaces; Female; Humans; Image Processing, Computer-Assisted; Male; Neural Networks, Computer; Signal Processing, Computer-Assisted},
correspondence_address = {M. Ferrante; Department of Biomedicine and Prevention, University of Rome Tor Vergata (IT), Italy; email: matteo.ferrante@uniroma2.it},
publisher = {Elsevier Ltd},
issn = {18790534; 00104825},
coden = {CBMDA},
pmid = {38901186},
language = {English},
abbrev_source_title = {Comput. Biol. Med.},
type = {Article}}
@inproceedings{Huang2024Annual,
author = {Huang, Chin-Wei and Su, Chien-Hui and Kuo, Po-Chih},
booktitle = {2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
title = {Decoding Visual Perception from EEG Using Explainable Graph Neural Network},
year = {2024},
volume = {},
number = {},
pages = {1--4},
abstract = {Brain decoding is an emerging area in the fields of neuroscience and machine learning. The goal of decoding is to utilize measured brain activity to understand the thoughts or sensations of individuals. In the fields of computer vision and machine learning, Graph Neural Networks (GNNs) have demonstrated considerable success. Furthermore, the integration of attention mechanisms in these networks provides a pathway for improved model explainability. This study employs GNNs in the analysis of electroencephalography (EEG), aiming to explore how our brain handles visual information and uncover functional brain networks. We utilize GNNExplainer, a tool designed for GNN interpretation, to pinpoint critical EEG channels and their interconnections relevant to visual EEG tasks. Our findings, which align with existing neuroscience literature, underscore the significance of specific channels. This implies that GNNs have the capability to provide valuable insights within the field of neuroscience research.},
keywords = {Visualization;Neuroscience;Hafnium;Nearest neighbor methods;Brain modeling;Electroencephalography;Graph neural networks;Decoding;Engineering in medicine and biology;Visual perception;Visual decoding;EEG;Graph neural networks;Explainability},
doi = {10.1109/EMBC53108.2024.10782730},
issn = {2694-0604},
month = {July}}
@article{Ikegawa2024Text,
author = {Ikegawa, Yuya and Fukuma, Ryohei and Sugano, Hidenori Sugano and Oshino, Satoru and Tani, Naoki and Tamura, Kentaro and Iimura, Yasushi Iimura and Suzuki, Hiroharu and Yamamoto, Shota and Fujita, Yuya and Nishimoto, Shinji and Kishima, Haruhiko and Yanagisawa, Takufumi},
title = {Text and image generation from intracranial electroencephalography using an embedding space for text and images},
year = {2024},
journal = {Journal of Neural Engineering},
volume = {21},
number = {3},
pages = {},
doi = {10.1088/1741-2552/ad417a},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193773877&doi=10.1088%2F1741-2552%2Fad417a&partnerID=40&md5=6944c52692d205487b14736a17db2770},
affiliations = {The University of Osaka, Institute for Advanced Co-Creation Studies, Suita, Osaka, Japan; Graduate School of Medicine, Department of Neurosurgery, Suita, Osaka, Japan; Juntendo University, Department of Neurosurgery, Tokyo, Japan; Nara Medical University, Department of Neurosurgery, Kashihara, Nara, Japan; National Institute of Information and Communications Technology, Koganei, Tokyo, Japan; The University of Osaka, Graduate School of Frontier Biosciences, Suita, Osaka, Japan},
abstract = {Objective. Invasive brain-computer interfaces (BCIs) are promising communication devices for severely paralyzed patients. Recent advances in intracranial electroencephalography (iEEG) coupled with natural language processing have enhanced communication speed and accuracy. It should be noted that such a speech BCI uses signals from the motor cortex. However, BCIs based on motor cortical activities may experience signal deterioration in users with motor cortical degenerative diseases such as amyotrophic lateral sclerosis. An alternative approach to using iEEG of the motor cortex is necessary to support patients with such conditions. Approach. In this study, a multimodal embedding of text and images was used to decode visual semantic information from iEEG signals of the visual cortex to generate text and images. We used contrastive language-image pretraining (CLIP) embedding to represent images presented to 17 patients implanted with electrodes in the occipital and temporal cortices. A CLIP image vector was inferred from the high-γ power of the iEEG signals recorded while viewing the images. Main results. Text was generated by CLIPCAP from the inferred CLIP vector with better-than-chance accuracy. Then, an image was created from the generated text using StableDiffusion with significant accuracy. Significance. The text and images generated from iEEG through the CLIP embedding vector can be used for improved communication. © 2024 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Brain computer interface; Decoding; Deterioration; Electrophysiology; Embeddings; Image enhancement; Medical computing; Natural language processing systems; Neurodegenerative diseases; Semantics; Communication device; Image generations; Intracranial EEG; Motor-cortex; Natural languages; Neural decoding; Paralyzed patients; Pre-training; Text generations; Electroencephalography; Article; clinical article; contrastive language image pretraining; controlled study; electrocorticography; electroencephalography; embedding; human; multilayer perceptron; nested cross validation; occipital cortex; temporal cortex; adult; brain computer interface; electrode implant; female; male; middle aged; photostimulation; procedures; young adult; Adult; Brain-Computer Interfaces; Electrocorticography; Electrodes, Implanted; Female; Humans; Male; Middle Aged; Photic Stimulation; Young Adult},
correspondence_address = {T. Yanagisawa; Institute for Advanced Co-Creation Studies, Osaka University, Suita, Japan; email: tyanagisawa@nsurg.med.osaka-u.ac.jp},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {38648781},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Iwama2024Rapidiaf,
author = {Iwama, Seitaro and Ushiba, Junichi},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
title = {Rapid-IAF: Rapid Identification of Individual Alpha Frequency in EEG Data Using Sequential Bayesian Estimation},
year = {2024},
volume = {32},
number = {},
pages = {915--922},
abstract = {Rapid and robust identification of the individual alpha frequency (IAF) in electroencephalogram (EEG) is an essential factor for successful brain-computer interface (BCI) use. Here we demonstrate an algorithm to determine the IAF from short-term resting-state scalp EEG data. First, we outlined the algorithm to determine IAF from short-term resting scalp EEG data and evaluated its reliability using a large-scale dataset of scalp EEG during motor imagery-based BCI use and independent dataset for generalizability confirmation (N = 147). Next, we characterized the relationship between IAF and responsive frequency band of sensorimotor rhythm, which exhibits prominent event-related desynchronization (SMR-ERD) while attempting unilateral and movement. The proposed sequential Bayesian estimation algorithm (Rapid-IAF) determined IAF from less than 26-second resting EEG data among 95% of participants, indicating a clear advance over the conventional methods, which uses 2–15 minutes of data in previous literatures. We confirmed that the determined IAF corresponded to the frequency of SMR, which exhibits the most prominent event-related desynchronization during BCI use (individual SMR-ERD frequency, ISF). Moreover, intraclass correlation revealed that the estimated IAF was more stable than ISF across sessions, suggesting its reliability and utility for robust BCI use without intermittent recalibration. In summary, our method rapidly and reliably determined IAF compared to the conventional method using the spectral power change based on task-related response. The method can be utilized to quick BCI initialization. The demonstration of rapid, task-free parametrization of individual variability of neural responses would be of importance for future BCI systems including neural communication via a cursor, an avatar or robots, and closed-loop neurofeedback training.},
keywords = {Electroencephalography;Task analysis;Estimation;Frequency estimation;Bayes methods;Scalp;Rhythm;Brain-computer interface;brain-machine interface;individual alpha frequency;scalp electroencephalogram;sensorimotor rhythm},
doi = {10.1109/TNSRE.2024.3365197},
issn = {1558-0210},
month = {}}
@article{KhaliqFard2024Decoding,
author = {Khaliq Fard, Mahdie and Fallah, Ali and Maleki, Ali},
title = {Decoding temporal muscle synergy patterns based on brain activity for upper extremity in ADL movements},
year = {2024},
journal = {Cognitive Neurodynamics},
volume = {18},
number = {2},
pages = {349 - 356},
doi = {10.1007/s11571-022-09885-0},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139709002&doi=10.1007%2Fs11571-022-09885-0&partnerID=40&md5=929ea71991a7e6a104001572b3f75ac8},
affiliations = {Amirkabir University of Technology, Department of Biomedical Engineering, Tehran, Tehran, Iran; Semnan University, Semnan, Iran},
abstract = {Muscle synergies have been hypothesized as specific predefined motor primitives that the central nervous system can reduce the complexity of motor control by using them, but how these are expressed in brain activity is ambiguous yet. The main purpose of this paper is to develop synergy-based neural decoding of motor primitives, so for the first time, brain activity and muscle synergy map of the upper extremity was investigated in the activity of daily living movements. To find the relationship between brain activities and muscle synergies, electroencephalogram (EEG) and electromyogram (EMG) signals were acquired simultaneously during activities of daily living. To extract the maximum correlation of neural commands with muscle synergies, application of a combined partial least squares and canonical correlation analysis (PLS-CCA) method was proposed. The Elman neural network was used to decode the relationship between extracted motor commands and muscle synergies. The performance of proposed method was evaluated with tenfold cross-validation and muscle synergy estimation of brain activity with R, VAF, and MSE of 84 ± 2.6%, 70 ± 4.7%, and 0.00011 ± 0.00002 were quantified respectively. Furthermore, the similarity between actual and reconstructed muscle activations was achieved more than 92% for correlation coefficient. To compare with the existing methods, our results showed significantly more accuracy of the model performance. Our results confirm that use of the expression of muscle synergies in brain activity can estimate the neural decoding performance for motor control that can be used to develop neurorehabilitation tools such as neuroprosthesis. © The Author(s), under exclusive licence to Springer Nature B.V. 2022. Springer Nature or its licensor holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.}, keywords = {adult; Article; artificial neural network; controlled study; correlation analysis; correlation coefficient; cross validation; daily life activity; electroencephalogram; electromyogram; Elman neural network; female; human; human experiment; limb movement; male; motor control; motor dysfunction; muscle activation; muscle contraction; muscle function; muscle synergy; neurorehabilitation; normal human; partial least squares regression; temporalis muscle; upper limb},
correspondence_address = {A. Fallah; Biomedical Engineering Department, Amirkabir University of Technology, Tehran, Iran; email: afallah@aut.ac.ir},
publisher = {Springer Science and Business Media B.V.},
issn = {18714080; 18714099},
language = {English},
abbrev_source_title = {Cogn. Neurodynamics},
type = {Article}}
@article{Kim2024Brain,
author = {Kim, Hongji and Lux, Byeol Kim and Lee, Eunjin and Finn, Emily S. and Woo, Choongwan},
title = {Brain decoding of spontaneous thought: Predictive modeling of self-relevance and valence using personal narratives},
year = {2024},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
volume = {121},
number = {14},
pages = {},
doi = {10.1073/pnas.2401959121},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194715278&doi=10.1073%2Fpnas.2401959121&partnerID=40&md5=48737692194a78757439e2837779bab4},
affiliations = {Institute for Basic Science, Daejeon, Daejeon, South Korea; Sungkyunkwan University, Department of Biomedical Engineering, Seoul, South Korea; Sungkyunkwan University, Department of Intelligent Precision Healthcare Convergence, Seoul, South Korea; Faculty of Arts and Sciences, Hanover, NH, United States; Life-inspired Neural Network for Prediction and Optimization Research Group, Suwon, South Korea},
abstract = {The contents and dynamics of spontaneous thought are important factors for personality traits and mental health. However, assessing spontaneous thoughts is challenging due to their unconstrained nature, and directing participants’ attention to report their thoughts may fundamentally alter them. Here, we aimed to decode two key content dimensions of spontaneous thought—self-relevance and valence—directly from brain activity. To train functional MRI-based predictive models, we used individually generated personal stories as stimuli in a story-reading task to mimic narrative-like spontaneous thoughts (n = 49). We then tested these models on multiple test datasets (total n = 199). The default mode, ventral attention, and frontoparietal networks played key roles in the predictions, with the anterior insula and midcingulate cortex contributing to self-relevance prediction and the left temporoparietal junction and dorsomedial prefrontal cortex contributing to valence prediction. Overall, this study presents brain models of internal thoughts and emotions, highlighting the potential for the brain decoding of spontaneous thought. © 2024 the Author(s). Published by PNAS.}, keywords = {adult; anterior insula; article; attention; brain; dorsomedial prefrontal cortex; electroencephalogram; emotion; female; frontoparietal network; functional magnetic resonance imaging; human; male; mental health; narrative; personality; prediction; predictive model; temporoparietal junction; thinking},
correspondence_address = {C.-W. Woo; Center for Neuroscience Imaging Research, Institute for Basic Science, Suwon, 16419, South Korea; email: waniwoo@skku.edu},
publisher = {National Academy of Sciences},
issn = {10916490; 00278424},
coden = {PNASA},
language = {English},
abbrev_source_title = {Proc. Natl. Acad. Sci. U. S. A.},
type = {Article}}
@article{KoideMajima2024Mental,
author = {Koide-Majima, Naoko and Nishimoto, Shinji and Majima, Kei},
title = {Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based Bayesian estimation},
year = {2024},
journal = {Neural Networks},
volume = {170},
pages = {349 - 363},
doi = {10.1016/j.neunet.2023.11.024},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178352755&doi=10.1016%2Fj.neunet.2023.11.024&partnerID=40&md5=0c0563507dbd1c34cef2ea6a0fa4f2fd},
affiliations = {National Institute of Information and Communications Technology, Koganei, Tokyo, Japan; The University of Osaka, Suita, Osaka, Japan; Graduate School of Medicine, Suita, Osaka, Japan; Institute for Quantum Life Science, Chiba, Chiba, Japan; Japan Science and Technology Agency, Kawaguchi, Saitama, Japan},
abstract = {Visual images observed by humans can be reconstructed from their brain activity. However, the visualization (externalization) of mental imagery is challenging. Only a few studies have reported successful visualization of mental imagery, and their visualizable images have been limited to specific domains such as human faces or alphabetical letters. Therefore, visualizing mental imagery for arbitrary natural images stands as a significant milestone. In this study, we achieved this by enhancing a previous method. Specifically, we demonstrated that the visual image reconstruction method proposed in the seminal study by Shen et al. (2019) heavily relied on low-level visual information decoded from the brain and could not efficiently utilize the semantic information that would be recruited during mental imagery. To address this limitation, we extended the previous method to a Bayesian estimation framework and introduced the assistance of semantic information into it. Our proposed framework successfully reconstructed both seen images (i.e., those observed by the human eye) and imagined images from brain activity. Quantitative evaluation showed that our framework could identify seen and imagined images highly accurately compared to the chance accuracy (seen: 90.7%, imagery: 75.6%, chance accuracy: 50.0%). In contrast, the previous method could only identify seen images (seen: 64.3%, imagery: 50.4%). These results suggest that our framework would provide a unique tool for directly investigating the subjective contents of the brain such as illusions, hallucinations, and dreams. © 2023}, keywords = {Bayesian networks; Decoding; Deep neural networks; Image reconstruction; Neurophysiology; Semantics; Visualization; Bayesian estimations; Brain activity; Brain decoding; Human brain; Images reconstruction; Mental imagery; Mental images; Semantic representation; Semantics Information; Visual image; Brain; adult; Article; Bayesian network; comparative study; deep neural network; dream; electroencephalogram; female; hallucination; human; illusion; image reconstruction; male; measurement accuracy; neuroimaging; semantics; visual information; young adult; artificial neural network; Bayes theorem; brain; brain mapping; diagnostic imaging; image processing; imagination; nuclear magnetic resonance imaging; procedures; Bayes Theorem; Brain Mapping; Humans; Image Processing, Computer-Assisted; Imagination; Magnetic Resonance Imaging; Neural Networks, Computer},
correspondence_address = {K. Majima; Institute for Quantum Life Science, National Institutes for Quantum Science and Technology, Chiba, 263-8555, Japan; email: majima.kei@qst.go.jp},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
pmid = {38016230},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{Lee2024Selective,
author = {Lee, Juho and Choi, Jin-woo and Jo, Sungho},
title = {Selective Multi-Source Domain Adaptation Network for Cross-Subject Motor Imagery Discrimination},
year = {2024},
journal = {IEEE Transactions on Cognitive and Developmental Systems},
volume = {16},
number = {3},
pages = {923 - 934},
doi = {10.1109/TCDS.2023.3314351},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171755967&doi=10.1109%2FTCDS.2023.3314351&partnerID=40&md5=477979df5187a924abb8dac2f6b931e4},
affiliations = {Korea Advanced Institute of Science and Technology, School of Computing, Daejeon, South Korea; LG Electronics, Korea, AI Lab, Seoul, Yeongdeungpo-gu, South Korea; Korea Advanced Institute of Science and Technology, Information and Electronics Research Institute, Daejeon, South Korea; Stanford University School of Medicine, Stanford, CA, United States},
abstract = {Discriminating motor imagery with electroencephalogram (EEG)-based brain-computer interface (BCI) poses a challenge as it involves an extensive data acquisition phase that demands a substantial amount of effort from the user. To address this issue, one approach is to use unsupervised domain adaptation, where classification models are constructed using data from multiple subjects, and only the unlabeled data from the target user is used for model calibration. However, since brain patterns from motor imagery vary between individuals, the reliability of each subject must be considered when multiple subjects are used to build the classification model. Thus, in this article, we propose Selective-MDA that performs domain adaptation on each source subject and selectively limits influences based on their domain discrepancies. To evaluate our approach, we assess our results with two public dataset, BCI Competition IV IIa and the Autocalibration and Recurrent Adaptation dataset. We further investigate the effect of source selection by comparing the discrimination performance when different numbers of source domains are selected based on discrepancy measures. Our results demonstrate that Selective-MDA not only integrates multisource domain adaptation to cross-subject motor imagery discrimination but also highlights the impact of source domain selection when using data from multiple subjects for model training. © 2016 IEEE.}, keywords = {Data acquisition; Data mining; Electroencephalography; Electrophysiology; Feature extraction; Interfaces (computer); Job analysis; Adaptation models; Brain modeling; Classification models; Domain adaptation; Features extraction; Motor imagery; Multi-Sources; Neural decoding; Task analysis; Unsupervised domain adaptation; Brain computer interface},
correspondence_address = {S. Jo; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, 34141, South Korea; email: shjo@kaist.ac.kr},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {23798939; 23798920},
language = {English},
abbrev_source_title = {IEEE Trans. Cogn. Dev. Syst.},
type = {Article}}
@article{Li2024Registered,
author = {Li, Bingbing and Zhang, Shuhui},
title = {Registered Report Stage II: Decoding the category information from evoked potentials to visible and invisible visual objects},
year = {2024},
journal = {International Journal of Psychophysiology},
volume = {205},
pages = {},
doi = {10.1016/j.ijpsycho.2024.112446},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206260327&doi=10.1016%2Fj.ijpsycho.2024.112446&partnerID=40&md5=0764e25bbc0b0888d63565fd376a2db9},
affiliations = {Jiangsu Normal University, School of Education Science, Xuzhou, Jiangsu, China},
abstract = {Previous studies that use decoding methods and EEG to investigate the neural representation of the category information of visual objects focused mainly on consciously processed visual objects. It remains unclear whether the category information of unconsciously processed visual objects can be decoded and whether the decoding performance is different for consciously and unconsciously processed visual objects. The present study compared the neural decoding of the animacy category of visible and invisible visual objects via EEG and decoding methods. The results revealed that the animacy of visible visual objects could be decoded above the chance level by the P200, N300, and N400, but not by the early N/P100. However, the animacy of invisible visual objects could not be decoded above the chance level by neither early nor late ERP components. The decoding accuracy was greater for visible visual objects than that for invisible visual objects for the P200, N300 and N400. These results suggested that access to animacy category information for visual objects requires conscious processing. © 2024 Elsevier B.V.}, keywords = {Article; category information; comparative study; consciousness; controlled study; decision making; electroencephalogram; event related potential; evoked response; human; image processing; information; invisible visual object; measurement accuracy; nerve cell; neural decoding; object-based attention; registered report; unconsciousness; visible visual object; visual evoked potential; adolescent; adult; electroencephalography; female; male; photostimulation; physiology; procedures; reaction time; visual pattern recognition; young adult; Adolescent; Adult; Electroencephalography; Evoked Potentials; Evoked Potentials, Visual; Female; Humans; Male; Pattern Recognition, Visual; Photic Stimulation; Pre-Registration Publication; Reaction Time; Young Adult},
correspondence_address = {B. Li; School of Education Science, Jiangsu Normal University, Xuzhou, Jiangsu, China; email: lbb@jsnu.edu.cn},
publisher = {Elsevier B.V.},
issn = {18727697; 01678760},
coden = {IJPSE},
pmid = {39389167},
language = {English},
abbrev_source_title = {Int. J. Psychophysiol.},
type = {Article}}
@conference{Li2024Neurobolt,
author = {Li, Yamin and Lou, Ange and Xu, Ziyuan and Zhang, Shengchao and Wang, Shiyu and Englot, Dario J. and Kolouri, Soheil and Moyer, Daniel C. and Bayrak, Roza G. and Chang, Catie E.},
title = {NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping},
year = {2024},
journal = {Advances in Neural Information Processing Systems},
volume = {37},
pages = {},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000548354&partnerID=40&md5=40b02ce145bd8c375d203a06c8d65c5d},
affiliations = {Vanderbilt University, Nashville, TN, United States; Vanderbilt University Medical Center, Nashville, TN, United States},
abstract = {Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities. © 2024 Neural information processing systems foundation. All rights reserved.},
editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
publisher = {Neural information processing systems foundation},
issn = {10495258},
isbn = {9781558602748; 1558602747; 0262100762; 0262122413; 0262025507; 9780262232531; 0262042088; 9780262025508; 0262100657; 9781627480031},
language = {English},
abbrev_source_title = {Adv. neural inf. proces. syst.},
type = {Conference paper}}
@article{Liu2024Stbn,
author = {Liu, Chunyu and Cao, Bokai and Zhang, Jiacai},
title = {s-TBN: A New Neural Decoding Model to Identify Stimulus Categories From Brain Activity Patterns},
year = {2024},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {32},
pages = {1934 - 1943},
doi = {10.1109/TNSRE.2024.3399191},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192778867&doi=10.1109%2FTNSRE.2024.3399191&partnerID=40&md5=eb87ee8332244ca98c8435937767b6f7},
affiliations = {North China Electric Power University, School of Control and Computer Engineering, Beijing, China; Meta, Menlo Park, CA, United States; Beijing Normal University, School of Artificial Intelligence, Beijing, China},
abstract = {Neural decoding is still a challenging and a hot topic in neurocomputing science. Recently, many studies have shown that brain network patterns containing rich spatiotemporal structural information represent the brain's activation information under external stimuli. In the traditional method, brain network features are directly obtained using the standard machine learning method and provide to a classifier, subsequently decoding external stimuli. However, this method cannot effectively extract the multidimensional structural information hidden in the brain network. Furthermore, studies on tensors have show that the tensor decomposition model can fully mine unique spatiotemporal structural characteristics of a spatiotemporal structure in data with a multidimensional structure. This research proposed a stimulus-constrained Tensor Brain Network (s-TBN) model that involves the tensor decomposition and stimulus category-constraint information. The model was verified on real neuroimaging data obtained via magnetoencephalograph and functional mangetic resonance imaging). Experimental results show that the s-TBN model achieve accuracy matrices of greater than 11.06% and 18.46% on the accuracy matrix compared with other methods on two modal datasets. These results prove the superiority of extracting discriminative characteristics using the STN model, especially for decoding object stimuli with semantic information. © 2001-2011 IEEE.}, keywords = {Brain mapping; Decoding; Learning systems; Magnetic resonance imaging; Magnetoencephalography; Matrix algebra; Semantics; Tensors; Brain modeling; Brain networks; External stimulus; Matrix decomposition; Neural decoding; Structural information; Tensor brain network; Tensor decomposition; Brain; article; classifier; controlled study; decomposition; electroencephalogram; human; human experiment; machine learning; nerve cell network; neuroimaging; rat; stimulus; adult; algorithm; artificial neural network; biological model; brain; diagnostic imaging; female; magnetoencephalography; male; nuclear magnetic resonance imaging; physiology; procedures; reproducibility; young adult; Adult; Algorithms; Female; Humans; Machine Learning; Magnetic Resonance Imaging; Male; Models, Neurological; Nerve Net; Neural Networks, Computer; Reproducibility of Results; Young Adult},
correspondence_address = {J. Zhang; Beijing Normal University, School of Artificial Intelligence, Beijing, 100875, China; email: jiacai.zhang@bnu.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {38722722},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Liu2024Neural,
author = {Liu, Manyu and Liu, Ying and Feleke, A. Genetu and Fei, Weijie and Bi, Luzheng},
title = {Neural Signature and Decoding of Unmanned Aerial Vehicle Operators in Emergency Scenarios Using Electroencephalography},
year = {2024},
journal = {Sensors},
volume = {24},
number = {19},
pages = {},
doi = {10.3390/s24196304},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206343449&doi=10.3390%2Fs24196304&partnerID=40&md5=cb7ce9e3c972ae298dab354559d743d7},
affiliations = {Beijing Institute of Technology, School of Mechanical Engineering, Beijing, China},
abstract = {Brain–computer interface (BCI) offers a novel means of communication and control for individuals with disabilities and can also enhance the interactions between humans and machines for the broader population. This paper explores the brain neural signatures of unmanned aerial vehicle (UAV) operators in emergencies and develops an operator’s electroencephalography (EEG) signals-based detection method for UAV emergencies. We found regularity characteristics similar to classic event-related potential (ERP) components like visual mismatch negativity (vMMN) and contingent negative variation (CNV). Source analysis revealed a sequential activation of the occipital, temporal, and frontal lobes following the onset of emergencies, corresponding to the processing of attention, emotion, and motor intention triggered by visual stimuli. Furthermore, an online detection system was implemented and tested. Experimental results showed that the system achieved an average accuracy of over 88% in detecting emergencies with a detection latency of 431.95 ms from the emergency onset. This work lays a foundation for understanding the brain activities of operators in emergencies and developing an EEG-based detection method for emergencies to assist UAV operations. © 2024 by the authors.}, keywords = {Aircraft communication; Image coding; Interfaces (computer); Neurons; Aerial vehicle; Brain neural signature; Communication and control; Detection methods; Emergency detection; Emergency scenario; Event related potentials; Neural decoding; Neural signatures; Vehicle operators; Unmanned aerial vehicles (UAV); adult; aircraft; brain; brain computer interface; electroencephalography; emergency; evoked response; female; human; male; physiology; procedures; young adult; Adult; Aircraft; Brain; Brain-Computer Interfaces; Electroencephalography; Emergencies; Evoked Potentials; Female; Humans; Male; Young Adult},
correspondence_address = {W. Fei; School of Mechanical Engineering, Beijing Institute of Technology, Beijing, 100081, China; email: 7520230128@bit.edu.cn},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {14248220},
pmid = {39409342},
language = {English},
abbrev_source_title = {Sensors},
type = {Article}}
@article{Liu2024Joint,
author = {Liu, Qile and Zhou, Zhihao and Wang, Jiyuan and Liang, Zhen},
title = {Joint Contrastive Learning with Feature Alignment for Cross-Corpus EEG-based Emotion Recognition},
year = {2024},
pages = {9 - 17},
doi = {10.1145/3688862.3689112},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210850760&doi=10.1145%2F3688862.3689112&partnerID=40&md5=ac114a477edf36b8c30b4517bce8cdbc},
affiliations = {Shenzhen University, Shenzhen, Guangdong, China},
abstract = {The integration of human emotions into multimedia applications shows great potential for enriching user experiences and enhancing engagement across various digital platforms. Unlike traditional methods such as questionnaires, facial expressions, and voice analysis, brain signals offer a more direct and objective understanding of emotional states. However, in the field of electroencephalography (EEG)-based emotion recognition, previous studies have primarily concentrated on training and testing EEG models within a single dataset, overlooking the variability across different datasets. This oversight leads to significant performance degradation when applying EEG models to cross-corpus scenarios. In this study, we propose a novel Joint Contrastive learning framework with Feature Alignment (JCFA) to address cross-corpus EEG-based emotion recognition. The JCFA model operates in two main stages. In the pre-training stage, a joint domain contrastive learning strategy is introduced to characterize generalizable time-frequency representations of EEG signals, without the use of labeled data. It extracts robust time-based and frequency-based embeddings for each EEG sample, and then aligns them within a shared latent time-frequency space. In the fine-tuning stage, JCFA is refined in conjunction with downstream tasks, where the structural connections among brain electrodes are considered. The model capability could be further enhanced for the application in emotion detection and interpretation. Extensive experimental results on two well-recognized emotional datasets show that the proposed JCFA model achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy increase of 6.58% in cross-corpus EEG-based emotion recognition tasks. © 2024 Copyright held by the owner/author(s).}, keywords = {Biomedical signal processing; Brain; Connectors (structural); Speech recognition; Affective Computing; Cross-corpus; Digital platforms; Emotion recognition; Feature alignment; Human emotion; Joint contrastive learning; Multimedia applications; Neural decoding; Users' experiences; Emotion Recognition},
correspondence_address = {Z. Liang; Shenzhen University, Shenzhen, China; email: janezliang@szu.edu.cn},
publisher = {Association for Computing Machinery, Inc},
isbn = {9798400711893},
language = {English},
abbrev_source_title = {BCIMM - Proc. Int. Workshop Brain-Comput. Interfaces (BCI) Multimed. Underst., Co-Located: MM},
type = {Conference paper}}
@article{MacIntyre2024Neural,
author = {MacIntyre, Alexis Deighton and Carlyon, Robert P. and Goehring, Tobias},
title = {Neural Decoding of the Speech Envelope: Effects of Intelligibility and Spectral Degradation},
year = {2024},
journal = {Trends in Hearing},
volume = {28},
pages = {},
doi = {10.1177/23312165241266316},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202072709&doi=10.1177%2F23312165241266316&partnerID=40&md5=6c1d95cb763f1d0b18fa5b532c7e35f6},
affiliations = {MRC Cognition and Brain Sciences Unit, Cambridge, Cambridgeshire, United Kingdom},
abstract = {During continuous speech perception, endogenous neural activity becomes time-locked to acoustic stimulus features, such as the speech amplitude envelope. This speech–brain coupling can be decoded using non-invasive brain imaging techniques, including electroencephalography (EEG). Neural decoding may provide clinical use as an objective measure of stimulus encoding by the brain—for example during cochlear implant listening, wherein the speech signal is severely spectrally degraded. Yet, interplay between acoustic and linguistic factors may lead to top-down modulation of perception, thereby complicating audiological applications. To address this ambiguity, we assess neural decoding of the speech envelope under spectral degradation with EEG in acoustically hearing listeners (n = 38; 18–35 years old) using vocoded speech. We dissociate sensory encoding from higher-order processing by employing intelligible (English) and non-intelligible (Dutch) stimuli, with auditory attention sustained using a repeated-phrase detection task. Subject-specific and group decoders were trained to reconstruct the speech envelope from held-out EEG data, with decoder significance determined via random permutation testing. Whereas speech envelope reconstruction did not vary by spectral resolution, intelligible speech was associated with better decoding accuracy in general. Results were similar across subject-specific and group analyses, with less consistent effects of spectral degradation in group decoding. Permutation tests revealed possible differences in decoder statistical significance by experimental condition. In general, while robust neural decoding was observed at the individual and group level, variability within participants would most likely prevent the clinical use of such a measure to differentiate levels of spectral degradation and intelligibility on an individual basis. © The Author(s) 2024.}, keywords = {adolescent; adult; auditory stimulation; brain; electroencephalography; female; human; male; physiology; speech; speech intelligibility; speech perception; young adult; Acoustic Stimulation; Adolescent; Adult; Brain; Electroencephalography; Female; Humans; Male; Speech Acoustics; Speech Intelligibility; Speech Perception; Young Adult},
correspondence_address = {A.D. MacIntyre; MRC Cognition and Brain Sciences Unit, University of Cambridge, Cambridge, United Kingdom; email: AlexisDeighton.MacIntyre@mrc-cbu.cam.ac.uk},
publisher = {SAGE Publications Inc.},
issn = {23312165},
pmid = {39183533},
language = {English},
abbrev_source_title = {Trends Hear.},
type = {Article}}
@article{Meng2024Semanticsguided,
author = {Meng, Lu and Yang, Chuanhao},
title = {Semantics-Guided Hierarchical Feature Encoding Generative Adversarial Network for Visual Image Reconstruction from Brain Activity},
year = {2024},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {32},
pages = {1267 - 1283},
doi = {10.1109/TNSRE.2024.3377698},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188682462&doi=10.1109%2FTNSRE.2024.3377698&partnerID=40&md5=f154724b0cac48389337846fb00c6677},
affiliations = {College of Information Science and Engineering, Northeastern University, Shenyang, Liaoning, China},
abstract = {The utilization of deep learning techniques for decoding visual perception images from brain activity recorded by functional magnetic resonance imaging (fMRI) has garnered considerable attention in recent research. However, reconstructed images from previous studies still suffer from low quality or unreliability. Moreover, the complexity inherent to fMRI data, characterized by high dimensionality and low signal-to-noise ratio, poses significant challenges in extracting meaningful visual information for perceptual reconstruction. In this regard, we proposes a novel neural decoding model, named the hierarchical semantic generative adversarial network (HS-GAN), inspired by the hierarchical encoding of the visual cortex and the homology theory of convolutional neural networks (CNNs), which is capable of reconstructing perceptual images from fMRI data by leveraging the hierarchical and semantic representations. The experimental results demonstrate that HS-GAN achieved the best performance on Horikawa2017 dataset (histogram similarity: 0.447, SSIM-Acc: 78.9%, Peceptual-Acc: 95.38%, AlexNet(2): 96.24% and AlexNet(5): 94.82%) over existing advanced methods, indicating improved naturalness and fidelity of the reconstructed image. The versatility of the HS-GAN was also highlighted, as it demonstrated promising generalization capabilities in reconstructing handwritten digits, achieving the highest SSIM (0.783±0.038), thus extending its application beyond training solely on natural images. © 2001-2011 IEEE.}, keywords = {Brain; Decoding; Deep learning; Encoding (symbols); Functional neuroimaging; Generative adversarial networks; Image enhancement; Magnetic resonance imaging; Network coding; Neural networks; Semantic Web; Semantics; Signal to noise ratio; Brain activity; Features extraction; Functional magnetic resonance imaging; Images reconstruction; Reconstructed image; Resonance imaging data; Visual decoding; Image reconstruction; Article; attention network; convolutional neural network; electroencephalogram; functional magnetic resonance imaging; generative adversarial network; Hierarchical Encoder; image analysis; image reconstruction; image segmentation; information processing; lateral geniculate body; learning algorithm; machine learning; mathematical model; measurement accuracy; nerve cell network; nonhuman; nuclear magnetic resonance imaging; retina image; semantics; structural homology; visual image reconstruction; visual selective attention; artificial neural network; brain; human; image processing; procedures; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer},
correspondence_address = {L. Meng; Northeastern University, College of Information Science and Engineering, Shenyang, 110819, China; email: menglu1982@gmail.com},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {38498745},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@conference{Mentzelopoulos2024Neural,
author = {Mentzelopoulos, Georgios and Chatzipantazis, Evangelos and Ramayya, Ashwin G. and Hedlund, Michelle J. and Buch, Vivek P. and Daniilidis, Kostas and Kording, Konrad P. and Vitale, Flavia},
title = {Neural decoding from stereotactic EEG: accounting for electrode variability across subjects},
year = {2024},
journal = {Advances in Neural Information Processing Systems},
volume = {37},
pages = {},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000477733&partnerID=40&md5=c59c006aa088f53e47fff97da0a721f4},
affiliations = {University of Pennsylvania, Philadelphia, PA, United States; Stanford University, Stanford, CA, United States; Athena RC, Athena, United Kingdom},
abstract = {Deep learning based neural decoding from stereotactic electroencephalography (sEEG) would likely benefit from scaling up both dataset and model size. To achieve this, combining data across multiple subjects is crucial. However, in sEEG cohorts, each subject has a variable number of electrodes placed at distinct locations in their brain, solely based on clinical needs. Such heterogeneity in electrode number/placement poses a significant challenge for data integration, since there is no clear correspondence of the neural activity recorded at distinct sites between individuals. Here we introduce seegnificant: a training framework and architecture that can be used to decode behavior across subjects using sEEG data. We tokenize the neural activity within electrodes using convolutions and extract long-term temporal dependencies between tokens using self-attention in the time dimension. The 3D location of each electrode is then mixed with the tokens, followed by another self-attention in the electrode dimension to extract effective spatiotemporal neural representations. Subject-specific heads are then used for downstream decoding tasks. Using this approach, we construct a multi-subject model trained on the combined data from 21 subjects performing a behavioral task. We demonstrate that our model is able to decode the trial-wise response time of the subjects during the behavioral task solely from neural data. We also show that the neural representations learned by pretraining our model across individuals can be transferred in a few-shot manner to new subjects. This work introduces a scalable approach towards sEEG data integration for multi-subject model training, paving the way for cross-subject generalization for sEEG decoding. © 2024 Neural information processing systems foundation. All rights reserved.},
correspondence_address = {G. Mentzelopoulos; University of Pennsylvania, United States; email: gment@upenn.edu; F. Vitale; University of Pennsylvania, United States; email: vitalef@pennmedicine.upenn.edu},
editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
publisher = {Neural information processing systems foundation},
issn = {10495258},
isbn = {9781558602748; 1558602747; 0262100762; 0262122413; 0262025507; 9780262232531; 0262042088; 9780262025508; 0262100657; 9781627480031},
language = {English},
abbrev_source_title = {Adv. neural inf. proces. syst.},
type = {Conference paper}}
@article{Mishra2024Classification,
author = {Mishra, Rahul and Bhavsar, Arnav V.},
title = {EEG classification based on visual stimuli via adversarial learning},
year = {2024},
journal = {Cognitive Neurodynamics},
volume = {18},
number = {3},
pages = {1135 - 1151},
doi = {10.1007/s11571-023-09967-7},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158097867&doi=10.1007%2Fs11571-023-09967-7&partnerID=40&md5=51cef5989d28048bdfb502d2da54b90e},
affiliations = {Indian Institute of Technology Mandi, School of Computing and Electrical Engineering, Mandi, HP, India},
abstract = {In this work, we propose a dual path deep learning architecture for the application of visual brain decoding. The inputs to the proposed network are the electroencephalogram (EEG) signals which are evoked due to the external stimuli, specifically, images in this case. The objective is to classify the EEG signals based on the image categories under which they were evoked. Our approach involves the combinations of convolution neural networks (CNN) on the time axis and the channel axis. Importantly, for the purpose of learning subject-invariant features, we also make use of the gradient reversal layer (GRL). This addition to our network boosts the performance of our system. In addition, we also propose to use guided back-propagation for the selection of more informative EEG channels, and finally, with the reduced number of channels, we estimate the performance of the proposed network which is almost similar to the version when considering all EEG channels. © The Author(s), under exclusive licence to Springer Nature B.V. 2023.}, keywords = {article; back propagation; convolutional neural network; deep learning; electroencephalogram; learning},
correspondence_address = {R. Mishra; MANAS Lab, School of Computing and Electrical Engineering, Indian Institute of Technology Mandi, Mandi, India; email: d16043@students.iitmandi.ac.in},
publisher = {Springer Science and Business Media B.V.},
issn = {18714080; 18714099},
language = {English},
abbrev_source_title = {Cogn. Neurodynamics},
type = {Article}}
@article{Mou2024Chineseeeg,
author = {Mou, Xinyu and He, Cuilin and Tan, Liwei and Yu, Junjie and Liang, Huadong and Zhang, Jianyu and Tian, Yan and Yang, Yufang and Xu, Ting and Wang, Qing and Cao, Miao and Chen, Zijiao and Hu, Chuanpeng and Wang, Xindi and Liu, Quanying and Wu, Haiyan},
title = {ChineseEEG: A Chinese Linguistic Corpora EEG Dataset for Semantic Alignment and Neural Decoding},
year = {2024},
journal = {Scientific Data},
volume = {11},
number = {1},
pages = {},
doi = {10.1038/s41597-024-03398-7},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194813748&doi=10.1038%2Fs41597-024-03398-7&partnerID=40&md5=65b1dec615b95e0575e4f662a119e021},
affiliations = {Southern University of Science and Technology, Department of Biomedical Engineering, Shenzhen, Guangdong, China; University of Macau, Department of Psychology, Taipa, Macao; IFLYTEK Co., Ltd., AI Research Institute, Hefei, Anhui, China; Freie Universität Berlin, Department of Education and Psychology, Berlin, Berlin, Germany; Child Mind Institute, Inc., Center for the Integrative Developmental Neuroscience, New York, NY, United States; Shanghai Mental Health Center, Shanghai, China; Swinburne University of Technology, Hawthorn, VIC, Australia; NUS Yong Loo Lin School of Medicine, Centre for Sleep and Cognition, Singapore City, Singapore; Nanjing Normal University, School of Psychology, Nanjing, Jiangsu, China},
abstract = {An Electroencephalography (EEG) dataset utilizing rich text stimuli can advance the understanding of how the brain encodes semantic information and contribute to semantic decoding in brain-computer interface (BCI). Addressing the scarcity of EEG datasets featuring Chinese linguistic stimuli, we present the ChineseEEG dataset, a high-density EEG dataset complemented by simultaneous eye-tracking recordings. This dataset was compiled while 10 participants silently read approximately 13 hours of Chinese text from two well-known novels. This dataset provides long-duration EEG recordings, along with pre-processed EEG sensor-level data and semantic embeddings of reading materials extracted by a pre-trained natural language processing (NLP) model. As a pilot EEG dataset derived from natural Chinese linguistic stimuli, ChineseEEG can significantly support research across neuroscience, NLP, and linguistics. It establishes a benchmark dataset for Chinese semantic decoding, aids in the development of BCIs, and facilitates the exploration of alignment between large language models and human cognitive processes. It can also aid research into the brain’s mechanisms of language processing within the context of the Chinese natural language. © The Author(s) 2024.},
keywords = {brain; brain computer interface; China; electroencephalography; human; language; linguistics; natural language processing; physiology; reading; semantics; Brain; Brain-Computer Interfaces; Electroencephalography; Humans; Language; Linguistics; Natural Language Processing; Reading; Semantics},
correspondence_address = {Q. Liu; Department of Biomedical Engineering, Southern University of Science and Technology, Shenzhen, China; email: liuqy@sustech.edu.cn; H. Wu; Centre for Cognitive and Brain Sciences, Department of Psychology, Faculty of Social Sciences, University of Macau, Taipa, SAR, Macao; email: haiyanwu@um.edu.mo},
publisher = {Nature Research},
issn = {20524463},
pmid = {38811613},
language = {English},
abbrev_source_title = {Sci. Data},
type = {Data paper}}
@article{Pan2024Reconstructing,
author = {Pan, Honggguang and Li, Zhuoyi and Fu, Yunpeng and Qin, Xuebin and Hu, Jianchen},
title = {Reconstructing Visual Stimulus Representation from EEG Signals Based on Deep Visual Representation Model},
year = {2024},
journal = {IEEE Transactions on Human-Machine Systems},
volume = {54},
number = {6},
pages = {711 - 722},
doi = {10.1109/THMS.2024.3407875},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204587450&doi=10.1109%2FTHMS.2024.3407875&partnerID=40&md5=80c4cce05a9d00268f9ae0473fbc6919},
affiliations = {Xi'an University of Science and Technology, College of Electrical and Control Engineering, Xi'an, Shaanxi, China; Xi’an Key Laboratory of Electrical Equipment Condition Monitoring and Power Supply Security, Xi'an, China; Xi'an Jiaotong University, School of Automation Science and Engineering, Xi'an, Shaanxi, China},
abstract = {Reconstructing visual stimulus representation is a significant task in neural decoding. Until now, most studies have considered functional magnetic resonance imaging (fMRI) as the signal source. However, fMRI-based image reconstruction methods are challenging to apply widely due to the complexity and high cost of acquisition equipment. Taking into account the advantages of the low cost and easy portability of electroencephalogram (EEG) acquisition equipment, we propose a novel image reconstruction method based on EEG signals in this article. First, to meet the high recognizability of visual stimulus images in a fast-switching manner, we construct a visual stimuli image dataset and obtain the corresponding EEG dataset through EEG signals collection experiment. Second, we introduce the deep visual representation model (DVRM), comprising a primary encoder and a subordinate decoder, to reconstruct visual stimuli representation. The encoder is designed based on residual-in-residual dense blocks to learn the distribution characteristics between EEG signals and visual stimulus images. Meanwhile, the decoder is designed using a deep neural network to reconstruct the visual stimulus representation from the learned deep visual representation. The DVRM can accommodate the deep and multiview visual features of the human natural state, resulting in more precise reconstructed images. Finally, we evaluate the DVRM based on the quality of the generated images using our EEG dataset. The results demonstrate that the DVRM exhibits an excellent performance in learning deep visual representation from EEG signals, generating reconstructed representation of images that are realistic and highly resemble the original images. © 2013 IEEE.}, keywords = {Amplitude modulation; Brain mapping; Deep neural networks; Image acquisition; Image reconstruction; Magnetic resonance imaging; Deep visual representation model; Electroencephalogram dataset; Electroencephalogram signals; Functional magnetic resonance imaging; Image reconstruction methods; Images reconstruction; Neural decoding; Representation model; Visual representations; Visual stimulus; Electroencephalography},
correspondence_address = {H. Pan; Xi'An University of Science and Technology, College of Electrical and Control Engineering, Xi'an, 710054, China; email: hongguangpan@163.com},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21682291},
language = {English},
abbrev_source_title = {IEEE Trans. Human Mach. Syst.},
type = {Article}}
@article{Park2024Decoding,
author = {Park, Seungbin and Lipton, Megan H. and Dadarlat, Maria C.},
title = {Decoding multi-limb movements from two-photon calcium imaging of neuronal activity using deep learning},
year = {2024},
journal = {Journal of Neural Engineering},
volume = {21},
number = {6},
pages = {},
doi = {10.1088/1741-2552/ad83c0},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209159480&doi=10.1088%2F1741-2552%2Fad83c0&partnerID=40&md5=10aed331d16477c49785aae0080f41a7},
affiliations = {Weldon School of Biomedical Engineering, West Lafayette, IN, United States},
abstract = {Objective. Brain-machine interfaces (BMIs) aim to restore sensorimotor function to individuals suffering from neural injury and disease. A critical step in implementing a BMI is to decode movement intention from recorded neural activity patterns in sensorimotor areas. Optical imaging, including two-photon (2p) calcium imaging, is an attractive approach for recording large-scale neural activity with high spatial resolution using a minimally-invasive technique. However, relating slow two-photon calcium imaging data to fast behaviors is challenging due to the relatively low optical imaging sampling rates. Nevertheless, neural activity recorded with 2p calcium imaging has been used to decode information about stereotyped single-limb movements and to control BMIs. Here, we expand upon prior work by applying deep learning to decode multi-limb movements of running mice from 2p calcium imaging data. Approach. We developed a recurrent encoder-decoder network (LSTM-encdec) in which the output is longer than the input. Main results. LSTM-encdec could accurately decode information about all four limbs (contralateral and ipsilateral front and hind limbs) from calcium imaging data recorded in a single cortical hemisphere. Significance. Our approach provides interpretability measures to validate decoding accuracy and expands the utility of BMIs by establishing the groundwork for control of multiple limbs. Our work contributes to the advancement of neural decoding techniques and the development of next-generation optical BMIs. © 2024 The Author(s).}, keywords = {Brain; Deep neural networks; Image coding; Image resolution; Network coding; Optical image storage; Brain-machine interface; Calcium imaging; Deep learning; Machine interfaces; Multi-limb; Neural decoding; Optical brain-machine interface; Optical-; Sensorimotors; Two photon; Two-photon calcium imaging; Neurons; calcium; isoflurane; adult; animal behavior; animal experiment; Article; autoencoder; comparative study; controlled study; deconvolution algorithm; deep learning; electroencephalogram; explainable artificial intelligence; fluorescence imaging; habituation; hindlimb; least absolute shrinkage and selection operator; limb movement; long short term memory network; male; mouse; nerve potential; neuroimaging; neuropil; nonhuman; recurrent encoder decoder network; regression model; root mean squared error; running; sensorimotor function; animal; brain computer interface; C57BL mouse; innervation; limb; metabolism; movement (physiology); nerve cell; physiology; Animals; Brain-Computer Interfaces; Calcium; Deep Learning; Extremities; Male; Mice; Mice, Inbred C57BL; Movement},
correspondence_address = {M.C. Dadarlat; Weldon School of Biomedical Engineering, Purdue University, West Lafayette, 47906, United States; email: mdadarla@purdue.edu},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {39508456},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Pei2024Investigating,
author = {Pei, Dingyi and Vinjamuri, Ramana Kumar},
title = {Investigating the role of cortical rhythms in modulating kinematic synergies and exploring their potential for stroke rehabilitation},
year = {2024},
pages = {253 - 263},
doi = {10.1016/B978-0-323-95439-6.00019-3},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213783750&doi=10.1016%2FB978-0-323-95439-6.00019-3&partnerID=40&md5=5303a625e6b137d7871d2b12211b2a22},
affiliations = {College of Engineering and Information Technology, Vinjamuri Lab, Baltimore, MD, United States},
abstract = {Synergies have been demonstrated to play a significant role in brain–machine interfaces controlling hand exoskeletons or robotic systems for motor assistance and rehabilitation. However, one major challenge of utilizing BMIs for individuals with stroke is the changes in cortical rhythms encoded in the motor-related areas. The suppression of sensorimotor rhythms in individuals with stroke may affect the decoding accuracy of BMIs. Therefore, investigating how changes in cortical rhythms influence synergy modulation is of paramount importance. This study aimed to explore the performance of the neural decoding of hand kinematics based on a linear model established between kinematic synergies and corresponding cortical rhythms. Our analysis successfully decoded two typical hand grasps representative of daily activities from cortical rhythms obtained from electroencephalography Results offer promise for applications in noninvasive, synergy-based neuromotor control and rehabilitation, particularly for individuals with upper limb motor disabilities due to stroke. © 2025 Elsevier Inc. All rights are reserved including those for text and data mining AI training and similar technologies.},
keywords = {Brain–machine interfaces; EEG; Hand kinematics; Kinematic synergies; Motor control; Neural decoding},
publisher = {Elsevier},
isbn = {9780323954396; 9780323954402},
language = {English},
abbrev_source_title = {Brain-Computer Interfaces},
type = {Book chapter}}
@conference{Rambhia2024Cognisonance,
author = {Rambhia, Jeenal and Sutar, Rajendra G.},
title = {Cogni-Sonance: Navigating the Neurological Orchestra of Imagined Speech},
year = {2024},
pages = {178 - 183},
doi = {10.1109/GlobalAISummit62156.2024.10947883},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003242739&doi=10.1109%2FGlobalAISummit62156.2024.10947883&partnerID=40&md5=da81ac715d1461ece4ce0f29395558ac},
affiliations = {Sardar Patel Institute of Technology, Mumbai, Department of Electronics and Telecommunication Engineering, Mumbai, MH, India},
abstract = {The silent expression of language in the mind, or imagined speech, is an intriguing yet mysterious feature of human thought. This paper offers a thorough analysis of the illusory speech using Electroencephalography signals. We examine the complex interactions between brain oscillations and functional connectivity patterns related to imagined speech tasks, building on recent developments in EEG methodology and processing tools. In order to clarify the cortical networks and activation patterns involved in the production and processing of inner speech, we investigate the spatiotemporal dynamics of EEG signals. The dataset contains 3 different paradigms of imagination i.e. imagined digits, imagined alphabets and imagined images. The bagged tree model has given the best classification accuracy. For three class the classification accuracy is 56.7% and for binary class the maximum obtained accuracy is 70.1%. We have also emphasized on having lighter models which have less training time. EEG research provides us with a window into the inner workings of the human mind and may open up new avenues for helping those who struggle with language comprehension or speech. Through the use of EEG to decode the neurological foundations of imagined speech, we have found the source localization for all the different modalities of imagination. we hope to shed light on human inner monologue and open new avenues for creative applications in neurorehabilitation, brain-computer interfaces, and mental health therapies. © 2024 IEEE.}, keywords = {Brain computer interface; Neurons; Speech analysis; Brain oscillations; Classification accuracy; Cognitive neurosciences; Cortical network; Functional connectivity patterns; Imagined speech; Machine-learning; Mental imagery; Neural decoding; Processing tools; Electroencephalography},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9798350379716},
language = {English},
abbrev_source_title = {Proc. Int. Conf. Artif. Intell. Emerg. Technol., Global AI Summit},
type = {Conference paper}}
@article{Rudoler2024Decoding,
author = {Rudoler, Joseph H. and Bruska, James P. and Chang, Woohyeuk and Dougherty, Matthew R. and Katerman, Brandon S. and Halpern, David J. and Diamond, Nicholas B. and Kahana, Michael Jacob},
title = {Decoding EEG for optimizing naturalistic memory},
year = {2024},
journal = {Journal of Neuroscience Methods},
volume = {410},
pages = {},
doi = {10.1016/j.jneumeth.2024.110220},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200819833&doi=10.1016%2Fj.jneumeth.2024.110220&partnerID=40&md5=1abf60247f278f0e1cea81e78bdc5578},
affiliations = {University of Pennsylvania, Philadelphia, PA, United States},
abstract = {Background: Spectral features of human electroencephalographic (EEG) recordings during learning predict subsequent recall variability. New method: Capitalizing on these fluctuating neural features, we develop a non-invasive closed-loop (NICL) system for real-time optimization of human learning. Participants play a virtual navigation-and-memory game; recording multi-session data across days allowed us to build participant-specific classification models of recall success. In subsequent closed-loop sessions, our platform manipulated the timing of memory encoding, selectively presenting items during periods of predicted good or poor memory function based on EEG features decoded in real time. Results: The induced memory effect (the difference between recall rates when presenting items during predicted good vs. poor learning periods) increased with the accuracy of neural decoding. Comparison with Existing Methods: This study demonstrates greater-than-chance memory decoding from EEG recordings in a naturalistic virtual navigation task with greater real-world validity than basic word-list recall paradigms. Here we modulate memory by timing stimulus presentation based on noninvasive scalp EEG recordings, whereas prior closed-loop studies for memory improvement involved intracranial recordings and direct electrical stimulation. Other noninvasive studies have investigated the use of neurofeedback or remedial study for memory improvement. Conclusions: These findings present a proof-of-concept for using non-invasive closed-loop technology to optimize human learning and memory through principled stimulus timing, but only in those participants for whom classifiers reliably predict out-of-sample memory function. © 2024 Elsevier B.V.}, keywords = {adult; Article; binary classification; clinical article; controlled study; coronavirus disease 2019; electroencephalogram; electrostimulation; episodic memory; feature extraction; human; informed consent; machine learning; memory disorder; Morlet wavelet transform; motion sickness; neurofeedback; receiver operating characteristic; brain; electroencephalography; female; male; physiology; procedures; recall; young adult; Adult; Brain; Electroencephalography; Female; Humans; Male; Mental Recall; Young Adult},
correspondence_address = {M.J. Kahana; University of Pennsylvania, United States; email: kahana@psych.upenn.edu},
publisher = {Elsevier B.V.},
issn = {01650270; 1872678X},
coden = {JNMED},
pmid = {39033965},
language = {English},
abbrev_source_title = {J. Neurosci. Methods},
type = {Article}}
@article{Ryb2024Data,
author = {Rybář, Milan and Poli, Riccardo and Daly, Ian},
title = {Using data from cue presentations results in grossly overestimating semantic BCI performance},
year = {2024},
journal = {Scientific Reports},
volume = {14},
number = {1},
pages = {},
doi = {10.1038/s41598-024-79309-y},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209101763&doi=10.1038%2Fs41598-024-79309-y&partnerID=40&md5=28599e779375d082f74cff5d8d2d4302},
affiliations = {University of Essex, Brain-Computer Interfaces and Neural Engineering Laboratory, Colchester, Essex, United Kingdom},
abstract = {Neuroimaging studies have reported the possibility of semantic neural decoding to identify specific semantic concepts from neural activity. This offers promise for brain-computer interfaces (BCIs) for communication. However, translating these findings into a BCI paradigm has proven challenging. Existing EEG-based semantic decoding studies often rely on neural activity recorded when a cue is present, raising concerns about decoding reliability. To address this, we investigate the effects of cue presentation on EEG-based semantic decoding. In an experiment with a clear separation between cue presentation and mental task periods, we attempt to differentiate between semantic categories of animals and tools in four mental tasks. By using state-of-the-art decoding analyses, we demonstrate significant mean classification accuracies up to 71.3% during cue presentation but not during mental tasks, even with adapted analyses from previous studies. These findings highlight a potential issue when using neural activity recorded during cue presentation periods for semantic decoding. Additionally, our results show that semantic decoding without external cues may be more challenging than current state-of-the-art research suggests. By bringing attention to these issues, we aim to stimulate discussion and drive advancements in the field toward more effective semantic BCI applications. © The Author(s) 2024.}, keywords = {adult; association; brain; brain computer interface; electroencephalography; female; human; male; physiology; procedures; semantics; young adult; Adult; Brain; Brain-Computer Interfaces; Cues; Electroencephalography; Female; Humans; Male; Semantics; Young Adult},
correspondence_address = {M. Rybář; Brain-Computer Interfaces and Neural Engineering Laboratory, School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, United Kingdom; email: contact@milanrybar.cz; I. Daly; Brain-Computer Interfaces and Neural Engineering Laboratory, School of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, United Kingdom; email: i.daly@essex.ac.uk},
publisher = {Nature Research},
issn = {20452322},
pmid = {39543314},
language = {English},
abbrev_source_title = {Sci. Rep.},
type = {Article}}
@conference{Sharma2024Classification,
author = {Sharma, Akanksha and Nigam, Jyoti and Rathore, Abhishek and Bhavsar, Arnav V.},
title = {EEG classification for visual brain decoding with spatio-temporal and transformer based paradigms},
year = {2024},
pages = {},
doi = {10.1145/3702250.3702286},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216804791&doi=10.1145%2F3702250.3702286&partnerID=40&md5=a9fb369060a07d8b6f1ee3650bce6d4e},
affiliations = {Indian Institute of Technology Mandi, Mandi, HP, India},
abstract = {In this work, we delve into the EEG classification task in the domain of visual brain decoding via two frameworks, involving two different learning paradigms. Considering the spatio-temporal nature of EEG data, one of our frameworks is based on a CNN-BiLSTM model. The other involves a CNN-Transformer architecture which inherently involves the more versatile attention based learning paradigm. In both cases, a special 1D-CNN feature extraction module is used to generate the initial embeddings with 1D convolutions in the time and the EEG channel domains. Considering the EEG signals are noisy, non stationary and the discriminative features are even less clear (than in semantically structured data such as text or image), we also follow a window-based classification followed by majority voting during inference, to yield labels at a signal level. To illustrate how brain patterns correlate with different image classes, we visualize t-SNE plots of the embeddings from the better performing method alongside brain activation maps for the top 10 classes. These visualizations provide insightful revelations into the distinct associations of neural signatures and learnt embeddings that capture and represent the discriminative brain activity linked to visual stimuli. We demonstrate the performance of our approach on the updated EEG-Imagenet dataset with positive comparisons with state-of-the-art methods. Our best performing model (CNN-BiLSTM) yields an accuracy of 71%, a significant improvement over various existing methods. While the CNN-Transformer approach does not perform as well, its performance is still shown to be better than the existing Transformer based methods in this domain. © 2024 Copyright held by the owner/author(s).}, keywords = {Brain mapping; Brain decoding; CNN-BiLSTM; CNN-transformer; EEG classification; EEG-imagenet dataset; Embeddings; Learning paradigms; Performance; Spatio-temporal; Visual brain decoding; Electroencephalography},
correspondence_address = {A. Sharma; SCEE, IIT Mandi, MANDI, HIMACHAL PRADESH, India; email: t22110@students.iitmandi.ac.in},
publisher = {Association for Computing Machinery},
isbn = {9798400710759; 9798400717260; 9798400711855},
language = {English},
abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
type = {Conference paper}}
@conference{Song2024Decoding,
author = {Song, Yonghao and Liu, Bingchuan and Li, Xiang and Shi, Nanlin and Wang, Yijun and Gao, Xiaorong Rong},
title = {DECODING NATURAL IMAGES FROM EEG FOR OBJECT RECOGNITION},
year = {2024},
pages = {},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200580106&partnerID=40&md5=c79185bbf6fc81f4e06798b4df495ac0},
affiliations = {Tsinghua University, Department of Biomedical Engineering, Beijing, China; Institute of Semiconductors Chinese Academy of Sciences, Beijing, China},
abstract = {Electroencephalography (EEG) signals, known for convenient non-invasive acquisition but low signal-to-noise ratio, have recently gained substantial attention due to the potential to decode natural images. This paper presents a self-supervised framework to demonstrate the feasibility of learning image representations from EEG signals, particularly for object recognition. The framework utilizes image and EEG encoders to extract features from paired image stimuli and EEG responses. Contrastive learning aligns these two modalities by constraining their similarity. Our approach achieves state-of-the-art results on a comprehensive EEG-image dataset, with a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in 200-way zero-shot tasks. Moreover, we perform extensive experiments to explore the biological plausibility by resolving the temporal, spatial, spectral, and semantic aspects of EEG signals. Besides, we introduce attention modules to capture spatial correlations, providing implicit evidence of the brain activity perceived from EEG data. These findings yield valuable insights for neural decoding and brain-computer interfaces in real-world scenarios. Code available at https://github.com/eeyhsong/NICE-EEG. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.},
keywords = {Brain; Brain computer interface; Decoding; Electrophysiology; Object recognition; Semantics; Signal to noise ratio; Zero-shot learning; Brain activity; Image datasets; Image representations; Image stimulus; Low signal-to-noise ratio; Natural images; Objects recognition; Spatial correlations; Spatial semantics; State of the art; Electroencephalography},
correspondence_address = {X. Gao; Department of Biomedical Engineering, Tsinghua University, China; email: gxr-dea@tsinghua.edu.cn},
publisher = {International Conference on Learning Representations, ICLR},
language = {English},
abbrev_source_title = {Int. Conf. Learn. Represent., ICLR},
type = {Conference paper}}
@article{Sorensen2024Neural,
author = {Sorensen, David O. and Avcu, Enes and Lynch, Skyla and Ahlfors, Seppo P. and Gow, David W.},
title = {Neural representation of phonological wordform in temporal cortex},
year = {2024},
journal = {Psychonomic Bulletin and Review},
volume = {31},
number = {6},
pages = {2659 - 2671},
doi = {10.3758/s13423-024-02511-6},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191744223&doi=10.3758%2Fs13423-024-02511-6&partnerID=40&md5=b0e916d2e08b3678904255b02635a3e0},
affiliations = {Harvard Medical School, Division of Medical Sciences, Boston, MA, United States; Harvard Medical School, Boston, MA, United States; Massachusetts General Hospital, Boston, MA, United States; Harvard Medical School, Department of Radiology, Boston, MA, United States; Salem State University, Department of Psychology, Salem, MA, United States; Massachusetts General Hospital, Neurodynamics and Neural Decoding Group, Boston, MA, United States},
abstract = {While the neural bases of the earliest stages of speech categorization have been widely explored using neural decoding methods, there is still a lack of consensus on questions as basic as how wordforms are represented and in what way this word-level representation influences downstream processing in the brain. Isolating and localizing the neural representations of wordform is challenging because spoken words activate a variety of representations (e.g., segmental, semantic, articulatory) in addition to form-based representations. We addressed these challenges through a novel integrated neural decoding and effective connectivity design using region of interest (ROI)-based, source-reconstructed magnetoencephalography/electroencephalography (MEG/EEG) data collected during a lexical decision task. To identify wordform representations, we trained classifiers on words and nonwords from different phonological neighborhoods and then tested the classifiers' ability to discriminate between untrained target words that overlapped phonologically with the trained items. Training with word neighbors supported significantly better decoding than training with nonword neighbors in the period immediately following target presentation. Decoding regions included mostly right hemisphere regions in the posterior temporal lobe implicated in phonetic and lexical representation. Additionally, neighbors that aligned with target word beginnings (critical for word recognition) supported decoding, but equivalent phonological overlap with word codas did not, suggesting lexical mediation. Effective connectivity analyses showed a rich pattern of interaction between ROIs that support decoding based on training with lexical neighbors, especially driven by right posterior middle temporal gyrus. Collectively, these results evidence functional representation of wordforms in temporal lobes isolated from phonemic or semantic representations. © The Author(s) 2024.}, keywords = {adult; electroencephalography; female; human; magnetoencephalography; male; phonetics; physiology; psycholinguistics; speech perception; temporal lobe; young adult; Adult; Electroencephalography; Female; Humans; Magnetoencephalography; Male; Phonetics; Psycholinguistics; Speech Perception; Temporal Lobe; Young Adult},
correspondence_address = {D.W. Gow; Division of Medical Sciences, Harvard Medical School, Cambridge, United States; email: dgow@mgh.harvard.edu},
publisher = {Springer},
issn = {10699384; 15315320},
pmid = {38689188},
language = {English},
abbrev_source_title = {Psychonom. Bull. Rev.},
type = {Article}}
@conference{Sugimoto2024Image,
author = {Sugimoto, Yuma and Pongthanisorn, Goragod and Capi, Genci},
title = {Image Generation using EEG data: A Contrastive Learning based Approach},
year = {2024},
journal = {Canadian Conference on Electrical and Computer Engineering},
pages = {794 - 798},
doi = {10.1109/CCECE59415.2024.10667256},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205008610&doi=10.1109%2FCCECE59415.2024.10667256&partnerID=40&md5=ca49f28a96db04b22ceb7a190fa9d066},
affiliations = {Hosei University, Department of Mechanical Engineering, Tokyo, Japan; Hosei University, Graduate School of Science and Engineering, Tokyo, Japan},
abstract = {In recent years, there has been active research on Neural Decoding aims to decrypt perceptual cognitive content, recall content, and motor information directly from brain signals. Simultaneously, deep learning has been widely implemented especially in generative models. Diverse architectures and learning methods have been formulated and applied across numerous fields. In this work, we focus on generating perceptual and cognitive contents using brain electroencephalography (EEG) data. We propose and demonstrate the efficacy of contrastive learning to generate images only from EEG data. We compare the performance of two electrode settings 1) visual cortex and 2) motor cortex. © 2024 IEEE.}, keywords = {Active learning; Adversarial machine learning; Brain signals; Electroencephalography signal; Generative model; Image generations; Learning methods; Learning-based approach; Neural decoding; Performance; Two electrodes; Visual imagery; Contrastive Learning},
correspondence_address = {Y. Sugimoto; Hosei University, Department of Mechanical Engineering, Tokyo, Japan; email: yuma.sugimoto.6t@stu.hosei.ac.jp},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {08407789},
isbn = {9781424497898; 9781424416431; 0780382536; 9781728154428; 9781538624104; 0780327667; 0780314433; 9781424400386; 0780375149; 9798350371628},
coden = {CCCEF},
language = {English},
abbrev_source_title = {Can Conf Electr Comput Eng},
type = {Conference paper}}
@article{Tan2024Effective,
author = {Tan, Xianhan and Lian, Qi and Zhu, Junming and Zhang, Jianmin and Wang, Yueming and Qi, Yu},
title = {Effective Phoneme Decoding with Hyperbolic Neural Networks for High-Performance Speech BCIs},
year = {2024},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {32},
pages = {3432 - 3441},
doi = {10.1109/TNSRE.2024.3457313},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204235667&doi=10.1109%2FTNSRE.2024.3457313&partnerID=40&md5=325b34efce1e78e2400bec0fe8188686},
affiliations = {College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Nanhu Brain-computer Interface Institute, Hangzhou, Zhejiang, China; The Second Affiliated Hospital Zhejiang University School of Medicine, Hangzhou, Zhejiang, China; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, MOE Frontier Science Center for Brain Science and Brain-machine Integration, Hangzhou, Zhejiang, China},
abstract = {Objective: Speech brain-computer interfaces (speech BCIs), which convert brain signals into spoken words or sentences, have demonstrated great potential for high-performance BCI communication. Phonemes are the basic pronunciation units. For monosyllabic languages such as Chinese Mandarin, where a word usually contains less than three phonemes, accurate decoding of phonemes plays a vital role. We found that in the neural representation space, phonemes with similar pronunciations are often inseparable, leading to confusion in phoneme classification. Methods: We mapped the neural signals of phoneme pronunciation into a hyperbolic space for a more distinct phoneme representation. Critically, we proposed a hyperbolic hierarchical clustering approach to specifically learn a phoneme-level structure to guide the representation. Results: We found such representation facilitated greater distance between similar phonemes, effectively reducing confusion. In the phoneme decoding task, our approach demonstrated an average accuracy of 75.21% for 21 phonemes and outperformed existing methods across different experimental days. Conclusion: Our approach showed high accuracy in phoneme classification. By learning the phoneme-level neural structure, the representations of neural signals were more discriminative and interpretable. Significance: Our approach can potentially facilitate high-performance speech BCIs for Chinese and other monosyllabic languages. © 2001-2011 IEEE.}, keywords = {Image coding; Brain signals; Clusterings; Hyperbolic clustering; Hyperbolic networks; Neural decoding; Neural signals; Neural-networks; Performance; Phoneme classification; Speech brain-computer interface; Neural networks; accuracy; Article; artificial neural network; classifier; clinical article; clinical trial; confusion; consonant; controlled study; cross validation; electroencephalogram; gated recurrent unit network; hierarchical clustering; human; human experiment; hyperbolic neural network; learning; mandarin; nerve cell network; phoneme; primary motor cortex; single unit activity; speech; speech perception; support vector machine; task performance; training; vowel; adult; algorithm; brain computer interface; cluster analysis; electroencephalography; female; language; male; phonetics; physiology; procedures; young adult; Adult; Algorithms; Brain-Computer Interfaces; Cluster Analysis; Electroencephalography; Female; Humans; Language; Male; Neural Networks, Computer; Phonetics; Speech; Young Adult},
correspondence_address = {Y. Wang; Zhejiang University, Qiushi Academy for Advanced Studies, Hangzhou, 310027, China; email: ymingwang@zju.edu.cn; Y. Qi; Zhejiang University, MOE Frontier Science Center for Brain Science and Brain-Machine Integration, Mental Health Center and Hangzhou Seventh People's Hospital, State Key Laboratory of Brain-Machine Intelligence, Hangzhou, 310027, China; email: qiyu@zju.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {39255187},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Wang2024Humanmachine,
author = {Wang, Hanwen and Qi, Yu and Yao, Lin and Wang, Yueming and Farina, Dario and Pan, Gang},
title = {A Human-Machine Joint Learning Framework to Boost Endogenous BCI Training},
year = {2024},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
volume = {35},
number = {12},
pages = {17534 - 17548},
doi = {10.1109/TNNLS.2023.3305621},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169693215&doi=10.1109%2FTNNLS.2023.3305621&partnerID=40&md5=b8e09c2ac26924f494f989d2f70b688c},
affiliations = {College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University School of Medicine, MOE Frontier Science Center for Brain Science and Brain-Machine Integration, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University School of Medicine, Department of Neurobiology, Hangzhou, Zhejiang, China; College of Biomedical Engineering & Instrument Science, Zhejiang University, MOE Frontier Science Center for Brain Science and Brain-machine Integration, Hangzhou, Zhejiang, China; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, Zhejiang, China; Imperial College London, Department of Bioengineering, London, United Kingdom; The First Affiliated Hospital, Zhejiang University School of Medicine, The First Affiliated Hospital, Hangzhou, Zhejiang, China},
abstract = {Brain-computer interfaces (BCIs) provide a direct pathway from the brain to external devices and have demonstrated great potential for assistive and rehabilitation technologies. Endogenous BCIs based on electroencephalogram (EEG) signals, such as motor imagery (MI) BCIs, can provide some level of control. However, mastering spontaneous BCI control requires the users to generate discriminative and stable brain signal patterns by imagery, which is challenging and is usually achieved over a very long training time (weeks/months). Here, we propose a human-machine joint learning framework to boost the learning process in endogenous BCIs, by guiding the user to generate brain signals toward an optimal distribution estimated by the decoder, given the historical brain signals of the user. To this end, we first model the human-machine joint learning process in a uniform formulation. Then a human-machine joint learning framework is proposed: 1) for the human side, we model the learning process in a sequential trial-and-error scenario and propose a novel "copy/new"feedback paradigm to help shape the signal generation of the subject toward the optimal distribution and 2) for the machine side, we propose a novel adaptive learning algorithm to learn an optimal signal distribution along with the subject's learning process. Specifically, the decoder reweighs the brain signals generated by the subject to focus more on "good"samples to cope with the learning process of the subject. Online and psuedo-online BCI experiments with 18 healthy subjects demonstrated the advantages of the proposed joint learning process over coadaptive approaches in both learning efficiency and effectiveness. © 2012 IEEE.}, keywords = {Biomedical signal processing; Decoding; Electroencephalography; Electrophysiology; Learning algorithms; Machine learning; Process control; Brain modeling; Brain–computer interface; Electroencephalogram; Features extraction; Learning process; Machine-learning; Motor imagery; Neural decoding; Brain computer interface; algorithm; artificial neural network; brain; brain computer interface; electroencephalography; human; imagination; machine learning; physiology; procedures; signal processing; Algorithms; Brain; Brain-Computer Interfaces; Humans; Imagination; Machine Learning; Neural Networks, Computer; Signal Processing, Computer-Assisted},
correspondence_address = {Y. Qi; Zhejiang University School of Medicine, Affiliated Mental Health Center and Hangzhou Seventh People's Hospital, MOE Frontier Science Center for Brain Science and Brain-Machine Integration, Hangzhou, China; email: qiyu@zju.edu.cn; G. Pan; Zhejiang University, State Key Laboratory of Brain-Machine Intelligence, College of Computer Science and Technology, First Affiliated Hospital, Hangzhou, 310058, China; email: gpan@zju.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {2162237X; 21622388},
pmid = {37647178},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Networks Learn. Sys.},
type = {Article}}
@article{Wang2024Decoding,
author = {Wang, Xiangcun and Shi, Ran and Wu, Xia and Zhang, Jiacai},
title = {Decoding Human Interaction Type from Inter-Brain Synchronization by Using EEG Brain Network},
year = {2024},
journal = {IEEE Journal of Biomedical and Health Informatics},
volume = {28},
number = {1},
pages = {204 - 215},
doi = {10.1109/JBHI.2023.3329742},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181815611&doi=10.1109%2FJBHI.2023.3329742&partnerID=40&md5=f7feded7afc0abe248bdf89302828ea1},
affiliations = {Beijing Normal University, School of Artificial Intelligence, Beijing, China},
abstract = {Cooperation and competition are two common forms of interpersonal interactions and exploring inter-brain synchronization in these two forms can help to further deliberate the underlying neural mechanisms of interpersonal interactions. Recently, studies revealed that electrode-paired inter-brain synchronization plays an important role in human interactions. This study investigated the neural correlates of interpersonal synchronization at the brain network scale and interaction type. Firstly, the network-wise inter-brain synchronization (NIBS) index reflecting cross-brain network synchronization from the global brain perspective was advanced. Secondly, statistical analysis demonstrated that there are differences in NIBS activities between cooperative and competitive interactions. And a row-filtered depthwise separable convolution network was proposed to classify the NIBS features. Results of EEG hyper-scanning data showed significant differences in NIBS between cooperative and competitive tasks, and a comparative study manifested that the cross-brain synchronization in cooperative tasks is more consistent than that of competitive tasks. The neural decoder using a modified convolution network achieved a peak accuracy of 96.05% under the binary classification(cooperation vs competition). © 2013 IEEE.}, keywords = {Behavioral research; Convolution; Decoding; Electrophysiology; Job analysis; Synchronization; Behavioral science; Correlation; EEG hyper-scanning; Hyper-scanning; Index; Inter-brain synchronization; Interpersonal interaction; Neural decoding; Task analysis; Electroencephalography; accuracy; Article; binary classification; brain; brain region; classifier; comparative study; competitive task; convolutional neural network; cooperative task; correlation analysis; correlation coefficient; cortical synchronization; data processing; decoding human interaction; deep neural network; discriminant analysis; electroencephalography; electroencephalography phase synchronization; feedback system; functional connectivity; head movement; heart rate; human; information processing; inter brain synchronization; inter subject variability; intracerebral phase synchronization; mathematical variable; network analysis; network wise inter brain synchronization index; neural decoding; neuromodulation; neurorehabilitation; occipital lobe; phase synchronization function connection matrix; social synchronization; task performance; training; university student; verbal communication},
correspondence_address = {J. Zhang; Beijing Normal University, School of Artificial Intelligence, Beijing, 100875, China; email: jiacai.zhang@bnu.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21682208; 21682194},
coden = {ITIBF},
pmid = {37917521},
language = {English},
abbrev_source_title = {IEEE J. Biomedical Health Informat.},
type = {Article}}
@article{Wang2024Efficient,
author = {Wang, Yun},
title = {Efficient Neural Decoding Based on Multimodal Training},
year = {2024},
journal = {Brain Sciences},
volume = {14},
number = {10},
pages = {},
doi = {10.3390/brainsci14100988},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207646135&doi=10.3390%2Fbrainsci14100988&partnerID=40&md5=dcea41147346389c36e5c9e10c1b8362},
affiliations = {Fudan University, Institute of Science and Technology for Brain-Inspired Intelligence, Shanghai, China; Fudan University, Key Laboratory of Computational Neuroscience and Brain-Inspired Intelligence, Shanghai, China},
abstract = {Background/Objectives: Neural decoding methods are often limited by the performance of brain encoders, which map complex brain signals into a latent representation space of perception information. These brain encoders are constrained by the limited amount of paired brain and stimuli data available for training, making it challenging to learn rich neural representations. Methods: To address this limitation, we present a novel multimodal training approach using paired image and functional magnetic resonance imaging (fMRI) data to establish a brain masked autoencoder that learns the interactions between images and brain activities. Subsequently, we employ a diffusion model conditioned on brain data to decode realistic images. Results: Our method achieves high-quality decoding results in semantic contents and low-level visual attributes, outperforming previous methods both qualitatively and quantitatively, while maintaining computational efficiency. Additionally, our method is applied to decode artificial patterns across region of interests (ROIs) to explore their functional properties. We not only validate existing knowledge concerning ROIs but also unveil new insights, such as the synergy between early visual cortex and higher-level scene ROIs, as well as the competition within the higher-level scene ROIs. Conclusions: These findings provide valuable insights for future directions in the field of neural decoding. © 2024 by the author.}, keywords = {adult; article; autoencoder; diffusion; electroencephalogram; female; functional magnetic resonance imaging; human; human experiment; male; nerve cell; nonhuman; training; visual cortex},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {20763425},
language = {English},
abbrev_source_title = {Brain Sci.},
type = {Article}}
@article{Wei2024Bdanspd,
author = {Wei, Fulin and Xu, Xueyuan and Li, Xiuxing and Wu, Xia},
title = {BDAN-SPD: A Brain Decoding Adversarial Network Guided by Spatiotemporal Pattern Differences for Cross-Subject MI-BCI},
year = {2024},
journal = {IEEE Transactions on Industrial Informatics},
volume = {20},
number = {12},
pages = {14321 - 14329},
doi = {10.1109/TII.2024.3450010},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204800456&doi=10.1109%2FTII.2024.3450010&partnerID=40&md5=f6a21cbdde161ed7984f40e7171280b9},
affiliations = {Beijing Institute of Technology, School of Computer Science and Technology, Beijing, China; Beijing Normal University, School of Artificial Intelligence, Beijing, China; Beijing University of Technology, School of Information Science and Technology, Beijing, China},
abstract = {Although advances in deep learning technologies have greatly facilitated the brain intention decoding from electroencephalogram (EEG) in motor imagery brain-computer interfaces (MI-BCIs), significant individual differences hinder the practical cross-subject MI-BCI applications. Unlike other existing domain adversarial transfer networks that focus on designing different discriminators to reduce individual differences, inspired by the motor lateralization phenomenon, we innovatively utilize transformer and the spatiotemporal pattern differences of EEG as prior knowledge to enhance the feature discriminability in our brain decoding adversarial network. In addition, to address adversarial network decision boundaries bias toward the source domain, we propose a data augmentation method, EEGMix to rapidly mix and enrich the target domain data. With an adaptive adversarial factor, our decoding model reduces the differences in marginal and conditional distribution simultaneously. Three public MI datasets, 2a, 2b, and OpenBMI verified our model's effectiveness. The accuracy achieved 77.49%, 85.19%, and 79.37%, superior to other state-of-the-art algorithms. © 2024 IEEE.}, keywords = {Deep learning; Adversarial networks; Brain decoding; Cross-subject; Domain adversarial transfer learning; Motor imagery; Motor imagery brain-computer interface; Spatiotemporal pattern difference; Spatiotemporal patterns; Transfer learning; Transformer; Brain computer interface},
correspondence_address = {X. Wu; Beijing Institute of Technology, School of Computer Science and Technology, Beijing, 100081, China; email: wuxia@bit.edu.cn},
publisher = {IEEE Computer Society},
issn = {15513203},
language = {English},
abbrev_source_title = {IEEE Trans. Ind. Inf.},
type = {Article}}
@conference{Wei2024Multimodal,
author = {Wei, Yayun and Cao, Lei and Li, Hao and Dong, Yilin},
title = {MB2C: Multimodal Bidirectional Cycle Consistency for Learning Robust Visual Neural Representations},
year = {2024},
pages = {8992 - 9000},
doi = {10.1145/3664647.3681292},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206805009&doi=10.1145%2F3664647.3681292&partnerID=40&md5=6b793a9799d08c0190b95dab6eaeb4d0},
affiliations = {Shanghai Maritime University, College of Information Engineering, Shanghai, China},
abstract = {Decoding human visual representations from brain activity data is a challenging but arguably essential task with an understanding of the real world and the human visual system. However, decoding semantically similar visual representations from brain recordings is difficult, especially for electroencephalography (EEG), which has excellent temporal resolution but suffers from spatial precision. Prevailing methods mainly focus on matching brain activity data with corresponding stimuli-responses using contrastive learning. They rely on massive and high-quality paired data and omit semantically aligned modalities distributed in distinct regions of the latent space. This paper proposes a novel Multimodal Bidirectional Cycle Consistency (MB2C) framework for learning robust visual neural representations. Specifically, we utilize dual-GAN to generate modality-related features and inversely translate back to the corresponding semantic latent space to close the modality gap and guarantee that embeddings from different modalities with similar semantics are in the same region of representation space. We perform zero-shot tasks on the ThingsEEG dataset. Additionally, we conduct EEG classification and image reconstruction on both the ThingsEEG and EEGCVPR40 datasets, achieving state-of-the-art performance compared to other baselines. © 2024 ACM.}, keywords = {Contrastive Learning; Semantic Segmentation; Zero-shot learning; Brain activity; Cycle consistency; Human visual; Images reconstruction; Multi-modal; Multi-modal learning; Neural decoding; Neural representations; Real-world; Visual representations; Electroencephalography},
correspondence_address = {L. Cao; College of Information Engineering, Shanghai Maritime University, Shanghai, China; email: lcao@shmtu.edu.cn},
publisher = {Association for Computing Machinery, Inc},
isbn = {9798400706868},
language = {English},
abbrev_source_title = {MM - Proc. ACM Int. Conf. Multimed.},
type = {Conference paper}}
@article{Wiese2024Neural,
author = {Wiese, Holger and Schweinberger, Stefan Robert and Kovács, Gyula Zoltán},
title = {The neural dynamics of familiar face recognition},
year = {2024},
journal = {Neuroscience and Biobehavioral Reviews},
volume = {167},
pages = {},
doi = {10.1016/j.neubiorev.2024.105943},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209062560&doi=10.1016%2Fj.neubiorev.2024.105943&partnerID=40&md5=4adfdd8f6fe105bdd2aec1b654cbe011},
affiliations = {Durham University, Durham, County Durham, United Kingdom; Friedrich-Schiller-Universität Jena, Jena, Thuringen, Germany},
abstract = {Humans are highly efficient at recognising familiar faces. However, previous EEG/ERP research has given a partial and fragmented account of the neural basis of this remarkable ability. We argue that this is related to insufficient consideration of fundamental characteristics of familiar face recognition. These include image-independence (recognition across different pictures), levels of familiarity (familiar faces vary hugely in duration and intensity of our exposure to them), automaticity (we cannot voluntarily withhold from recognising a familiar face), and domain-selectivity (the degree to which face familiarity effects are selective). We review recent EEG/ERP work, combining uni- and multivariate methods, that has systematically targeted these shortcomings. We present a theoretical account of familiar face recognition, dividing it into early visual, domain-sensitive and domain-general phases, and integrating image-independence and levels of familiarity. Our account incorporates classic and more recent concepts, such as multi-dimensional face representation and course-to-fine processing. While several questions remain to be addressed, this new account represents a major step forward in our understanding of the neurophysiological basis of familiar face recognition. © 2024 The Authors}, keywords = {adult; clinical article; diagnosis; duration; dynamics; electroencephalogram; electroencephalography; event related potential; evoked response; facial recognition; human; human experiment; male; review; therapy; young adult; brain; diagnostic imaging; physiology; recognition; Brain; Electroencephalography; Evoked Potentials; Facial Recognition; Humans; Recognition, Psychology},
correspondence_address = {H. Wiese; Durham University, United Kingdom; email: holger.wiese@durham.ac.uk},
publisher = {Elsevier Ltd},
issn = {18737528; 01497634},
coden = {NBRED},
pmid = {39557351},
language = {English},
abbrev_source_title = {Neurosci. Biobehav. Rev.},
type = {Review}}
@article{Wilson2024Feasibility,
author = {Wilson, Holly and Chen, Xi and Golbabaee, Mohammad and Proulx, Michael J. and O'Neill, Eamonn J.},
title = {Feasibility of decoding visual information from EEG},
year = {2024},
journal = {Brain-Computer Interfaces},
volume = {11},
number = {1-2},
pages = {33 - 60},
doi = {10.1080/2326263X.2023.2287719},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196397850&doi=10.1080%2F2326263X.2023.2287719&partnerID=40&md5=0050a5d9f9a4cf7db19914cd685f2503},
affiliations = {University of Bath, Department of Computer Science, Bath, Somerset, United Kingdom; University of Bath, Bath, Somerset, United Kingdom; University of Bristol, Department of Engineering Mathematics, Bristol, United Kingdom; University of Bath, Department of Psychology, Bath, Somerset, United Kingdom},
abstract = {Decoding visual information, such as visual imagery and perception, from EEG data can be used to improve understanding of the neural representation of visual information and to provide commands for BCI systems. The appeal of EEG as a neuroimaging tool lies in its high temporal resolution, cost-effectiveness, and portability. Nevertheless, the feasibility of using EEG for visual information decoding remains a subject of ongoing inquiry. In this review, we explore the neural correlates of this visual information, specifically focusing on visual features such as colour, shapes, texture, and also naturalistic whole objects. We begin to examine which visual features can be effectively measured using EEG, taking into account its inherent characteristics, such as its measurement depth, limited spatial resolution, and high temporal resolution. Using a systematic approach, the review provides an in-depth analysis of the current state-of-the-art in EEG-based decoding of visual features for BCI purposes. Finally, we address some potential methodological improvements that can be made to the experimental design in EEG visual information decoding studies, such as palette cleansing, augmentation to bolster dataset size, and fusion of neuroimaging techniques. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.}, keywords = {Brain computer interface; Cost effectiveness; Decoding; Electroencephalography; Electrophysiology; Image classification; Image enhancement; Neuroimaging; Textures; High temporal resolution; Information decoding; Machine-learning; Neural decoding; Neural representations; Resolution costs; Visual feature; Visual imagery; Visual information; Visual perception; Machine learning},
correspondence_address = {H. Wilson; Department of Computer Science, University of Bath, Bath, United Kingdom; email: hlw69@bath.ac.uk},
publisher = {Taylor and Francis Ltd.},
issn = {23262621; 2326263X},
language = {English},
abbrev_source_title = {Brain-Computer Interfaces},
type = {Review}}
@article{Yang2024Increasing,
author = {Yang, Shih Hung and Huang, Chunjui and Huang, Jhihsiang},
title = {Increasing Robustness of Intracortical Brain-Computer Interfaces for Recording Condition Changes via Data Augmentation},
year = {2024},
journal = {Computer Methods and Programs in Biomedicine},
volume = {251},
pages = {},
doi = {10.1016/j.cmpb.2024.108208},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193079634&doi=10.1016%2Fj.cmpb.2024.108208&partnerID=40&md5=638e03e823efb6297481743e279cca31},
affiliations = {National Cheng Kung University, Tainan, Taiwan},
abstract = {Background and Objective: Intracortical brain-computer interfaces (iBCIs) aim to help paralyzed individuals restore their motor functions by decoding neural activity into intended movement. However, changes in neural recording conditions hinder the decoding performance of iBCIs, mainly because the neural-to-kinematic mappings shift. Conventional approaches involve either training the neural decoders using large datasets before deploying the iBCI or conducting frequent calibrations during its operation. However, collecting data for extended periods can cause user fatigue, negatively impacting the quality and consistency of neural signals. Furthermore, frequent calibration imposes a substantial computational load. Methods: This study proposes a novel approach to increase iBCIs’ robustness against changing recording conditions. The approach uses three neural augmentation operators to generate augmented neural activity that mimics common recording conditions. Then, contrastive learning is used to learn latent factors by maximizing the similarity between the augmented neural activities. The learned factors are expected to remain stable despite varying recording conditions and maintain a consistent correlation with the intended movement. Results: Experimental results demonstrate that the proposed iBCI outperformed the state-of-the-art iBCIs and was robust to changing recording conditions across days for long-term use on one publicly available nonhuman primate dataset. It achieved satisfactory offline decoding performance, even when a large training dataset was unavailable. Conclusions: This study paves the way for reducing the need for frequent calibration of iBCIs and collecting a large amount of annotated training data. Potential future works aim to improve offline decoding performance with an ultra-small training dataset and improve the iBCIs’ robustness to severely disabled electrodes. © 2024}, keywords = {Brain; Calibration; Decoding; Electroencephalography; Large datasets; Neurons; Condition; Contrastive learning; Data augmentation; Decoding performance; Intracortical brain-computer interface; Latent factor; Neural activity; Neural decoding; Neural recording condition; Neural recordings; Brain computer interface; Article; learning; operator; recording; algorithm; animal; brain computer interface; calibration; human; movement (physiology); signal processing; Algorithms; Animals; Brain-Computer Interfaces; Humans; Movement; Signal Processing, Computer-Assisted},
correspondence_address = {S.-H. Yang; Department of Mechanical Engineering, National Cheng Kung University, Tainan, 701, Taiwan; email: vssyang@gs.ncku.edu.tw},
publisher = {Elsevier Ireland Ltd},
issn = {01692607; 18727565},
coden = {CMPBE},
pmid = {38754326},
language = {English},
abbrev_source_title = {Comput. Methods Programs Biomed.},
type = {Article}}
@article{Yu2024Deep,
author = {Yu, Haitao and Hu, Zhiwen and Zhao, Quanfa and Liu, Jing},
title = {Deep source transfer learning for the estimation of internal brain dynamics using scalp EEG},
year = {2024},
journal = {Cognitive Neurodynamics},
volume = {18},
number = {6},
pages = {3507 - 3520},
doi = {10.1007/s11571-024-10149-2},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198650072&doi=10.1007%2Fs11571-024-10149-2&partnerID=40&md5=6f56db1fa6866da24b4ca6c732f0b10f},
affiliations = {Tianjin University, Tianjin, China; Tangshan Gongren Hospital, Department of Neurology, Tangshan, Hebei, China},
abstract = {Electroencephalography (EEG) provides high temporal resolution neural data for brain-computer interfacing via noninvasive electrophysiological recording. Estimating the internal brain activity by means of source imaging techniques can further improve the spatial resolution of EEG and enhance the reliability of neural decoding and brain-computer interaction. In this work, we propose a novel EEG data-driven source imaging scheme for precise and efficient estimation of macroscale spatiotemporal brain dynamics across thalamus and cortical regions with deep learning methods. A deep source imaging framework with a convolutional-recurrent neural network is designed to estimate the internal brain dynamics from high-density EEG recordings. Moreover, a brain model including 210 cortical regions and 16 thalamic nuclei is established based on human brain connectome to provide synthetic training data, which manifests intrinsic characteristics of underlying brain dynamics in spontaneous, stimulation-evoked, and pathological states. Transfer learning algorithm is further applied to the trained network to reduce the dynamical differences between synthetic and realistic EEG. Extensive experiments exhibit that the proposed deep-learning method can accurately estimate the spatial and temporal activity of brain sources and achieves superior performance compared to the state-of-the-art approaches. Moreover, the EEG data-driven source imaging framework is effective in the location of seizure onset zone in epilepsy and reconstruction of dynamical thalamocortical interactions during sensory processing of acupuncture stimulation, implying its applicability in brain-computer interfacing for neuroscience research and clinical applications. © The Author(s), under exclusive licence to Springer Nature B.V. 2024.}, keywords = {Article; brain cortex; connectome; convolutional neural network; deep learning; electroencephalogram; image reconstruction; learning algorithm; neuroimaging; recurrent neural network; thalamocortical tract; thalamus; transfer learning algorithm},
correspondence_address = {J. Liu; Department of Neurology, Tangshan Gongren Hospital, Tangshan, 063000, China; email: angel.jsea@163.com},
publisher = {Springer Science and Business Media B.V.},
issn = {18714080; 18714099},
language = {English},
abbrev_source_title = {Cogn. Neurodynamics},
type = {Article}}
@article{Zhang2024Braintotext,
author = {Zhang, Daohan and Wang, Zhenjie and Qian, Youkun and Zhao, Zehao and Liu, Yan and Hao, Xiaotao and Li, Wanxin and Lu, Shuo and Zhu, Honglin and Chen, Luyao and Xu, Kunyu and Li, Yuanning and Lu, Junfeng},
title = {A brain-to-text framework for decoding natural tonal sentences},
year = {2024},
journal = {Cell Reports},
volume = {43},
number = {11},
pages = {},
doi = {10.1016/j.celrep.2024.114924},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207872929&doi=10.1016%2Fj.celrep.2024.114924&partnerID=40&md5=7f3cb497db9dcc7245f6da373ba829d0},
affiliations = {Fudan University, Department of Neurosurgery, Shanghai, China; Shanghai Key Laboratory of Brain Function and Restoration and Neural Regeneration, Shanghai, Shanghai, China; Fudan University, National Center for Neurological Disorders, Shanghai, China; ShanghaiTech University, School of Biomedical Engineering, Shanghai, China; Sun Yat-Sen University, Department of Chinese Language and Literature, Guangzhou, Guangdong, China; Faculty of Life Sciences & Medicine, London, United Kingdom; Beijing Normal University, School of International Chinese Language Education, Beijing, China; Fudan University, Institute of Modern Languages and Linguistics, Shanghai, China; ShanghaiTech University, State Key Laboratory of Advanced Medical Materials and Devices, Shanghai, China; Fudan University, MOE Frontiers Center for Brain Science, Shanghai, China; Shanghai Clinical Research and Trial Center, Shanghai, Shanghai, China},
abstract = {Speech brain-computer interfaces (BCIs) directly translate brain activity into speech sound and text. Despite successful applications in non-tonal languages, the distinct syllabic structures and pivotal lexical information conveyed through tonal nuances present challenges in BCI decoding for tonal languages like Mandarin Chinese. Here, we designed a brain-to-text framework to decode Mandarin sentences from invasive neural recordings. Our framework dissects speech onset, base syllables, and lexical tones, integrating them with contextual information through Bayesian likelihood and a Viterbi decoder. The results demonstrate accurate tone and syllable decoding during naturalistic speech production. The overall word error rate (WER) for 10 offline-decoded tonal sentences with a vocabulary of 40 high-frequency Chinese characters is 21% (chance: 95.3%) averaged across five participants, and tone decoding accuracy reaches 93% (chance: 25%), surpassing previous intracranial Mandarin tonal syllable decoders. This study provides a robust and generalizable approach for brain-to-text decoding of continuous tonal speech sentences. © 2024 The Author(s)}, keywords = {Article; artificial intelligence; Bayesian learning; brain to text framework; Chinese script; data processing; decoding; deep neural network; electrocorticography; electroencephalogram; human; language model; mandarin; nervous system function; speech; speech analysis; vocabulary; adult; Bayes theorem; brain; brain computer interface; female; language; male; physiology; young adult; Adult; Bayes Theorem; Brain; Brain-Computer Interfaces; Female; Humans; Language; Male; Speech; Young Adult},
correspondence_address = {Y. Li; School of Biomedical Engineering, ShanghaiTech University, Shanghai, 201210, China; email: liyn2@shanghaitech.edu.cn; J. Lu; Department of Neurosurgery, Huashan Hospital, Shanghai Medical College, Fudan University, Shanghai, 200040, China; email: junfeng_lu@fudan.edu.cn},
publisher = {Elsevier B.V.},
issn = {22111247; 26391856},
pmid = {39485790},
language = {English},
abbrev_source_title = {Cell Rep.},
type = {Article}}
@article{Zhang2024Oxcarnet,
author = {Zhang, Runkai and Rong, Rong and Xu, Yun and Wang, Haixian and Wang, Xiaoyun},
title = {OxcarNet: sinc convolutional network with temporal and channel attention for prediction of oxcarbazepine monotherapy responses in patients with newly diagnosed epilepsy},
year = {2024},
journal = {Journal of Neural Engineering},
volume = {21},
number = {5},
pages = {},
doi = {10.1088/1741-2552/ad788c},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204510564&doi=10.1088%2F1741-2552%2Fad788c&partnerID=40&md5=a1811439913e3ef70af7ad208c96f75a},
affiliations = {Key Laboratory of Child Development and Learning Science, Ministry of Education, School of Biological Science and Medical Engineering, Nanjing, Jiangsu, China; Nanjing Drum Tower Hospital, Department of Neurology, Nanjing, Jiangsu, China},
abstract = {Objective. Monotherapy with antiepileptic drugs (AEDs) is the preferred strategy for the initial treatment of epilepsy. However, an inadequate response to the initially prescribed AED is a significant indicator of a poor long-term prognosis, emphasizing the importance of precise prediction of treatment outcomes with the initial AED regimen in patients with epilepsy. Approach. We introduce OxcarNet, an end-to-end neural network framework developed to predict treatment outcomes in patients undergoing oxcarbazepine monotherapy. The proposed predictive model adopts a Sinc Module in its initial layers for adaptive identification of discriminative frequency bands. The derived feature maps are then processed through a Spatial Module, which characterizes the scalp distribution patterns of the electroencephalography (EEG) signals. Subsequently, these features are fed into an attention-enhanced Temporal Module to capture temporal dynamics and discrepancies. A channel module with an attention mechanism is employed to reveal inter-channel dependencies within the output of the Temporal Module, ultimately achieving response prediction. OxcarNet was rigorously evaluated using a proprietary dataset of retrospectively collected EEG data from newly diagnosed epilepsy patients at Nanjing Drum Tower Hospital. This dataset included patients who underwent long-term EEG monitoring in a clinical inpatient setting. Main results. OxcarNet demonstrated exceptional accuracy in predicting treatment outcomes for patients undergoing Oxcarbazepine monotherapy. In the ten-fold cross-validation, the model achieved an accuracy of 97.27%, and in the validation involving unseen patient data, it maintained an accuracy of 89.17%, outperforming six conventional machine learning methods and three generic neural decoding networks. These findings underscore the model’s effectiveness in accurately predicting the treatment responses in patients with newly diagnosed epilepsy. The analysis of features extracted by the Sinc filters revealed a predominant concentration of predictive frequencies in the high-frequency range of the gamma band. Significance. The findings of our study offer substantial support and new insights into tailoring early AED selection, enhancing the prediction accuracy for the responses of AEDs. © 2024 IOP Publishing Ltd. All rights, including for text and data mining, AI training, and similar technologies, are reserved.}, keywords = {Arthroplasty; Benchmarking; Brain mapping; Breath controlled devices; Convolutional neural networks; Deep learning; Drug therapy; Electrotherapeutics; Hospital data processing; Multilayer neural networks; Neurons; Signal sampling; Antiepileptics drugs; Attention mechanisms; Convolutional networks; End to end; Initial treatment; Mono-therapy; Neural-networks; Oxcarbazepine; Treatment outcomes; Electroencephalography; oxcarbazepine; anticonvulsive agent; adult; Article; clinical article; convolutional neural network; electroencephalography monitoring; epilepsy; epileptic patient; female; human; male; monotherapy; retrospective study; treatment outcome; treatment response; artificial neural network; attention; diagnosis; drug effect; drug therapy; electroencephalography; middle aged; pathophysiology; physiology; procedures; young adult; Adult; Anticonvulsants; Attention; Epilepsy; Female; Humans; Male; Middle Aged; Neural Networks, Computer; Retrospective Studies; Treatment Outcome; Young Adult},
correspondence_address = {H. Wang; Key Laboratory of Child Development and Learning Science, Ministry of Education, School of Biological Science and Medical Engineering, Southeast University, Nanjing, Jiangsu, 210096, China; email: hxwang@seu.edu.cn; X. Wang; Department of Neurology, Nanjing Drum Tower Hospital, Nanjing, Jiangsu, 210008, China; email: wxysypy@126.com},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {39250934},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Zhang2024Unsupervised,
author = {Zhang, Yameng and Gao, Yufei and Xu, Jing and Zhao, Guohua and Shi, Lei and Kong, Lingfei},
title = {Unsupervised Joint Domain Adaptation for Decoding Brain Cognitive States From tfMRI Images},
year = {2024},
journal = {IEEE Journal of Biomedical and Health Informatics},
volume = {28},
number = {3},
pages = {1494 - 1503},
doi = {10.1109/JBHI.2023.3348130},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181571236&doi=10.1109%2FJBHI.2023.3348130&partnerID=40&md5=f6839066be439060a1f869f78332ae7f},
affiliations = {Zhengzhou University, Department of Pathology, Zhengzhou, Henan, China; Zhengzhou University, School of Cyber Science and Engineering, Zhengzhou, Henan, China; Zhengzhou University, School of Computer and Artificial Intelligence, Zhengzhou, Henan, China; First Affiliated Hospital of Zhengzhou University, Department of Magnetic Resonance Imaging, Zhengzhou, Henan, China},
abstract = {Recent advances in large model and neuroscience have enabled exploration of the mechanism of brain activity by using neuroimaging data. Brain decoding is one of the most promising researches to further understand the human cognitive function. However, current methods excessively depends on high-quality labeled data, which brings enormous expense of collection and annotation of neural images by experts. Besides, the performance of cross-individual decoding suffers from inconsistency in data distribution caused by individual variation and different collection equipments. To address mentioned above issues, a Join Domain Adapative Decoding (JDAD) framework is proposed for unsupervised decoding specific brain cognitive state related to behavioral task. Based on the volumetric feature extraction from task-based functional Magnetic Resonance Imaging (tfMRI) data, a novel objective loss function is designed by the combination of joint distribution regularizer, which aims to restrict the distance of both the conditional and marginal probability distribution of labeled and unlabeled samples. Experimental results on the public Human Connectome Project (HCP) S1200 dataset show that JDAD achieves superior performance than other prevalent methods, especially for fine-grained task with 11.5%-21.6% improvements of decoding accuracy. The learned 3D features are visualized by Grad-CAM to build a combination with brain functional regions, which provides a novel path to learn the function of brain cortex regions related to specific cognitive task in group level. © 2013 IEEE.}, keywords = {Brain; Decoding; Extraction; Job analysis; Magnetic resonance imaging; Neuroimaging; Three dimensional displays; Brain cognitive; Brain cognitive state; Brain modeling; Cognitive state; Domain adaptions; Features extraction; Functional magnetic resonance imaging; Task analysis; Tfmri; Three-dimensional display; Unsupervised domain adaption; Working memory; Feature extraction; adaptation; article; brain; brain cortex; cognition; connectome; electroencephalogram; feature extraction; functional magnetic resonance imaging; human; human experiment; neuroimaging; probability; working memory; diagnostic imaging; nuclear magnetic resonance imaging; procedures; Cognition; Connectome; Humans; Magnetic Resonance Imaging},
correspondence_address = {Y. Gao; Zhengzhou University, School of Cyber Science and Engineering, Zhengzhou, 450003, China; email: yfgao@zzu.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21682208; 21682194},
coden = {ITIBF},
pmid = {38157464},
language = {English},
abbrev_source_title = {IEEE J. Biomedical Health Informat.},
type = {Article}}
@article{Zhang2024Chisco,
author = {Zhang, Zihan and Ding, Xiao and Bao, Yu and Zhao, Yi and Liang, Xia and Qin, Bing and Liu, Ting},
title = {Chisco: An EEG-based BCI dataset for decoding of imagined speech},
year = {2024},
journal = {Scientific Data},
volume = {11},
number = {1},
pages = {},
doi = {10.1038/s41597-024-04114-1},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209709077&doi=10.1038%2Fs41597-024-04114-1&partnerID=40&md5=b51d4a94459806c3f3b3eb114e267e03},
affiliations = {Harbin Institute of Technology, Department of Computer Science, Harbin, Heilongjiang, China; Harbin Institute of Technology, School of Space Environment and Material Science, Harbin, Heilongjiang, China},
abstract = {The rapid advancement of deep learning has enabled Brain-Computer Interfaces (BCIs) technology, particularly neural decoding techniques, to achieve higher accuracy and deeper levels of interpretation. Interest in decoding imagined speech has significantly increased because its concept akin to “mind reading”. However, previous studies on decoding neural language have predominantly focused on brain activity patterns during human reading. The absence of imagined speech electroencephalography (EEG) datasets has constrained further research in this field. We present the Chinese Imagined Speech Corpus (Chisco), including over 20,000 sentences of high-density EEG recordings of imagined speech from healthy adults. Each subject’s EEG data exceeds 900 minutes, representing the largest dataset per individual currently available for decoding neural language to date. Furthermore, the experimental stimuli include over 6,000 everyday phrases across 39 semantic categories, covering nearly all aspects of daily language. We believe that Chisco represents a valuable resource for the fields of BCIs, facilitating the development of more user-friendly BCIs. © The Author(s) 2024.},
keywords = {adult; brain; brain computer interface; electroencephalography; human; imagination; language; physiology; speech; young adult; Adult; Brain; Brain-Computer Interfaces; Electroencephalography; Humans; Imagination; Language; Speech; Young Adult},
correspondence_address = {X. Ding; Harbin Institute of Technology, Department of Computer Science, Harbin, 150000, China; email: xding@ir.hit.edu.cn; X. Liang; Harbin Institute of Technology, The School of Space Environment and Material Science, Harbin, 150000, China; email: xia.liang@hit.edu.cn},
publisher = {Nature Research},
issn = {20524463},
pmid = {39572577},
language = {English},
abbrev_source_title = {Sci. Data},
type = {Data paper}}
@article{Zhao2024Task,
author = {Zhao, Shijie and Fang, Long and Yang, Yang and Tang, Guochang and Luo, Guoxin and Han, Junwei and Liu, Tianming and Hu, Xintao},
title = {Task sub-type states decoding via group deep bidirectional recurrent neural network},
year = {2024},
journal = {Medical Image Analysis},
volume = {94},
pages = {},
doi = {10.1016/j.media.2024.103136},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187777231&doi=10.1016%2Fj.media.2024.103136&partnerID=40&md5=625b25842c100dc7b753f97a14629004},
affiliations = {Northwestern Polytechnical University, Xi'an, Shaanxi, China; Nanyang First People's Hospital Affiliated to Henan University, Department of Ophthalmology, Nanyang, Hubei, China; University of Georgia School of Computing, Athens, GA, United States; Northwestern Polytechnical University, Xi'an, Shaanxi, China},
abstract = {Decoding brain states under different cognitive tasks from functional magnetic resonance imaging (fMRI) data has attracted great attention in the neuroimaging filed. However, the well-known temporal dependency in fMRI sequences has not been fully exploited in existing studies, due to the limited temporal-modeling capacity of the backbone machine learning algorithms and rigid training sample organization strategies upon which the brain decoding methods are built. To address these limitations, we propose a novel method for fine-grain brain state decoding, namely, group deep bidirectional recurrent neural network (Group-DBRNN) model. We first propose a training sample organization strategy that consists of a group-task sample generation module and a multiple-scale random fragment strategy (MRFS) module to collect training samples that contain rich task-relevant brain activity contrast (i.e., the comparison of neural activity patterns between different tasks) and maintain the temporal dependency. We then develop a novel decoding model by replacing the unidirectional RNNs that are widely used in existing brain state decoding studies with bidirectional stacked RNNs to better capture the temporal dependency, and by introducing a multi-task interaction layer (MTIL) module to effectively model the task-relevant brain activity contrast. Our experimental results on the Human Connectome Project task fMRI dataset (7 tasks consisting of 23 task sub-type states) show that the proposed model achieves an average decoding accuracy of 94.7% over the 23 fine-grain sub-type states. Meanwhile, our extensive interpretations of the intermediate features learned in the proposed model via visualizations and quantitative assessments of their discriminability and inter-subject alignment evidence that the proposed model can effectively capture the temporal dependency and task-relevant contrast. © 2024}, keywords = {Decoding; Deep neural networks; Learning algorithms; Magnetic resonance imaging; Neuroimaging; Neurons; Recurrent neural networks; Sampling; Bidirectional recurrent neural networks; Bidirectional stacked RNN; Brain activity; Brain stage decoding; Finer grains; Functional magnetic resonance imaging; Task relevant; Task-relevant brain activity contrast; Temporal dependency; Training sample; Brain; adult; article; clinical article; cognition; connectome; controlled study; electroencephalogram; female; functional magnetic resonance imaging; grain; human; human experiment; learning algorithm; machine learning; male; neuroimaging; normal human; recurrent neural network; algorithm; artificial neural network; brain; diagnostic imaging; nuclear magnetic resonance imaging; procedures; Algorithms; Connectome; Humans; Magnetic Resonance Imaging; Neural Networks, Computer},
correspondence_address = {X. Hu; School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; email: xhu@nwpu.edu.cn},
publisher = {Elsevier B.V.},
issn = {13618415; 13618423},
coden = {MIAEC},
pmid = {38489895},
language = {English},
abbrev_source_title = {Med. Image Anal.},
type = {Article}}
@conference{Zheng2024Neural,
author = {Zheng, Lei and Chang, Wenwen and Lu, Jialei and Kong, Weixuan and Yan, Guanghui Huiy and Guo, Bin},
title = {Neural correlation study of swinging and standing during dual-task walking-A pilot study},
year = {2024},
pages = {929 - 934},
doi = {10.1109/CAC63892.2024.10865097},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000750990&doi=10.1109%2FCAC63892.2024.10865097&partnerID=40&md5=e0f0e3f0a37150529297e6898e1464e1},
affiliations = {Lanzhou Jiaotong University, School of Electronic and Information Engineering, Lanzhou, Gansu, China; Northwestern Polytechnical University, School of Computer Science, Xi'an, Shaanxi, China},
abstract = {The walking process is often accompanied by other cognitive tasks, which will cause certain interference to the gait. It is important to explore the cognitive characteristics of brain at different stages during dual-task walking for the neural decoding of real gait features. By collecting and analyzing EEG signals of healthy subjects during walking of different cognitive tasks, this study completed the analysis of EEG characteristics during the standing and swinging stages of the whole gait cycle. The time-frequency characteristics of EEG signals on the main electrodes in the locomotor brain, the topological distribution of power spectral density in the swinging and standing phases, and the corresponding brain functional network structure were analyzed. The results showed that the temporal-frequency characteristics and brain functional network connections differed significantly between the non-cognitive and cognitive tasks, and the power spectral characteristics and network topological connections of the EEG signals also showed significant differences between the swinging and the standing phases. However, the power spectral values and functional brain connectivity relationships did not differ between arithmetic operation task and left and right brain task. There was no significant variability in the topological distribution of power spectrum values across brain regions. These results suggest that it is feasible to achieve effective recognition of swing and stand phases within the gait cycle based on EEG features analysis, and that the identification of EEG signals in different walking phases can contribute to the development of brain-computer interface-based rehabilitation and robotic assistive systems. © 2024 IEEE.}, keywords = {Electrotherapeutics; Gait analysis; Neurophysiology; Power spectral density; Walking aids; Brain activity; Brain functional networks; Cognitive task; Dual-task walking; Dual-tasks; EEG signals; Gait cycles; Gait phasis; Gait-phase; Power spectral; Electroencephalography},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9798350368604},
language = {English},
abbrev_source_title = {Proc. - China Autom. Congr., CAC},
type = {Conference paper}}
@article{Zhou2024Multiband,
author = {Zhou, Yusong and Yang, Banghua and Wang, Changyong},
title = {Multiband task related components enhance rapid cognition decoding for both small and similar objects},
year = {2024},
journal = {Neural Networks},
volume = {175},
pages = {},
doi = {10.1016/j.neunet.2024.106313},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190511367&doi=10.1016%2Fj.neunet.2024.106313&partnerID=40&md5=a6443387433b7497c552a546912d8b71},
affiliations = {Shanghai University, School of Mechanical Engineering and Automation, Shanghai, China; Beijing Institute of Basic Medical Sciences, Beijing, Beijing, China},
abstract = {The cortically-coupled target recognition system based on rapid serial visual presentation (RSVP) has a wide range of applications in brain computer interface (BCI) fields such as medical and military. However, in the complex natural environment backgrounds, the identification of event-related potentials (ERP) of both small and similar objects that are quickly presented is a research challenge. Therefore, we designed corresponding experimental paradigms and proposed a multi-band task related components matching (MTRCM) method to improve the rapid cognitive decoding of both small and similar objects. We compared the areas under the receiver operating characteristic curve (AUC) between MTRCM and other 9 methods under different numbers of training sample using RSVP-ERP data from 50 subjects. The results showed that MTRCM maintained an overall superiority and achieved the highest average AUC (0.6562 ± 0.0091). We also optimized the frequency band and the time parameters of the method. The verification on public data sets further showed the necessity of designing MTRCM method. The MTRCM method provides a new approach for neural decoding of both small and similar RSVP objects, which is conducive to promote the further development of RSVP-BCI. © 2024}, keywords = {Decoding; Military applications; Both small and similar object; Event related potentials; Matching methods; Matchings; Multi band; Multiband task related component; Natural environments; Rapid serial visual presentations; Recognition systems; Target recognition; Brain computer interface; adult; alpha rhythm; Article; beta rhythm; classifier; convolutional neural network; event related potential; female; gamma rhythm; human; male; nervous system parameters; normal human; process optimization; rapid cognition decoding; recognition; task positive network; technology; training; brain; cognition; electroencephalography; evoked response; photostimulation; physiology; procedures; young adult; Adult; Brain; Brain-Computer Interfaces; Cognition; Electroencephalography; Evoked Potentials; Female; Humans; Male; Photic Stimulation; Young Adult},
correspondence_address = {B. Yang; School of Mechanical Engineering and Automation, Shanghai University, Shanghai, 200444, China; email: yangbanghua@shu.edu.cn},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
pmid = {38640695},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{An2023Neural,
author = {An, Hyunjung and Lee, Jee-won and Suh, Myung-Whan and Lim, Yoonseob},
title = {Neural correlation of speech envelope tracking for background noise in normal hearing},
year = {2023},
journal = {Frontiers in Neuroscience},
volume = {17},
pages = {},
doi = {10.3389/fnins.2023.1268591},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175649524&doi=10.3389%2Ffnins.2023.1268591&partnerID=40&md5=a421781c02d95935a05e9f768746f037},
affiliations = {Korea Institute of Science and Technology, Center for Intelligent and Interactive Robotics, Seoul, South Korea; Ewha Womans University, Department of Electrical and Electronic Engineering, Seoul, Seoul, South Korea; Seoul National University Hospital, Department of Otolaryngology-Head and Neck Surgery, Seoul, South Korea; Hanyang University, Seoul, South Korea},
abstract = {Everyday speech communication often occurs in environments with background noise, and the impact of noise on speech recognition can vary depending on factors such as noise type, noise intensity, and the listener’s hearing ability. However, the extent to which neural mechanisms in speech understanding are influenced by different types and levels of noise remains unknown. This study aims to investigate whether individuals exhibit distinct neural responses and attention strategies depending on noise conditions. We recorded electroencephalography (EEG) data from 20 participants with normal hearing (13 males) and evaluated both neural tracking of speech envelopes and behavioral performance in speech understanding in the presence of varying types of background noise. Participants engaged in an EEG experiment consisting of two separate sessions. The first session involved listening to a 12-min story presented binaurally without any background noise. In the second session, speech understanding scores were measured using matrix sentences presented under speech-shaped noise (SSN) and Story noise background noise conditions at noise levels corresponding to sentence recognitions score (SRS). We observed differences in neural envelope correlation depending on noise type but not on its level. Interestingly, the impact of noise type on the variation in envelope tracking was more significant among participants with higher speech perception scores, while those with lower scores exhibited similarities in envelope correlation regardless of the noise condition. The findings suggest that even individuals with normal hearing could adopt different strategies to understand speech in challenging listening environments, depending on the type of noise. © © 2023 An, Lee, Suh and Lim.}, keywords = {adult; article; attention; clinical article; controlled study; electroencephalography; hearing; human; human experiment; male; masking; nerve potential; noise; speech; speech perception},
correspondence_address = {Y. Lim; Center for Intelligent and Interactive Robotics, Korea Institute of Science and Technology, Seoul, South Korea; email: yslim@kist.re.kr},
publisher = {Frontiers Media SA},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@article{Bai2023Hybrid,
author = {Bai, Xin and Li, Minglun and Qi, Shouliang and Ng, Anna Ching Mei and Ng, Tit and Qian, Wei},
title = {A hybrid P300-SSVEP brain-computer interface speller with a frequency enhanced row and column paradigm},
year = {2023},
journal = {Frontiers in Neuroscience},
volume = {17},
pages = {},
doi = {10.3389/fnins.2023.1133933},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151259418&doi=10.3389%2Ffnins.2023.1133933&partnerID=40&md5=202567b012bf7232d2624ac0d1faf76f},
affiliations = {College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, Liaoning, China; NEU’s Key Laboratory of Medical Image Intelligent Computing of MOE, Shenyang, Liaoning, China; Tianjin University, Department of Biomedical Engineering, Tianjin, China; Ltd., Shenzhen, Guangdong, China},
abstract = {Objective: This study proposes a new hybrid brain-computer interface (BCI) system to improve spelling accuracy and speed by stimulating P300 and steady-state visually evoked potential (SSVEP) in electroencephalography (EEG) signals. Methods: A frequency enhanced row and column (FERC) paradigm is proposed to incorporate the frequency coding into the row and column (RC) paradigm so that the P300 and SSVEP signals can be evoked simultaneously. A flicker (white-black) with a specific frequency from 6.0 to 11.5 Hz with an interval of 0.5 Hz is assigned to one row or column of a 6 × 6 layout, and the row/column flashes are carried out in a pseudorandom sequence. A wavelet and support vector machine (SVM) combination is adopted for P300 detection, an ensemble task-related component analysis (TRCA) method is used for SSVEP detection, and the two detection possibilities are fused using a weight control approach. Results: The implemented BCI speller achieved an accuracy of 94.29% and an information transfer rate (ITR) of 28.64 bit/min averaged across 10 subjects during the online tests. An accuracy of 96.86% is obtained during the offline calibration tests, higher than that of only using P300 (75.29%) or SSVEP (89.13%). The SVM in P300 outperformed the previous linear discrimination classifier and its variants (61.90–72.22%), and the ensemble TRCA in SSVEP outperformed the canonical correlation analysis method (73.33%). Conclusion: The proposed hybrid FERC stimulus paradigm can improve the performance of the speller compared with the classical single stimulus paradigm. The implemented speller can achieve comparable accuracy and ITR to its state-of-the-art counterparts with advanced detection algorithms. © © 2023 Bai, Li, Qi, Ng, Ng and Qian.}, keywords = {accuracy; adult; Article; calibration; classification; classifier; correlation analysis; electroencephalography; feature extraction; human; human experiment; machine learning; male; normal human; signal processing; spelling; support vector machine; velocity; visual evoked potential; wavelet analysis; young adult},
correspondence_address = {S. Qi; College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China; email: qisl@bmie.neu.edu.cn},
publisher = {Frontiers Media S.A.},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@article{Berezutskaya2023Direct,
author = {Berezutskaya, Julia and Freudenburg, Zachary V. and Vansteensel, Mariska J. and Aarnoutse, Erik J. and Ramsey, Nick Franciscus and Van Gerven, Marcel A.J.},
title = {Direct speech reconstruction from sensorimotor brain activity with optimized deep learning models},
year = {2023},
journal = {Journal of Neural Engineering},
volume = {20},
number = {5},
pages = {},
doi = {10.1088/1741-2552/ace8be},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171900901&doi=10.1088%2F1741-2552%2Face8be&partnerID=40&md5=698777bb411c75962425db25d45880b5},
affiliations = {University Medical Center Utrecht, Department of Neurology and Neurosurgery, Utrecht, Netherlands; Donders Institute for Brain, Cognition and Behaviour, Nijmegen, Netherlands},
abstract = {Objective. Development of brain-computer interface (BCI) technology is key for enabling communication in individuals who have lost the faculty of speech due to severe motor paralysis. A BCI control strategy that is gaining attention employs speech decoding from neural data. Recent studies have shown that a combination of direct neural recordings and advanced computational models can provide promising results. Understanding which decoding strategies deliver best and directly applicable results is crucial for advancing the field. Approach. In this paper, we optimized and validated a decoding approach based on speech reconstruction directly from high-density electrocorticography recordings from sensorimotor cortex during a speech production task. Main results. We show that (1) dedicated machine learning optimization of reconstruction models is key for achieving the best reconstruction performance; (2) individual word decoding in reconstructed speech achieves 92%-100% accuracy (chance level is 8%); (3) direct reconstruction from sensorimotor brain activity produces intelligible speech. Significance. These results underline the need for model optimization in achieving best speech decoding results and highlight the potential that reconstruction-based speech decoding from sensorimotor cortex can offer for development of next-generation BCI technology for communication. © 2023 The Author(s). Published by IOP Publishing Ltd}, keywords = {Audio recordings; Brain; Brain computer interface; Decoding; Electroencephalography; Electrophysiology; Learning systems; Neurophysiology; Speech communication; Audio reconstruction; Brain activity; Electrocorticography; Interface control; Interface technology; Learning models; Neural decoding; Sensorimotor cortex; Sensorimotors; Speech decoding; Deep neural networks; adult; Article; artificial neural network; audio recording; classification algorithm; comparative study; convolutional neural network; deep learning; electrocorticography; electroencephalogram; female; human; human tissue; image reconstruction; leave one out cross validation; logistic regression analysis; machine learning; male; middle aged; multilayer perceptron; normal human; random forest; recurrent neural network; sensorimotor cortex; speech analysis; speech intelligibility; speech perception; word recognition; young adult; interpersonal communication; procedures; speech; Brain-Computer Interfaces; Communication; Deep Learning; Humans; Sensorimotor Cortex; Speech},
correspondence_address = {J. Berezutskaya; Brain Center, Department of Neurology and Neurosurgery, University Medical Center Utrecht, Utrecht, 3584 CX, Netherlands; email: y.berezutskaya@umcutrecht.nl},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {37467739},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Borra2023Bayesianoptimized,
author = {Borra, Davide and Filippini, Matteo and Ursino, Mauro and Fattori, Patrizia and Magosso, Elisa},
title = {A Bayesian-Optimized Convolutional Neural Network to Decode Reach-to-Grasp from Macaque Dorsomedial Visual Stream},
year = {2023},
journal = {Lecture Notes in Computer Science},
volume = {13811 LNCS},
pages = {473 - 487},
doi = {10.1007/978-3-031-25891-6_36},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151048820&doi=10.1007%2F978-3-031-25891-6_36&partnerID=40&md5=8afa14c185f7c2ae7a1f628fd752abe6},
affiliations = {Alma Mater Studiorum Università di Bologna, Cesena, Department of Electrical, Cesena, FC, Italy; Università degli Studi di Bologna, Facoltà di Medicina e Chirurgia, Bologna, BO, Italy; Alma Mater Studiorum Università di Bologna, Bologna, BO, Italy},
abstract = {Neural decoding is crucial to translate the neural activity for Brain-Computer Interfaces (BCIs) and provides information on how external variables (e.g., movement) are represented and encoded in the neural system. Convolutional neural networks (CNNs) are emerging as neural decoders for their high predictive power and are largely applied with electroencephalographic signals; these algorithms, by automatically learning the more relevant class-discriminative features, improve decoding performance over classic decoders based on handcrafted features. However, applications of CNNs for single-neuron decoding are still scarce and require further validation. In this study, a CNN architecture was designed via Bayesian optimization and was applied to decode different grip types from the activity of single neurons of the posterior parietal cortex of macaque (area V6A). The Bayesian-optimized CNN significantly outperformed a naïve Bayes classifier, commonly used for neural decoding, and proved to be robust to a reduction of the number of cells and of training trials. Adopting a sliding window decoding approach with a high time resolution (5 ms), the CNN was able to capture grip-discriminant features early after cuing the animal, i.e., when the animal was only attending the object to grasp, further supporting that grip-related neural signatures are strongly encoded in V6A already during movement preparation. The proposed approach may have practical implications in invasive BCIs to realize accurate and robust decoders, and may be used together with explanation techniques to design a general tool for neural decoding and analysis, boosting our comprehension of neural encoding. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.}, keywords = {Animals; Barium compounds; Bayesian networks; Brain; Convolution; Convolutional neural networks; Decoding; Electroencephalography; Neurons; Bayesian; Bayesian optimization; Convolutional neural network; Dorsomedial visual stream; Neural activity; Neural decoding; Neural systems; Reach to grasp; Single neuron; V6A; Brain computer interface},
correspondence_address = {D. Borra; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi” (DEI), University of Bologna, Cesena, Cesena Campus, Italy; email: davide.borra2@unibo.it},
editor = {Nicosia, G. and Giuffrida, G. and Ojha, V. and La Malfa, E. and La Malfa, G. and Pardalos, P. and Di Fatta, G. and Umeton, R.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {16113349; 03029743},
isbn = {9789819698936; 9789819698042; 9789819698110; 9789819698905; 9783032004949; 9789819512324; 9783032026019; 9783032008909; 9783031915802; 9789819698141},
language = {English},
abbrev_source_title = {Lect. Notes Comput. Sci.},
type = {Conference paper}}
@conference{Chen2023Auditory,
author = {Chen, Xiaoyu and Du, Changde and Zhou, Qiongyi and He, Huiguang},
title = {Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning},
year = {2023},
pages = {6025 - 6033},
doi = {10.1145/3581783.3611869},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179552152&doi=10.1145%2F3581783.3611869&partnerID=40&md5=1e020772eb0458c9c0429be71ff4067f},
affiliations = {Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, China; University of Chinese Academy of Sciences, School of Artificial Intelligence, Beijing, China},
abstract = {The human brain can easily focus on one speaker and suppress others in scenarios such as a cocktail party. Recently, researchers found that auditory attention can be decoded from the electroencephalogram (EEG) data. However, most existing deep learning methods are difficult to use prior knowledge of different views (that is attended speech and EEG are task-related views) and extract an unsatisfactory representation. Inspired by Broadbent's filter model, we decode auditory attention in a multi-view paradigm and extract the most relevant and important information utilizing the missing view. Specifically, we propose an auditory attention decoding (AAD) method based on multi-view VAE with task-related multi-view contrastive (TMC) learning. Employing TMC learning in multi-view VAE can utilize the missing view to accumulate prior knowledge of different views into the fusion of representation, and extract the approximate task-related representation. We examine our method on two popular AAD datasets, and demonstrate the superiority of our method by comparing it to the state-of-the-art method. © 2023 Owner/Author.}, keywords = {Deep learning; Electroencephalography; Learning systems; Auditory attention; Auditory attention decoding; Cocktail party; Filter model; Human brain; Learning methods; Multi-view learning; Multi-views; Neural decoding; Prior-knowledge; Decoding},
correspondence_address = {H. He; Laboratory of Brain Atlas and Brain-Inspired Intelligence, State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; email: huiguang.he@ia.ac.cn},
publisher = {Association for Computing Machinery, Inc},
isbn = {9798400701085},
language = {English},
abbrev_source_title = {MM - Proc. ACM Int. Conf. Multimed.},
type = {Conference paper}}
@inproceedings{Cho2023Ieee,
author = {Cho, Julia and Gupta, Ranajoy and Johnson, Kaelyn and Sawhney, Ritvik and Zeltser, Daniel and Dasgupta, Sabar},
booktitle = {2023 IEEE MIT Undergraduate Research Technology Conference (URTC)},
title = {Machine Learning for Neural Decoding: Using EEG Signals to Detect Freezing of Gait in Parkinson's Patients},
year = {2023},
volume = {},
number = {},
pages = {1--5},
abstract = {Freezing of gait (FOG) is one of the most serious and poorly understood symptoms of Parkinson's Disease (PD). The main focus of this paper is to examine correlations between electroencephalography (EEG) data and episodes of FOG. Ultimately, the goal of the project is the development of a machine-learning model capable of detecting instances of FOG in Parkinson's patients. EEG data was retrieved from an existing dataset, processed, and analyzed using linear regressions to evaluate relationships between the variables. Then, the data was run through four predictive models: a polynomial regression, a dense neural network, a Bayesian neural network, and an autoencoder. The dense neural network generated the best results for multi-patient data with an accuracy rate of 66%. However, when the Dense neural network analyzed data from an individual patient engaged in one task, its accuracy increased sharply to 80–94%. One potential application of this research would be the development of personal EEG tracking and Parkinson's symptom prediction systems.},
keywords = {Correlation;Parkinson's disease;Neural networks;Machine learning;Predictive models;Brain modeling;Electroencephalography},
doi = {10.1109/URTC60662.2023.10534919},
issn = {},
month = {Oct}}
@article{Daly2023Neural,
author = {Daly, Ian},
title = {Neural decoding of music from the EEG},
year = {2023},
journal = {Scientific Reports},
volume = {13},
number = {1},
pages = {},
doi = {10.1038/s41598-022-27361-x},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146195134&doi=10.1038%2Fs41598-022-27361-x&partnerID=40&md5=c0dedbf3c6eefac0b76e61d4bde8f3ff},
affiliations = {University of Essex, Department of Electronics and Computer Science, Colchester, Essex, United Kingdom},
abstract = {Neural decoding models can be used to decode neural representations of visual, acoustic, or semantic information. Recent studies have demonstrated neural decoders that are able to decode accoustic information from a variety of neural signal types including electrocortiography (ECoG) and the electroencephalogram (EEG). In this study we explore how functional magnetic resonance imaging (fMRI) can be combined with EEG to develop an accoustic decoder. Specifically, we first used a joint EEG-fMRI paradigm to record brain activity while participants listened to music. We then used fMRI-informed EEG source localisation and a bi-directional long-term short term deep learning network to first extract neural information from the EEG related to music listening and then to decode and reconstruct the individual pieces of music an individual was listening to. We further validated our decoding model by evaluating its performance on a separate dataset of EEG-only recordings. We were able to reconstruct music, via our fMRI-informed EEG source analysis approach, with a mean rank accuracy of 71.8% (n=18, p<0.05). Using only EEG data, without participant specific fMRI-informed source analysis, we were able to identify the music a participant was listening to with a mean rank accuracy of 59.2% (n=19, p<0.05). This demonstrates that our decoding model may use fMRI-informed source analysis to aid EEG based decoding and reconstruction of acoustic information from brain activity and makes a step towards building EEG-based neural decoders for other complex information domains such as other acoustic, visual, or semantic information. © 2023, The Author(s).},
keywords = {adult; article; brain function; clinical article; controlled study; deep learning; electroencephalogram; female; functional magnetic resonance imaging; human; human experiment; male; music; auscultation; brain mapping; electroencephalography; hearing; procedures; Auditory Perception; Auscultation; Brain Mapping; Electroencephalography; Humans; Music},
correspondence_address = {I. Daly; Brain-Computer Interfacing and Neural Engineering Lab, Department of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, United Kingdom; email: i.daly@essex.ac.uk},
publisher = {Nature Research},
issn = {20452322},
pmid = {36635340},
language = {English},
abbrev_source_title = {Sci. Rep.},
type = {Article}}
@article{Duan2023Uncer,
author = {Duan, Tiehang and Wang, Zhenyi and Liu, Sheng and Yin, Yiyi and Srihari, Sargur N.},
title = {UNCER: A framework for uncertainty estimation and reduction in neural decoding of EEG signals},
year = {2023},
journal = {Neurocomputing},
volume = {538},
pages = {},
doi = {10.1016/j.neucom.2023.03.071},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151684495&doi=10.1016%2Fj.neucom.2023.03.071&partnerID=40&md5=69fc4741a9e466c2edb188fcd4262a86},
affiliations = {Facebook Artificial Intelligence Research, Menlo Park, CA, United States; University at Buffalo, The State University of New York, Buffalo, NY, United States},
abstract = {EEG decoding systems based on deep neural networks have been widely used in the decision-making of brain-computer interfaces (BCI). Their predictions, however, can be unreliable given the significant variance and noise in EEG signals. Previous works on EEG analysis mainly focus on the exploration of noise patterns in the source signal, while the uncertainty during the decoding process is largely unexplored. Automatically detecting and reducing such decoding uncertainty is important for BCI motor imagery applications such as robotic arm control etc. In this work, we proposed an uncertainty estimation and reduction model (UNCER) to quantify and mitigate the uncertainty during the EEG decoding process. It utilized a combination of dropout oriented method and Bayesian neural network for uncertainty estimation to incorporate both the uncertainty in the input signal and the uncertainty in the model parameters. We further proposed an adaptive data augmentation-based approach for uncertainty reduction. The model can be integrated into the current widely used EEG neural decoders without change of the architecture. We performed extensive experiments for uncertainty estimation and its reduction in both intra-subject EEG decoding and cross-subject EEG decoding on two public motor imagery datasets, where the proposed model achieves significant improvement both in the quality of estimated uncertainty and the effectiveness of uncertainty reduction. © 2023 Elsevier B.V.}, keywords = {Decision making; Decoding; Deep neural networks; Image enhancement; Uncertainty analysis; Decoding process; EEG decoding; EEG signals; Estimation models; Motor imagery; Neural decoding; Reduction models; Uncertainty; Uncertainty estimation; Uncertainty reduction; Brain computer interface; analytic method; Article; Bayesian network; controlled study; data analysis; electroencephalogram; human; imagery; mathematical computing; quality control; quantitative analysis; radiographic parameter},
publisher = {Elsevier B.V.},
issn = {18728286; 09252312},
coden = {NRCGE},
language = {English},
abbrev_source_title = {Neurocomputing},
type = {Article}}
@article{Fan2023Hyperflexible,
author = {Fan, Jie and Li, Xiaocheng and Wang, Peiyu and Yang, Fan and Zhao, Bingzhen and Yang, Jianing and Zhao, Zhengtuo and Li, Xue},
title = {A Hyperflexible Electrode Array for Long-Term Recording and Decoding of Intraspinal Neuronal Activity},
year = {2023},
journal = {Advanced Science},
volume = {10},
number = {33},
pages = {},
doi = {10.1002/advs.202303377},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174592439&doi=10.1002%2Fadvs.202303377&partnerID=40&md5=117a90b90dceb5ed41cd76fc8cdf419f},
affiliations = {Center for Excellence in Brain Science and Intelligence Technology, Institute of Neuroscience, Shanghai, China},
abstract = {Neural interfaces for stable access to the spinal cord (SC) electrical activity can benefit patients with motor dysfunctions. Invasive high-density electrodes can directly extract signals from SC neuronal populations that can be used for the facilitation, adjustment, and reconstruction of motor actions. However, developing neural interfaces that can achieve high channel counts and long-term intraspinal recording remains technically challenging. Here, a biocompatible SC hyperflexible electrode array (SHEA) with an ultrathin structure that minimizes mechanical mismatch between the interface and SC tissue and enables stable single-unit recording for more than 2 months in mice is demonstrated. These results show that SHEA maintains stable impedance, signal-to-noise ratio, single-unit yield, and spike amplitude after implantation into mouse SC. Gait analysis and histology show that SHEA implantation induces negligible behavioral effects and Inflammation. Additionally, multi-unit signals recorded from the SC ventral horn can predict the mouse's movement trajectory with a high decoding coefficient of up to 0.95. Moreover, during step cycles, it is found that the neural trajectory of spikes and low-frequency local field potential (LFP) signal exhibits periodic geometry patterns. Thus, SHEA can offer an efficient and reliable SC neural interface for monitoring and potentially modulating SC neuronal activity associated with motor dysfunctions. © 2023 The Authors. Advanced Science published by Wiley-VCH GmbH.}, keywords = {Biocompatibility; Decoding; Electrodes; Mammals; Neurons; Signal processing; Signal to noise ratio; Electrical activities; Electrode arrays; Hyperflexible electrode; Intraspinal recording; Long-term recording; Neural decoding; Neural interfaces; Neural trajectory; Neuronal activities; Spinal-cord; Trajectories; animal; electroencephalography; human; mouse; movement (physiology); nerve cell; physiology; procedures; Animals; Electroencephalography; Humans; Mice; Movement},
correspondence_address = {X. Li; Center for Excellence in Brain Science and Intelligence Technology, Institute of Neuroscience, Chinese Academy of Sciences, Shanghai, 200031, China; email: xli@ion.ac.cn},
publisher = {John Wiley and Sons Inc},
issn = {21983844},
pmid = {37870208},
language = {English},
abbrev_source_title = {Adv. Sci.},
type = {Article}}
@conference{Feng2023Research,
author = {Feng, Jiaqi},
title = {Research on Physiological Signal Analysis Based on Clinical Statistics and Machine Learning},
year = {2023},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
volume = {12611},
pages = {},
doi = {10.1117/12.2669358},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159098213&doi=10.1117%2F12.2669358&partnerID=40&md5=9fe2003eab04059cd24806fdce8446f9},
affiliations = {University of Melbourne, Melbourne, VIC, Australia},
abstract = {Neural network evaluation and decoding technology based on physiological signal analysis has become a hot research topic in the field of life and health. The decoding of physiological signals can enable people to break through physical limitations to communicate with the external environment, and can also enhance the effectiveness of the human body by evaluating people's cognitive and motor functions. Electroencephalogram (EEG), as a kind of physiological signal, has become a hot spot of much research due to its non-invasive, high time precision and strong explanatory characteristics. This paper summarizes the applied research based on physiological signal analysis such as EEG from three aspects of evaluation technology, human-computer interaction and clinical application, and looks forward to the development direction in the future. © 2023 SPIE.}, keywords = {Biomedical signal processing; Clinical research; Decoding; Electroencephalography; Physiological models; Psychophysiology; Signal analysis; Clinical statistics; Decoding technology; Machine-learning; Network decoding; Network evaluation; Neural decoding; Neural-networks; Physiological signals; Signals analysis; Statistics learning; Machine learning},
correspondence_address = {J. Feng; The University of Melbourne, Parkville, Grattan Street, 3010, Australia; email: jiaqfeng@student.unimelb.edu.au},
editor = {Royle, G. and Lipkin, S.M.},
publisher = {SPIE},
issn = {0277786X; 1996756X},
isbn = {9781510692657; 9781510690561; 9781510693302; 9781510692251; 9781510692275; 9781510693081; 9781510688728; 9781510688629; 9781510692671; 9781510693326},
coden = {PSISD},
language = {English},
abbrev_source_title = {Proc SPIE Int Soc Opt Eng},
type = {Conference paper}}
@article{Fu2023Eegnetmsd,
author = {Fu, Rongrong and Wang, Zeyi and Wang, Shiwei and Xu, Xuechen and Chen, Junxiang and Wen, Guilin},
title = {EEGNet-MSD: A Sparse Convolutional Neural Network for Efficient EEG-Based Intent Decoding},
year = {2023},
journal = {IEEE Sensors Journal},
volume = {23},
number = {17},
pages = {19684 - 19691},
doi = {10.1109/JSEN.2023.3295407},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165236863&doi=10.1109%2FJSEN.2023.3295407&partnerID=40&md5=26cc655f712cfe4922ff37f95eb94ec6},
affiliations = {Yanshan University, Department of Electrical Engineering, Qinhuangdao, Hebei, China; School of Photovoltaic Materials, Xinyu, China; University of Pittsburgh School of Medicine, Pittsburgh, PA, United States; Yanshan University, School of Mechanical Engineering, Qinhuangdao, Hebei, China},
abstract = {Electroencephalography (EEG) is a noninvasive technique that can be used in brain machine interface (BMI) systems to measure and record brain electrical activity. Deep learning (DL) techniques have proved superior to conventional methods in EEG-based intent decoding. However, some DL models have overly complex structures while ensuring the accuracy of EEG recognition, resulting in reduced training and recognition speed. In this study, we proposed a compact multihead self-attention DL decoder that combined the convolutional neural network (CNN)-based EEGNet decoder with the ProbSparse multihead self-attention mechanism. Compared with traditional self-attention methods, this decoder ensures alignment dependent on both time complexity and memory usage of O (L log L) and it has been demonstrated to enhance the accuracy of EEG-based intent recognition. The test results on dataset 2a from BCI Competition IV showed that the EEGNet multihead self-attention decoding (EEGNet-MSD) decoder performed approximately 8% better than the competition-winning decoder filter bank common spatial pattern (FBCSP) and namely batch and pairwise (NBPW), and achieved better results than the latest long short-term memory (LSTM) neural decoding method. In addition, a binary classification test was performed on the Physiobank EEG motor imagery (MI) dataset, and the results showed that the accuracy of EEGNet-MSD was approximately 4% higher than EEGNet, validating the stability of the EEGNet-MSD decoder. This study provides a new solution for enhancing the performance of EEG-based intent decoding in both accuracy and decoding speed. © 2001-2012 IEEE.}, keywords = {Brain; Brain computer interface; Classification (of information); Complex networks; Convolution; Decoding; Electrophysiology; Long short-term memory; Statistical tests; Brain electrical activity; Convolutional neural network; Electroencephalography; Intent decoding; Interface system; Machine interfaces; Motor imagery; Noninvasive technique; Probsparse self-attention},
correspondence_address = {R. Fu; Yanshan University, Measurement Technology and Instrumentation Key Laboratory of Hebei Province, Department of Electrical Engineering, Qinhuangdao, 066004, China; email: frr1102@aliyun.com},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {1530437X},
language = {English},
abbrev_source_title = {IEEE Sensors J.},
type = {Article}}
@article{Gow2023Abstract,
author = {Gow, David W. and Avcu, Enes and Schoenhaut, Adriana M. and Sorensen, David O. and Ahlfors, Seppo P.},
title = {Abstract representations in temporal cortex support generative linguistic processing},
year = {2023},
journal = {Language, Cognition and Neuroscience},
volume = {38},
number = {6},
pages = {765 - 778},
doi = {10.1080/23273798.2022.2157029},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144282819&doi=10.1080%2F23273798.2022.2157029&partnerID=40&md5=3dd9feb7c03158fe560d9acd91c8899b},
affiliations = {Harvard Medical School, Boston, MA, United States; Salem State University, Department of Psychology, Salem, MA, United States; Massachusetts General Hospital, Boston, MA, United States; Harvard Medical School, Program in Speech and Hearing Bioscience and Technology, Boston, MA, United States; Harvard Medical School, Department of Radiology, Boston, MA, United States},
abstract = {Generativity, the ability to create and evaluate novel constructions, is a fundamental property of human language and cognition. The productivity of generative processes is determined by the scope of the representations they engage. Here we examine the neural representation of reduplication, a productive phonological process that can create novel forms through patterned syllable copying (e.g. ba-mih → ba-ba-mih, ba-mih-mih, or ba-mih-ba). Using MRI-constrained source estimates of combined MEG/EEG data collected during an auditory artificial grammar task, we identified localised cortical activity associated with syllable reduplication pattern contrasts in novel trisyllabic nonwords. Neural decoding analyses identified a set of predominantly right hemisphere temporal lobe regions whose activity reliably discriminated reduplication patterns evoked by untrained, novel stimuli. Effective connectivity analyses suggested that sensitivity to abstracted reduplication patterns was propagated between these temporal regions. These results suggest that localised temporal lobe activity patterns function as abstract representations that support linguistic generativity. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.}, keywords = {article; electroencephalogram; grammar; human; human experiment; magnetoencephalography; nuclear magnetic resonance imaging; phonetics; right hemisphere; temporal cortex; temporal lobe},
correspondence_address = {D.W. Gow; Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, United States; email: gow@helix.mgh.harvard.edu},
publisher = {Routledge},
issn = {23273798; 23273801},
language = {English},
abbrev_source_title = {Lang. Cogn. Neurosci.},
type = {Article}}
@article{Granjon2023Neural,
author = {Granjon, Marine and Doignon-Camus, Nad́ege and Popa-Roch, Maria Antoneta and Rohmer, Odile},
title = {Neural empathic response to disability: An ERP study of prejudice},
year = {2023},
journal = {Biological Psychology},
volume = {177},
pages = {},
doi = {10.1016/j.biopsycho.2023.108507},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146857028&doi=10.1016%2Fj.biopsycho.2023.108507&partnerID=40&md5=d635d3024ef38f7fab63c39643fe3405},
affiliations = {Laboratoire de Psychologie des Cognitions (LPC), Strasbourg, Grand Est, France; Laboratoire Interuniversitaire des Sciences de l’Éducation et de la Communication, Strasbourg, Grand Est, France},
abstract = {While social neuroscience has already provided evidence for a deficit of affective empathy in racial prejudice, little is known about other less visible social categories when considered as an outgroup. We studied the process of empathy through event-related brain potentials (ERPs). We focused on the group “people with disabilities” as they are the target of a large amount of prejudice. Twenty-six participants performed a pain decision task. The mean amplitudes of N1, P2, N2–N3 and P3 components were recorded. Our results are consistent with previous work on prejudice, showing that the pain detection is modulated by group membership (with disabilities vs. without disabilities) on N2–N3, suggesting a better neural decoding of pain vs. non-pain in the without-disability condition. Critically, no effect of early sensory components (N1, P2) was found, and P3 was not moderated by disability. These findings indicate a different time course of empathic responses depending on the condition, suggesting that people with disabilities trigger specific empathic responses. Our results contribute to disentangling perceptual processes from affective empathy reactions. © 2023}, keywords = {adult; amygdala; Article; electroencephalography; electrooculography; empathy; female; functional connectivity; human; human experiment; male; neurodisability; young adult; brain; evoked response; pain; physiology; psychology; racism; Brain; Electroencephalography; Empathy; Evoked Potentials; Humans; Pain; Racism},
correspondence_address = {M. Granjon; University of Strasbourg, Laboratoire de Psychologie des Cognitions, France; email: granjon@unistra.fr},
publisher = {Elsevier B.V.},
issn = {18736246; 03010511},
coden = {BLPYA},
pmid = {36706863},
language = {English},
abbrev_source_title = {Biol. Psychol.},
type = {Article}}
@article{Guo2023Editorial,
author = {Guo, Yuzhu and Li, Yang and Wei, H. and Zhao, Yifan},
title = {Editorial: New theories, models, and AI methods of brain dynamics, brain decoding and neuromodulation},
year = {2023},
journal = {Frontiers in Neuroscience},
volume = {17},
pages = {},
doi = {10.3389/fnins.2023.1302505},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180680648&doi=10.3389%2Ffnins.2023.1302505&partnerID=40&md5=15c2c170e903c81f135038a89856d4e4},
affiliations = {Beihang University, School of Automation Science and Electrical Engineering, Beijing, China; Boardware-Barco-Beihang BAIoT Brain Computer Intelligence Joint Laboratory, Beijing, Beijing, China; The University of Sheffield, Department of Automatic Control and Systems Engineering, Sheffield, South Yorkshire, United Kingdom; Cranfield University, Centre for Life-Cycle Engineering and Management, Cranfield, Bedfordshire, United Kingdom}, keywords = {artificial intelligence; brain decoding; brain dynamics; brain function; cerebrovascular accident; cross frequency coupling; dynamics; Editorial; electroencephalogram; functional connectivity; human; microvasculature; myelooptic neuropathy; nerve excitability; neural coupling; neuromodulation; physical medicine; primary motor cortex; reaction time; repetitive transcranial magnetic stimulation; retinal microvascular changes; somatosensory cortex; transcranial alternating current stimulation; working memory},
correspondence_address = {Y. Guo; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; email: yuzhuguo@buaa.edu.cn},
publisher = {Frontiers Media SA},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Editorial}}
@article{Hasan2023Eegbased,
author = {Hasan, Haitham S. and Al-Sharqi, Mais A.},
title = {EEG-based image classification using an efficient geometric deep network based on functional connectivity},
year = {2023},
journal = {Periodicals of Engineering and Natural Sciences},
volume = {11},
number = {1},
pages = {208 - 215},
doi = {10.21533/pen.v11i1.3450},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150248349&doi=10.21533%2Fpen.v11i1.3450&partnerID=40&md5=d02d47259ad7b9cad3f38e28f852efc5},
affiliations = {University of Information Technology and Communications, Department of Business Information Technology, Baghdad, Baghdad, Iraq; University of Information Technology and Communications, Department of Bioinformatics, Baghdad, Baghdad, Iraq},
abstract = {To ensure that the FC-GDN is properly calibrated for the EEG-ImageNet dataset, we subject it to extensive training and gather all of the relevant weights for its parameters. Making use of the FC-GDN pseudo-code. The dataset is split into a "train" and "test" section in Kfold cross-validation. Ten-fold recommends using ten folds, with one fold being selected as the test split at each iteration. This divides the dataset into 90% training data and 10% test data. In order to train all 10 folds without overfitting, it is necessary to apply this procedure repeatedly throughout the whole dataset. Each training fold is arrived at after several iterations. After training all ten folds, results are analyzed. For each iteration, the FC-GDN weights are optimized by the SGD and ADAM optimizers. The ideal network design parameters are based on the convergence of the trains and the precision of the tests. This study offers a novel geometric deep learning-based network architecture for classifying visual stimulation categories using electroencephalogram (EEG) data from human participants while they watched various sorts of images. The primary goals of this study are to (1) eliminate feature extraction from GDL-based approaches and (2) extract brain states via functional connectivity. Tests with the EEG-ImageNet database validate the suggested method's efficacy. FC-GDN is more efficient than other cutting-edge approaches for boosting classification accuracy, requiring fewer iterations. In computational neuroscience, neural decoding addresses the problem of mind-reading. Because of its simplicity of use and temporal precision, Electroencephalographys (EEG) are commonly employed to monitor brain activity. Deep neural networks provide a variety of ways to detecting brain activity. Using a Function Connectivity (FC) - Geometric Deep Network (GDN) and EEG channel functional connectivity, this work directly recovers hidden states from high-resolution temporal data. The time samples taken from each channel are utilized to represent graph signals on a topological connection network based on EEG channel functional connectivity. A novel graph neural network architecture evaluates users' visual perception state utilizing extracted EEG patterns associated to various picture categories using graphically rendered EEG recordings as training data. The efficient graph representation of EEG signals serves as the foundation for this design. Proposal for an FC-GDN EEG-ImageNet test. Each category has a maximum of 50 samples. Nine separate EEG recorders were used to obtain these images. The FC-GDN approach yields 99.4% accuracy, which is 0.1% higher than the most sophisticated method presently available © The Author 2023. This work is licensed under a Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/) that allows others to share and adapt the material for any purpose (even commercially), in any medium with an acknowledgement of the work's authorship and initial publication in this journal},
keywords = {Deep Learning; EEG; Functional Connectivity; Neural Network; Visual Stimulus Decoding},
correspondence_address = {H.S. Hasan; Business Information Technology Department, Business Informatics College, University of Information Technology and Communications, Iraq; email: Haitham@uoitc.edu.iq},
publisher = {International University of Sarajevo},
issn = {23034521},
language = {English},
abbrev_source_title = {Period. eng. nat. sci.},
type = {Article}}
@article{Hua2023Neural,
author = {Hua, Lin and Gao, Fei and Leong, Chantat and Yuan, Zhen},
title = {Neural decoding dissociates perceptual grouping between proximity and similarity in visual perception},
year = {2023},
journal = {Cerebral Cortex},
volume = {33},
number = {7},
pages = {3803 - 3815},
doi = {10.1093/cercor/bhac308},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159549724&doi=10.1093%2Fcercor%2Fbhac308&partnerID=40&md5=d59ef52a17392dc7345eadb6c638313c},
affiliations = {University of Macau, Centre for Cognitive and Brain Sciences, Taipa, Macao; Faculty of Health Sciences, Taipa, Macao},
abstract = {Unlike single grouping principle, cognitive neural mechanism underlying the dissociation across two or more grouping principles is still unclear. In this study, a dimotif lattice paradigm that can adjust the strength of one grouping principle was used to inspect how, when, and where the processing of two grouping principles (proximity and similarity) were carried out in human brain. Our psychophysical findings demonstrated that similarity grouping effect was enhanced with reduced proximity effect when the grouping cues of proximity and similarity were presented simultaneously. Meanwhile, EEG decoding was performed to reveal the specific cognitive patterns involved in each principle by using time-resolved MVPA. More importantly, the onsets of dissociation between 2 grouping principles coincided within 3 time windows: the early-stage proximity-defined local visual element arrangement in middle occipital cortex, the middle-stage processing for feature selection modulating low-level visual cortex such as inferior occipital cortex and fusiform cortex, and the high-level cognitive integration to make decisions for specific grouping preference in the parietal areas. In addition, it was discovered that the brain responses were highly correlated with behavioral grouping. Therefore, our study provides direct evidence for a link between the human perceptual space of grouping decision-making and neural space of brain activation patterns. © The Author(s) 2022. Published by Oxford University Press. All rights reserved.}, keywords = {article; brain cortex; decision making; dissociation; electroencephalogram; feature selection; human; human experiment; occipital cortex; vision; visual cortex},
correspondence_address = {Z. Yuan; Centre for Cognitive and Brain Sciences, N21 Research Building, University of Macau, Taipa, Avenida da Universidade, 999078, Macao; email: zhenyuan@um.edu.mo},
publisher = {Oxford University Press},
issn = {14602199; 10473211},
coden = {CECOE},
pmid = {35973163},
language = {English},
abbrev_source_title = {Cereb. Cortex},
type = {Article}}
@conference{Jaswal2023Learning,
author = {Jaswal, Gaurav and Sharma, Geetanjali and Dutt, Varun and Bhavsar, Arnav V.},
title = {Learning channel attention for decoding of visual imagined text from multi-band EEG using metric learning},
year = {2023},
pages = {720 - 727},
doi = {10.1145/3594806.3596566},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170395064&doi=10.1145%2F3594806.3596566&partnerID=40&md5=751cbd2de95e8f7b46570a26ba61b5fd},
affiliations = {Indian Institute of Technology Mandi, Mandi, HP, India; Indian Institute of Technology Mandi, School of Computing and Electrical Engineering, Mandi, HP, India},
abstract = {The electroencephalogram (EEG) signals represent proxies of the perceptual cognitive, and emotional processes of the user, which allows neuro-engineering researchers to decode the activities of the human mind non-invasively. Many studies resulting in visual imagined brain decoding have been proposed for the classification of EEG using traditional machine learning and deep learning algorithms. Although deep learning methods have achieved great success in many fields, their performance in EEG-based brain decoding analysis is still limited mainly due to the relatively small sizes of available datasets and low signal-to-noise ratio characteristics. Since some channels in the input feature maps contain more prominent information, we capitalize discriminative feature learning across the channels through the channel attention mechanism. This paper employs a low parametric channel attention module that adopts the idea of sparse cross-channel relationships and involves much fewer parameters but brings a performance gain to the baseline. Prior embedding is obtained using multi-block 1DCNN and then the feature vector is optimized using triplet loss for inducting better discriminator power. In order to show network generality, experiments have been conducted on an In-house visual imagery EEG dataset and as well a case study has been presented on a standard visual imagery EEG dataset. Our framework has achieved outperforming results on standard EEG datasets as well as showing gains in performance on in-house data (Dataset and code will be made publically available). © 2023 ACM.}, keywords = {Brain computer interface; Decoding; Deep learning; Learning algorithms; Learning systems; Signal to noise ratio; Brain decoding; Dataset; Electroencephalogram signals; Gaze detection; Metric learning; Multi band; Neural-networks; Performance; Text tagging; Visual imagery; Electroencephalography},
publisher = {Association for Computing Machinery},
isbn = {9798400700699},
language = {English},
abbrev_source_title = {ACM Int. Conf. Proc. Ser.},
type = {Conference paper}}
@article{Jiang2023Cybersecurity,
author = {Jiang, Xinyu and Fan, Jiahao and Zhu, Ziyue and Wang, Zihao and Guo, Yao and Liu, Xiangyu and Jia, Fumin and Dai, Chenyun},
title = {Cybersecurity in neural interfaces: Survey and future trends},
year = {2023},
journal = {Computers in Biology and Medicine},
volume = {167},
pages = {},
doi = {10.1016/j.compbiomed.2023.107604},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174835427&doi=10.1016%2Fj.compbiomed.2023.107604&partnerID=40&md5=9c8d06f9a33c462d5c9c898e8c879fc6},
affiliations = {Fudan University, School of Information Science and Technology, Shanghai, China; Penn State College of Engineering, University Park, PA, United States; Imperial College London, Department of Bioengineering, London, United Kingdom; University of Shanghai for Science and Technology, College of Communication and Art Design, Shanghai, Shanghai, China; Fudan University, Institute of Science and Technology for Brain-Inspired Intelligence, Shanghai, China},
abstract = {With the joint advancement in areas such as pervasive neural data sensing, neural computing, neuromodulation and artificial intelligence, neural interface has become a promising technology facilitating both the closed-loop neurorehabilitation for neurologically impaired patients and the intelligent man–machine interactions for general application purposes. However, although neural interface has been widely studied, few previous studies focused on the cybersecurity issues in related applications. In this survey, we systematically investigated possible cybersecurity risks in neural interfaces, together with potential solutions to these problems. Importantly, our survey considers interfacing techniques on both central nervous systems (i.e., brain–computer interfaces) and peripheral nervous systems (i.e., general neural interfaces), covering diverse neural modalities such as electroencephalography, electromyography and more. Moreover, our survey is organized on three different levels: (1) the data level, which mainly focuses on the privacy leakage issue via attacking and analyzing neural database of users; (2) the permission level, which mainly focuses on the prospects and risks to directly use real time neural signals as biometrics for continuous and unobtrusive user identity verification; and (3) the model level, which mainly focuses on adversarial attacks and defenses on both the forward neural decoding models (e.g. via machine learning) and the backward feedback implementation models (e.g. via neuromodulation and stimulation). This is the first study to systematically investigate cybersecurity risks and possible solutions in neural interfaces which covers both central and peripheral nervous systems, and considers multiple different levels to provide a complete picture of this issue. © 2023 Elsevier Ltd}, keywords = {Artificial intelligence; Electroencephalography; Electrophysiology; Real time systems; Adversarial attack and defense; Central nervous systems; Cyber security; Future trends; Neural data; Neural interfaces; Neuromodulation; Peripheral nervous system; Privacy preservation; System security; Cybersecurity; anonymization; biometry; central nervous system; computer security; data privacy; electroencephalography; electromyography; encryption; human; machine learning; nerve stimulation; neurofeedback; neuromodulation; peripheral nervous system; Review; transfer of learning; artificial intelligence; nervous system; Artificial Intelligence; Brain-Computer Interfaces; Computer Security; Electromyography; Humans; Nervous System},
correspondence_address = {F. Jia; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China; email: jfmin@fudan.edu.cn},
publisher = {Elsevier Ltd},
issn = {18790534; 00104825},
coden = {CBMDA},
pmid = {37883851},
language = {English},
abbrev_source_title = {Comput. Biol. Med.},
type = {Review}}
@article{Khaleghi2023Salient,
author = {Khaleghi, Nastaran and Hashemi, Shaghayegh and Ardabili, Sevda Zafarmandi and Sheykhivand, Sobhan and Daneshvar, Sabalan},
title = {Salient Arithmetic Data Extraction from Brain Activity via an Improved Deep Network},
year = {2023},
journal = {Sensors},
volume = {23},
number = {23},
pages = {},
doi = {10.3390/s23239351},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179136822&doi=10.3390%2Fs23239351&partnerID=40&md5=6ccff6212b83e6cfd5e342f3985110b5},
affiliations = {University of Tabriz, Department of Electrical & Computer Engineering, Tabriz, East Azerbaijan, Iran; Shahid Beheshti University, Department of Computer Science and Engineering, Tehran, Tehran, Iran; Bobby B. Lyle School of Engineering, Dallas, TX, United States; University of Bonab, Department of Biomedical Engineering, Bonab, East Azerbaijan, Iran; Brunel University London, Design and Physical Sciences, Uxbridge, Middlesex, United Kingdom},
abstract = {Interpretation of neural activity in response to stimulations received from the surrounding environment is necessary to realize automatic brain decoding. Analyzing the brain recordings corresponding to visual stimulation helps to infer the effects of perception occurring by vision on brain activity. In this paper, the impact of arithmetic concepts on vision-related brain records has been considered and an efficient convolutional neural network-based generative adversarial network (CNN-GAN) is proposed to map the electroencephalogram (EEG) to salient parts of the image stimuli. The first part of the proposed network consists of depth-wise one-dimensional convolution layers to classify the brain signals into 10 different categories according to Modified National Institute of Standards and Technology (MNIST) image digits. The output of the CNN part is fed forward to a fine-tuned GAN in the proposed model. The performance of the proposed CNN part is evaluated via the visually provoked 14-channel MindBigData recorded by David Vivancos, corresponding to images of 10 digits. An average accuracy of 95.4% is obtained for the CNN part for classification. The performance of the proposed CNN-GAN is evaluated based on saliency metrics of SSIM and CC equal to 92.9% and 97.28%, respectively. Furthermore, the EEG-based reconstruction of MNIST digits is accomplished by transferring and tuning the improved CNN-GAN’s trained weights. © 2023 by the authors.}, keywords = {Brain; Convolution; Convolutional neural networks; Deep learning; Electroencephalography; Neurons; Arithmetic content; Brain activity; Convolutional neural network; Data extraction; Modified national institute of standard and technology; National Institute of Standards and Technology; Network-based; Performance; Visual perception; Generative adversarial networks; artificial neural network; brain; head; Head; Neural Networks, Computer},
correspondence_address = {S. Danishvar; College of Engineering, Design and Physical Sciences, Brunel University London, Uxbridge, UB8 3PH, United Kingdom; email: sebelan.danishvar@brunel.ac.uk},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {14248220},
pmid = {38067727},
language = {English},
abbrev_source_title = {Sensors},
type = {Article}}
@article{Khaleghi2023Developing,
author = {Khaleghi, Nastaran and Yousefi Rezaii, Tohid and Beheshti, Soosan and Meshgini, Saeed},
title = {Developing an efficient functional connectivity-based geometric deep network for automatic EEG-based visual decoding},
year = {2023},
journal = {Biomedical Signal Processing and Control},
volume = {80},
pages = {},
doi = {10.1016/j.bspc.2022.104221},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140733886&doi=10.1016%2Fj.bspc.2022.104221&partnerID=40&md5=3217ccf4d3e2e099170105e3d62fb66c},
affiliations = {University of Tabriz, Faculty of Electrical and Computer Engineering, Tabriz, East Azerbaijan, Iran; Toronto Metropolitan University, Department of Electrical & Computer Engineering, Toronto, ON, Canada},
abstract = {Neural decoding is of great importance in computational neuroscience to automatically interpret brain activities in order to address the challenging problem of mind-reading. Analyzing the vision-related EEG records is of great importance to discern the relation between visual perception and brain activity. Considering the recent advances and achievements in the field of deep neural networks, several architectures have been implemented to decode brain activities. In this paper, functional connectivity-based geometric deep network (FC-GDN) is proposed to leverage the spatio-temporal distributed information in EEG recordings evoked by images to directly extract hidden states of high-resolution time samples considering the functional connectivity between EEG channels. To this end, a topological connectivity graph is constructed based on the functional connectivity between EEG channels and time samples of each EEG channel are considered as a graph signal on top of corresponding graph node. Furthermore, a novel graph neural network architecture based on this efficient graph representation of EEG signals is proposed, in which visually provoked EEG recordings are used as training data in order to decode visual perception state of the participants in terms of extracted EEG patterns related to different image categories. The performance of the proposed FC-GDN is evaluated on the EEG-ImageNet dataset, consisting of 40 image categories and each category includes 50 sample images, shown to 6 participants while their EEG signals were recorded. The average accuracy of 98.4% is obtained for FC-GDN, showing an average improvement of 1.1% compared to the best state-of-the-art method. © 2022 Elsevier Ltd}, keywords = {Brain; Data mining; Data visualization; Deep neural networks; Electroencephalography; Geometry; Network architecture; Neurophysiology; Brain activity; Decoding visual stimulus; EEG recording; EEG signals; Functional connectivity; Geometric deep learning; Neural decoding; Time samples; Visual perception; Visual stimulus; Decoding; adult; article; clinical article; deep learning; electroencephalogram; female; functional connectivity; human; human experiment; male; vision},
correspondence_address = {T.Y. Rezaii; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; email: yousefi@tabrizu.ac.ir},
publisher = {Elsevier Ltd},
issn = {17468108; 17468094},
language = {English},
abbrev_source_title = {Biomed. Signal Process. Control},
type = {Article}}
@conference{Lee2023Towards,
author = {Lee, Seohyun and Lee, Young-eun and Kim, Soowon and Ko, Byung-kwan and Lee, Seongwhan},
title = {Towards Neural Decoding of Imagined Speech based on Spoken Speech},
year = {2023},
journal = {International Winter Conference on Brain-Computer Interface, BCI},
volume = {2023-February},
pages = {},
doi = {10.1109/BCI57258.2023.10078707},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152204812&doi=10.1109%2FBCI57258.2023.10078707&partnerID=40&md5=0a6f136db8ade377a71afe1adf34a4d7},
affiliations = {Korea University, Department of Brain and Cognitive Engineering, Seoul, South Korea; Korea University, Department of Artificial Intelligence, Seoul, South Korea},
abstract = {Decoding imagined speech from human brain signals is a challenging and important issue that may enable human communication via brain signals. While imagined speech can be the paradigm for silent communication via brain signals, it is always hard to collect enough stable data to train the decoding model. Meanwhile, spoken speech data is relatively easy and to obtain, implying the significance of utilizing spoken speech brain signals to decode imagined speech. In this paper, we performed a preliminary analysis to find out whether if it would be possible to utilize spoken speech electroencephalography data to decode imagined speech, by simply applying the pre-trained model trained with spoken speech brain signals to decode imagined speech. While the classification performance of imagined speech data solely used to train and validation was 30. 5±4.9 %, the transferred performance of spoken speech based classifier to imagined speech data displayed average accuracy of 26.8±2.0 % which did not have statistically significant difference compared to the imagined speech based classifier (p=0.0983, chi-square =4.64). For more comprehensive analysis, we compared the result with the visual imagery dataset, which would naturally be less related to spoken speech compared to the imagined speech. As a result, visual imagery have shown solely trained performance of 31.8±4.1 % and transferred performance of 26.3±2.4 % which had shown statistically significant difference between each other (p=0.022, chi-square =7.64). Our results imply the potential of applying spoken speech to decode imagined speech, as well as their underlying common features. © 2023 IEEE.}, keywords = {Audio signal processing; Biomedical signal processing; Brain computer interface; Decoding; Electroencephalography; Electrophysiology; Speech communication; Brain signals; Human brain; Human communications; Imagined speech; Neural decoding; Performance; Speech data; Spoken speech; Statistically significant difference; Visual imagery; Speech recognition},
correspondence_address = {S.-H. Lee; Korea University, Dept. Brain and Cognitive Engineering, Seoul, South Korea; email: seohyunlee@korea.ac.kr},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {25727672},
isbn = {9781665464444; 9798350309430; 9781665413374; 9798331521929},
language = {English},
abbrev_source_title = {Int. Winter Conf. Brain-Comput. Interface, BCI},
type = {Conference paper}}
@article{Li2023Encrypt,
author = {Li, Ming and Qi, Yu and Pan, Gang},
title = {Encrypt with Your Mind: Reliable and Revocable Brain Biometrics via Multidimensional Gaussian Fitted Bit Allocation},
year = {2023},
journal = {Bioengineering},
volume = {10},
number = {8},
pages = {},
doi = {10.3390/bioengineering10080912},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168874695&doi=10.3390%2Fbioengineering10080912&partnerID=40&md5=992322b0dc4504a2eecc6a09e9fd65ad},
affiliations = {Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou, Zhejiang, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University School of Medicine, Affiliated Mental Health Center, Hangzhou, Zhejiang, China},
abstract = {Biometric features, e.g., fingerprints, the iris, and the face, have been widely used to authenticate individuals. However, most biometrics are not cancellable, i.e., once these biometric features are cloned or stolen, they cannot be replaced easily. Unlike traditional biometrics, brain biometrics are extremely difficult to clone or forge due to the natural randomness across different individuals, which makes them an ideal option for identity authentication. Most existing brain biometrics are based on electroencephalogram (EEG), which is usually demonstrated unstable performance due to the low signal-to-noise ratio (SNR). For the first time, we propose the use of intracortical brain signals, which have higher resolution and SNR, to realize the construction of the high-performance brain biometrics. Specifically, we put forward a novel brain-based key generation approach called multidimensional Gaussian fitted bit allocation (MGFBA). The proposed MGFBA method extracts keys from the local field potential of ten rats with high reliability and high entropy. We found that with the proposed MGFBA, the average effective key length of the brain biometrics was 938 bits, while achieving high authentication accuracy of 88.1% at a false acceptance rate of 1.9%, which is significantly improved compared to conventional EEG-based approaches. In addition, the proposed MGFBA-based keys can be conveniently revoked using different motor behaviors with high entropy. Experimental results demonstrate the potential of using intracortical brain signals for reliable authentication and other security applications. © 2023 by the authors.},
keywords = {authentication; biometrics; brain decoding; electroencephalogram; intracortical brain signals; key generation; local field potential},
correspondence_address = {M. Li; State Key Lab of Brain-Machine Intelligence, Hangzhou, 310018, China; email: lming@zju.edu.cn},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {23065354},
language = {English},
abbrev_source_title = {Bioeng.},
type = {Article}}
@article{Li2023Optimal,
author = {Li, Ming and Qi, Yu and Pan, Gang},
title = {Optimal Feature Analysis for Identification Based on Intracranial Brain Signals with Machine Learning Algorithms},
year = {2023},
journal = {Bioengineering},
volume = {10},
number = {7},
pages = {},
doi = {10.3390/bioengineering10070801},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166331648&doi=10.3390%2Fbioengineering10070801&partnerID=40&md5=587896189ec3b04be9b3ccd9f314b863},
affiliations = {Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou, Zhejiang, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University School of Medicine, MOE Frontier Science Center for Brain Science and Brain-Machine Integration, Hangzhou, Zhejiang, China},
abstract = {Biometrics, e.g., fingerprints, the iris, and the face, have been widely used to authenticate individuals. However, most biometrics are not cancellable, i.e., once these traditional biometrics are cloned or stolen, they cannot be replaced easily. Unlike traditional biometrics, brain biometrics are extremely difficult to clone or forge due to the natural randomness across different individuals, which makes them an ideal option for identity authentication. Most existing brain biometrics are based on an electroencephalogram (EEG), which typically demonstrates unstable performance due to the low signal-to-noise ratio (SNR). Thus, in this paper, we propose the use of intracortical brain signals, which have higher resolution and SNR, to realize the construction of a high-performance brain biometric. Significantly, this is the first study to investigate the features of intracortical brain signals for identification. Specifically, several features based on local field potential are computed for identification, and their performance is compared with different machine learning algorithms. The results show that frequency domain features and time-frequency domain features are excellent for intra-day and inter-day identification. Furthermore, the energy features perform best among all features with 98% intra-day and 93% inter-day identification accuracy, which demonstrates the great potential of intracraial brain signals to be biometrics. This paper may serve as a guidance for future intracranial brain researches and the development of more reliable and high-performance brain biometrics. © 2023 by the authors.},
keywords = {biometrics; brain decoding; electroencephalogram; identification; intracranial brain signals; local field potential},
correspondence_address = {M. Li; State Key Lab of Brain-Machine Intelligence, Hangzhou, 310018, China; email: lming@zju.edu.cn},
publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
issn = {23065354},
language = {English},
abbrev_source_title = {Bioeng.},
type = {Article}}
@article{Li2023Correntropybased,
author = {Li, Yuanhao and Chen, Badong and Shi, Yuxi and Yoshimura, Natsue and Koike, Yasuharu},
title = {Correntropy-Based Logistic Regression With Automatic Relevance Determination for Robust Sparse Brain Activity Decoding},
year = {2023},
journal = {IEEE Transactions on Biomedical Engineering},
volume = {70},
number = {8},
pages = {2416 - 2429},
doi = {10.1109/TBME.2023.3246599},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159696240&doi=10.1109%2FTBME.2023.3246599&partnerID=40&md5=19a7662593adcc7dda7761ecd1f791e0},
affiliations = {Institute of Science Tokyo, Institute of Innovative Research, Tokyo, Japan; Xi'an Jiaotong University, Institute of Artificial Intelligence and Robotics, Xi'an, Shaanxi, China},
abstract = {Objective: Recent studies have used sparse classifications to predict categorical variables from high-dimensional brain activity signals to expose human's mental states and intentions, selecting the relevant features automatically in the model training process. However, existing sparse classification models will likely be prone to the performance degradation which is caused by the noise inherent in the brain recordings. To address this issue, we aim to propose a new robust and sparse classification algorithm in this study. Methods: To this end, we introduce the correntropy learning framework into the automatic relevance determination based sparse classification model, proposing a new correntropy-based robust sparse logistic regression algorithm. To demonstrate the superior brain activity decoding performance of the proposed algorithm, we evaluate it on a synthetic dataset, an electroencephalogram (EEG) dataset, and a functional magnetic resonance imaging (fMRI) dataset. Results: The extensive experimental results confirm that not only the proposed method can achieve higher classification accuracy in a noisy and high-dimensional classification task, but also it would select those more informative features for the decoding tasks. Conclusion: Integrating the correntropy learning approach with the automatic relevance determination technique will significantly improve the robustness with respect to the noise, leading to more adequate robust sparse brain decoding algorithm. Significance: It provides a more powerful approach in the real-world brain activity decoding and the brain-computer interfaces. © 1964-2012 IEEE.}, keywords = {Brain; Brain computer interface; Decoding; Electrophysiology; Learning algorithms; Magnetic resonance imaging; Neurophysiology; Regression analysis; Automatic relevance determination; Brain decoding; Brain modeling; Correntropy; Correntropy learning; Functional magnetic resonance imaging; Prediction algorithms; Signal processing algorithms; Task analysis; Electroencephalography; Article; bandwidth; binary classification; classification algorithm; correlation function; cross validation; cslr algorithm; electroencephalogram; feature selection; functional magnetic resonance imaging; functional neuroimaging; galvanic vestibular test; human; human experiment; intermethod comparison; learning algorithm; logistic regression analysis; machine learning; mean squared error; measurement accuracy; multiclass classification; noise; normal human; sensory feedback; stochastic model; visual stimulation; algorithm; brain; brain mapping; diagnostic imaging; electroencephalography; learning; nuclear magnetic resonance imaging; procedures; statistical model; Algorithms; Brain Mapping; Brain-Computer Interfaces; Humans; Learning; Logistic Models; Magnetic Resonance Imaging},
correspondence_address = {Y. Li; Tokyo Institute of Technology, Institute of Innovative Research, Yokohama, 226-8503, Japan; email: li.y.ay@m.titech.ac.jp},
publisher = {IEEE Computer Society},
issn = {00189294; 15582531},
coden = {IEBEA},
pmid = {37093731},
language = {English},
abbrev_source_title = {IEEE Trans. Biomed. Eng.},
type = {Article}}
@article{Luo2023Stable,
author = {Luo, Shiyu and Angrick, Miguel and Coogan, Christopher G. and Candrea, Daniel N. and Wyse-Sookoo, Kimberley R. and Shah, Samyak and Rabbani, Qinwan and Milsap, Griffin W. and Weiss, Alexander R. and Anderson, William Stanley and Tippett, Donna Clark and Maragakis, Nicholas J. and Clawson, Lora L. and Vansteensel, Mariska J. and Wester, Brock A. and Tenore, Francesco V.G. and Heřmanský, Hynek and Fifer, Matthew S. and Ramsey, Nick Franciscus and Crone, Nathan Earl},
title = {Stable Decoding from a Speech BCI Enables Control for an Individual with ALS without Recalibration for 3 Months},
year = {2023},
journal = {Advanced Science},
volume = {10},
number = {35},
pages = {},
doi = {10.1002/advs.202304853},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174563307&doi=10.1002%2Fadvs.202304853&partnerID=40&md5=df655b8bc8fbe08608849f30cfc08655},
affiliations = {Johns Hopkins University School of Medicine, Department of Biomedical Engineering, Baltimore, MD, United States; Johns Hopkins University School of Medicine, Department of Neurology, Baltimore, MD, United States; Whiting School of Engineering, Baltimore, MD, United States; Whiting School of Engineering, Baltimore, MD, United States; Johns Hopkins University Applied Physics Laboratory, Research and Exploratory Development Department, Laurel, MD, United States; Johns Hopkins University School of Medicine, Department of Neurosurgery, Baltimore, MD, United States; Johns Hopkins University School of Medicine, Department of Otolaryngology-Head and Neck Surgery, Baltimore, MD, United States; Johns Hopkins University School of Medicine, Department of Physical Medicine and Rehabilitation, Baltimore, MD, United States; University Medical Center Utrecht, Department of Neurology and Neurosurgery, Utrecht, Netherlands},
abstract = {Brain-computer interfaces (BCIs) can be used to control assistive devices by patients with neurological disorders like amyotrophic lateral sclerosis (ALS) that limit speech and movement. For assistive control, it is desirable for BCI systems to be accurate and reliable, preferably with minimal setup time. In this study, a participant with severe dysarthria due to ALS operates computer applications with six intuitive speech commands via a chronic electrocorticographic (ECoG) implant over the ventral sensorimotor cortex. Speech commands are accurately detected and decoded (median accuracy: 90.59%) throughout a 3-month study period without model retraining or recalibration. Use of the BCI does not require exogenous timing cues, enabling the participant to issue self-paced commands at will. These results demonstrate that a chronically implanted ECoG-based speech BCI can reliably control assistive devices over long time periods with only initial model training and calibration, supporting the feasibility of unassisted home use. © 2023 The Authors. Advanced Science published by Wiley-VCH GmbH.}, keywords = {Decoding; Electroencephalography; Electrophysiology; Medical computing; Neurodegenerative diseases; Amyotrophic lateral sclerose; Amyotrophic lateral sclerosis; Assistive; Assistive devices; Interface system; Neural decoding; Neurological disorders; Recalibrations; Speech brain-computer interface; Speech commands; Brain computer interface; amyotrophic lateral sclerosis; complication; electrocorticography; human; speech; Amyotrophic Lateral Sclerosis; Brain-Computer Interfaces; Electrocorticography; Humans; Speech},
correspondence_address = {N.E. Crone; Department of Neurology, Johns Hopkins University School of Medicine, Baltimore, 21287, United States; email: ncrone@jhmi.edu},
publisher = {John Wiley and Sons Inc},
issn = {21983844},
pmid = {37875404},
language = {English},
abbrev_source_title = {Adv. Sci.},
type = {Article}}
@article{Ngan2023Early,
author = {Ngan, Vince Siu Hin and Cheung, Leo Y.T. and Ng, Hezul Tin Yan and Yip, Ken H.M. and Wong, Yetta Kwailing and Wong, Alan Chun Nang},
title = {An early perceptual locus of absolute pitch},
year = {2023},
journal = {Psychophysiology},
volume = {60},
number = {2},
pages = {},
doi = {10.1111/psyp.14170},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137819355&doi=10.1111%2Fpsyp.14170&partnerID=40&md5=c89e68b011e7aa3ead483b69a8a3b20a},
affiliations = {Chinese University of Hong Kong, Department of Psychology, Hong Kong, Hong Kong; Chinese University of Hong Kong, Department of Educational Psychology, Hong Kong, Hong Kong},
abstract = {Absolute pitch (AP) refers to the naming of musical tone without external reference. The influential two-component model states that AP is limited by the late-emerging pitch labeling process only and not the earlier perceptual and memory processes. Over the years, however, support for this model at the neural level has been mixed with various methodological limitations. Here, the electroencephalography responses of 27 AP possessors and 27 non-AP possessors were recorded. During both name verification and passive listening, event-related potential analyses showed a difference between AP and non-AP possessors at about 200 ms in their response toward tones compared with noise stimuli. Multivariate pattern analyses suggested that pitch naming was subserved by a series of transient processes for the first 250 ms, followed by a stage-like process for both AP and non-AP possessors with no group differences between them. These findings are inconsistent with the predictions of the two-component model, and instead suggest the existence of an early perceptual locus of AP. © 2022 Society for Psychophysiological Research.}, keywords = {auditory stimulation; electroencephalography; hearing; human; memory; multivariate analysis; music; physiology; Acoustic Stimulation; Auditory Perception; Electroencephalography; Humans; Memory; Multivariate Analysis; Music},
correspondence_address = {A.C.N. Wong; Department of Psychology, The Chinese University of Hong Kong, Shatin, Hong Kong; email: alancnwong@gmail.com; Y.K. Wong; Department of Educational Psychology, Faculty of Education, The Chinese University of Hong Kong, Shatin, Hong Kong; email: yetta.wong@gmail.com},
publisher = {John Wiley and Sons Inc},
issn = {00485772; 14698986},
coden = {PSPHA},
pmid = {36094011},
language = {English},
abbrev_source_title = {Psychophysiology},
type = {Article}}
@article{Orima2023Spatiotemporal,
author = {Orima, Taiki and Motoyoshi, Isamu},
title = {Spatiotemporal cortical dynamics for visual scene processing as revealed by EEG decoding},
year = {2023},
journal = {Frontiers in Neuroscience},
volume = {17},
pages = {},
doi = {10.3389/fnins.2023.1167719},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176385805&doi=10.3389%2Ffnins.2023.1167719&partnerID=40&md5=190881585b2b7a88a98803f2917b66de},
affiliations = {The University of Tokyo, Department of Life Science, Tokyo, Japan; Japan Society for the Promotion of Science, Tokyo, Tokyo, Japan},
abstract = {The human visual system rapidly recognizes the categories and global properties of complex natural scenes. The present study investigated the spatiotemporal dynamics of neural signals involved in visual scene processing using electroencephalography (EEG) decoding. We recorded visual evoked potentials from 11 human observers for 232 natural scenes, each of which belonged to one of 13 natural scene categories (e.g., a bedroom or open country) and had three global properties (naturalness, openness, and roughness). We trained a deep convolutional classification model of the natural scene categories and global properties using EEGNet. Having confirmed that the model successfully classified natural scene categories and the three global properties, we applied Grad-CAM to the EEGNet model to visualize the EEG channels and time points that contributed to the classification. The analysis showed that EEG signals in the occipital electrodes at short latencies (approximately 80 ~ ms) contributed to the classifications, whereas those in the frontal electrodes at relatively long latencies (200 ~ ms) contributed to the classification of naturalness and the individual scene category. These results suggest that different global properties are encoded in different cortical areas and with different timings, and that the combination of the EEGNet model and Grad-CAM can be a tool to investigate both temporal and spatial distribution of natural scene processing in the human brain. © © 2023 Orima and Motoyoshi.}, keywords = {accuracy; Article; brain cortex; brain function; computer model; deep convolutional classification model; electroencephalogram; electroencephalography; event related potential; human; human experiment; image analysis; natural scene processing; spatiotemporal cortical dynamics; support vector machine; vision; visual evoked potential; visual stimulation; visual system},
correspondence_address = {T. Orima; Department of Life Sciences, The University of Tokyo, Tokyo, Japan; email: taikiorima@gmail.com},
publisher = {Frontiers Media SA},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@article{Pan2023Images,
author = {Pan, Honggguang and Fu, Yunpeng and Li, Zhuoyi and Wen, Fan and Hu, Jianchen and Wu, Bo},
title = {Images Reconstruction from Functional Magnetic Resonance Imaging Patterns Based on the Improved Deep Generative Multiview Model},
year = {2023},
journal = {Neuroscience},
volume = {509},
pages = {103 - 112},
doi = {10.1016/j.neuroscience.2022.11.021},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145571633&doi=10.1016%2Fj.neuroscience.2022.11.021&partnerID=40&md5=6b69e10a82387ac87fe9d1484abe6dbe},
affiliations = {Xi'an University of Science and Technology, College of Electrical and Control Engineering, Xi'an, Shaanxi, China; Ministry of Education of the People's Republic of China, Key Laboratory of Industrial Internet of Things and Networked Control, Beijing, Beijing, China; Ltd., Xi'an, Shaanxi, China; Xi'an Jiaotong University, School of Automation Science and Engineering, Xi'an, Shaanxi, China; Ltd., Xi'an, Shaanxi, China},
abstract = {Reconstructing visual stimulus images from the brain activity signals is an important research task in the field of brain decoding. Many methods of reconstructing visual stimulus images mainly focus on how to use deep learning to classify the brain activities measured by functional magnetic resonance imaging or identify visual stimulus images. Accurate reconstruction of visual stimulus images by using deep learning still remains challenging. This paper proposes an improved deep generative multiview model to further promote the accuracy of reconstructing visual stimulus images. Firstly, an encoder based on residual-in-residual dense blocks is designed to fit the deep and multiview visual features of human natural state, and extract the features of visual stimulus images. Secondly, the structure of original decoder is extended to a deeper network in the deep generative multiview model, which makes the features obtained by each deconvolution layer more distinguishable. Finally, we configure the parameters of the optimizer and compare the performance of various optimizers under different parameter values, and then the one with the best performance is chosen and adopted to the whole model. The performance evaluations conducted on two publicly available datasets demonstrate that the improved model has more accurate reconstruction effectiveness than the original deep generative multiview model. © 2022 IBRO}, keywords = {accuracy; Article; deconvolution; deep learning; electroencephalogram; functional magnetic resonance imaging; human; image reconstruction; visual stimulation; brain; diagnostic imaging; head; image processing; nuclear magnetic resonance imaging; procedures; Brain; Head; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging},
correspondence_address = {H. Pan; College of Electrical and Control Engineering, Xi'an University of Science and Technology, Xi'an, 710054, China; email: hongguangpan@163.com},
publisher = {Elsevier Ltd},
issn = {18737544; 03064522},
coden = {NRSCD},
pmid = {36460220},
language = {English},
abbrev_source_title = {Neuroscience},
type = {Article}}
@article{Park2023Effect,
author = {Park, Jonghwa Jeonglok and Baek, Seung-cheol and Suh, Myung-Whan and Choi, Jongsuk and Kim, Sung-june and Lim, Yoonseob},
title = {The effect of topic familiarity and volatility of auditory scene on selective auditory attention},
year = {2023},
journal = {Hearing Research},
volume = {433},
pages = {},
doi = {10.1016/j.heares.2023.108770},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153294877&doi=10.1016%2Fj.heares.2023.108770&partnerID=40&md5=b614f9255489785610f98f1f6c4bdb2d},
affiliations = {Korea Institute of Science and Technology, Center for Intelligent and Interactive Robotics, Seoul, South Korea; SNU College of Engineering, Department of Electrical & Computer Engineering, Seoul, South Korea; Max Planck Institute for Empirical Aesthetics, Research Group Neurocognition of Music and Language, Frankfurt am Main, Hessen, Germany; Seoul National University Hospital, Department of Otolaryngology-Head and Neck Surgery, Seoul, South Korea; University of Science and Technology (UST), Division of AI-Robotics, Daejeon, South Korea; Hanyang University, Seoul, South Korea},
abstract = {Selective auditory attention has been shown to modulate the cortical representation of speech. This effect has been well documented in acoustically more challenging environments. However, the influence of top-down factors, in particular topic familiarity, on this process remains unclear, despite evidence that semantic information can promote speech-in-noise perception. Apart from individual features forming a static listening condition, dynamic and irregular changes of auditory scenes—volatile listening environments—have been less studied. To address these gaps, we explored the influence of topic familiarity and volatile listening on the selective auditory attention process during dichotic listening using electroencephalography. When stories with unfamiliar topics were presented, participants’ comprehension was severely degraded. However, their cortical activity selectively tracked the speech of the target story well. This implies that topic familiarity hardly influences the speech tracking neural index, possibly when the bottom-up information is sufficient. However, when the listening environment was volatile and the listeners had to re-engage in new speech whenever auditory scenes altered, the neural correlates of the attended speech were degraded. In particular, the cortical response to the attended speech and the spatial asymmetry of the response to the left and right attention were significantly attenuated around 100–200 ms after the speech onset. These findings suggest that volatile listening environments could adversely affect the modulation effect of selective attention, possibly by hampering proper attention due to increased perceptual load. © 2023}, keywords = {adult; article; attention; auditory selective attention; comprehension; dichotic listening; electroencephalography; female; human; human experiment; male; speech; hearing; physiology; speech perception; Attention; Auditory Perception; Electroencephalography; Hearing; Humans; Speech Perception},
correspondence_address = {Y. Lim; Center for Intelligent & Interactive Robotics, Artificial Intelligence and Robot Institute, Korea Institute of Science and Technology, Seoul, 02792, South Korea; email: yslim@kist.re.kr},
publisher = {Elsevier B.V.},
issn = {03785955; 18785891},
coden = {HERED},
pmid = {37104990},
language = {English},
abbrev_source_title = {Hear. Res.},
type = {Article}}
@article{Rangel2023Lingering,
author = {Rangel, Benjamin O. and Hazeltine, Eliot and Wessel, Jan R.},
title = {Lingering Neural Representations of Past Task Features Adversely Affect Future Behavior},
year = {2023},
journal = {Journal of Neuroscience},
volume = {43},
number = {2},
pages = {282 - 292},
doi = {10.1523/JNEUROSCI.0464-22.2022},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146365278&doi=10.1523%2FJNEUROSCI.0464-22.2022&partnerID=40&md5=14b5a8aaf4805ec1178142ace8df8061},
affiliations = {University of Iowa, Interdisciplinary Graduate Program in Neuroscience, Iowa City, IA, United States; University of Iowa, Cognitive Control Collaborative, Iowa City, IA, United States; College of Liberal Arts and Sciences, Iowa City, IA, United States; University of Iowa Hospitals & Clinics, Department of Neurology, Iowa City, IA, United States},
abstract = {During goal-directed behavior, humans purportedly form and retrieve so-called event files, conjunctive representations that link context-specific information about stimuli, their associated actions, and the expected action outcomes. The automatic formation, and later retrieval, of such conjunctive representations can substantially facilitate efficient action selection. However, recent behavioral work suggests that these event files may also adversely affect future behavior, especially when action requirements have changed between successive instances of the same task context (e.g., during task switching). Here, we directly tested this hypothesis with a recently developed method for measuring the strength of the neural representations of context-specific stimulus-action conjunctions (i.e., event files). Thirty-five male and female adult humans performed a task switching paradigm while undergoing EEG recordings. Replicating previous behavioral work, we found that changes in action requirements between two spaced repetitions of the same task incurred a significant reaction time cost. By combining multivariate pattern analysis and representational similarity analysis of the EEG recordings with linear mixed-effects modeling of trial-to-trial behavior, we then found that the magnitude of this behavioral cost was directly proportional to the strength of the conjunctive representation formed during the most recent previous exposure to the same task, that is, the most recent event file. This confirms that the formation of conjunctive representations of specific task contexts, stimuli, and actions in the brain can indeed adversely affect future behavior. Moreover, these findings demonstrate the potential of neural decoding of complex task set representations toward the prediction of behavior beyond the current trial. © 2023 the authors.}, keywords = {adult; article; electroencephalogram; electroencephalography; executive function; female; human; male; prediction; reaction time; brain; brain mapping; cognition; physiology; Adult; Brain; Brain Mapping; Cognition; Electroencephalography; Female; Humans; Male; Reaction Time},
correspondence_address = {B.O. Rangel; Interdisciplinary Graduate Program in Neuroscience, University of Iowa, Iowa City, 52245, United States; email: benjamin-rangel@uiowa.edu; J.R. Wessel; Cognitive Control Collaborative, University of Iowa, Iowa City, 52245, United States; email: jan-wessel@uiowa.edu},
publisher = {Society for Neuroscience},
issn = {02706474; 15292401},
coden = {JNRSD},
pmid = {36639905},
language = {English},
abbrev_source_title = {J. Neurosci.},
type = {Article}}
@article{Rodriguez2023Potential,
author = {Rodriguez, Fernando and He, Shenghong and Tan, Huiling},
title = {The potential of convolutional neural networks for identifying neural states based on electrophysiological signals: experiments on synthetic and real patient data},
year = {2023},
journal = {Frontiers in Human Neuroscience},
volume = {17},
pages = {},
doi = {10.3389/fnhum.2023.1134599},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162002318&doi=10.3389%2Ffnhum.2023.1134599&partnerID=40&md5=46ee649469d8a2b002224041a0cd2159},
affiliations = {University of Oxford Medical Sciences Division, Oxford, Oxfordshire, United Kingdom},
abstract = {Processing incoming neural oscillatory signals in real-time and decoding from them relevant behavioral or pathological states is often required for adaptive Deep Brain Stimulation (aDBS) and other brain-computer interface (BCI) applications. Most current approaches rely on first extracting a set of predefined features, such as the power in canonical frequency bands or various time-domain features, and then training machine learning systems that use those predefined features as inputs and infer what the underlying brain state is at each given time point. However, whether this algorithmic approach is best suited to extract all available information contained within the neural waveforms remains an open question. Here, we aim to explore different algorithmic approaches in terms of their potential to yield improvements in decoding performance based on neural activity such as measured through local field potentials (LFPs) recordings or electroencephalography (EEG). In particular, we aim to explore the potential of end-to-end convolutional neural networks, and compare this approach with other machine learning methods that are based on extracting predefined feature sets. To this end, we implement and train a number of machine learning models, based either on manually constructed features or, in the case of deep learning-based models, on features directly learnt from the data. We benchmark these models on the task of identifying neural states using simulated data, which incorporates waveform features previously linked to physiological and pathological functions. We then assess the performance of these models in decoding movements based on local field potentials recorded from the motor thalamus of patients with essential tremor. Our findings, derived from both simulated and real patient data, suggest that end-to-end deep learning-based methods may surpass feature-based approaches, particularly when the relevant patterns within the waveform data are either unknown, difficult to quantify, or when there may be, from the point of view of the predefined feature extraction pipeline, unidentified features that could contribute to decoding performance. The methodologies proposed in this study might hold potential for application in adaptive deep brain stimulation (aDBS) and other brain-computer interface systems. © © 2023 Rodriguez, He and Tan.}, keywords = {Article; brain depth stimulation; convolutional neural network; cross validation; decomposition; deep learning; electric potential; electrocorticography; electrode implantation; electroencephalogram; electrophysiological procedures; electrophysiology; Fourier transform; human; machine learning; Markov chain; patient coding; power spectrum; signal noise ratio; signal processing; spatial memory; two-dimensional imaging; upregulation; waveform},
correspondence_address = {H. Tan; MRC Brain Network Dynamics Unit, Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, United Kingdom; email: huiling.tan@ndcn.ox.ac.uk},
publisher = {Frontiers Media SA},
issn = {16625161},
language = {English},
abbrev_source_title = {Front. Human Neurosci.},
type = {Article}}
@article{Rubinstein2023Direct,
author = {Rubinstein, Daniel Y. and Weidemann, Christoph T. and Sperling, Michael R. and Kahana, Michael Jacob},
title = {Direct brain recordings suggest a causal subsequent-memory effect},
year = {2023},
journal = {Cerebral Cortex},
volume = {33},
number = {11},
pages = {6891 - 6901},
doi = {10.1093/cercor/bhad008},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160965358&doi=10.1093%2Fcercor%2Fbhad008&partnerID=40&md5=e3cf405e841513278d2207609508a71c},
affiliations = {Sidney Kimmel Medical College, Philadelphia, PA, United States; Swansea University, Department of Psychology, Swansea, West Glamorgan, Wales, United Kingdom; Columbia University, Department of Bioengineering, New York, NY, United States; University of Pennsylvania, Department of Psychology, Philadelphia, PA, United States},
abstract = {Endogenous variation in brain state and stimulus-specific evoked activity can both contribute to successful encoding. Previous studies, however, have not clearly distinguished among these components. We address this question by analysing intracranial EEG recorded from epilepsy patients as they studied and subsequently recalled lists of words. We first trained classifiers to predict recall of either single items or entire lists and found that both classifiers exhibited similar performance. We found that list-level classifier output—a biomarker of successful encoding—tracked item presentation and recall events, despite having no information about the trial structure. Across widespread brain regions, decreased low- and increased high-frequency activity (HFA) marked successful encoding of both items and lists. We found regional differences in the hippocampus and prefrontal cortex, where in the hippocampus HFA correlated more strongly with item recall, whereas, in the prefrontal cortex, HFA correlated more strongly with list performance. Despite subtle differences in item- and list-level features, the similarity in overall classification performance, spectral signatures of successful recall and fluctuations of spectral activity across the encoding period argue for a shared endogenous process that causally impacts the brain’s ability to learn new information. © The Author(s) 2023. Published by Oxford University Press. All rights reserved.}, keywords = {Article; brain depth stimulation; brain region; classifier; electrode implantation; electroencephalogram; episodic memory; hippocampus; human; inferior frontal gyrus; inferior parietal cortex; major clinical study; memory; middle frontal gyrus; occipital cortex; parahippocampal gyrus; prediction; prefrontal cortex; recall; refractory epilepsy; superior frontal gyrus; superior parietal lobule; temporal cortex; brain; brain mapping; electrocorticography; physiology; Brain; Brain Mapping; Electrocorticography; Hippocampus; Humans; Mental Recall; Prefrontal Cortex},
correspondence_address = {D.Y. Rubinstein; Department of Neurology, Thomas Jefferson University, Philadelphia, 19107, United States; email: daniel.rubinstein@jefferson.edu},
publisher = {Oxford University Press},
issn = {14602199; 10473211},
coden = {CECOE},
pmid = {36702495},
language = {English},
abbrev_source_title = {Cereb. Cortex},
type = {Article}}
@inproceedings{Sameh2023International,
author = {Sameh, Ahmed and Magdy, Helmy and Shady, Mario and Wael, Hady and ElBohy, Shereen Essam},
booktitle = {2023 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)},
title = {Brain Decoding using EEG Signals: Detection for Human Daily Activities},
year = {2023},
volume = {},
number = {},
pages = {1--6},
abstract = {Electroencephalography (EEG) is a useful method for acquiring brain signals related to various states from the scalp surface that can used to solve many challenges that can face Humans. This study provides Brain-Computer Interfaces that use a hierarchical method for feature extraction based on deep learning. The proposed EEG-based system employs non-invasive EEG sensors to continuously monitor brain activity, focusing on distinct neural patterns associated with each of the target activities. Machine learning algorithms are trained on a dataset of EEG signals corresponding to these activities to achieve accurate and robust activity detection. By analyzing the temporal patterns and spectral features of EEG signals, the system can reliably identify when a user is engaged in one of the four activities (Sleep, Food, Toilet, and Drink) and for more accurate check by face detection Module to detect patient's eye blinks.},
keywords = {Privacy;Sleep;Scalp;Feature extraction;Ubiquitous computing;Electroencephalography;Sensor systems;Electroencephalography;EEG;brain activity;BCI},
doi = {10.1109/MIUCC58832.2023.10278316},
issn = {},
month = {Sep.}}
@article{Shirzadi2023Accurate,
author = {Shirzadi, Mehdi and Marateb, Hamid Reza and McGill, Kevin C. and Muceli, Silvia and Mañanas, Miguel Angel and Farina, Dario},
title = {An Accurate and Real-Time Method for Resolving Superimposed Action Potentials in MultiUnit Recordings},
year = {2023},
journal = {IEEE Transactions on Biomedical Engineering},
volume = {70},
number = {1},
pages = {378 - 389},
doi = {10.1109/TBME.2022.3192119},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135233823&doi=10.1109%2FTBME.2022.3192119&partnerID=40&md5=da413d802eb79bc1dedca99ac0ed6707},
affiliations = {Universitat Politècnica de Catalunya, Department of Automatic Control, Barcelona, Barcelona, Spain; University of Isfahan, Department of Biomedical Engineering, Isfahan, Isfahan, Iran; VA Palo Alto Health Care System, Palo Alto, CA, United States; Chalmers University of Technology, Department of Electrical Engineering, Gothenburg, Vastra Gotaland, Sweden; Centro de Investigación Biomédica en Red de Bioingeniería, Biomateriales y Nanomedicina, Madrid, Madrid, Spain; Imperial College London, London, United Kingdom},
abstract = {Objective: Spike sorting of muscular and neural recordings requires separating action potentials that overlap in time (superimposed action potentials (APs)). We propose a new algorithm for resolving superimposed action potentials, and we test it on intramuscular EMG (iEMG) and intracortical recordings. Methods: Discrete-time shifts of the involved APs are first selected based on a heuristic extension of the peel-off algorithm. Then, the time shifts that provide the minimal residual Euclidean norm are identified (Discrete Brute force Correlation (DBC)). The optimal continuous-time shifts are then estimated (High-Resolution BC (HRBC)). In Fusion HRBC (FHRBC), two other cost functions are used. A parallel implementation of the DBC and HRBC algorithms was developed. The performance of the algorithms was assessed on 11,000 simulated iEMG and 14,000 neural recording superpositions, including two to eight APs, and eight experimental iEMG signals containing four to eleven active motor units. The performance of the proposed algorithms was compared with that of the Branch-and-Bound (BB) algorithm using the Rank-Product (RP) method in terms of accuracy and efficiency. Results: The average accuracy of the DBC, HRBC and FHRBC methods on the entire simulated datasets was 92.16±17.70, 93.65±16.89, and 94.90±15.15 (%). The DBC algorithm outperformed the other algorithms based on the RP method. The average accuracy and running time of the DBC algorithm on 10.5 ms superimposed spikes of the experimental signals were 92.1±21.7 (%) and 2.3±15.3 (ms). Conclusion and Significance: The proposed algorithm is promising for real-time neural decoding, a central problem in neural and muscular decoding and interfacing. © 2022 IEEE.}, keywords = {Biomedical signal processing; Continuous time systems; Cost functions; Decoding; Electroencephalography; Electrophysiology; Heuristic methods; Neurophysiology; Action-potentials; Biomedical signals processing; Correlation; Neural decoding; Overlapping spike; Recording; Resolving superimposition; Signal processing algorithms; Signal resolution; Spike-sorting; Electromyography; action potential; algorithm; article; controlled study; decomposition; electromyography; motor unit; needle electromyography; running; signal processing; simulation; spike; physiology; Action Potentials; Algorithms; Signal Processing, Computer-Assisted},
correspondence_address = {H.R. Marateb; Department of Automatic Control, Biomedical Engineering Research Center, Universitat Politècnica de Catalunya, BarcelonaTech (UPC), Barcelona, 08034, Spain; email: h.marateb@eng.ui.ac.ir},
publisher = {IEEE Computer Society},
issn = {00189294; 15582531},
coden = {IEBEA},
pmid = {35862323},
language = {English},
abbrev_source_title = {IEEE Trans. Biomed. Eng.},
type = {Article}}
@article{Tai2023Braincomputer,
author = {Tai, Pengrui and Ding, Peng and Wang, Fan and Gong, Anming Min and Li, Tianwen and Zhao, Lei and Su, Lei and Fu, Yunfa},
title = {Brain-computer interface paradigms and neural coding},
year = {2023},
journal = {Frontiers in Neuroscience},
volume = {17},
pages = {},
doi = {10.3389/fnins.2023.1345961},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183647678&doi=10.3389%2Ffnins.2023.1345961&partnerID=40&md5=81d89d309834dd88cf9437ab5740a0fc},
affiliations = {Kunming University of Science and Technology, Faculty of Information Engineering and Automation, Kunming, Yunnan, China; Kunming University of Science and Technology, Brain Cognition and Brain-Computer Intelligence Integration Group, Kunming, Yunnan, China; Chinese People's Armed Police Force Engineering University, School of Information Engineering, Xi'an, China; Kunming University of Science and Technology, Faculty of Science, Kunming, Yunnan, China},
abstract = {Brain signal patterns generated in the central nervous system of brain-computer interface (BCI) users are closely related to BCI paradigms and neural coding. In BCI systems, BCI paradigms and neural coding are critical elements for BCI research. However, so far there have been few references that clearly and systematically elaborated on the definition and design principles of the BCI paradigm as well as the definition and modeling principles of BCI neural coding. Therefore, these contents are expounded and the existing main BCI paradigms and neural coding are introduced in the review. Finally, the challenges and future research directions of BCI paradigm and neural coding were discussed, including user-centered design and evaluation for BCI paradigms and neural coding, revolutionizing the traditional BCI paradigms, breaking through the existing techniques for collecting brain signals and combining BCI technology with advanced AI technology to improve brain signal decoding performance. It is expected that the review will inspire innovative research and development of the BCI paradigm and neural coding. © © 2024 Tai, Ding, Wang, Gong, Li, Zhao, Su and Fu.}, keywords = {deoxyhemoglobin; oxyhemoglobin; action potential; brain disease; brain function; cortical synchronization; electrocorticography; electroencephalography; evoked response; eye movement; fatigue; functional connectivity; functional magnetic resonance imaging; functional near-infrared spectroscopy; handwriting; heart beat; human; limb movement; local field potential; magnetoencephalography; mental health; mental load; movement related cortical potential; nervous system function; neural coding; neuroimaging; neurological and sensorial procedures; Review; task performance; visual evoked potential},
correspondence_address = {P. Ding; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; email: ausarschorr@foxmail.com; Y. Fu; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; email: fyf@ynu.edu.cn},
publisher = {Frontiers Media SA},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Review}}
@article{Thlke2023Class,
author = {Thölke, Philipp and Mantilla-Ramos, Yorguin Jose and Abdelhedi, Hamza and Maschke, Charlotte and Dehgan, Arthur and Harel, Yann and Kemtur, Anirudha and Mekki Berrada, Loubna and Sahraoui, Myriam and Young, Tammy and Bellemare-Pepin, Antoine and El Khantour, Clara and Landry, Mathieu and Pascarella, Annalisa and Hadid, Vanessa and Combrisson, Etienne and O'Byrne, Jordan N. and Jerbi, Karim},
title = {Class imbalance should not throw you off balance: Choosing the right classifiers and performance metrics for brain decoding with imbalanced data},
year = {2023},
journal = {NeuroImage},
volume = {277},
pages = {},
doi = {10.1016/j.neuroimage.2023.120253},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164370859&doi=10.1016%2Fj.neuroimage.2023.120253&partnerID=40&md5=872ca9dbdcdaf19f72fe0987a7197413},
affiliations = {University of Montreal, Cognitive and Computational Neuroscience Laboratory (CoCo Lab), Montreal, QC, Canada; Osnabrück University, Institute of Cognitive Science, Osnabruck, Niedersachsen, Germany; Universidad de Antioquia, Faculty of Medicine, Medellin, Antioquia, Colombia; Université McGill, Montreal, QC, Canada; University of Alberta, Edmonton, AB, Canada; Concordia University, Department of Music, Montreal, QC, Canada; Consiglio Nazionale delle Ricerche, Rome, RM, Italy; Institut de Neurosciences de la Timone, Marseille, Provence-Alpes-Cote d'Azur, France; Montreal Institute for Learning Algorithms, Montreal, QC, Canada; UNIQUE Center (Quebec Neuro-AI Research Center), Montreal, QC, Canada},
abstract = {Machine learning (ML) is increasingly used in cognitive, computational and clinical neuroscience. The reliable and efficient application of ML requires a sound understanding of its subtleties and limitations. Training ML models on datasets with imbalanced classes is a particularly common problem, and it can have severe consequences if not adequately addressed. With the neuroscience ML user in mind, this paper provides a didactic assessment of the class imbalance problem and illustrates its impact through systematic manipulation of data imbalance ratios in (i) simulated data and (ii) brain data recorded with electroencephalography (EEG), magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI). Our results illustrate how the widely-used Accuracy (Acc) metric, which measures the overall proportion of successful predictions, yields misleadingly high performances, as class imbalance increases. Because Acc weights the per-class ratios of correct predictions proportionally to class size, it largely disregards the performance on the minority class. A binary classification model that learns to systematically vote for the majority class will yield an artificially high decoding accuracy that directly reflects the imbalance between the two classes, rather than any genuine generalizable ability to discriminate between them. We show that other evaluation metrics such as the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC), and the less common Balanced Accuracy (BAcc) metric - defined as the arithmetic mean between sensitivity and specificity, provide more reliable performance evaluations for imbalanced data. Our findings also highlight the robustness of Random Forest (RF), and the benefits of using stratified cross-validation and hyperprameter optimization to tackle data imbalance. Critically, for neuroscience ML applications that seek to minimize overall classification error, we recommend the routine use of BAcc, which in the specific case of balanced data is equivalent to using standard Acc, and readily extends to multi-class settings. Importantly, we present a list of recommendations for dealing with imbalanced data, as well as open-source code to allow the neuroscience community to replicate and extend our observations and explore alternative approaches to coping with imbalanced data. © 2023}, keywords = {area under the curve; arithmetic; Article; binary classification; brain; classification error; classifier; cross validation; electroencephalography; functional magnetic resonance imaging; machine learning; magnetoencephalography; measurement accuracy; neuroscience; performance indicator; process optimization; random forest; receiver operating characteristic; sensitivity and specificity; simulation; algorithm; benchmarking; human; Algorithms; Benchmarking; Brain; Electroencephalography; Humans; Machine Learning; Magnetoencephalography},
correspondence_address = {P. Thölke; Cognitive and Computational Neuroscience Laboratory (CoCo Lab), University of Montreal, Montreal, 2900, boul. Edouard-Montpetit, H3T 1J4, Canada; email: philipp.thoelke@posteo.de},
publisher = {Academic Press Inc.},
issn = {10959572; 10538119},
coden = {NEIME},
pmid = {37385392},
language = {English},
abbrev_source_title = {NeuroImage},
type = {Article}}
@article{Wang2023Mrcpsandersdoscillationsdriven,
author = {Wang, Jiarong and Bi, Luzheng and Feleke, A. Genetu and Fei, Weijie},
title = {MRCPs-and-ERS/D-Oscillations-Driven Deep Learning Models for Decoding Unimanual and Bimanual Movements},
year = {2023},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {31},
pages = {1384 - 1393},
doi = {10.1109/TNSRE.2023.3245617},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149182240&doi=10.1109%2FTNSRE.2023.3245617&partnerID=40&md5=aea402df8cf43abd63095ae921efa64c},
affiliations = {Beijing Institute of Technology, School of Mechanical Engineering, Beijing, China},
abstract = {Motor brain-computer interface (BCI) can intend to restore or compensate for central nervous system functionality. In the motor-BCI, motor execution (ME), which relies on patients' residual or intact movement functions, is a more intuitive and natural paradigm. Based on the ME paradigm, we can decode voluntary hand movement intentions from electroencephalography (EEG) signals. Numerous studies have investigated EEG-based unimanual movement decoding. Moreover, some studies have explored bimanual movement decoding since bimanual coordination is important in daily-life assistance and bilateral neurorehabilitation therapy. However, the multi-class classification of the unimanual and bimanual movements shows weak performance. To address this problem, in this work, we propose a neurophysiological signatures-driven deep learning model utilizing the movement-related cortical potentials (MRCPs) and event-related synchronization/ desynchronization (ERS/D) oscillations for the first time, inspired by the finding that brain signals encode motor-related information with both evoked potentials and oscillation components in ME. The proposed model consists of a feature representation module, an attention-based channel-weighting module, and a shallow convolutional neural network module. Results show that our proposed model has superior performance to the baseline methods. Six-class classification accuracies of unimanual and bimanual movements achieved 80.3%. Besides, each feature module of our model contributes to the performance. This work is the first to fuse the MRCPs and ERS/D oscillations of ME in deep learning to enhance the multi-class unimanual and bimanual movements' decoding performance. This work can facilitate the neural decoding of unimanual and bimanual movements for neurorehabilitation and assistance. © 2001-2011 IEEE.}, keywords = {Brain; Brain computer interface; Classification (of information); Decoding; Deep learning; Electrophysiology; Learning systems; Neural networks; Neurophysiology; Bimanual movement; Central nervous systems; Event-related synchronization/desynchronization; Learning models; Motor execution; Movement decoding; Movement Related Cortical Potentials; Neurorehabilitation []; Performance; System functionality; Electroencephalography; Article; artifact; artificial neural network; attention; Bimanual Movement; central nervous system; classification algorithm; continuous wavelet transform; convolutional neural network; cross validation; deep learning; electroencephalography; electrooculogram; evoked response; eye movement; human; human experiment; imagery; learning algorithm; machine learning; mathematical model; motor execution; movement related cortical potential; multiclass classification; neurophysiology; neurorehabilitation; oscillation; article; brain; controlled study},
correspondence_address = {W. Fei; Beijing Institute of Technology, School of Mechanical Engineering, Beijing, 100081, China; email: 3120170241@bit.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {37027527},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Wang2023Decoding,
author = {Wang, Pengpai and Gong, Peiliang and Zhou, Yueying and Wen, Xuyun and Zhang, Daoqiang},
title = {Decoding the Continuous Motion Imagery Trajectories of Upper Limb Skeleton Points for EEG-Based Brain-Computer Interface},
year = {2023},
journal = {IEEE Transactions on Instrumentation and Measurement},
volume = {72},
pages = {},
doi = {10.1109/TIM.2022.3224991},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144059238&doi=10.1109%2FTIM.2022.3224991&partnerID=40&md5=0000f5a757fe32217a659421ec2c0ef5},
affiliations = {Nanjing University of Aeronautics and Astronautics, MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, Jiangsu, China},
abstract = {In the field of brain-computer interface (BCI), brain decoding using electroencephalography (EEG) is an essential direction, and motion imagery EEG-based BCI can not only help rehabilitation of patients with physical disabilities, but also enhance the endurance and power of people. Most of the existing MI-based BCI studies are limited to discrete EEG classification or 3-D directional limb trajectory reconstruction. To suitable for the requirements of BCI systems in practical applications, here, we explored the decoding of trajectories of continuous nondirectional motion imagination in 3-D space based on Chinese sign language. We propose a motion imagery trajectory reconstruction Transformer (MITRT) model to decode the EEG signals of the subjects performing motion imagery, and obtain the positional changes in the 3-D space of the shoulder, elbow, and wrist skeleton points in the neural activity. We add the geometric constraint features of upper limb skeleton points to the model, and the MITRT decoding model can obtain prior knowledge to improve the reconstruction accuracy of spatial positions. To verify the decoding performance of our proposed model, we collected motor imagery (MI) EEG signals of 20 subjects based on Chinese sign language for experiments. The experimental results show that the average Pearson correlation coefficient of the six skeleton points was 0.975, which was significantly higher than the contrast models. This study is the first attempt to reconstruct multidirectional continuous nondirectional upper limb MI trajectories based on Chinese sign language. The experimental results show that it is feasible to decode and reconstruct imagined 3-D trajectories of human upper limb skeleton points from scalp EEG. © 1963-2012 IEEE.}, keywords = {Brain; Correlation methods; Decoding; Electroencephalography; Image enhancement; Image reconstruction; Medical computing; Neurons; Patient rehabilitation; Three dimensional computer graphics; Trajectories; Brain decoding; Chinese sign language; Continous motion; EEG signals; Limb motion decoding; Limb motions; Motion imagery; Motor imagery; Trajectory reconstruction; Upper limbs; Brain computer interface},
correspondence_address = {D. Zhang; Nanjing University of Aeronautics and Astronauctics, College of Computer Science and Technology, Miit Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, 211106, China; email: dqzhang@nuaa.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {00189456; 15579662},
coden = {IEIMA},
language = {English},
abbrev_source_title = {IEEE Trans. Instrum. Meas.},
type = {Article}}
@article{Watanabe2023Multimodal,
author = {Watanabe, Noriya and Miyoshi, Kosuke and Jimura, Koji and Shimane, Daisuke and Keerativittayayut, Ruedeerat and Nakahara, Kiyoshi and Takeda, Masaki},
title = {Multimodal deep neural decoding reveals highly resolved spatiotemporal profile of visual object representation in humans},
year = {2023},
journal = {NeuroImage},
volume = {275},
pages = {},
doi = {10.1016/j.neuroimage.2023.120164},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159223238&doi=10.1016%2Fj.neuroimage.2023.120164&partnerID=40&md5=ac2df86f45d76931eefee8e476a8ff2a},
affiliations = {Kochi University of Technology, Research Center for Brain Communication, Kami, Kochi, Japan; Gunma University, Department of Informatics, Maebashi, Gunma, Japan; Chulabhorn Royal Academy, Bangkok, Thailand},
abstract = {Perception and categorization of objects in a visual scene are essential to grasp the surrounding situation. Recently, neural decoding schemes, such as machine learning in functional magnetic resonance imaging (fMRI), has been employed to elucidate the underlying neural mechanisms. However, it remains unclear as to how spatially distributed brain regions temporally represent visual object categories and sub-categories. One promising strategy to address this issue is neural decoding with concurrently obtained neural response data of high spatial and temporal resolution. In this study, we explored the spatial and temporal organization of visual object representations using concurrent fMRI and electroencephalography (EEG), combined with neural decoding using deep neural networks (DNNs). We hypothesized that neural decoding by multimodal neural data with DNN would show high classification performance in visual object categorization (faces or non-face objects) and sub-categorization within faces and objects. Visualization of the fMRI DNN was more sensitive than that in the univariate approach and revealed that visual categorization occurred in brain-wide regions. Interestingly, the EEG DNN valued the earlier phase of neural responses for categorization and the later phase of neural responses for sub-categorization. Combination of the two DNNs improved the classification performance for both categorization and sub-categorization compared with fMRI DNN or EEG DNN alone. These deep learning-based results demonstrate a categorization principle in which visual objects are represented in a spatially organized and coarse-to-fine manner, and provide strong evidence of the ability of multimodal deep learning to uncover spatiotemporal neural machinery in sensory processing. © 2023}, keywords = {article; brain; controlled study; deep learning; deep neural network; electroencephalography; functional magnetic resonance imaging; human; machine learning; nerve potential; visual system; artificial neural network; brain mapping; nuclear magnetic resonance imaging; physiology; procedures; vision; visual pattern recognition; Brain; Brain Mapping; Electroencephalography; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Pattern Recognition, Visual; Visual Perception},
correspondence_address = {M. Takeda; Research Center for Brain Communication, Kochi University of Technology, Kami, Kochi, 782-8502, Japan; email: takeda.masaki@kochi-tech.ac.jp},
publisher = {Academic Press Inc.},
issn = {10959572; 10538119},
coden = {NEIME},
pmid = {37169115},
language = {English},
abbrev_source_title = {NeuroImage},
type = {Article}}
@conference{Yan2023Multidof,
author = {Yan, Weidong and Xu, Zhaoliang and Li, Yang},
title = {A Multi-DOF Robot System Based on LightGBM-Driven EEG Decoding Model for BCI Human-Machine Interaction},
year = {2023},
journal = {Chinese Control Conference, CCC},
volume = {2023-July},
pages = {8605 - 8610},
doi = {10.23919/CCC58697.2023.10240771},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175571962&doi=10.23919%2FCCC58697.2023.10240771&partnerID=40&md5=6bcd57c9d63ebd32b2f2a5fcd1e68229},
affiliations = {Beihang University, School of Automation Science and Electrical Engineering, Beijing, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beijing, Beijing, China; Beihang University, Beijing, China; Peking University, Beijing, China},
abstract = {In this study, a multi-degree-of-freedom (Multi-DOF) robot (MDR) system based on a LightGBM-driven electroencephalogram (EEG) decoding model is designed and developed to assist subjects with hand motor dysfunction in their daily activities and neurorehabilitation. The system mainly consists of a motor imagery electroencephalogram (MI-EEG) evoking layer, an intention decoding layer, and an interaction executive layer. The MI-EEG evoking layer initially displays a virtual reality (VR) motion imagination scenario, instructing the subjects to imagine a real hand gripping movement, simultaneously collecting the electrical EEG signals and preprocessing the EEG signals. Secondly, in the intention decoding layer, a network combining temporal-spectral feature fusion and LightGBM (TSFF-LightGBM) for MI-BCI classification is used to more effectively boost brain decoding accuracy and decrease decoding time. Finally, in the interaction executive layer, the Multi-DOF wearable robot is developed to offer hand grasp motion kinesthetic feedback and visual feedback synced with MI. The following are the key benefits of the proposed MDR system: (1) We propose a new lightweight network structure more suitable for brain computer interface (BCI) interaction systems, achieving more accurate decoding and shorter identification time in different data sets, which helps to improve the practicability of the system and promote the practical clinical application of BCI rehabilitation technology. (2) We integrate BCI, VR, a wearable Multi-DOF robot, motion kinesthetic feedback, and visual feedback to improve human-machine interaction. Compared to the most recent investigations, the average accuracy of the MDR system on the publicly accessible datasets BCI IV 2a and HGD reached 75.89% and 93.53%, respectively. © 2023 Technical Committee on Control Theory, Chinese Association of Automation.}, keywords = {Decoding; Degrees of freedom (mechanics); Electroencephalography; Human robot interaction; Man machine systems; Ultrasonic devices; Virtual reality; Visual communication; Visual servoing; Wearable technology; Degrees of freedom robots; Electroencephalogram signals; Human machine interaction; Kinaesthetic feedback; Lightgbm; Motor imagery; Multi degree-of-freedom; Multi-degree-of-freedom robot; Robots system; Visual feedback; Brain computer interface},
correspondence_address = {Y. Li; Beihang University, The Department of Automation Science and Electrical Engineering, Beijing, 100191, China; email: liyang@buaa.edu.cn},
publisher = {IEEE Computer Society},
issn = {21612927; 19341768},
isbn = {9789881563941; 9789887581581; 9789887581536; 9789881563804; 9789881563910; 9789881563842; 9789881563972; 9789881563811; 9789881563835; 9789887581543},
language = {English},
abbrev_source_title = {Chinese Control Conf., CCC},
type = {Conference paper}}
@conference{Yu2023Classification,
author = {Yu, Yue and Ji, Wenkai and Zhao, Liming and Sun, Zhongbo and Liu, Keping},
title = {Classification of Motor Imagery EEG Signals Based on Channel Attention Mechanism},
year = {2023},
pages = {1720 - 1725},
doi = {10.1109/DDCLS58216.2023.10167410},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166015610&doi=10.1109%2FDDCLS58216.2023.10167410&partnerID=40&md5=aa533ec0ec5976abe6b82ad07d7cc155},
affiliations = {Changchun University of Technology, Department of Control Engineering, Changchun, Jilin, China; Jilin Engineering Normal University, School of Information Engineering, Changchun, Jilin, China},
abstract = {Brain-computer interface (BCI) technology establishes communication between the brain and external devices by decoding EEG signals. BCI technology based on motor imagery (MI) has great application potential. There are many different methods to extract motor intention from electroencephalogram (EEG) based on motor imagery (MI).These methods rely on extracting the unique features of EEG in the process of imaginary movement, which directly affect the performance of neural decoding algorithm of BCI. Convolutional neural network (CNN) shows outstanding advantages in automatic extraction of image features. In this paper, an image representation method based on the EEG is proposed as the input of the network. Then, a CNN and a CNN based on Channel Attention Mechanism (CAM) are built as the classifier, convolution layers and activation functions of different sizes are validated. The performance of the method is evaluated. A CNN framework based on CAM, which contained three convolution layers (3-L) is better than the other state-of-the-art approaches. The accuracy on dataset IV from BCI competition II reaches 72.6%. © 2023 IEEE.}, keywords = {Convolution; Convolutional neural networks; Decoding; Electroencephalography; Image classification; Image representation; Attention mechanisms; Brain-computer interface; Channel attention mechanism; Convolutional neural network; Electroencephalogram signals; Interface technology; Motor imagery; Performance; Brain computer interface},
correspondence_address = {Z. Sun; Changchun University of Technology, Department of Control Engineering, Changchun, 130012, China; email: zhongbosun2012@163.com},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9798350321050},
language = {English},
abbrev_source_title = {Proc. IEEE Data Driven Control Learn. Syst. Conf., DDCLS},
type = {Conference paper}}
@article{Cui2022Spatialtemporal,
author = {Cui, Yujie and Xie, Songyun and Xie, Xinzhou and Duan, Xu and Gao, Chuanlin},
title = {A spatial-temporal hybrid feature extraction method for rapid serial visual presentation of electroencephalogram signals},
year = {2022},
journal = {Shengwu Yixue Gongchengxue Zazhi/Journal of Biomedical Engineering},
volume = {39},
number = {1},
pages = {39 - 46},
doi = {10.7507/1001-5515.202104049},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125581569&doi=10.7507%2F1001-5515.202104049&partnerID=40&md5=c799b3382384be9b857b6f1385268e2f},
affiliations = {Northwestern Polytechnical University, School of Electronics and Information, Xi'an, Shaanxi, China},
abstract = {快速序列视觉呈现脑机接口（RSVP-BCI）是目前基于人脑对目标进行早期发现任务中最为常用的技术，该技术能够获取人脑对环境的快速感知。脑电信号（EEG）具有非平稳和信噪比较低的特点，因此在单次试验中准确解码大脑活动较为困难。为解决RSVP-BCI技术中单次试验分类准确率不高的问题，本文提出一种基于时空域混合特征提取的新方法。该方法充分考虑大脑活动的时空模式，分别在时域和空域采用主成分分析（PCA）和共空间模式（CSP）对EEG信号进行特征提取，构成时空混合CSP-PCA（STHCP）方法，通过时域、空域两次特征提取最大化目标类与非目标类之间的判别距离，有效地降低特征维数。STHCP的单试次解码曲线下面积（AUC）较三种基准算法［空间加权费希尔（Fisher）线性判决-PCA（SWFP）、CSP及PCA算法］分别提高了17.9%、22.2%及29.2%，为利用RSVP-BCI技术进行快速高效的目标检测提供了新方法。.; Rapid serial visual presentation-brain computer interface (RSVP-BCI) is the most popular technology in the early discover task based on human brain. This algorithm can obtain the rapid perception of the environment by human brain. Decoding brain state based on single-trial of multichannel electroencephalogram (EEG) recording remains a challenge due to the low signal-to-noise ratio (SNR) and nonstationary. To solve the problem of low classification accuracy of single-trial in RSVP-BCI, this paper presents a new feature extraction algorithm which uses principal component analysis (PCA) and common spatial pattern (CSP) algorithm separately in spatial domain and time domain, creating a spatial-temporal hybrid CSP-PCA (STHCP) algorithm. By maximizing the discrimination distance between target and non-target, the feature dimensionality was reduced effectively. The area under the curve (AUC) of STHCP algorithm is higher than that of the three benchmark algorithms (SWFP, CSP and PCA) by 17.9%, 22.2% and 29.2%, respectively. STHCP algorithm provides a new method for target detection.}, keywords = {algorithm; brain; brain computer interface; electroencephalography; human; principal component analysis; procedures; signal processing; Algorithms; Brain; Brain-Computer Interfaces; Electroencephalography; Humans; Principal Component Analysis; Signal Processing, Computer-Assisted},
publisher = {NLM (Medline)},
issn = {10015515},
pmid = {35231964},
language = {Chinese},
abbrev_source_title = {Sheng Wu Yi Xue Gong Cheng Xue Za Zhi},
type = {Article}}
@article{Einizade2022Neural,
author = {Einizade, Aref and Mozafari, Mohsen and Jalilpour, Shayan and Bagheri, Sara and Hajipour, Sepideh},
title = {Neural decoding of imagined speech from EEG signals using the fusion of graph signal processing and graph learning techniques},
year = {2022},
journal = {Neuroscience Informatics},
volume = {2},
number = {3},
pages = {},
doi = {10.1016/j.neuri.2022.100091},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166466928&doi=10.1016%2Fj.neuri.2022.100091&partnerID=40&md5=8d02e506895e2c75dd444c408cbac9c2},
affiliations = {Sharif University of Technology, Department of Electrical Engineering, Tehran, Tehran, Iran},
abstract = {Imagined Speech (IS) is the imagination of speech without using the tongue or muscles. In recent studies, IS tasks are increasingly investigated for the Brain-Computer Interface (BCI) applications. Electroencephalography (EEG) signals, which record brain activity, can be used to analyze BCI-based tasks utilizing Machine Learning (ML) methods. The current paper considers decoding IS brain waves using the fusion of classical signal processing, Graph Signal Processing (GSP), and Graph Learning (GL) based features. The proposed fusion method, named GraphIS (short for a Graph-based Imagined Speech BCI decoder), is applied to the four-class classification (three classes of the imagined words, in addition to the rest state) on EEG recordings of fifteen subjects. Results show that GSP and GL-based features can highly improve the performance of classification outcomes compared to using only classical signal processing features and over the state-of-the-art Common Spatial Pattern (CSP) feature extractor by considering the spatial information of the signals as well as interactions between channels in regions of interest. The proposed GraphIS method leads to a mean accuracy of 50.10% in the studied four-class IS classification task, compared to using only one feature set with an accuracy of 47.86% and 46.10%, and also the state-of-the-art CSP with an accuracy of 47.10%. Additionally, using an EEG connectivity map of the electrode signals obtained from GL methods, we also found a strong connection in the right frontal region as well as in the left frontal regions during IS, which had not been focused on in the previous IS papers. © 2022 The Author(s)}, keywords = {accuracy; aphasia; Article; electroencephalogram; electroencephalography; functional connectivity; glioma; hemispherectomy; learning algorithm; machine learning; mathematical analysis; measurement accuracy; neural decoding; signal processing; spatial analysis; training; visual stimulation; wavelet transform},
correspondence_address = {A. Einizade; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran; email: einizade.aref@ee.sharif.edu},
publisher = {Elsevier Masson s.r.l.},
issn = {27725286},
language = {English},
abbrev_source_title = {Neurosci. Inform.},
type = {Article}}
@article{Girdler2022Neural,
author = {Girdler, Benton and Caldbeck, William and Bae, Jihye},
title = {Neural Decoders Using Reinforcement Learning in Brain Machine Interfaces: A Technical Review},
year = {2022},
journal = {Frontiers in Systems Neuroscience},
volume = {16},
pages = {},
doi = {10.3389/fnsys.2022.836778},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138067937&doi=10.3389%2Ffnsys.2022.836778&partnerID=40&md5=a00ed9fd5d410a9219e590bcb9880897},
affiliations = {Stanley and Karen Pigman College of Engineering, Lexington, KY, United States},
abstract = {Creating flexible and robust brain machine interfaces (BMIs) is currently a popular topic of research that has been explored for decades in medicine, engineering, commercial, and machine-learning communities. In particular, the use of techniques using reinforcement learning (RL) has demonstrated impressive results but is under-represented in the BMI community. To shine more light on this promising relationship, this article aims to provide an exhaustive review of RL’s applications to BMIs. Our primary focus in this review is to provide a technical summary of various algorithms used in RL-based BMIs to decode neural intention, without emphasizing preprocessing techniques on the neural signals and reward modeling for RL. We first organize the literature based on the type of RL methods used for neural decoding, and then each algorithm’s learning strategy is explained along with its application in BMIs. A comparative analysis highlighting the similarities and uniqueness among neural decoders is provided. Finally, we end this review with a discussion about the current stage of RLBMIs including their limitations and promising directions for future research. © © 2022 Girdler, Caldbeck and Bae.}, keywords = {algorithm; artificial neural network; chemical reaction kinetics; correlation function; discriminant analysis; electroencephalography; electrostimulation; functional electrical stimulation; hospitalization; learning algorithm; machine learning; mathematical model; nerve cell network; nerve cell plasticity; performance; reinforcement learning (machine learning); review; Review; reward; signal noise ratio; support vector machine; systematic review},
correspondence_address = {J. Bae; Department of Electrical and Computer Engineering, University of Kentucky, Lexington, United States; email: jihye.bae@uky.edu},
publisher = {Frontiers Media S.A.},
issn = {16625137},
language = {English},
abbrev_source_title = {Front. Syst. Neurosci.},
type = {Review}}
@article{Haruvi2022Measuring,
author = {Haruvi, Aia and Kopito, Ronen and Brande-Eilat, Noa and Kalev, Shai and Kay, Eitan and Furman, Daniel},
title = {Measuring and Modeling the Effect of Audio on Human Focus in Everyday Environments Using Brain-Computer Interface Technology},
year = {2022},
journal = {Frontiers in Computational Neuroscience},
volume = {15},
pages = {},
doi = {10.3389/fncom.2021.760561},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124515090&doi=10.3389%2Ffncom.2021.760561&partnerID=40&md5=e6e7ccabaae4cb51b7893e12066b98ee},
affiliations = {Arctop Inc, San Francisco, CA, United States},
abstract = {The goal of this study was to investigate the effect of audio listened to through headphones on subjectively reported human focus levels, and to identify through objective measures the properties that contribute most to increasing and decreasing focus in people within their regular, everyday environment. Participants (N = 62, 18–65 years) performed various tasks on a tablet computer while listening to either no audio (silence), popular audio playlists designed to increase focus (pre-recorded music arranged in a particular sequence of songs), or engineered soundscapes that were personalized to individual listeners (digital audio composed in real-time based on input parameters such as heart rate, time of day, location, etc.). Audio stimuli were delivered to participants through headphones while their brain signals were simultaneously recorded by a portable electroencephalography headband. Participants completed four 1-h long sessions at home during which different audio played continuously in the background. Using brain-computer interface technology for brain decoding and based on an individual’s self-report of their focus, we obtained individual focus levels over time and used this data to analyze the effects of various properties of the sounds contained in the audio content. We found that while participants were working, personalized soundscapes increased their focus significantly above silence (p = 0.008), while music playlists did not have a significant effect. For the young adult demographic (18–36 years), all audio tested was significantly better than silence at producing focus (p = 0.001–0.009). Personalized soundscapes increased focus the most relative to silence, but playlists of pre-recorded songs also increased focus significantly during specific time intervals. Ultimately we found it is possible to accurately predict human focus levels a priori based on physical properties of audio content. We then applied this finding to compare between music genres and revealed that classical music, engineered soundscapes, and natural sounds were the best genres for increasing focus, while pop and hip-hop were the worst. These insights can enable human and artificial intelligence composers to produce increases or decreases in listener focus with high temporal (millisecond) precision. Future research will include real-time adaptation of audio for other functional objectives beyond affecting focus, such as affecting listener enjoyment, drowsiness, stress and memory. © © 2022 Haruvi, Kopito, Brande-Eilat, Kalev, Kay and Furman.}, keywords = {Artificial intelligence; Audio acoustics; Brain computer interface; Electrophysiology; Eye tracking; Headphones; Loudspeakers; Audio; Audio content; Digital audio; Human; Interface technology; Objective measure; Property; Real- time; Soundscapes; Tablet computer; Electroencephalography; accuracy; adult; aged; Article; artificial intelligence; attention; auditory stimulation; controlled study; electroencephalography; feature extraction; female; heart rate; human; human experiment; learning environment; machine learning; male; mental health; music; normal human; self report; sound; task performance; time series analysis; work environment},
correspondence_address = {D. Furman; Arctop Inc, San Francisco, United States; email: df@arctop.com},
publisher = {Frontiers Media S.A.},
issn = {16625188},
language = {English},
abbrev_source_title = {Front. Comput. Neurosci.},
type = {Article}}
@inproceedings{Hazarika2022Ieeeembs,
author = {Hazarika, Doli and Chanda, Souptick and Gupta, Cota Navin},
booktitle = {2022 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)},
title = {Smartphone-Based Natural Environment Electroencephalogram Experimentation-Opportunities and Challenges},
year = {2022},
volume = {},
number = {},
pages = {370--375},
abstract = {Using a smartphone for Electroencephalogram (EEG) based research in the natural environment is a growing field of study. It brings attention to device portability, participant mobility, and system specifications. This article discusses the most recent developments in the field of EEG investigations using smartphones in natural environments, for healthy and clinical applications. We integrate the current trends in smartphone-based EEG studies, namely experimental paradigms, electrode/hardware compatibility, preprocessing frameworks, classifiers, and software apps. However, smartphone devices have inherent limitations like computational time and algorithm performance. Implementing artifact reduction and classification algorithms together in an android smartphone app is still speculative, and possible solutions are proposed. This review presents a holistic insight into our current understanding and challenges of the smartphone’s role in natural environment electroencephalogram trials. Clinical Relevance-These portable smartphone-based EEG systems will be useful in monitoring individuals with psychiatric diseases, in addition to human brain applications in a natural setting. With ubiquitous availability of internet on smartphones, telemedicine is another possible application.},
keywords = {Performance evaluation;Telemedicine;Software algorithms;Market research;Electroencephalography;Software;Natural language processing},
doi = {10.1109/IECBES54088.2022.10079412},
issn = {},
month = {Dec}}
@article{Jain2022Premovnet,
author = {Jain, Anant and Kumar, Lalan},
journal = {IEEE Sensors Letters},
title = {PreMovNet: Premovement EEG-Based Hand Kinematics Estimation for Grasp-and-Lift Task},
year = {2022},
volume = {6},
number = {7},
pages = {1--4},
abstract = {Kinematics decoding from brain activity helps in developing rehabilitation or power-augmenting brain–computer interface (BCI) devices. Low-frequency signals recorded from noninvasive electroencephalography (EEG) are associated with the neural motor correlation utilized for motor trajectory decoding (MTD). In this communication, the ability to decode motor kinematics trajectory from premovement delta-band (0.5–3 Hz) EEG is investigated for the healthy participants. In particular, two deep learning-based neural decoders called PreMovNet-I and PreMovNet-II are proposed that make use of motor-related neural information existing in the premovement EEG data. EEG data segments with various time lags of 150, 200, 250, 300, and 350 ms before the movement onset are utilized for the same. The MTD is presented for grasp-and-lift task (WAY-EEG-GAL dataset) using EEG with the various lags taken as input to the neural decoders. The performance of the proposed decoders is compared with the state-of-the-art multivariable linear regression model. Pearson correlation coefficient and hand trajectory are utilized as performance metric. The results demonstrate the viability of decoding 3-D hand kinematics using premovement EEG data, enabling better control of BCI-based external devices such as exoskeleton/exosuit.},
keywords = {Electroencephalography;Decoding;Kinematics;Brain modeling;Trajectory;Three-dimensional displays;Finite impulse response filters;Sensor signal processing;brain–computer interface (BCI);deep learning;electroencephalography (EEG);multivariable linear regression (mLR);premovement},
doi = {10.1109/LSENS.2022.3183284},
issn = {2475-1472},
month = {July}}
@article{Jeong2022International,
author = {Jeong, Ji-hoon and Cho, Jeong-hyun and Lee, Young-eun and Lee, Seohyun and Shin, Gi-hwan and Kweon, Young-seok and Millán, José del R. and Müller, Klaus Robert and Lee, Seongwhan},
title = {2020 International brain–computer interface competition: A review},
year = {2022},
journal = {Frontiers in Human Neuroscience},
volume = {16},
pages = {},
doi = {10.3389/fnhum.2022.898300},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135449323&doi=10.3389%2Ffnhum.2022.898300&partnerID=40&md5=19595a3454da4046e76ce57cf883b24c},
affiliations = {Chungbuk National University, School of Computer Science, Cheongju, Chungcheongbuk-do, South Korea; Korea University, Department of Brain and Cognitive Engineering, Seoul, South Korea; Cockrell School of Engineering, Austin, TX, United States; Technische Universität Berlin, Department of Computer Science, Berlin, Germany; Max Planck Institute for Informatics, Saarbrucken, Saarland, Germany; Korea University, Department of Artificial Intelligence, Seoul, South Korea},
abstract = {The brain-computer interface (BCI) has been investigated as a form of communication tool between the brain and external devices. BCIs have been extended beyond communication and control over the years. The 2020 international BCI competition aimed to provide high-quality neuroscientific data for open access that could be used to evaluate the current degree of technical advances in BCI. Although there are a variety of remaining challenges for future BCI advances, we discuss some of more recent application directions: (i) few-shot EEG learning, (ii) micro-sleep detection (iii) imagined speech decoding, (iv) cross-session classification, and (v) EEG(+ear-EEG) detection in an ambulatory environment. Not only did scientists from the BCI field compete, but scholars with a broad variety of backgrounds and nationalities participated in the competition to address these challenges. Each dataset was prepared and separated into three data that were released to the competitors in the form of training and validation sets followed by a test set. Remarkable BCI advances were identified through the 2020 competition and indicated some trends of interest to BCI researchers. © © 2022 Jeong, Cho, Lee, Lee, Shin, Kweon, Millán, Müller and Lee.}, keywords = {artificial neural network; blood glucose monitoring; clinical outcome; competition; drowsiness; electroencephalography; human; imagery; machine learning; measurement accuracy; motor evoked potential; non invasive procedure; Review; signal noise ratio; spatial learning; stimulus; task performance; validation process; visual feedback},
correspondence_address = {S.-W. Lee; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; email: sw.lee@korea.ac.kr},
publisher = {Frontiers Media S.A.},
issn = {16625161},
language = {English},
abbrev_source_title = {Front. Human Neurosci.},
type = {Review}}
@conference{Jin2022Channel,
author = {Jin, Yingxin and Shang, Shaohua and Tang, Liwei and He, Lianghua and Zhou, Mengchu Chu},
title = {EEG channel selection algorithm based on Reinforcement Learning},
year = {2022},
pages = {},
doi = {10.1109/ICNSC55942.2022.10004161},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146919112&doi=10.1109%2FICNSC55942.2022.10004161&partnerID=40&md5=18f3d75824d6310b23f3d601cd0205e4},
affiliations = {Tongji University, Electronics and Information Engineering, Shanghai, China; Newark College of Engineering, Newark, NJ, United States},
abstract = {Multichannel EEG is generally used to collect brain activities from various locations across the brain. However, BCIs using lesser channels will be more convenient for subjects. What's more, information acquired from adjacent channels is usually inter-correlated or irrelevant to the task. And some channels are noisy. This paper proposes a novel channel selection algorithm based on reinforcement learning. It can adaptively transform the full-channel EEG data to the optimal-channel-number EEG format conditioned on different input trials to make a trade-off between brain decoding accuracy and efficiency. Experimen-tal results showed that the proposed model can improve the classification accuracy by 2% 6% compared to channel set C3,C4,Cz. © 2022 IEEE.}, keywords = {Brain; Brain computer interface; Economic and social effects; Electroencephalography; Learning algorithms; Adjacent channels; Brain activity; Channel number; Channel selection; EEG channel selections; Multichannel EEG; Optimal channels; Reinforcement learnings; Selection algorithm; Trade off; Reinforcement learning},
correspondence_address = {L. He; Tongji University, Electronic and Information Engineering, Shanghai, China; email: helianghua@tongji.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9781665472432},
language = {English},
abbrev_source_title = {ICNSC - Proc. IEEE Int. Conf. Netw., Sens. Control: Auton. Intell. Syst.},
type = {Conference paper}}
@conference{Ke2022Decoding,
author = {Ke, Xi and Bi, Luzheng and Fei, Weijie and Feleke, A. Genetu},
title = {A Decoding Model of Upper Limb Movement Intention Based on Data Augmentation},
year = {2022},
volume = {2022-January},
pages = {4257 - 4260},
doi = {10.1109/CAC57257.2022.10055468},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151122483&doi=10.1109%2FCAC57257.2022.10055468&partnerID=40&md5=9052e94222b0f43e8574e677ec4b0bb5},
affiliations = {Beijing Institute of Technology, School of Mechanical Engineering, Beijing, China},
abstract = {With the aging of the population, the application of brain-computer interfaces(BCIs) in the neural decoding of upper limb motion direction is becoming more extensive. However, how to improve the recognition accuracy of neural decoding is one of the key problems given limited training samples. In this paper, we proposed a neural decoding method of upper limb motion direction based on data augmentation. We used the deep convolutional generative adversarial networks(DCGANs), which is a data augmentation algorithm to generate more data to expand the training set to improve the accuracy of the model. We completed analysis on different numbers of real training data across eight subjects. The analysis results show that after using data augmentation, the average decoding accuracy given small amounts of training samples significantly increases, showing that the DCGANs algorithm can indeed improve the accuracy of the neural decoding model, and help to improve the practical application of BCIs in decoding upper limb motion. © 2022 IEEE.}, keywords = {Brain computer interface; Decoding; Generative adversarial networks; Sampling; Data augmentation; Decoding methods; Limb movements; Motion direction; Movement intentions; Neural decoding; Recognition accuracy; Training sample; Upper limb motion; Upper limbs; Electroencephalography},
correspondence_address = {A.G. Feleke; Beijing Institute of Technology, School of Mechanical Engineering, Beijing, China; email: abrucag@gmail.com},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9781665465335},
language = {English},
abbrev_source_title = {Proc. - Chin. Autom. Congr., CAC},
type = {Conference paper}}
@article{Khaleghi2022Visual,
author = {Khaleghi, Nastaran and Yousefi Rezaii, Tohid and Beheshti, Soosan and Meshgini, Saeed and Sheykhivand, Sobhan and Daneshvar, Sabalan},
title = {Visual Saliency and Image Reconstruction from EEG Signals via an Effective Geometric Deep Network-Based Generative Adversarial Network},
year = {2022},
journal = {Electronics (Switzerland)},
volume = {11},
number = {21},
pages = {},
doi = {10.3390/electronics11213637},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141696795&doi=10.3390%2Felectronics11213637&partnerID=40&md5=62ebda7f159fabddc49ee42a27519fd7},
affiliations = {University of Tabriz, Department of Biomedical Engineering, Tabriz, East Azerbaijan, Iran; Toronto Metropolitan University, Department of Electrical, Toronto, ON, Canada; Brunel University London, Design and Physical Sciences, Uxbridge, Middlesex, United Kingdom},
abstract = {Reaching out the function of the brain in perceiving input data from the outside world is one of the great targets of neuroscience. Neural decoding helps us to model the connection between brain activities and the visual stimulation. The reconstruction of images from brain activity can be achieved through this modelling. Recent studies have shown that brain activity is impressed by visual saliency, the important parts of an image stimuli. In this paper, a deep model is proposed to reconstruct the image stimuli from electroencephalogram (EEG) recordings via visual saliency. To this end, the proposed geometric deep network-based generative adversarial network (GDN-GAN) is trained to map the EEG signals to the visual saliency maps corresponding to each image. The first part of the proposed GDN-GAN consists of Chebyshev graph convolutional layers. The input of the GDN part of the proposed network is the functional connectivity-based graph representation of the EEG channels. The output of the GDN is imposed to the GAN part of the proposed network to reconstruct the image saliency. The proposed GDN-GAN is trained using the Google Colaboratory Pro platform. The saliency metrics validate the viability and efficiency of the proposed saliency reconstruction network. The weights of the trained network are used as initial weights to reconstruct the grayscale image stimuli. The proposed network realizes the image reconstruction from EEG signals. © 2022 by the authors.},
keywords = {electroencephalogram; generative adversarial network; geometric deep network; image reconstruction; visual saliency},
correspondence_address = {T.Y. Rezaii; Biomedical Engineering Department, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, 51666-16471, Iran; email: yousefi@tabrizu.ac.ir; S. Sheykhivand; Biomedical Engineering Department, Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, 51666-16471, Iran; email: s.sheykhivand@tabrizu.ac.ir},
publisher = {MDPI},
issn = {20799292},
language = {English},
abbrev_source_title = {Electronics (Switzerland)},
type = {Article}}
@article{KhaliqFard2022Neural,
author = {Khaliq Fard, Mahdie and Fallah, Ali and Maleki, Ali},
title = {Neural decoding of continuous upper limb movements: a meta-analysis},
year = {2022},
journal = {Disability and Rehabilitation: Assistive Technology},
volume = {17},
number = {7},
pages = {731 - 737},
doi = {10.1080/17483107.2020.1842919},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096102960&doi=10.1080%2F17483107.2020.1842919&partnerID=40&md5=32e0d5b8e20c1a78abcbe9776e0e8417},
affiliations = {Amirkabir University of Technology, Department of Biomedical Engineering, Tehran, Tehran, Iran; Semnan University, Semnan, Iran},
abstract = {Objective: EEG-based motion trajectory decoding makes a promising approach for neurotechnology which can be used for neural control of motion reconstruction and neurorehabilitation tools. However, the feasibility and validity of continuous motion decoding by non-invasive brain activity are not clear. The main aim of this study was to perform a meta-analysis across studies that examined the ability of EEG-based continuous motion decoding of upper limb movements. Approach: Pearson’s correlation coefficient (CC) was used to evaluate the model performance of the studies and considered as an effect size. To estimate the overall effect size of neural decoding of motion trajectory across studies, characteristics of included studies were addressed and the random effect model was applied to the heterogeneous studies which estimated overall effect size distribution. Furthermore, the significant difference between the two subgroups of imagined and executed movements was analysed. Main results: The mean of the overall effect size was computed 0.46 across the nonhomogeneous studies. The results showed no significant difference between imagined and executed movements (Chi2=0.28, df = 1, p = 0.60). Significance: Meta-analysis results confirm that imagination like execution movements can be used for neural decoding of motion trajectory in neural motor control systems. Also, nonlinear compare with linear model statistically confirmed to be more beneficial for complex movements. Furthermore, a new approach of synergy-based motion decoding can be significantly effective to increase model performance and more research needs to evaluate this method for different levels of complexity of movements.IMPLICATIONS FOR REHABILITATION Neural decoding methods base on EEG as a non-invasive brain activity, are more user friendly for neurorehabilitation than invasive methods that developing of it makes it more applicable for reconstructing activities of daily living. Neurotechnology for neural control of motion reconstruction, makes the rehabilitation tools to be more synchrony with human intentional movement that can be used to improve the brain neuroplastisity in stroke or other paralysed people. The feasibility and validity of imagined movements equal with executed movements show that amputee people also can benefit EEG-based motion decoding for controling rehabilitation tools just by imagination of their intentional movements. For neurorehabilitation tools, comparing the study outcomes illucidate that the approach of synergy-based motor control in brain activities concluded significantly high performance that highlighted the need it to more investigated in future research. © 2020 Informa UK Limited, trading as Taylor & Francis Group.}, keywords = {brain computer interface; daily life activity; electroencephalography; human; imagination; meta analysis; movement (physiology); upper limb; Activities of Daily Living; Brain-Computer Interfaces; Electroencephalography; Humans; Imagination; Movement; Upper Extremity},
correspondence_address = {A. Fallah; Department of Biomedical Engineering, Amirkabir University of Technology, Tehran, Iran; email: afallah@aut.ac.ir},
publisher = {Taylor and Francis Ltd.},
issn = {17483107; 17483115},
pmid = {33186068},
language = {English},
abbrev_source_title = {Disabil. Rehabil. Assistive Technol.},
type = {Review}}
@article{Kwak2022Fganet,
author = {Kwak, Youngchul and Song, Woo-Jin and Kim, Seong-Eun},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
title = {FGANet: fNIRS-Guided Attention Network for Hybrid EEG-fNIRS Brain-Computer Interfaces},
year = {2022},
volume = {30},
number = {},
pages = {329--339},
abstract = {Non-invasive brain-computer interfaces (BCIs) have been widely used for neural decoding, linking neural signals to control devices. Hybrid BCI systems using electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) have received significant attention for overcoming the limitations of EEG- and fNIRS-standalone BCI systems. However, most hybrid EEG-fNIRS BCI studies have focused on late fusion because of discrepancies in their temporal resolutions and recording locations. Despite the enhanced performance of hybrid BCIs, late fusion methods have difficulty in extracting correlated features in both EEG and fNIRS signals. Therefore, in this study, we proposed a deep learning-based early fusion structure, which combines two signals before the fully-connected layer, called the fNIRS-guided attention network (FGANet). First, 1D EEG and fNIRS signals were converted into 3D EEG and fNIRS tensors to spatially align EEG and fNIRS signals at the same time point. The proposed fNIRS-guided attention layer extracted a joint representation of EEG and fNIRS tensors based on neurovascular coupling, in which the spatially important regions were identified from fNIRS signals, and detailed neural patterns were extracted from EEG signals. Finally, the final prediction was obtained by weighting the sum of the prediction scores of the EEG and fNIRS-guided attention features to alleviate performance degradation owing to delayed fNIRS response. In the experimental results, the FGANet significantly outperformed the EEG-standalone network. Furthermore, the FGANet has 4.0% and 2.7% higher accuracy than the state-of-the-art algorithms in mental arithmetic and motor imagery tasks, respectively.},
keywords = {Electroencephalography;Functional near-infrared spectroscopy;Feature extraction;Three-dimensional displays;Tensors;Task analysis;Electrodes;Brain-computer interface (BCI);deep learning;electroencephalography (EEG);functional near-infrared spectroscopy (fNIRS);hybrid BCI;fNIRS-guided attention networks},
doi = {10.1109/TNSRE.2022.3149899},
issn = {1558-0210},
month = {}}
@article{Li2022Biologically,
author = {Li, Peiwen and Cai, Siqi and Su, Enze and Xie, Longhan},
journal = {IEEE Signal Processing Letters},
title = {A Biologically Inspired Attention Network for EEG-Based Auditory Attention Detection},
year = {2022},
volume = {29},
number = {},
pages = {284--288},
abstract = {Decoding auditory attention in a cocktail party from neural activities is crucial in the brain-computer interfaces (BCIs). Given that the speech-electroencephalography (EEG) relationships are informative about attentional focus, we propose a novel framework called the biologically inspired attention network (BIAnet) to capture the interactions between EEG and speech. With the neural attention mechanism, the BIAnet can model how each EEG frequency band is related to the subband envelopes of speech by dynamically assigning weights to individual frequency bands at run-time. Results show that the proposed BIAnet outperforms state-of-the-art AAD methods on two publicly available datasets. We also analyze how the BIAnet works and the frequency-specific interactions between EEG and speech signals through data visualization. Overall, the proposed BIAnet provides an accurate, low-latency, and interpretable AAD approach, which has the potential to be extended to general problems in BCIs.},
keywords = {Electroencephalography;Decoding;Brain modeling;Frequency modulation;Speech processing;Training;Three-dimensional displays;Auditory attention;brain-computer interfaces;cross-modal attention;electroencephalography},
doi = {10.1109/LSP.2021.3134563},
issn = {1558-2361},
month = {}}
@article{Loriette2022Beyond,
author = {Loriette, Celia and Amengual, Julià L. and Ben-Hamed, Suliann B.},
title = {Beyond the brain-computer interface: Decoding brain activity as a tool to understand neuronal mechanisms subtending cognition and behavior},
year = {2022},
journal = {Frontiers in Neuroscience},
volume = {16},
pages = {},
doi = {10.3389/fnins.2022.811736},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138522778&doi=10.3389%2Ffnins.2022.811736&partnerID=40&md5=7747ba72bc98c456a3835f925f4096b1},
affiliations = {Institut des Sciences Cognitives Marc Jeannerod, Institut des Sciences Cognitives, Bron, Auvergne-Rhone-Alpes, France},
abstract = {One of the major challenges in system neurosciences consists in developing techniques for estimating the cognitive information content in brain activity. This has an enormous potential in different domains spanning from clinical applications, cognitive enhancement to a better understanding of the neural bases of cognition. In this context, the inclusion of machine learning techniques to decode different aspects of human cognition and behavior and its use to develop brain–computer interfaces for applications in neuroprosthetics has supported a genuine revolution in the field. However, while these approaches have been shown quite successful for the study of the motor and sensory functions, success is still far from being reached when it comes to covert cognitive functions such as attention, motivation and decision making. While improvement in this field of BCIs is growing fast, a new research focus has emerged from the development of strategies for decoding neural activity. In this review, we aim at exploring how the advanced in decoding of brain activity is becoming a major neuroscience tool moving forward our understanding of brain functions, providing a robust theoretical framework to test predictions on the relationship between brain activity and cognition and behavior. © © 2022 Loriette, Amengual and Ben Hamed.}, keywords = {attention; behavior; brain cortex; brain decoding; brain electrophysiology; brain function; brain nerve cell; coding; cognition; confusion; deep learning; electroencephalography; functional magnetic resonance imaging; functional neuroimaging; human; machine learning; magnetoencephalography; model; neurofeedback; nonhuman; Review; task performance},
correspondence_address = {C. Loriette; Institut des Sciences, Cognitives Marc Jeannerod, CNRS, UMR 5229, Université Claude Bernard Lyon 1, Bron, France; email: celia.loriette@isc.cnrs.fr; S. Ben Hamed; Institut des Sciences, Cognitives Marc Jeannerod, CNRS, UMR 5229, Université Claude Bernard Lyon 1, Bron, France; email: benhamed@isc.cnrs.fr},
publisher = {Frontiers Media S.A.},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Review}}
@article{Luo2022Braincomputer,
author = {Luo, Shiyu and Rabbani, Qinwan and Crone, Nathan Earl},
title = {Brain-Computer Interface: Applications to Speech Decoding and Synthesis to Augment Communication},
year = {2022},
journal = {Neurotherapeutics},
volume = {19},
number = {1},
pages = {263 - 273},
doi = {10.1007/s13311-022-01190-2},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124133288&doi=10.1007%2Fs13311-022-01190-2&partnerID=40&md5=326ccd040a391bd953cfd20121e5968e},
affiliations = {Johns Hopkins University School of Medicine, Department of Biomedical Engineering, Baltimore, MD, United States; Whiting School of Engineering, Baltimore, MD, United States; Johns Hopkins University School of Medicine, Department of Neurology, Baltimore, MD, United States},
abstract = {Damage or degeneration of motor pathways necessary for speech and other movements, as in brainstem strokes or amyotrophic lateral sclerosis (ALS), can interfere with efficient communication without affecting brain structures responsible for language or cognition. In the worst-case scenario, this can result in the locked in syndrome (LIS), a condition in which individuals cannot initiate communication and can only express themselves by answering yes/no questions with eye blinks or other rudimentary movements. Existing augmentative and alternative communication (AAC) devices that rely on eye tracking can improve the quality of life for people with this condition, but brain-computer interfaces (BCIs) are also increasingly being investigated as AAC devices, particularly when eye tracking is too slow or unreliable. Moreover, with recent and ongoing advances in machine learning and neural recording technologies, BCIs may offer the only means to go beyond cursor control and text generation on a computer, to allow real-time synthesis of speech, which would arguably offer the most efficient and expressive channel for communication. The potential for BCI speech synthesis has only recently been realized because of seminal studies of the neuroanatomical and neurophysiological underpinnings of speech production using intracranial electrocorticographic (ECoG) recordings in patients undergoing epilepsy surgery. These studies have shown that cortical areas responsible for vocalization and articulation are distributed over a large area of ventral sensorimotor cortex, and that it is possible to decode speech and reconstruct its acoustics from ECoG if these areas are recorded with sufficiently dense and comprehensive electrode arrays. In this article, we review these advances, including the latest neural decoding strategies that range from deep learning models to the direct concatenation of speech units. We also discuss state-of-the-art vocoders that are integral in constructing natural-sounding audio waveforms for speech BCIs. Finally, this review outlines some of the challenges ahead in directly synthesizing speech for patients with LIS. © 2022, The American Society for Experimental NeuroTherapeutics, Inc.}, keywords = {Article; augmentative and alternative communication; brain electrophysiology; convolutional neural network; deep learning; deep neural network; electrocorticography; electroencephalography; feed forward neural network; functional magnetic resonance imaging; functional near-infrared spectroscopy; human; locked in syndrome; magnetoencephalography; paralanguage; phoneme; speech; support vector machine; vocabulary; brain computer interface; interpersonal communication; physiology; quality of life; Brain-Computer Interfaces; Communication; Electrocorticography; Humans; Quality of Life; Speech},
correspondence_address = {S. Luo; Department of Biomedical Engineering, The Johns Hopkins University School of Medicine, Baltimore, United States; email: sluo15@jhu.edu},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {18787479; 19337213},
pmid = {35099768},
language = {English},
abbrev_source_title = {Neurotherapeutics},
type = {Article}}
@inproceedings{Mathur2022Ieee,
author = {Mathur, Priyanka and Chakka, Vijay Kumar and Garg, Vidhi},
booktitle = {2022 IEEE 19th India Council International Conference (INDICON)},
title = {Weighted Vector Visibility based Graph Signal Processing (WVV-GSP) for Neural Decoding of Motor Imagery EEG signals},
year = {2022},
volume = {},
number = {},
pages = {1--6},
abstract = {The paper deals with weighted vector visibility-based graph signal processing (WVV-GSP) to decode motor imagery tasks from multi-channel EEG signals, which plays a key role in developing brain-computer interface (BCI) systems. Initially, multichannel EEG data at each time state is mapped into a vector which is defined as a node of the graph. Functional connectivity between different temporal states is determined by the visibility between the vector’s norm and the norm of its projections onto subsequent time states. To create a weighted vector visibility graph using the connections among the nodes, it presents edge weights between the nodes using Gaussian kernels. The MI task-based WVV graph is then used to obtain GSP-based spectral features of Laplacian energy (LE) and Fiedler vector energy (FE). On a publicly accessible Physio-net database, the performance of the suggested technique is evaluated with a subject-specific and subject-independent basis using an SVM classifier. Average subject-specific accuracy of 98.99% with AUC of 99.8% and subject independent accuracy of 95.8% with AUC 96.4% is achieved for decoding left and right hand MI tasks. Extensive comparative analysis with the current literature exhibits the efficacy of the proposed WVV-GSP-based method for the neural decoding of multichannel MI EEG signals.},
keywords = {Support vector machines;Computer interfaces;Laplace equations;Databases;Signal processing;Electroencephalography;Iron;Weighted Vector visibility (WVV);graph signal processing (GSP);brain computer interface (BCI);Motor Imagery (MI) EEG;Laplacian energy;Fiedler energy},
doi = {10.1109/INDICON56171.2022.10039945},
issn = {2325-9418},
month = {Nov}}
@article{Pei2022Reconstructing,
author = {Pei, Dingyi and Olikkal, Parthan and Adali, Tülay and Vinjamuri, Ramana Kumar},
title = {Reconstructing Synergy-Based Hand Grasp Kinematics from Electroencephalographic Signals},
year = {2022},
journal = {Sensors},
volume = {22},
number = {14},
pages = {},
doi = {10.3390/s22145349},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135132624&doi=10.3390%2Fs22145349&partnerID=40&md5=6fa1f71c57619637245c007423fbd659},
affiliations = {College of Engineering and Information Technology, Baltimore, MD, United States},
abstract = {Brain-machine interfaces (BMIs) have become increasingly popular in restoring the lost motor function in individuals with disabilities. Several research studies suggest that the CNS may employ synergies or movement primitives to reduce the complexity of control rather than controlling each DoF independently, and the synergies can be used as an optimal control mechanism by the CNS in simplifying and achieving complex movements. Our group has previously demonstrated neural decoding of synergy-based hand movements and used synergies effectively in driving hand exoskeletons. In this study, ten healthy right-handed participants were asked to perform six types of hand grasps representative of the activities of daily living while their neural activities were recorded using electroencephalography (EEG). From half of the participants, hand kinematic synergies were derived, and a neural decoder was developed, based on the correlation between hand synergies and corresponding cortical activity, using multivariate linear regression. Using the synergies and the neural decoder derived from the first half of the participants and only cortical activities from the remaining half of the participants, their hand kinematics were reconstructed with an average accuracy above 70%. Potential applications of synergy-based BMIs for controlling assistive devices in individuals with upper limb motor deficits, implications of the results in individuals with stroke and the limitations of the study were discussed. © 2022 by the authors.}, keywords = {Brain computer interface; Decoding; Electrophysiology; Exoskeleton (Robotics); Kinematics; Mergers and acquisitions; Signal reconstruction; Brain-machine interface; Cortical activity; Electroencephalographic signals; Hand grasps; Hand kinematics; Hand synergy; Machine interfaces; Motor function; Neural decoding; Research studies; Electroencephalography; biomechanics; brain computer interface; daily life activity; electroencephalography; hand; hand strength; human; movement (physiology); procedures; Activities of Daily Living; Biomechanical Phenomena; Brain-Computer Interfaces; Hand; Hand Strength; Humans; Movement},
correspondence_address = {R. Vinjamuri; Department of Computer Science and Electrical Engineering, University of Maryland Baltimore County, Baltimore, 21250, United States; email: rvinjam1@umbc.edu},
publisher = {MDPI},
issn = {14248220},
pmid = {35891029},
language = {English},
abbrev_source_title = {Sensors},
type = {Article}}
@inproceedings{Perumal2022International,
author = {Perumal, Varalakshmi and Medikanda, Jeevan},
booktitle = {2022 International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics ( DISCOVER)},
title = {Tracing and decoding of covert phonemes using single channel Electroencephalogram with Machine Learning Techniques},
year = {2022},
volume = {},
number = {},
pages = {320--324},
abstract = {A Brain-computer interface BCI is a technology that interfaces the brain and computer for communication without the person expressing it. Amongst concepts of reading thoughts of the brain, decoding covert speech is a popular application in BCI which can be able to translate the imagined voice inside a person. In this study, Electroencephalogram (EEGs) has been used to interpret the covert speech of a person. On the other hand, reading the brain with EEG is a complicated task to use in daily life applications as it needs multichannel spatial information to be extracted by connecting leads all over the scalp. In the direction of overcoming this complexity, this study uses only single-channel EEG Fpz, which is much easier to access than channels. In this study, Multilayer Perceptron (MLP), K-nearest neighbour Classifier (KNN), Support Vector Classifier (SVC), and Random Forest (RF) models are proposed to classify a single channel Fpz of EEG by extracting spectral information in form of wavelet decomposition coefficients and an energy level over Alpha, Beta, Gamma, Delta and Theta bands to show the evidence that covert speech can be derived through single channel EEG with basics classifiers.},
keywords = {Support vector machine classification;Static VAr compensators;Very large scale integration;Brain modeling;Electroencephalography;Brain-computer interfaces;Decoding;Brain-computer interface;Covert speech;EEG;Multilayer Perceptron;K-nearest neighbour Classifier;Support Vector Classifier;Random Forest},
doi = {10.1109/DISCOVER55800.2022.9974955},
issn = {},
month = {Oct}}
@article{Peterson2022Learning,
author = {Peterson, Steven M. and Rao, Rajesh P.N. and Brunton, Bingni Wen},
title = {Learning neural decoders without labels using multiple data streams},
year = {2022},
journal = {Journal of Neural Engineering},
volume = {19},
number = {4},
pages = {},
doi = {10.1088/1741-2552/ac857c},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135977540&doi=10.1088%2F1741-2552%2Fac857c&partnerID=40&md5=673d0d55b66bd555ef0c0faed0abe6cc},
affiliations = {University of Washington, Department of Biology, Seattle, WA, United States; University of Washington, eScience Institute, Seattle, WA, United States; UW College of Engineering, Seattle, WA, United States; UW College of Engineering, Seattle, WA, United States; University of Washington, Center for Neurotechnology, Seattle, WA, United States},
abstract = {Objective. Recent advances in neural decoding have accelerated the development of brain-computer interfaces aimed at assisting users with everyday tasks such as speaking, walking, and manipulating objects. However, current approaches for training neural decoders commonly require large quantities of labeled data, which can be laborious or infeasible to obtain in real-world settings. Alternatively, self-supervised models that share self-generated pseudo-labels between two data streams have shown exceptional performance on unlabeled audio and video data, but it remains unclear how well they extend to neural decoding. Approach. We learn neural decoders without labels by leveraging multiple simultaneously recorded data streams, including neural, kinematic, and physiological signals. Specifically, we apply cross-modal, self-supervised deep clustering to train decoders that can classify movements from brain recordings. After training, we then isolate the decoders for each input data stream and compare the accuracy of decoders trained using cross-modal deep clustering against supervised and unimodal, self-supervised models. Main results. We find that sharing pseudo-labels between two data streams during training substantially increases decoding performance compared to unimodal, self-supervised models, with accuracies approaching those of supervised decoders trained on labeled data. Next, we extend cross-modal decoder training to three or more modalities, achieving state-of-the-art neural decoding accuracy that matches or slightly exceeds the performance of supervised models. Significance. We demonstrate that cross-modal, self-supervised decoding can be applied to train neural decoders when few or no labels are available and extend the cross-modal framework to share information among three or more data streams, further improving self-supervised training. © 2022 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Brain computer interface; Computer hardware description languages; Data streams; Decoding; Deep learning; Electrophysiology; Walking aids; Clusterings; Cross-modal; Cross-modal learning; Data stream; Deep clustering; Labeled data; Neural decoding; Performance; Self-supervised learning; Unimodal; Electroencephalography; article; brain; electroencephalography; human; human experiment; learning; videorecording; brain computer interface; movement (physiology); physiology; supervised machine learning; walking; Brain-Computer Interfaces; Learning; Movement; Supervised Machine Learning; Walking},
correspondence_address = {B.W. Brunton; Department of Biology, University of Washington, Seattle, 98195, United States; email: bbrunton@uw.edu},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {35905727},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Peterson2022Motor,
author = {Peterson, Victoria and Galvan, Catalina María and Hernández, Hugo Sacha and Saavedra, María Paula and Spies, Rubén Daniel},
title = {A motor imagery vs. rest dataset with low-cost consumer grade EEG hardware},
year = {2022},
journal = {Data in Brief},
volume = {42},
pages = {},
doi = {10.1016/j.dib.2022.108225},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130263276&doi=10.1016%2Fj.dib.2022.108225&partnerID=40&md5=28ee4b02e4b3cfb7cfea96061f50daf3},
affiliations = {Instituto de Matemática Aplicada del Litoral, Santa Fe, Santa Fe, Argentina; Universidad Nacional de Entre Rios, Faculty of Engineering, Concepcion del Uruguay, Entre Rios, Argentina; Consejo Nacional de Investigaciones Científicas y Técnicas, Instituto de Investigación en Señales, Buenos Aires, Provincia de Buenos Aires, Argentina; Universidad Nacional del Litoral, Department of Mathematics, Santa Fe, Santa Fe, Argentina},
abstract = {The data consist of electroencephalography (EEG) signals acquired by means of low-cost consumer-grade devices from 10 participants (four females, right-handed, mean age ± SD = 26.1 ± 4.0 years) without any previous experience in Brain-Computer Interfaces (BCIs) usage. The BCI protocol consisted of two conditions, namely the kinesthetic imagination of grasping movement (motor imagery, MI) of the dominant hand and a rest/idle condition. Five protocol runs were required to be performed by each participant in a single-day session, of about 1.5 h. The first run, called RUN0, involved 5 trials of real grasping movement together with the same number of trials in a rest condition. This first run was done to both better explain the protocol and to encourage the participant to focus on the sensation of executing the movement. The rest of the runs (RUN1-RUN4) were identical, consisting of 20 trials for each condition presented in a random order. The electrical brain activity was registered from 15 electrodes covering the sensorimotor area, at a sampling frequency of 125 Hz. Muscle activity of the dominant hand was controlled via the electromyography (EMG) activity by two electrodes placed at two antagonist muscles involved in the flexion/extension of the wrist. The recordings were performed in a non-shielded office, by means of low-cost consumer grade devices and free multi-platform open source software. The EMG corruption level was analyzed and EEG trials for which the EMG activity was higher than a prescribed threshold value, were discarded. During acquisition, EEG data was digitally band-pass filtered between 0.5 and 45 Hz. These data provide a motor imagery vs. rest EEG dataset, relevant for BCI for motor rehabilitation applications. Since the recordings were performed by means of low-cost consumer grade devices in a non-controlled environment, this dataset provides an excellent source for exploring robust brain decoding techniques for future in-home BCI usage. © 2022 The Author(s)},
keywords = {Brain-computer interfaces; Electroencephalography (EEG); Low-cost technologies},
correspondence_address = {V. Peterson; Instituto de Matemática Aplicada del Litoral, IMAL, CONICET-UNL, Santa Fe, 3000, Argentina; email: vpeterson@santafe-conicet.gov.ar},
publisher = {Elsevier Inc.},
issn = {23523409},
language = {English},
abbrev_source_title = {Data Brief},
type = {Data paper}}
@article{Petschenig2022Classification,
author = {Petschenig, Horst and Bisio, Marta and Maschietto, Marta and Leparulo, Alessandro and Legenstein, Robert A. and Vassanelli, Stefano},
title = {Classification of Whisker Deflections From Evoked Responses in the Somatosensory Barrel Cortex With Spiking Neural Networks},
year = {2022},
journal = {Frontiers in Neuroscience},
volume = {16},
pages = {},
doi = {10.3389/fnins.2022.838054},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128891683&doi=10.3389%2Ffnins.2022.838054&partnerID=40&md5=127f1b5a52c07681285e01c1068ff72e},
affiliations = {Technische Universitat Graz, Faculty of Computer Science and Biomedical Engineering, Graz, Styria, Austria; Università degli Studi di Padova, Department of Biomedical Sciences, Padua, PD, Italy},
abstract = {Spike-based neuromorphic hardware has great potential for low-energy brain-machine interfaces, leading to a novel paradigm for neuroprosthetics where spiking neurons in silicon read out and control activity of brain circuits. Neuromorphic processors can receive rich information about brain activity from both spikes and local field potentials (LFPs) recorded by implanted neural probes. However, it was unclear whether spiking neural networks (SNNs) implemented on such devices can effectively process that information. Here, we demonstrate that SNNs can be trained to classify whisker deflections of different amplitudes from evoked responses in a single barrel of the rat somatosensory cortex. We show that the classification performance is comparable or even superior to state-of-the-art machine learning approaches. We find that SNNs are rather insensitive to recorded signal type: both multi-unit spiking activity and LFPs yield similar results, where LFPs from cortical layers III and IV seem better suited than those of deep layers. In addition, no hand-crafted features need to be extracted from the data—multi-unit activity can directly be fed into these networks and a simple event-encoding of LFPs is sufficient for good performance. Furthermore, we find that the performance of SNNs is insensitive to the network state—their performance is similar during UP and DOWN states. © © 2022 Petschenig, Bisio, Maschietto, Leparulo, Legenstein and Vassanelli.}, keywords = {animal experiment; animal tissue; Article; classification; controlled study; cortex layer III; cortex layer IV; electroencephalogram; evoked cortical response; feature extraction; feed forward neural network; firing rate; intermethod comparison; liquid state machine model; local field potential; long short term memory network; measurement accuracy; model; negative peak amplitude; nonhuman; physical parameters; physical phenomena; positive rebound amplitude; random forest; rat; response onset latency; sensory evoked potential; signal noise ratio; somatosensory cortex; spiking neural network; stimulation; stimulation intensity; stimulus response; whisker deflection},
correspondence_address = {R. Legenstein; Faculty of Computer Science and Biomedical Engineering, Institute of Theoretical Computer Science, Graz University of Technology, Graz, Austria; email: robert.legenstein@igi.tugraz.at; S. Vassanelli; NeuroChip Laboratory, Department of Biomedical Sciences, University of Padova, Padova, Italy; email: stefano.vassanelli@unipd.it},
publisher = {Frontiers Media S.A.},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@conference{Puffay2022Relating,
author = {Puffay, Corentin and van Canneyt, Jana and Vanthornhout, Jonas and Van hamme, Hugo and Francart, Tom},
title = {Relating the fundamental frequency of speech with EEG using a dilated convolutional network},
year = {2022},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
volume = {2022-September},
pages = {4038 - 4042},
doi = {10.21437/Interspeech.2022-315},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140075159&doi=10.21437%2FInterspeech.2022-315&partnerID=40&md5=db12502cc96db731258bf6989018c406},
affiliations = {Departement Neurowetenschappen, Leuven, Vlaams-Brabant, Belgium; KU Leuven, Leuven, Vlaams-Brabant, Belgium},
abstract = {To investigate how speech is processed in the brain, we can model the relation between features of a natural speech signal and the corresponding recorded electroencephalogram (EEG). Usually, linear models are used in regression tasks. Either EEG is predicted, or speech is reconstructed, and the correlation between predicted and actual signal is used to measure the brain's decoding ability. However, given the nonlinear nature of the brain, the modeling ability of linear models is limited. Recent studies introduced nonlinear models to relate the speech envelope to EEG. We set out to include other features of speech that are not coded in the envelope, notably the fundamental frequency of the voice (f0). F0 is a higher-frequency feature primarily coded at the brainstem to midbrain level. We present a dilated-convolutional model to provide evidence of neural tracking of the f0. We show that a combination of f0 and the speech envelope improves the performance of a state-of-the-art envelope-based model. This suggests the dilated-convolutional model can extract non-redundant information from both f0 and the envelope. We also show the ability of the dilated-convolutional model to generalize to subjects not included during training. This latter finding will accelerate f0-based hearing diagnosis. © © 2022 ISCA.}, keywords = {Audition; Decoding; Electroencephalography; Natural frequencies; Speech communication; Speech recognition; Auditory systems; Brain decoding; Convolutional model; Convolutional networks; Electroencephalogram decoding; Envelope; Fundamental frequencies; Linear modeling; Natural speech; Speech signals; Convolution},
publisher = {International Speech Communication Association},
issn = {29581796; 19909772; 2308457X; 2958-1796},
isbn = {9781713836902; 9781713820697; 9781605603162; 9781617821233; 9781604234497},
language = {English},
abbrev_source_title = {Proc. Annu. Conf. Int. Speech. Commun. Assoc., INTERSPEECH},
type = {Conference paper}}
@article{Ran2022Hybrid,
author = {Ran, Xingchen and Chen, Weidong and Yvert, Blaise and Zhang, Shaomin},
title = {A hybrid autoencoder framework of dimensionality reduction for brain-computer interface decoding},
year = {2022},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {},
doi = {10.1016/j.compbiomed.2022.105871},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135537209&doi=10.1016%2Fj.compbiomed.2022.105871&partnerID=40&md5=c52a038b3c91f359b8ab98aa182ed4fe},
affiliations = {Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, Zhejiang, China; Ministry of Education of the People's Republic of China, Beijing, Beijing, China; Université Grenoble Alpes, Saint Martin d'Heres, Auvergne-Rhone-Alpes, France},
abstract = {Objective: As the scale of neural recording increases, Brain-computer interfaces (BCIs) are restrained by high-dimensional neural features, so dimensionality reduction is required as a preprocess of neural features. In this context, we propose a novel framework based on deep learning to reduce the dimensionality of neural features that are typically extracted from electrocorticography (ECoG) or local field potential (LFP). Approach: A high-performance autoencoder was implemented by chaining convolutional layers to deal with spatial and frequency dimensions with bottleneck long short-term memory (LSTM) layers to deal with the temporal dimension of the features. Furthermore, this autoencoder is combined with a fully connected layer to regularize the training. Main results: By applying the proposed method to two different datasets, we found that this dimensionality reduction method largely outperforms kernel principal component analysis (KPCA), partial least square (PLS), preferential subspace identification (PSID), and latent factor analysis via dynamical systems (LFADS). Besides, the new features obtained by our method can be applied to various BCI decoders, without significant differences in decoding performance. Significance: A novel method is proposed as a reliable tool for efficient dimensionality reduction of neural signals. Its high performance and robustness are promising to enhance the decoding accuracy and long-term stability of online BCI systems based on large-scale neural recordings. © 2022 Elsevier Ltd}, keywords = {Brain computer interface; Decoding; Dynamical systems; Electroencephalography; Learning systems; Least squares approximations; Long short-term memory; Online systems; Auto encoders; Dimensionality reduction; High-dimensional; Higher-dimensional; Local field potentials; Neural decoding; Neural recordings; Performance; Preprocess; Spatial dimension; Principal component analysis; adult; animal experiment; Article; autoencoder; controlled study; dimensionality reduction; electrocorticography; factor analysis; female; local field potential; long short term memory network; nonhuman; partial least squares regression; principal component analysis; brain computer interface; electroencephalography; least square analysis; Brain-Computer Interfaces; Electrocorticography; Least-Squares Analysis},
correspondence_address = {S. Zhang; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, China; email: shaomin@zju.edu.cn; B. Yvert; Univ. Grenoble Alpes, INSERM, Grenoble Institute of Neuroscience U1216, France; email: blaise.yvert@inserm.fr},
publisher = {Elsevier Ltd},
issn = {18790534; 00104825},
coden = {CBMDA},
pmid = {35933960},
language = {English},
abbrev_source_title = {Comput. Biol. Med.},
type = {Article}}
@article{Ryb2022Neural,
author = {Rybář, Milan and Daly, Ian},
title = {Neural decoding of semantic concepts: A systematic literature review},
year = {2022},
journal = {Journal of Neural Engineering},
volume = {19},
number = {2},
pages = {},
doi = {10.1088/1741-2552/ac619a},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128488349&doi=10.1088%2F1741-2552%2Fac619a&partnerID=40&md5=b82aee2426c22efa18a92ee6f44ed96d},
affiliations = {University of Essex, Brain-Computer Interfaces and Neural Engineering Laboratory, Colchester, Essex, United Kingdom},
abstract = {Objective. Semantic concepts are coherent entities within our minds. They underpin our thought processes and are a part of the basis for our understanding of the world. Modern neuroscience research is increasingly exploring how individual semantic concepts are encoded within our brains and a number of studies are beginning to reveal key patterns of neural activity that underpin specific concepts. Building upon this basic understanding of the process of semantic neural encoding, neural engineers are beginning to explore tools and methods for semantic decoding: identifying which semantic concepts an individual is focused on at a given moment in time from recordings of their neural activity. In this paper we review the current literature on semantic neural decoding. Approach. We conducted this review according to the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) guidelines. Specifically, we assess the eligibility of published peer-reviewed reports via a search of PubMed and Google Scholar. We identify a total of 74 studies in which semantic neural decoding is used to attempt to identify individual semantic concepts from neural activity. Main results. Our review reveals how modern neuroscientific tools have been developed to allow decoding of individual concepts from a range of neuroimaging modalities. We discuss specific neuroimaging methods, experimental designs, and machine learning pipelines that are employed to aid the decoding of semantic concepts. We quantify the efficacy of semantic decoders by measuring information transfer rates. We also discuss current challenges presented by this research area and present some possible solutions. Finally, we discuss some possible emerging and speculative future directions for this research area. Significance. Semantic decoding is a rapidly growing area of research. However, despite its increasingly widespread popularity and use in neuroscientific research this is the first literature review focusing on this topic across neuroimaging modalities and with a focus on quantifying the efficacy of semantic decoders. © 2022 IOP Publishing Ltd.}, keywords = {Brain; Electroencephalography; Electrophysiology; Functional neuroimaging; Infrared devices; Magnetic resonance imaging; Near infrared spectroscopy; Neurons; Semantics; Conceptual decoding; Functional magnetic resonance imaging; Functional near infrared spectroscopy; Intracranial electrode; Literature reviews; Semantic concept; Semantic decoding; Decoding; association; brain function; electrocorticography; electroencephalography; experimental design; feature extraction; feature selection; functional magnetic resonance imaging; functional near-infrared spectroscopy; human; machine learning; mental task; neural decoding; neuroimaging; neuroscience; quantitative analysis; Review; semantics; sensory system; stereoelectroencephalography; brain; diagnostic imaging; nuclear magnetic resonance imaging; Machine Learning; Magnetic Resonance Imaging; Neuroimaging},
correspondence_address = {I. Daly; Brain-Computer Interfacing and Neural Engineering Laboratory, School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom; email: i.daly@essex.ac.uk},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {35344941},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Review}}
@article{Tonoyan2022Subjective,
author = {Tonoyan, Yelena and Fornaciai, Michele and Parsons, Brent D. and Bueti, Domenica},
title = {Subjective time is predicted by local and early visual processing},
year = {2022},
journal = {NeuroImage},
volume = {264},
pages = {},
doi = {10.1016/j.neuroimage.2022.119707},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140969491&doi=10.1016%2Fj.neuroimage.2022.119707&partnerID=40&md5=65d5bb77b65c937e0007dc6f6d1251b0},
affiliations = {Scuola Internazionale Superiore di Studi Avanzati, Trieste, TS, Italy},
abstract = {Time is as pervasive as it is elusive to study, and how the brain keeps track of millisecond time is still unclear. Here we addressed the mechanisms underlying duration perception by looking for a neural signature of subjective time distortion induced by motion adaptation. We recorded electroencephalographic signals in human participants while they were asked to discriminate the duration of visual stimuli after different types of translational motion adaptation. Our results show that perceived duration can be predicted by the amplitude of the N200 event-related potential evoked by the adapted stimulus. Moreover, we show that the distortion of subjective time can be predicted by the activity in the Beta band frequency spectrum, at the offset of the adaptor and during the presentation of the subsequent adapted stimulus. Both effects were observed from posterior electrodes contralateral to the adapted stimulus. Overall, our findings suggest that local and low-level perceptual processes are involved in generating a subjective sense of time. © 2022}, keywords = {adaptation; adult; Article; behavior; compression; electroencephalography; electrooculogram; event related potential; eye movement; eye tracking; female; human; human experiment; inverse motion spatiotopic adaptation; local field potential; male; motion; normal human; perception; prediction; retinotopic adaptation; sensory system parameters; spatial position; spatiotopic adaptation; stimulus; stimulus response; time; time perception; vision; visual evoked potential; visual field; visual stimulation; evoked response; movement perception; photostimulation; physiology; procedures; Adaptation, Physiological; Electroencephalography; Evoked Potentials; Humans; Motion Perception; Photic Stimulation; Visual Perception},
correspondence_address = {Y. Tonoyan; International School for Advanced Studies (SISSA), Trieste, Via Bonomea 265, TS, 34136, Italy; email: ytonoyan@sissa.it},
publisher = {Academic Press Inc.},
issn = {10959572; 10538119},
coden = {NEIME},
pmid = {36341952},
language = {English},
abbrev_source_title = {NeuroImage},
type = {Article}}
@article{Wang2022Efficient,
author = {Wang, Jiaxing and Shi, Lei and Wang, Weiqun and Hou, Zeng-Guang},
journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
title = {Efficient Brain Decoding Based on Adaptive EEG Channel Selection and Transformation},
year = {2022},
volume = {6},
number = {6},
pages = {1314--1323},
abstract = {Electroencephalography (EEG) based brain-computer interface (BCI) has a wide range of applications in neuro-rehabilitation and motor assistance. However, brain activities, acquired from a large number of EEG channels, are highly inter-correlated or irrelevant to the brain decoding task, thus reducing the decoding efficiency and accuracy. How to adaptively select the optimal channel number depend on different trials remains a big challenge. To solve this problem, an efficient end-to-end brain decoding model named AdaEEGNet, is proposed in this study. It can reduce the computational cost by adaptively controlling the number of input channels and improve the classification accuracy by reducing over-fitting. Specifically, a lightweight policy module is designed to analyze which channel is needed for decoding current EEG trial. Due to the channel selection process is indifferentiable, we propose to use the Gumbel-Estimator to back-propagate the gradient to train the whole framework. Additionally, a weight coefficient is designed to make a trade-off between brain decoding accuracy and efficiency. To validate the proposed AdaEEGNet feasibility in improving decoding efficiency and accuracy, extensive experiments were conducted on BCI competition IV dataset. The results show that our methods can improve the decoding accuracy by 2% with only 65% computational cost significantly compared with the baseline method.},
keywords = {Electroencephalography;Decoding;Brain modeling;Computational modeling;Optimized production technology;Feature extraction;Data models;Channel selection;channel transformation;brain decoding;computational cost;classification accuracy},
doi = {10.1109/TETCI.2022.3147225},
issn = {2471-285X},
month = {Dec}}
@article{Zhang2022Neural,
author = {Zhang, Yijun and Yu, Zhaofei and Liu, Jian K. and Huang, Tie Jun},
title = {Neural Decoding of Visual Information Across Different Neural Recording Modalities and Approaches},
year = {2022},
journal = {Machine Intelligence Research},
volume = {19},
number = {5},
pages = {350 - 365},
doi = {10.1007/s11633-022-1335-2},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134292313&doi=10.1007%2Fs11633-022-1335-2&partnerID=40&md5=32a9c9b94e92bb18c3d907836a63c37c},
affiliations = {Shanghai Jiao Tong University, Department of Computer Science and Engineering, Shanghai, China; Peking University, School of Computer Science, Beijing, China; Peking University, Institute for Artificial Intelligence, Beijing, China; University of Leeds, Leeds, West Yorkshire, United Kingdom; Beijing Academy of Artificial Intelligence, Beijing, China},
abstract = {Vision plays a peculiar role in intelligence. Visual information, forming a large part of the sensory information, is fed into the human brain to formulate various types of cognition and behaviours that make humans become intelligent agents. Recent advances have led to the development of brain-inspired algorithms and models for machine vision. One of the key components of these methods is the utilization of the computational principles underlying biological neurons. Additionally, advanced experimental neuroscience techniques have generated different types of neural signals that carry essential visual information. Thus, there is a high demand for mapping out functional models for reading out visual information from neural signals. Here, we briefly review recent progress on this issue with a focus on how machine learning techniques can help in the development of models for contending various types of neural signals, from fine-scale neural spikes and single-cell calcium imaging to coarse-scale electroencephalography (EEG) and functional magnetic resonance imaging recordings of brain signals. © 2022, The Author(s).}, keywords = {Brain; Deep learning; Electroencephalography; Electrophysiology; Intelligent agents; Learning systems; Magnetic resonance imaging; Brain-inspired; Brain-inspired vision; Large parts; Machine-learning; Neural decoding; Neural recordings; Neural signals; Visual decoding; Visual information; Decoding},
correspondence_address = {Z.-F. Yu; School of Computer Science, Peking University, Beijing, 100190, China; email: yuzf12@pku.edu.cn},
publisher = {Chinese Academy of Sciences},
issn = {27315398; 2731538X},
language = {English},
abbrev_source_title = {Mach. Intell. Res.},
type = {Review}}
@article{Angjelichinoski2021Deep,
author = {Angjelichinoski, Marko and Soltani, Mohammadreza and Choi, John Stephen and Pesaran, Bijan and Tarokh, Vahid},
title = {Deep Pinsker and James-Stein Neural Networks for Decoding Motor Intentions from Limited Data},
year = {2021},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {29},
pages = {1058 - 1067},
doi = {10.1109/TNSRE.2021.3083755},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107212099&doi=10.1109%2FTNSRE.2021.3083755&partnerID=40&md5=96547699feac2db3f5cb86b633aee1b8},
affiliations = {Pratt School of Engineering, Durham, NC, United States; New York University, New York, NY, United States},
abstract = {Non-parametric regression has been shown to be useful in extracting relevant features from Local Field Potential (LFP) signals for decoding motor intentions. Yet, in many instances, brain-computer interfaces (BCIs) rely on simple classification methods, circumventing deep neural networks (DNNs) due to limited training data. This paper leverages the robustness of several important results in non-parametric regression to harness the potentials of deep learning in limited data setups. We consider a solution that combines Pinsker's theorem as well as its adaptively optimal counterpart due to James-Stein for feature extraction from LFPs, followed by a DNN for classifying motor intentions. We apply our approach to the problem of decoding eye movement intentions from LFPs collected in macaque cortex while the animals perform memory-guided visual saccades to one of eight target locations. The results demonstrate that a DNN classifier trained over the Pinsker features outperforms the benchmark method based on linear discriminant analysis (LDA) trained over the same features. © 2001-2011 IEEE.}, keywords = {Brain computer interface; Decoding; Deep learning; Deep neural networks; Discriminant analysis; Electrophysiology; Eye movements; Brain computer interfaces (BCIs); Classification methods; Limited training data; Linear discriminant analysis; Local field potentials; Movement intentions; Non-parametric regression; Relevant features; Neural networks; adult; Article; artificial neural network; audiometry; craniotomy; diagnostic accuracy; electroencephalography; electromyography; electrophysiology; extraction; eye movement; hypoxia; learning algorithm; local field potential; male; nerve cell network; nonhuman; refraction error; retina blood vessel; signal noise ratio; speech intelligibility; speech perception; task performance; animal; behavior; brain computer interface; movement (physiology); Animals; Brain-Computer Interfaces; Eye Movements; Intention; Movement; Neural Networks, Computer},
correspondence_address = {M. Angjelichinoski; Department of Electrical and Computer Engineering, Duke University, Durham, United States; email: ma393@duke.edu},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {34038363},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Balagourouchetty2021Scattering,
author = {Balagourouchetty, Lakshmipriya and Jayalakshmy, S. S. and Jayanthi, Pragatheeswaran K. and Saraswathi, D. and Poonguzhali, N.},
title = {Scattering convolutional network based predictive model for cognitive activity of brain using empirical wavelet decomposition},
year = {2021},
journal = {Biomedical Signal Processing and Control},
volume = {66},
pages = {},
doi = {10.1016/j.bspc.2021.102501},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101184324&doi=10.1016%2Fj.bspc.2021.102501&partnerID=40&md5=fe9369d1530c5e1fc1fac096e8557a38},
affiliations = {Manakula Vinayagar Institute of Technology, Department of Electrical & Computer Engineering, Puducherry, PY, India; IFET College of Engineering, Department of Electrical & Computer Engineering, Viluppuram, TN, India; Puducherry Technological University, Department of Electrical & Computer Engineering, Puducherry, PY, India; Manakula Vinayagar Institute of Technology, Department of CSE, Puducherry, PY, India},
abstract = {Understanding the cognitive activity of brain is a challenging task in brain computer interface (BCI) applications. This work aims at exploring the capability of empirical wavelet transform in decoding the brain wave pattern acquired in response to a thought process and visual stimuli. Empirical wavelet transform (EWT), when combined with the wavelet scattering coefficients is found to efficiently decode the brain wave using recurrent neural network (RNN) based classifier. Electroencephalogram (EEG) and magnetoencephalogram (MEG) are the two modalities considered in this work. The proposed framework is assessed using three different RNN architectures namely long short term memory (LSTM), bi-directional long short term memory (Bi-LSTM), gated recurrent units (GRU). The experimental results show that wavelet scattering coefficients extracted from the dominant mode of EWT decomposition record better performance of 90.23 % and 84.25 % for EEG and MEG signals using GRU as classifier. Furthermore, the wavelet scattering network which involves no learning process achieves better classification at reduced time and computational complexities. © 2021 Elsevier Ltd}, keywords = {Brain; Brain computer interface; Convolutional neural networks; Decoding; Electroencephalography; Predictive analytics; Wavelet decomposition; Cognitive activities; Convolutional networks; Electro-encephalogram (EEG); Magnetoencephalogram (MEG); Predictive modeling; Recurrent neural network (RNN); Scattering co-efficient; Scattering networks; Long short-term memory; article; brain function; classifier; decomposition; electroencephalogram; gated recurrent unit network; human; human experiment; learning; long short term memory network; wavelet transform},
correspondence_address = {B. Lakshmi Priya; Department of ECE, Manakula Vinayagar Institute of Technology, Puducherry, India; email: lakshmipriyaece@mvit.edu.in},
publisher = {Elsevier Ltd},
issn = {17468108; 17468094},
language = {English},
abbrev_source_title = {Biomed. Signal Process. Control},
type = {Article}}
@article{DiLiberto2021Accurate,
author = {Di Liberto, Giovanni M. and Marion, Guilhem and Shamma, Shihab A.},
title = {Accurate Decoding of Imagined and Heard Melodies},
year = {2021},
journal = {Frontiers in Neuroscience},
volume = {15},
pages = {},
doi = {10.3389/fnins.2021.673401},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113258558&doi=10.3389%2Ffnins.2021.673401&partnerID=40&md5=8e8564538531477c2d7a923957cb33bc},
affiliations = {CNRS Centre National de la Recherche Scientifique, Paris, Ile-de-France, France; École Normale Supérieure, Paris, Ile-de-France, France; Trinity College Dublin, Department of Mechanics, Dublin, Leinster, Ireland; University College Dublin, Dublin, Leinster, Ireland; A. James Clark School of Engineering, Department of Electrical & Computer Engineering, College Park, MD, United States},
abstract = {Music perception requires the human brain to process a variety of acoustic and music-related properties. Recent research used encoding models to tease apart and study the various cortical contributors to music perception. To do so, such approaches study temporal response functions that summarise the neural activity over several minutes of data. Here we tested the possibility of assessing the neural processing of individual musical units (bars) with electroencephalography (EEG). We devised a decoding methodology based on a maximum correlation metric across EEG segments (maxCorr) and used it to decode melodies from EEG based on an experiment where professional musicians listened and imagined four Bach melodies multiple times. We demonstrate here that accurate decoding of melodies in single-subjects and at the level of individual musical units is possible, both from EEG signals recorded during listening and imagination. Furthermore, we find that greater decoding accuracies are measured for the maxCorr method than for an envelope reconstruction approach based on backward temporal response functions (bTRFenv). These results indicate that low-frequency neural signals encode information beyond note timing, especially with respect to low-frequency cortical signals below 1 Hz, which are shown to encode pitch-related information. Along with the theoretical implications of these results, we discuss the potential applications of this decoding methodology in the context of novel brain-computer interface solutions. © Copyright © 2021 Di Liberto, Marion and Shamma.}, keywords = {adolescent; adult; Article; auditory stimulation; brain decoding; brain function; cantus firmus; data analysis software; electroencephalogram; electroencephalography; female; functional magnetic resonance imaging; human; human experiment; imagination; joint decorrelation analysis; listening; Lutheran; male; maximum correlation music decoding; monophonic music; multiway canonical correlation analysis; music; music perception; music therapy; musician; neuroscience; normal human; perception; signal noise ratio; signal processing; sound envelope reconstruction analysis; statistical analysis; task performance},
correspondence_address = {G.M. Di Liberto; Laboratoire des Systèmes Perceptifs, CNRS, Paris, France; email: diliberg@tcd.ie},
publisher = {Frontiers Media S.A.},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Article}}
@article{Fan2021Bilinear,
author = {Fan, Chenchen and Yang, Hongjun and Hou, Zengguang and Ni, Zhenliang and Chen, Sheng and Fang, Zhijie},
title = {Bilinear neural network with 3-D attention for brain decoding of motor imagery movements from the human EEG},
year = {2021},
journal = {Cognitive Neurodynamics},
volume = {15},
number = {1},
pages = {181 - 189},
doi = {10.1007/s11571-020-09649-8},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095794491&doi=10.1007%2Fs11571-020-09649-8&partnerID=40&md5=768565143bef1b7062caf3bd55e205ed},
affiliations = {Institute of Automation Chinese Academy of Sciences, State Key Laboratory of Management and Control for Complex Systems, Beijing, Beijing, China; University of Chinese Academy of Sciences, School of Artificial Intelligence, Beijing, China; Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China},
abstract = {Deep learning has achieved great success in areas such as computer vision and natural language processing. In the past, some work used convolutional networks to process EEG signals and reached or exceeded traditional machine learning methods. We propose a novel network structure and call it QNet. It contains a newly designed attention module: 3D-AM, which is used to learn the attention weights of EEG channels, time points, and feature maps. It provides a way to automatically learn the electrode and time selection. QNet uses a dual branch structure to fuse bilinear vectors for classification. It performs four, three, and two classes on the EEG Motor Movement/Imagery Dataset. The average cross-validation accuracy of 65.82%, 74.75%, and 82.88% was obtained, which are 7.24%, 4.93%, and 2.45% outperforms than the state-of-the-art, respectively. The article also visualizes the attention weights learned by QNet and shows its possible application for electrode channel selection. © 2020, Springer Nature B.V.}, keywords = {accuracy; Article; artificial neural network; attention; attention mechanism; automation; brain function; classification; convolutional neural network; cross validation; data base; electroencephalogram; imagery; mathematical analysis; mathematical computing; motor imagery movement; motor performance; time},
correspondence_address = {Z.-G. Hou; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; email: zengguang.hou@ia.ac.cn},
publisher = {Springer Science and Business Media B.V.},
issn = {18714080; 18714099},
language = {English},
abbrev_source_title = {Cogn. Neurodynamics},
type = {Article}}
@article{Hsu2021Application,
author = {Hsu, Chunhsien and Wu, Yaning},
title = {Application of empirical mode decomposition for decoding perception of faces using magnetoencephalography},
year = {2021},
journal = {Sensors},
volume = {21},
number = {18},
pages = {},
doi = {10.3390/s21186235},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115087713&doi=10.3390%2Fs21186235&partnerID=40&md5=00af7b9cbb1d602f25d76ba70387e33b},
affiliations = {National Central University, Institute of Cognitive Neuroscience, Taoyuan, Taiwan},
abstract = {Neural decoding is useful to explore the timing and source location in which the brain encodes information. Higher classification accuracy means that an analysis is more likely to succeed in extracting useful information from noises. In this paper, we present the application of a nonlinear, nonstationary signal decomposition technique—the empirical mode decomposition (EMD), on MEG data. We discuss the fundamental concepts and importance of nonlinear methods when it comes to analyzing brainwave signals and demonstrate the procedure on a set of open-source MEG facial recognition task dataset. The improved clarity of data allowed further decoding analysis to capture distinguishing features between conditions that were formerly over-looked in the existing literature, while raising interesting questions concerning hemispheric dominance to the encoding process of facial and identity information. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}, keywords = {Brain mapping; Classification (of information); Decoding; Encoding (symbols); Magnetoencephalography; Classification accuracy; Empirical Mode Decomposition; Encoding process; Facial recognition; Fundamental concepts; Identity information; Non-linear methods; Nonstationary signals; Face recognition; algorithm; electroencephalography; facial recognition; magnetoencephalography; signal processing; Algorithms; Electroencephalography; Facial Recognition; Signal Processing, Computer-Assisted},
correspondence_address = {Y.-N. Wu; Institute of Cognitive Neuroscience, National Central University, Taoyuan City, 320317, Taiwan; email: yaw141@ncu.edu.tw},
publisher = {MDPI},
issn = {14248220},
pmid = {34577441},
language = {English},
abbrev_source_title = {Sensors},
type = {Article}}
@article{Huang2021Neural,
author = {Huang, Wei and Yan, Hongmei and Cheng, Kaiwen and Wang, Chong and Li, Jiyi and Wang, Yuting and Li, Chen and Li, Chaorong and Li, Yunhan and Zuo, Zhentao and Chen, Huafu},
title = {A neural decoding algorithm that generates language from visual activity evoked by natural images},
year = {2021},
journal = {Neural Networks},
volume = {144},
pages = {90 - 100},
doi = {10.1016/j.neunet.2021.08.006},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113990280&doi=10.1016%2Fj.neunet.2021.08.006&partnerID=40&md5=045538c96a65b28da54eaac9b0444645},
affiliations = {University of Electronic Science and Technology of China, School of Life Science and Technology, Chengdu, Sichuan, China; Institute of Biophysics Chinese Academy of Sciences, State Key Laboratory of Brain and Cognitive Science, Beijing, China; Sichuan International Studies University, College of Language Intelligence, Chongqing, Chongqing, China; Sichuan University, Department of Medical Information Engineering, Chengdu, Sichuan, China},
abstract = {Transforming neural activities into language is revolutionary for human–computer interaction as well as functional restoration of aphasia. Present rapid development of artificial intelligence makes it feasible to decode the neural signals of human visual activities. In this paper, a novel Progressive Transfer Language Decoding Model (PT-LDM) is proposed to decode visual fMRI signals into phrases or sentences when natural images are being watched. The PT-LDM consists of an image-encoder, a fMRI encoder and a language-decoder. The results showed that phrases and sentences were successfully generated from visual activities. Similarity analysis showed that three often-used evaluation indexes BLEU, ROUGE and CIDEr reached 0.182, 0.197 and 0.680 averagely between the generated texts and the corresponding annotated texts in the testing set respectively, significantly higher than the baseline. Moreover, we found that higher visual areas usually had better performance than lower visual areas and the contribution curve of visual response patterns in language decoding varied at successively different time points. Our findings demonstrate that the neural representations elicited in visual cortices when scenes are being viewed have already contained semantic information that can be utilized to generate human language. Our study shows potential application of language-based brain–machine interfaces in the future, especially for assisting aphasics in communicating more efficiently with fMRI signals. © 2021 Elsevier Ltd}, keywords = {Brain; Human computer interaction; Semantics; Signal encoding; Visual languages; Computer interaction; Decoding algorithm; Language decoding; Natural images; Neural activity; Neural decoding; Neural signals; Progressive transfer; Visual activity; Visual areas; Decoding; adult; algorithm; Article; electroencephalogram; female; functional magnetic resonance imaging; human; human computer interaction; language; language processing; male; model; semantics; vision; visual cortex; visual evoked potential; artificial intelligence; brain mapping; nuclear magnetic resonance imaging; Algorithms; Artificial Intelligence; Brain Mapping; Humans; Language; Magnetic Resonance Imaging},
correspondence_address = {H. Yan; The Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, School of Life Science and Technology, University of Electronic Science and Technology of China, Chengdu, 610054, China; email: hmyan@uestc.edu.cn},
publisher = {Elsevier Ltd},
issn = {08936080; 18792782},
coden = {NNETE},
pmid = {34478941},
language = {English},
abbrev_source_title = {Neural Netw.},
type = {Article}}
@article{Kuratomi2021Realtime,
author = {Kuratomi, Takeru and Palmer, Jason A. and Chen, Peng and Jiang, Yinlai and Yokoi, Hiroshi and Hirata, Masayuki},
title = {Real-time 3D control of a robot arm based on a brain-machine interface using intracranial EEG},
year = {2021},
journal = {Transactions of Japanese Society for Medical and Biological Engineering},
volume = {Annual 59},
number = {Proc},
pages = {638 - 640},
doi = {10.11239/jsmbe.Annual59.638},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135136189&doi=10.11239%2Fjsmbe.Annual59.638&partnerID=40&md5=32c399c59c319d1f2e6fb610f6d033b4},
affiliations = {Graduate School of Medicine, Department of Neurological Diagnosis and Restoration, Suita, Osaka, Japan; The University of Electro-Communications, Department of Mechanical Engineering and Intelligent Systems, Chofu, Tokyo, Japan},
abstract = {Objective: We aimed to estimate the velocity vector of the wrist based on the human intracranial electroencephalography (iEEG) and to control a robot arm three-dimensionally. Methods: An epilepsy patient with implanted intracranial electrodes participated in this study. IEEGs were recorded while the patient imitated simulated movement of a robot arm. Independent component analysis (ICA) and partial least squares regression (PLS) were used to extract components specifically distributed over the sensorimotor areas. These components were used to estimate the wrist velocity vector using support vector regression (SVR). The robot arm was three-dimensionally controlled based on the estimated velocity using ROS where inverse kinematics were implemented. Results: We developed the system for real-time three-dimensional control of a robot arm by estimating the wrist velocity based on iEEG using ICA, PLS and SVR. Conclusion: Three-dimensional velocity control based on iEEG, ICA, PLS and SVR is feasible for real-time control of a robot arm. © 2021, Japan Soc. of Med. Electronics and Biol. Engineering. All rights reserved.}, keywords = {Brain computer interface; Electroencephalography; Electrophysiology; Independent component analysis; Inverse kinematics; Least squares approximations; Principal component analysis; Robotic arms; Velocity; Bio-robotics; Brain-machine interface; Independent components analysis; Intracranial electroencephalogram; Machine interfaces; Neural decoding; Partial least square regression; Real- time; Robot arms; Support vector regressions; Real time control; Article; electroencephalogram; electroencephalography; epilepsy; human; independent component analysis; intracranial electroencephalography; kinematics; partial least squares regression; support vector machine; wrist},
correspondence_address = {T. Kuratomi; Neurological Diagnosis and Restoration, Graduate School of Medicine, Osaka University, Japan; email: t-kuratomi@ndr.med.osaka-u.ac.jp},
publisher = {Japan Soc. of Med. Electronics and Biol. Engineering},
issn = {1347443X; 18814379},
language = {Japanese},
abbrev_source_title = {Trans. Jpn. Soc. Med. Biol. Eng.},
type = {Article}}
@article{Livezey2021Deep,
author = {Livezey, Jesse A. and Glaser, Joshua I.},
title = {Deep learning approaches for neural decoding across architectures and recording modalities},
year = {2021},
journal = {Briefings in Bioinformatics},
volume = {22},
number = {2},
pages = {1577 - 1591},
doi = {10.1093/bib/bbaa355},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103474539&doi=10.1093%2Fbib%2Fbbaa355&partnerID=40&md5=e334c26415b481ceb2c943d8ecd3dcf9},
affiliations = {University of California, Berkeley, Berkeley, CA, United States; Columbia University, New York, NY, United States},
abstract = {Decoding behavior, perception or cognitive state directly from neural signals is critical for brain-computer interface research and an important tool for systems neuroscience. In the last decade, deep learning has become the state-of-the-art method in many machine learning tasks ranging from speech recognition to image segmentation. The success of deep networks in other domains has led to a new wave of applications in neuroscience. In this article, we review deep learning approaches to neural decoding. We describe the architectures used for extracting useful features from neural recording modalities ranging from spikes to functional magnetic resonance imaging. Furthermore, we explore how deep learning has been leveraged to predict common outputs including movement, speech and vision, with a focus on how pretrained deep networks can be incorporated as priors for complex decoding targets like acoustic speech or images. Deep learning has been shown to be a useful tool for improving the accuracy and flexibility of neural decoding across a wide range of tasks, and we point out areas for future scientific development. © 2020 The Author(s) 2020. Published by Oxford University Press. All rights reserved.},
keywords = {deep learning; functional magnetic resonance imaging; human; human experiment; review; science; speech; spike; vision; action potential; electrocorticography; electroencephalography; image processing; metabolism; nuclear magnetic resonance imaging; procedures; calcium; Action Potentials; Calcium; Deep Learning; Electrocorticography; Electroencephalography; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Speech; Vision, Ocular},
correspondence_address = {J.A. Livezey; University of California, Berkeley, United States; email: jlivezey@lbl.gov; J.I. Glaser; Department of Statistics at Columbia University, United States; email: j.glaser@columbia.edu; J.A. Livezey; University of California, Berkeley, United States; email: jlivezey@lbl.gov; J.I. Glaser; Department of Statistics at Columbia University, United States; email: j.glaser@columbia.edu; J.A. Livezey; University of California, Berkeley, United States; email: jlivezey@lbl.gov},
publisher = {Oxford University Press},
issn = {14774054; 14675463},
pmid = {33372958},
language = {English},
abbrev_source_title = {Brief. Bioinform.},
type = {Review}}
@inproceedings{Mishra2021European,
author = {Mishra, Rahul and Sharma, Krishan and Bhavsar, Arnav},
booktitle = {2021 29th European Signal Processing Conference (EUSIPCO)},
title = {Visual Brain Decoding for Short Duration EEG Signals},
year = {2021},
volume = {},
number = {},
pages = {1226--1230},
abstract = {In this work, we propose a CNN-based approach for classification of short duration EEG signals for visual brain decoding. These signals are captured for a visual perception task by showing digit images on a computer screen, and the task involves classification of the EEG signals into 10 classes, corresponding to the digits shown. The captured EEG signals are of very short duration (approx. 2sec), which are typically very noisy. We use a correlation based technique for the removal of highly noisy samples. Further, a sample refinement approach for the selection of relevant channels is also proposed. Both these steps constitute the data refinement process, which we demonstrate has a significant effect on the CNN classification performance. We validate the proposed approach on a publicly available MindBigData (The “MNIST” of Brain Digits) dataset.},
keywords = {Deep learning;Visualization;Correlation;Europe;Signal processing;Electroencephalography;Decoding;CNN;Correlation coefficients;EEG Classification;visual brain decoding},
doi = {10.23919/EUSIPCO54536.2021.9616192},
issn = {2076-1465},
month = {Aug}}
@article{Narayanan2021Miniaturization,
author = {Narayanan, Abhijith Mundanad and Zink, Rob and Bertrand, Alexander},
title = {EEG miniaturization limits for stimulus decoding with EEG sensor networks},
year = {2021},
journal = {Journal of Neural Engineering},
volume = {18},
number = {5},
pages = {},
doi = {10.1088/1741-2552/ac2629},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117533601&doi=10.1088%2F1741-2552%2Fac2629&partnerID=40&md5=2baf95484d80cfbcd05c742818173a7a},
affiliations = {KU Leuven, Signal Processing and Data Analytics, Leuven, Vlaams-Brabant, Belgium; KU Leuven, Leuven, Vlaams-Brabant, Belgium},
abstract = {Objective. Unobtrusive electroencephalography (EEG) monitoring in everyday life requires the availability of highly miniaturized EEG devices (mini-EEGs), which ideally consist of a wireless node with a small scalp area footprint, in which the electrodes, amplifier and wireless radio are embedded. By attaching a multitude of mini-EEGs at relevant positions on the scalp, a wireless 'EEG sensor network' (WESN) can be formed. However, each mini-EEG in the network only has access to its own local electrodes, thereby recording local scalp potentials with short inter-electrode distances. This is unlike using traditional cap-EEG, which by the virtue of re-referencing can measure EEG across arbitrarily large distances on the scalp. We evaluate the implications and limitations of such far-driven miniaturization on neural decoding performance. Approach. We collected 255-channel EEG data in an auditory attention decoding (AAD) task. As opposed to previous studies with a lower channel density, this new high-density dataset allows emulation of mini-EEGs with inter-electrode distances down to 1 cm in order to identify and quantify the lower bound on miniaturization for EEG-based stimulus decoding. Main results. We demonstrate that the performance remains reasonably stable for inter-electrode distances down to 3 cm, but decreases quickly for shorter distances if the mini-EEG nodes can be placed at optimal scalp locations and orientations selected by a data-driven algorithm. Significance. The results indicate the potential for the use of mini-EEGs in a WESN context for AAD applications and provide guidance on inter-electrode distances while designing such devices for neuro-steered hearing devices. © 2021 IOP Publishing Ltd.}, keywords = {Audition; Decoding; Electrodes; Electroencephalography; Miniature instruments; Sensor networks; Signal processing; Auditory attention; Auditory attention decoding; Electrode distances; Local electrodes; Miniaturisation; Neural decoding; Neural signal processing; Sensors network; Wireless nodes; Wireless radios; Electrophysiology; algorithm; Article; autocorrelation; controlled study; cross validation; degree of freedom; density; electroencephalography; miniaturization; nerve potential; probability; stimulus; attention; electrode; scalp; Attention; Miniaturization; Scalp},
correspondence_address = {A. Mundanad Narayanan; KU Leuven, Dept. of Electrical Engineering (ESAT), Stadius Center for Dynamical Systems, Signal Processing and Data Analytics (STADIUS), Leuven, Kasteelpark Arenberg 10, B-3001, Belgium; email: abhijith@esat.kuleuven.be},
publisher = {IOP Publishing Ltd},
issn = {17412560},
pmid = {34517358},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Narayanan2021Optimal,
author = {Narayanan, Abhijith Mundanad and Patrinos, Panagiotis (Panos) and Bertrand, Alexander},
title = {Optimal Versus Approximate Channel Selection Methods for EEG Decoding with Application to Topology-Constrained Neuro-Sensor Networks},
year = {2021},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {29},
pages = {92 - 102},
doi = {10.1109/TNSRE.2020.3035499},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101972518&doi=10.1109%2FTNSRE.2020.3035499&partnerID=40&md5=3cec024c5b39b28bf60cb0c45c777f58},
affiliations = {KU Leuven, Leuven, Vlaams-Brabant, Belgium},
abstract = {Channel selection or electrode placement for neural decoding is a commonly encountered problem in electroencephalography (EEG). Since evaluating all possible channel combinations is usually infeasible, one usually has to settle for heuristic methods or convex approximations without optimality guarantees. To date, it remains unclear how large the gap is between the selection made by these approximate methods and the truly optimal selection. The goal of this paper is to quantify this optimality gap for several state-of-The-Art channel selection methods in the context of least-squares based neural decoding. To this end, we reformulate the channel selection problem as a mixed-integer quadratic program (MIQP), which allows the use of efficient MIQP solvers to find the optimal channel combination in a feasible computation time for up to 100 candidate channels. As this reveals the exact solution to the combinatorial problem, it allows to quantify the performance losses when using state-of-The-Art sub-optimal (yet faster) channel selection methods. In a context of auditory attention decoding, we find that a greedy channel selection based on the utility metric does not show a significant optimality gap compared to optimal channel selection, whereas other state-of-The-Art greedy or ${l}_{{1}}$-norm penalized methods do show a significant loss in performance. Furthermore, we demonstrate that the MIQP formulation also provides a natural way to incorporate topology constraints in the selection, e.g., for electrode placement in neuro-sensor networks with galvanic separation constraints. Furthermore, a combination of this utility-based greedy selection with an MIQP solver allows to perform a topology constrained electrode placement, even in large scale problems with more than 100 candidate positions. © 2001-2011 IEEE.}, keywords = {Decoding; Electrodes; Electroencephalography; Electrophysiology; Integer programming; Least squares approximations; Quadratic programming; Sensor networks; Topology; Candidate positions; Combinatorial problem; Convex approximation; Galvanic separations; Large-scale problem; Mixed integer quadratic program; Optimal channel selections; Topology constraint; Heuristic methods; Article; auditory network; channel selection; coding; electrode implantation; electroencephalogram; electroencephalography; galvanic current; hearing; human; human experiment; measurement accuracy; nerve cell network; stimulus; time series analysis; validation process; attention; electrode; least square analysis; Attention; Humans; Least-Squares Analysis},
correspondence_address = {A.M. Narayanan; Department of Electrical Engineering (ESAT), Stadius Center for Dynamical Systems, Signal Processing and Data Analytics (STADIUS), Ku Leuven, Leuven, Belgium; email: abhijith@esat.kuleuven.be},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {33141674},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Peterson2021Generalized,
author = {Peterson, Steven M. and Steine-Hanson, Zoe and Davis, Nathan and Rao, Rajesh P.N. and Brunton, Bingni Wen},
title = {Generalized neural decoders for transfer learning across participants and recording modalities},
year = {2021},
journal = {Journal of Neural Engineering},
volume = {18},
number = {2},
pages = {},
doi = {10.1088/1741-2552/abda0b},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102950572&doi=10.1088%2F1741-2552%2Fabda0b&partnerID=40&md5=5e9c2deb9d8b26bfb6ecec622e39e6dc},
affiliations = {University of Washington, Department of Biology, Seattle, WA, United States; University of Washington, Seattle, WA, United States; UW College of Engineering, Seattle, WA, United States; UW College of Engineering, Seattle, WA, United States; University of Washington, Center for Neurotechnology, Seattle, WA, United States},
abstract = {Objective. Advances in neural decoding have enabled brain-computer interfaces to perform increasingly complex and clinically-relevant tasks. However, such decoders are often tailored to specific participants, days, and recording sites, limiting their practical long-term usage. Therefore, a fundamental challenge is to develop neural decoders that can robustly train on pooled, multi-participant data and generalize to new participants. Approach. We introduce a new decoder, HTNet, which uses a convolutional neural network with two innovations: (a) a Hilbert transform that computes spectral power at data-driven frequencies and (b) a layer that projects electrode-level data onto predefined brain regions. The projection layer critically enables applications with intracranial electrocorticography (ECoG), where electrode locations are not standardized and vary widely across participants. We trained HTNet to decode arm movements using pooled ECoG data from 11 of 12 participants and tested performance on unseen ECoG or electroencephalography (EEG) participants; these pretrained models were also subsequently fine-tuned to each test participant. Main results. HTNet outperformed state-of-the-art decoders when tested on unseen participants, even when a different recording modality was used. By fine-tuning these generalized HTNet decoders, we achieved performance approaching the best tailored decoders with as few as 50 ECoG or 20 EEG events. We were also able to interpret HTNet's trained weights and demonstrate its ability to extract physiologically-relevant features. Significance. By generalizing to new participants and recording modalities, robustly handling variations in electrode placement, and allowing participant-specific fine-tuning with minimal data, HTNet is applicable across a broader range of neural decoding applications compared to current state-of-the-art decoders. © 2021 The Author(s). Published by IOP Publishing Ltd.}, keywords = {Brain; Brain computer interface; Convolutional neural networks; Electrodes; Electroencephalography; Electrophysiology; Mathematical transformations; Multilayer neural networks; Transfer learning; Tuning; Brain regions; Electrocorticography (ECoG); Electrode placement; Hilbert transform; Neural decoding; Relevant features; Spectral power; State of the art; Decoding; adult; arm movement; Article; artifact; clinical article; controlled study; convolutional neural network; data analysis software; elbow flexion; electrocorticography; electroencephalography; epilepsy; female; human; left hemisphere; male; priority journal; random forest; right hemisphere; transfer of learning; videorecording; wrist; x-ray computed tomography; brain computer interface; machine learning; procedures; Brain-Computer Interfaces; Electrocorticography; Humans; Machine Learning; Neural Networks, Computer},
correspondence_address = {B.W. Brunton; Department of Biology, University of Washington, Seattle, 98195, United States; email: bbrunton@uw.edu},
publisher = {IOP Publishing Ltd},
issn = {17412560},
pmid = {33418552},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Saeidi2021Neural,
author = {Saeidi, Maham and Karwowski, Waldemar and Farahani, Farzad Vasheghani and Fiok, Krzysztof and Redha, Taiar and Hancock, Peter A. and Al-Juaid, Awad},
title = {Neural decoding of eeg signals with machine learning: A systematic review},
year = {2021},
journal = {Brain Sciences},
volume = {11},
number = {11},
pages = {},
doi = {10.3390/brainsci11111525},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119693836&doi=10.3390%2Fbrainsci11111525&partnerID=40&md5=417a4cec14aa19439e13eb289665ce04},
affiliations = {College of Engineering and Computer Science, Computational Neuroergonomics Laboratory, Orlando, FL, United States; Johns Hopkins University, Department of Biostatistics, Baltimore, MD, United States; MATériaux et Ingénierie Mécanique (MATIM), Reims, Grand Est, France; University of Central Florida, Department of Psychology, Orlando, FL, United States; Taif University, Department of Industrial Engineering, Taif, Makkah al Mukarramah, Saudi Arabia},
abstract = {Electroencephalography (EEG) is a non-invasive technique used to record the brain’s evoked and induced electrical activity from the scalp. Artificial intelligence, particularly machine learning (ML) and deep learning (DL) algorithms, are increasingly being applied to EEG data for pattern analysis, group membership classification, and brain-computer interface purposes. This study aimed to systematically review recent advances in ML and DL supervised models for decoding and classifying EEG signals. Moreover, this article provides a comprehensive review of the state-of-the-art techniques used for EEG signal preprocessing and feature extraction. To this end, several academic databases were searched to explore relevant studies from the year 2000 to the present. Our results showed that the application of ML and DL in both mental workload and motor imagery tasks has received substantial attention in recent years. A total of 75% of DL studies applied convolutional neural networks with various learning algorithms, and 36% of ML studies achieved competitive accuracy by using a support vector machine algorithm. Wavelet transform was found to be the most common feature extraction method used for all types of tasks. We further examined the specific feature extraction methods and end classifier recommendations discovered in this systematic review. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.}, keywords = {attention; brain; convolutional neural network; deep learning; electroencephalography; feature extraction; human; human experiment; imagery; learning algorithm; review; support vector machine; systematic review; wavelet transform; workload},
correspondence_address = {M. Saeidi; Computational Neuroergonomics Laboratory, Department of Industrial Engineering and Management Systems, University of Central Florida, Orlando, 32816, United States; email: msaeidi@knights.ucf.edu; W. Karwowski; Computational Neuroergonomics Laboratory, Department of Industrial Engineering and Management Systems, University of Central Florida, Orlando, 32816, United States; email: wkar@ucf.edu},
publisher = {MDPI},
issn = {20763425},
language = {English},
abbrev_source_title = {Brain Sci.},
type = {Review}}
@conference{Samsonova2021Case,
author = {Samsonova, Alexandra and Devereux, Barry J. and Karakonstantis, Georgios and Mukhanov, Lev},
title = {A case study on profiling of an EEG-based brain decoding interface on Cloud and Edge servers},
year = {2021},
pages = {90 - 95},
doi = {10.1109/SmartCloud52277.2021.00023},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124028503&doi=10.1109%2FSmartCloud52277.2021.00023&partnerID=40&md5=9dde0b197bb9f9bc7e86f51620416bdb},
affiliations = {Queen's University Belfast, Belfast, Northern Ireland, United Kingdom},
abstract = {Brain-Computer Interfaces (BCIs) enable converting the brain electrical activity of an interface user to the user commands. BCI research studies demonstrated encouraging results in different areas such as neurorehabilitation, control of artificial limbs, control of computer environments, communication and detection of diseases. Most of BCIs use scalp-electroencephalography (EEG), which is a non-invasive method to capture the brain activity. Although EEG monitoring devices are available in the market, these devices are generally lab-oriented and expensive. Day-to-day use of BCIs is impractical at this time due to the complex techniques required for data preprocessing and signal analysis. This implies that BCI technologies should be improved to facilitate its widespread adoption in Cloud and Edge datacenters. This paper presents a case study on profiling the accuracy and performance of a brain-computer interface which runs on typical Cloud and Edge servers. In particular, we investigate how the accuracy and execution time of the preprocessing phase, i.e. the brain signal filtering phase, of a brain-computer interface varies when processing static and live streaming data obtained in real time BCI devices. We identify the optimal size of the packets for sampling brain signals which provides the best trade-off between the accuracy and performance. Finally, we discuss the pros and cons of using typical Cloud and Edge servers to perform the BCI filtering phase. © 2021 IEEE.}, keywords = {Brain; Disease control; Economic and social effects; Electroencephalography; Electrophysiology; Noninvasive medical procedures; Signal processing; Brain decoding; Brain electrical activity; Brain signals; Case-studies; Cloud servers; Decoding interfaces; Edge; Edge server; Performance; Profiling; Brain computer interface},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
isbn = {9781665443746},
language = {English},
abbrev_source_title = {Proc. - IEEE Int. Conf. Smart Cloud, SmartCloud},
type = {Conference paper}}
@article{Valeriani2021Editorial,
author = {Valeriani, Davide and Ayaz, Hasan and Kosmyna, Nataliya and Poli, Riccardo and Maes, Pattie},
title = {Editorial: Neurotechnologies for Human Augmentation},
year = {2021},
journal = {Frontiers in Neuroscience},
volume = {15},
pages = {},
doi = {10.3389/fnins.2021.789868},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120691694&doi=10.3389%2Ffnins.2021.789868&partnerID=40&md5=2b2894c3fb74ab2b9dd4ef308b9714ca},
affiliations = {Neurable, Boston, MA, United States; Drexel University, School of Biomedical Engineering, Philadelphia, PA, United States; MIT Media Lab, Cambridge, MA, United States; University of Essex, Colchester, Essex, United Kingdom}, keywords = {accuracy; age; arousal; brain depth stimulation; brain function; clinical effectiveness; convolutional neural network; deep learning; Editorial; electroencephalogram; functional status; hearing; human; human augmentation; imagery; metamemory; motor evoked potential; motor learning; motor performance; muscle strength; neural decoding; neurofeedback; neurotechnology; perception; technology; transcranial direct current stimulation; transcranial static magnetic field stimulation; virtual reality; work engagement},
correspondence_address = {D. Valeriani; Neurable Inc, Boston, United States; email: davide.valeriani@gmail.com},
publisher = {Frontiers Media S.A.},
issn = {1662453X; 16624548},
language = {English},
abbrev_source_title = {Front. Neurosci.},
type = {Editorial}}
@article{Wakita2021Photorealistic,
author = {Wakita, Suguru and Orima, Taiki and Motoyoshi, Isamu},
title = {Photorealistic Reconstruction of Visual Texture From EEG Signals},
year = {2021},
journal = {Frontiers in Computational Neuroscience},
volume = {15},
pages = {},
doi = {10.3389/fncom.2021.754587},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120531286&doi=10.3389%2Ffncom.2021.754587&partnerID=40&md5=0ab74dc3e7fe164f9c559c8075732fcf},
affiliations = {The University of Tokyo, Department of Life Science, Tokyo, Japan; Japan Society for the Promotion of Science, Tokyo, Tokyo, Japan},
abstract = {Recent advances in brain decoding have made it possible to classify image categories based on neural activity. Increasing numbers of studies have further attempted to reconstruct the image itself. However, because images of objects and scenes inherently involve spatial layout information, the reconstruction usually requires retinotopically organized neural data with high spatial resolution, such as fMRI signals. In contrast, spatial layout does not matter in the perception of “texture,” which is known to be represented as spatially global image statistics in the visual cortex. This property of “texture” enables us to reconstruct the perceived image from EEG signals, which have a low spatial resolution. Here, we propose an MVAE-based approach for reconstructing texture images from visual evoked potentials measured from observers viewing natural textures such as the textures of various surfaces and object ensembles. This approach allowed us to reconstruct images that perceptually resemble the original textures with a photographic appearance. The present approach can be used as a method for decoding the highly detailed “impression” of sensory stimuli from brain activity. © © 2021 Wakita, Orima and Motoyoshi.}, keywords = {Brain; Decoding; Electrophysiology; Image coding; Image reconstruction; Image resolution; Image texture; Network coding; Neurons; Textures; Auto encoders; Brain decoding; Deep neural network; EEG signals; Multi-modal; Multimodal variational auto encoder; Object and scenes; Photo-realistic; Spatial layout; Visual texture; Deep neural networks; article; autoencoder; brain function; deep neural network; electroencephalogram; human; perception; visual cortex; visual evoked potential},
correspondence_address = {S. Wakita; Department of Life Sciences, The University of Tokyo, Tokyo, Japan; email: suguruwakita.ac@gmail.com},
publisher = {Frontiers Media S.A.},
issn = {16625188},
language = {English},
abbrev_source_title = {Front. Comput. Neurosci.},
type = {Article}}
@article{Wang2021Neural,
author = {Wang, Pengpai and Zhou, Yueying and Li, Zhongnian and Huang, Shuo and Zhang, Daoqiang},
title = {Neural Decoding of Chinese Sign Language with Machine Learning for Brain-Computer Interfaces},
year = {2021},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {29},
pages = {2721 - 2732},
doi = {10.1109/TNSRE.2021.3137340},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122106995&doi=10.1109%2FTNSRE.2021.3137340&partnerID=40&md5=da9de220b2d7a16c6df95d1942ab13a5},
affiliations = {Nanjing University of Aeronautics and Astronautics, College of Computer Science and Technology, Nanjing, Jiangsu, China},
abstract = {Limb motion decoding is an important part of brain-computer interface (BCI) research. Among the limb motion, sign language not only contains rich semantic information and abundant maneuverable actions but also provides different executable commands. However, many researchers focus on decoding the gross motor skills, such as the decoding of ordinary motor imagery or simple upper limb movements. Here we explored the neural features and decoding of Chinese sign language from electroencephalograph (EEG) signal with motor imagery and motor execution. Sign language not only contains rich semantic information, but also has abundant maneuverable actions, and provides us with more different executable commands. In this paper, twenty subjects were instructed to perform movement execution and movement imagery based on Chinese sign language. Seven classifiers are employed to classify the selected features of sign language EEG. L1 regularization is used to learn and select features that contain more information from the mean, power spectral density, sample entropy, and brain network connectivity. The best average classification accuracy of the classifier is 89.90% (imagery sign language is 83.40%). These results have shown the feasibility of decoding between different sign languages. The source location reveals that the neural circuits involved in sign language are related to the visual contact area and the pre-movement area. Experimental evaluation shows that the proposed decoding strategy based on sign language can obtain outstanding classification results, which provides a certain reference value for the subsequent research of limb decoding based on sign language. © 2001-2011 IEEE.}, keywords = {Biomedical signal processing; Classification (of information); Decoding; Electroencephalography; Electromyography; Electrophysiology; Gesture recognition; Image recognition; Interfaces (computer); Learning systems; Semantics; Spectral density; Assistive technology; Brain-computer interface; Chinese sign language recognition; Electroencephalograph; Feature learning; Gestures recognition; License; Limb motion decoding; Limb motions; Sign language; Brain computer interface; algorithm; Article; artificial neural network; brain depth stimulation; Chinese; cognition; computer model; connectome; convolutional neural network; data processing; discriminant analysis; electroencephalogram; electroencephalography; electromyography; electrooculogram; event related potential; feature extraction; feature learning (machine learning); female; human; human experiment; impedance; machine learning; male; Mini Mental State Examination; nerve cell network; neural decoding; normal human; sign language; signal processing; spiking neural network; support vector machine; training; visual information; brain computer interface; China; imagination; movement (physiology); Brain-Computer Interfaces; Humans; Imagination; Machine Learning; Movement; Sign Language},
correspondence_address = {P. Wang; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; email: pengpaiwang@nuaa.edu.cn},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {34932480},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Wang2021Detecting,
author = {Wang, Yubo and Wan, Chenghao and Zhang, Yun and Zhou, Yu and Wang, Haidong and Yan, Fei and Song, Dawei and Du, Ruini and Wang, Qiang and Huang, Liyu},
title = {Detecting Connected Consciousness during Propofol-Induced Anesthesia Using EEG Based Brain Decoding},
year = {2021},
journal = {International Journal of Neural Systems},
volume = {31},
number = {6},
pages = {},
doi = {10.1142/S0129065721500210},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106007361&doi=10.1142%2FS0129065721500210&partnerID=40&md5=cb86b6c84566ca033db4e3c64ab3c421},
affiliations = {Xidian University, School of Life Science and Technology, Xi'an, Shaanxi, China; The First Hospital of Xian Jiaotong University, Center for Brain Science, Xi'an, Shaanxi, China},
abstract = {Connected consciousness refers to the state when external stimuli can enter into the stream of our consciousness experience. Emerging evidence suggests that although patients may not respond behaviorally to external stimuli during anesthesia, they may be aware of their surroundings. In this work, we investigated whether EEG based brain decoding could be used for detecting connected consciousness in the absence of behavioral responses during propofol infusion. A total of 14 subjects participated in our experiment. Subjects were asked to discriminate two types of auditory stimuli with a finger press during an ultraslow propofol infusion. We trained an EEG based brain decoding model using data collected in the awakened state using the same auditory stimuli and tested the model on data collected during the propofol infusion. The model provided a correct classification rate (CCR) of 78.74 ± 11.15% when subjects were able to respond to the stimuli during the propofol infusion. The CCR dropped to 67.29 ± 8.21% when subjects ceased responding and further decreased to 58.11 ± 8.15% when we increased the propofol concentration by another 0.2 μg/ml. After terminating the propofol infusion, we observed that the CCR rebounded to 65.31 ± 7.11% before the subjects regained consciousness. With the classification results, we provided evidence that loss of consciousness is a gradual process and may progress from full consciousness to connected consciousness and then to disconnected consciousness. © 2021 World Scientific Publishing Company.}, keywords = {Anesthesiology; Auditory stimuli; Behavioral response; Brain decoding; Classification rates; Classification results; External stimulus; Loss of consciousness; Propofol concentration; Decoding; propofol; anesthesia; brain; consciousness; electroencephalography; human; Anesthesia; Brain; Consciousness; Electroencephalography; Humans; Propofol},
correspondence_address = {L. Huang; School of Life Science and Technology, Xidian University, Xi'an, China; email: huangly@mail.xidian.edu.cn},
publisher = {World Scientific},
issn = {01290657; 17936462},
pmid = {33970056},
language = {English},
abbrev_source_title = {Int. J. Neural Syst.},
type = {Article}}
@article{Wei2021Editorial,
author = {Wei, Chunshu and Keller, Corey J. and Li, Junhua and Lin, Yuanpin and Nakanishi, Masaki and Wagner, Johanna and Wei, Wu and Zhang, Yu and Jung, Tzyy Ping},
title = {Editorial: Inter- and Intra-subject Variability in Brain Imaging and Decoding},
year = {2021},
journal = {Frontiers in Computational Neuroscience},
volume = {15},
pages = {},
doi = {10.3389/fncom.2021.791129},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121348884&doi=10.3389%2Ffncom.2021.791129&partnerID=40&md5=31dbdd6b504c0da614c41a6fdc20c9e5},
affiliations = {National Yang Ming Chiao Tung University, Department of Computer Science, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Institute of Education, Hsinchu, Taiwan; National Yang Ming Chiao Tung University, Institute of Electrical and Control Engineering, Hsinchu, Taiwan; Stanford University, Department of Psychiatry and Behavioral Sciences, Stanford, CA, United States; University of Essex, Colchester, Essex, United Kingdom; National Sun Yat-Sen University, Institute of Medical Science and Technology, Kaohsiung, Taiwan; National Sun Yat-Sen University, Kaohsiung, Taiwan; Institute for Neural Computation, La Jolla, CA, United States; P.C. Rossin College of Engineering & Applied Science, Bethlehem, PA, United States}, keywords = {Alzheimer disease; biological variation; brain decoding; brain function; brain metabolism; causality; corpus striatum; data analysis; DNA polymorphism; dorsolateral prefrontal cortex; early diagnosis; Editorial; electroencephalography; evoked response; frontal lobe; functional connectivity; functional magnetic resonance imaging; functional near-infrared spectroscopy; glucose metabolism; GRIN1 gene; GRIN2 gene; human; intersubject variability; intrasubject variability; machine learning; magnetoencephalography; major depression; mental stress; metabolic rate; mild cognitive impairment; Montreal Imaging Stress Task; nerve cell network; neuroimaging; schizophrenia; stress assessment; supplementary motor area; test retest reliability; thalamus; transfer of learning},
correspondence_address = {C.-S. Wei; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; email: wei@nycu.edu.tw; T.-P. Jung; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; email: tpjung@ucsd.edu},
publisher = {Frontiers Media S.A.},
issn = {16625188},
language = {English},
abbrev_source_title = {Front. Comput. Neurosci.},
type = {Editorial}}
@article{Yokoyama2021Neural,
author = {Yokoyama, Hikaru and Kaneko, Naotsugu and Watanabe, Katsumi and Nakazawa, Kimitaka},
title = {Neural decoding of gait phases during motor imagery and improvement of the decoding accuracy by concurrent action observation},
year = {2021},
journal = {Journal of Neural Engineering},
volume = {18},
number = {4},
pages = {},
doi = {10.1088/1741-2552/ac07bd},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111167084&doi=10.1088%2F1741-2552%2Fac07bd&partnerID=40&md5=c6cddbd9ee79c0cf781556b7380d49f6},
affiliations = {Tokyo University of Agriculture and Technology, Department of Electrical and Electronic Engineering, Fuchu, Tokyo, Japan; Japan Society for the Promotion of Science, Tokyo, Tokyo, Japan; The University of Tokyo, Department of Life Science, Tokyo, Japan; Waseda University, Faculty of Science and Engineering, Tokyo, Japan; UNSW Sydney, Design and Architecture, Sydney, NSW, Australia},
abstract = {Objective. Brain decoding of motor imagery (MI) not only is crucial for the control of neuroprosthesis but also provides insights into the underlying neural mechanisms. Walking consists of stance and swing phases, which are associated with different biomechanical and neural control features. However, previous knowledge on decoding the MI of gait is limited to simple information (e.g. the classification of 'walking' and 'rest'). Approach. Here, we investigated the feasibility of electroencephalogram (EEG) decoding of the two gait phases during the MI of walking and whether the combined use of MI and action observation (AO) would improve decoding accuracy. Main results. We demonstrated that the stance and swing phases could be decoded from EEGs during MI or AO alone. We also demonstrated the decoding accuracy during MI was improved by concurrent AO. The decoding models indicated that the improved decoding accuracy following the combined use of MI and AO was facilitated by the additional information resulting from the concurrent cortical activations related to sensorimotor, visual, and action understanding systems associated with MI and AO. Significance. This study is the first to show that decoding the stance versus swing phases during MI is feasible. The current findings provide fundamental knowledge for neuroprosthetic design and gait rehabilitation, and they expand our understanding of the neural activity underlying AO, MI, and AO + MI of walking. Novelty and significance Brain decoding of detailed gait-related information during motor imagery (MI) is important for brain-computer interfaces (BCIs) for gait rehabilitation. This study is the first to show the feasibility of EEG decoding of the stance versus swing phases during MI. We also demonstrated that the combined use of MI and action observation (AO) improves decoding accuracy, which is facilitated by the concurrent and synergistic involvement of the cortical activations for MI and AO. These findings extend the current understanding of neural activity and the combined effects of AO and MI and provide a basis for effective techniques for walking rehabilitation. © 2021 IOP Publishing Ltd.}, keywords = {Brain; Brain computer interface; Chemical activation; Classification (of information); Electroencephalography; Image enhancement; Neural prostheses; Neurons; Neurosurgery; Patient rehabilitation; Brain computer interfaces (BCIs); Cortical activation; Electro-encephalogram (EEG); Gait rehabilitation; Improve decoding; Neural mechanisms; Neuro-prosthesis; Simple informations; Decoding; adult; Article; electroencephalogram; feasibility study; gait; human; human experiment; imagery; male; motor activity; normal human; sensorimotor function; standing; vision; electroencephalography; imagination; walking; Gait; Imagery, Psychotherapy; Imagination; Walking},
correspondence_address = {K. Nakazawa; Department of Life Sciences, Graduate School of Arts and Sciences, University of Tokyo, Tokyo, 153-8902, Japan; email: nakazawa@idaten.c.u-tokyo.ac.jp},
publisher = {IOP Publishing Ltd},
issn = {17412560},
pmid = {34082405},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Baroni2020Converging,
author = {Baroni, Fabiano and Morillon, Benjamin and Trébuchon, Agnès S. and Liégeois-Chauvel, Catherine and Olasagasti, Itsaso and Giraud, Anne Lise},
title = {Converging intracortical signatures of two separated processing timescales in human early auditory cortex},
year = {2020},
journal = {NeuroImage},
volume = {218},
pages = {},
doi = {10.1016/j.neuroimage.2020.116882},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085235925&doi=10.1016%2Fj.neuroimage.2020.116882&partnerID=40&md5=74836f013f3f4f6be481a0c69e8a83f7},
affiliations = {Université de Genève, Department of Fundamental Neurosciences, Geneva, Switzerland; École Polytechnique Fédérale de Lausanne, School of Engineering, Lausanne, Switzerland; Institut de Neurosciences des Systèmes, Marseille, Provence-Alpes-Cote d'Azur, France; Hopital La Timone, Clinical Neurophysiology and Epileptology Department, Marseille, France; University of Pittsburgh School of Medicine, Pittsburgh, PA, United States},
abstract = {Neural oscillations in auditory cortex are argued to support parsing and representing speech constituents at their corresponding temporal scales. Yet, how incoming sensory information interacts with ongoing spontaneous brain activity, what features of the neuronal microcircuitry underlie spontaneous and stimulus-evoked spectral fingerprints, and what these fingerprints entail for stimulus encoding, remain largely open questions. We used a combination of human invasive electrophysiology, computational modeling and decoding techniques to assess the information encoding properties of brain activity and to relate them to a plausible underlying neuronal microarchitecture. We analyzed intracortical auditory EEG activity from 10 patients while they were listening to short sentences. Pre-stimulus neural activity in early auditory cortical regions often exhibited power spectra with a shoulder in the delta range and a small bump in the beta range. Speech decreased power in the beta range, and increased power in the delta-theta and gamma ranges. Using multivariate machine learning techniques, we assessed the spectral profile of information content for two aspects of speech processing: detection and discrimination. We obtained better phase than power information decoding, and a bimodal spectral profile of information content with better decoding at low (delta-theta) and high (gamma) frequencies than at intermediate (beta) frequencies. These experimental data were reproduced by a simple rate model made of two subnetworks with different timescales, each composed of coupled excitatory and inhibitory units, and connected via a negative feedback loop. Modeling and experimental results were similar in terms of pre-stimulus spectral profile (except for the iEEG beta bump), spectral modulations with speech, and spectral profile of information content. Altogether, we provide converging evidence from both univariate spectral analysis and decoding approaches for a dual timescale processing infrastructure in human auditory cortex, and show that it is consistent with the dynamics of a simple rate model. © 2020 The Author(s)}, keywords = {adult; article; auditory cortex; brain function; clinical article; computer model; electroencephalogram; female; human; machine learning; male; negative feedback; shoulder; spectroscopy; speech perception; computer simulation; electrocorticography; physiology; signal processing; Adult; Auditory Cortex; Computer Simulation; Electrocorticography; Female; Humans; Male; Signal Processing, Computer-Assisted; Speech Perception},
correspondence_address = {F. Baroni; Department of Fundamental Neuroscience, University of Geneva, Geneva, Switzerland; email: fabianobaroni@gmail.com},
publisher = {Academic Press Inc. apjcs@harcourt.com},
issn = {10959572; 10538119},
coden = {NEIME},
pmid = {32439539},
language = {English},
abbrev_source_title = {NeuroImage},
type = {Article}}
@article{Bouton2020Merging,
author = {Bouton, Chad E.},
title = {Merging brain-computer interface and functional electrical stimulation technologies for movement restoration},
year = {2020},
journal = {Handbook of Clinical Neurology},
volume = {168},
pages = {303 - 309},
doi = {10.1016/B978-0-444-63934-9.00022-6},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081035033&doi=10.1016%2FB978-0-444-63934-9.00022-6&partnerID=40&md5=9c806d5d430b44304563ba0b45c88f76},
affiliations = {Northwell Health System, Center for Bioelectronic Medicine, New York, NY, United States},
abstract = {BCI (brain-computer interface) and functional electrical stimulation (FES) technologies have advanced significantly over the last several decades. Recent efforts have involved the integration of these technologies with the goal of restoring functional movement in paralyzed patients. Implantable BCIs have provided neural recordings with increased spatial resolution and have been combined with sophisticated neural decoding algorithms and increasingly capable FES systems to advance efforts toward this goal. This chapter reviews historical developments that have occurred as the exciting fields of BCI and FES have evolved and now overlapped to allow new breakthroughs in medicine, targeting restoration of movement and lost function in users with disabilities. © 2020 Elsevier B.V.}, keywords = {brain; brain computer interface; electroencephalography; electrostimulation; human; movement (physiology); pathophysiology; physiology; procedures; spinal cord injury; Brain; Brain-Computer Interfaces; Electric Stimulation; Electroencephalography; Humans; Movement; Spinal Cord Injuries},
correspondence_address = {C.E. Bouton; Center for Bioelectronic Medicine, Feinstein Institute for Medical Research, Northwell Health, Manhasset, United States; email: boutonce@gmail.com},
publisher = {Elsevier B.V.},
issn = {22124152; 00729752},
isbn = {9780444514905; 9780444534866; 9780444518996; 9780444641892; 9780444520043; 9780444520104; 9780444513571; 9780444632470; 9780444518972; 9780444520050},
pmid = {32164861},
language = {English},
abbrev_source_title = {Handb. Clin. Neurol.},
type = {Book chapter}}
@article{Ceolini2020Braininformed,
author = {Ceolini, Enea and Hjortkjaer, Jens and Wong, Daniel D.E. and O'Sullivan, James A. and Raghavan, Vinay S. and Herrero, Jose L. and Mehta, Ashesh D. and Liu, Shihchii and Mesgarani, Nima},
title = {Brain-informed speech separation (BISS) for enhancement of target speaker in multitalker speech perception},
year = {2020},
journal = {NeuroImage},
volume = {223},
pages = {},
doi = {10.1016/j.neuroimage.2020.117282},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090141132&doi=10.1016%2Fj.neuroimage.2020.117282&partnerID=40&md5=d40330b0d69b3d3ad710e13362c29469},
affiliations = {Universität Zürich, Zurich, ZH, Switzerland; Technical University of Denmark, Department of Health Technology, Lyngby, Hovedstaden, Denmark; Hvidovre Hospital, Danish Research Centre for Magnetic Resonance, Hvidovre, Hovedstaden, Denmark; CNRS Centre National de la Recherche Scientifique, Paris, Ile-de-France, France; École Normale Supérieure, Département d'Etudes Cognitives, Paris, Ile-de-France, France; The Fu Foundation School of Engineering and Applied Science, New York, NY, United States; Columbia University, New York, NY, United States; Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Department of Neurosurgery, Hempstead, NY, United States},
abstract = {Hearing-impaired people often struggle to follow the speech stream of an individual talker in noisy environments. Recent studies show that the brain tracks attended speech and that the attended talker can be decoded from neural data on a single-trial level. This raises the possibility of “neuro-steered” hearing devices in which the brain-decoded intention of a hearing-impaired listener is used to enhance the voice of the attended speaker from a speech separation front-end. So far, methods that use this paradigm have focused on optimizing the brain decoding and the acoustic speech separation independently. In this work, we propose a novel framework called brain-informed speech separation (BISS) in which the information about the attended speech, as decoded from the subject's brain, is directly used to perform speech separation in the front-end. We present a deep learning model that uses neural data to extract the clean audio signal that a listener is attending to from a multi-talker speech mixture. We show that the framework can be applied successfully to the decoded output from either invasive intracranial electroencephalography (iEEG) or non-invasive electroencephalography (EEG) recordings from hearing-impaired subjects. It also results in improved speech separation, even in scenes with background noise. The generalization capability of the system renders it a perfect candidate for neuro-steered hearing-assistive devices. © 2020}, keywords = {Article; auditory network; brain informed speech separation; clinical article; controlled study; deep learning; electroencephalography; executive function; female; human; male; mathematical model; priority journal; separation technique; speech perception; adult; algorithm; auditory stimulation; brain; hearing impairment; middle aged; pathophysiology; physiology; signal processing; speech; Acoustic Stimulation; Adult; Algorithms; Brain; Deep Learning; Electroencephalography; Hearing Loss; Humans; Middle Aged; Signal Processing, Computer-Assisted; Speech Acoustics; Speech Perception},
correspondence_address = {E. Ceolini; University of Zürich and ETH Zürich, Institute of Neuroinformatics, Switzerland; email: enea.ceolini@ini.uzh.ch},
publisher = {Academic Press Inc. apjcs@harcourt.com},
issn = {10959572; 10538119},
coden = {NEIME},
pmid = {32828921},
language = {English},
abbrev_source_title = {NeuroImage},
type = {Article}}
@article{Duraivel2020Comparison,
author = {Duraivel, Suseendrakumar and Rao, Akshay T. and Lu, Charles W. and Bentley, J. Nicole and Stacey, William C. and Chestek, Cynthia A. and Patil, Parag G.},
title = {Comparison of signal decomposition techniques for analysis of human cortical signals},
year = {2020},
journal = {Journal of Neural Engineering},
volume = {17},
number = {5},
pages = {},
doi = {10.1088/1741-2552/abb63b},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092886593&doi=10.1088%2F1741-2552%2Fabb63b&partnerID=40&md5=146c2a5459369a4f5733c55e1c1d4f95},
affiliations = {Duke University, Durham, NC, United States; University of Michigan Medical School, Ann Arbor, MI, United States; University of Michigan, Ann Arbor, Ann Arbor, MI, United States; UAB Department of Neurosurgery, Birmingham, AL, United States; University of Michigan Medical School, Ann Arbor, MI, United States},
abstract = {Objective. Conventional neural signal analysis methods assume that features of interest are linear, time-invariant signals confined to well-delineated spectral bands. However, new evidence suggests that neural signals exhibit important non-stationary characteristics with ill-defined spectral distributions. These features pose a need for signal processing algorithms that can characterize temporal and spectral features of non-linear time series. This study compares the effectiveness of four algorithms in extracting neural information for use in decoding cortical signals: Fast Fourier Transform bandpass filtering (FFT), principal spectral component analysis (PSCA), wavelet analysis (WA), and empirical mode decomposition (EMD). Approach. Electrocorticographic signals were recorded from the motor and sensory cortex of two epileptic patients performing finger movements. Each signal processing algorithm was used to extract beta (10–30 Hz) and gamma (66–114 Hz) band power to detect thumb movement and decode finger flexions, respectively. Naïve-Bayes (NB), support vector machine (SVM), and linear discriminant analysis (LDA) classifiers using each signal were validated using leave-one-out cross-validation. Main results. Decoders using all four signal decompositions achieved above 90% average accuracy in finger movement detection using beta power. When decoding individual finger flexion using gamma, the PSCA NB classifiers achieved 78 ± 4% accuracy while FFT, WA, and EMD analysis achieved accuracies of 73 ± 8%, 68 ± 7%, and 62 ± 3% respectively, with similar results using SVM and LDA. Significance. These results illustrate the relative levels of useful information contributed by each decomposition method in the case of finger movement decoding, which can inform the development of effective neural decoding pipelines. Further analyses could compare performance using more specific non-sinusoidal features, such as transients and phase-amplitude coupling. © 2020 IOP Publishing Ltd}, keywords = {Decoding; Discriminant analysis; Fast Fourier transforms; Fourier series; Information filtering; Palmprint recognition; Signal distortion; Support vector machines; Wavelet decomposition; Electrocorticographic signals; Empirical Mode Decomposition; Leave-one-out cross validations; Linear discriminant analysis; Non stationary characteristics; Phaseamplitude couplings (PAC); Signal decomposition technique; Signal processing algorithms; Signal processing; adult; article; case report; clinical article; comparative effectiveness; discriminant analysis; electrocorticography; empirical mode decomposition; epileptic patient; female; filtration; Fourier transform; human; intermethod comparison; leave one out cross validation; male; pipeline; sensorimotor cortex; support vector machine; thumb; time series analysis; validation process; wavelet analysis; algorithm; Bayes theorem; brain cortex; electroencephalography; physiology; signal processing; Algorithms; Bayes Theorem; Cerebral Cortex; Electroencephalography; Humans; Signal Processing, Computer-Assisted; Support Vector Machine; Wavelet Analysis},
correspondence_address = {P.G. Patil; Department of Neurosurgery, University of Michigan, Ann Arbor, 48109, United States; email: pgpatil@umich.edu},
publisher = {IOP Publishing Ltd custserv@iop.org},
issn = {17412560},
pmid = {33047675},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Feng2020Decoding,
author = {Feng, Naishi and Hu, Fo and Wang, Hong and Gouda, Mohamed Amin},
title = {Decoding of voluntary and involuntary upper-limb motor imagery based on graph fourier transform and cross-frequency coupling coefficients},
year = {2020},
journal = {Journal of Neural Engineering},
volume = {17},
number = {5},
pages = {},
doi = {10.1088/1741-2552/abc024},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095668130&doi=10.1088%2F1741-2552%2Fabc024&partnerID=40&md5=e5aec6a62c945281c6d5349d503445e5},
affiliations = {School of Mechanical Engineering and Automation, Northeastern University, Shenyang, Liaoning, China},
abstract = {Objective. Brain-computer interface (BCI) technology based on motor imagery (MI) control has become a research hotspot but continues to encounter numerous challenges. BCI can assist in the recovery of stroke patients and serve as a key technology in robot control. Current research on MI almost exclusively focuses on the hands, feet, and tongue. Therefore, the purpose of this paper is to establish a four-class MI BCI system, in which the four types are the four articulations within the right upper limbs, involving the shoulder, elbow, wrist, and hand. Approach. Ten subjects were chosen to perform nine upper-limb analytic movements, after which the differences were compared in P300, movement-related potentials(MRPS), and event-related desynchronization/event-related synchronization under voluntary MI (V-MI) and involuntary MI (INV-MI). Next, the cross-frequency coupling (CFC) coefficient based on mutual information was extracted from the electrodes and frequency bands with interest. Combined with the image Fourier transform and twin bounded support vector machine classifier, four kinds of electroencephalography data were classified, and the classifier's parameters were optimized using a genetic algorithm. Main results. The results were shown to be encouraging, with an average accuracy of 93.2% and 92.2% for V-MI and INV-MI, respectively, and over 95% for any three classes and any two classes. In most cases, the accuracy of feature extraction using the proximal articulations as the basis was found to be relatively high and had better performance. Significance. This paper discussed four types of MI according to three aspects under two modes and classed them by combining graph Fourier transform and CFC. Accordingly, the theoretical discussion and classification methods may provide a fundamental theoretical basis for BCI interface applications. © 2020 IOP Publishing Ltd.}, keywords = {Classification (of information); Electroencephalography; Electrophysiology; Fourier series; Genetic algorithms; Patient rehabilitation; Support vector machines; Bounded support vector machines; Classification methods; Crossfrequency couplings (CFC); Event related desynchronization; Graph Fourier transforms; Interface applications; Movement related potentials; Mutual informations; Brain computer interface; adult; Article; convalescence; cortical synchronization; elbow; electroencephalography; evoked response; feature extraction; female; foot; Fourier transform; genetic algorithm; hand; human; human experiment; imagery; involuntary commitment; latent period; male; measurement accuracy; normal human; priority journal; reference electrode; robotics; shoulder; signal processing; stroke patient; support vector machine; technology; tongue; twin support vector machine; upper limb; voluntary movement; wrist; brain computer interface; Fourier analysis; imagination; Brain-Computer Interfaces; Fourier Analysis; Hand; Humans; Imagination},
correspondence_address = {H. Wang; Department of Mechanical Engineering and Automation, Northeastern University, Shenyang City, Liaoning, China; email: hongwang@mail.neu.edu.cn},
publisher = {IOP Publishing Ltd},
issn = {17412560},
pmid = {33045685},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{Glaser2020Machine,
author = {Glaser, Joshua I. and Benjamin, Ari S. and Chowdhury, Raeed H. and Perich, Matthew G. and Miller, Lee E. and Kording, Konrad P.},
title = {Machine learning for neural decoding},
year = {2020},
journal = {eNeuro},
volume = {7},
number = {4},
pages = {1 - 16},
doi = {10.1523/ENEURO.0506-19.2020},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090177300&doi=10.1523%2FENEURO.0506-19.2020&partnerID=40&md5=41ef00a322b7113cdb855aaae75e4ab6},
affiliations = {Northwestern University, Interdepartmental Neuroscience Program, Evanston, IL, United States; Northwestern University Feinberg School of Medicine, Department of Physical Medicine and Rehabilitation, Chicago, IL, United States; Shirley Ryan AbilityLab, Chicago, IL, United States; Northwestern University Feinberg School of Medicine, Department of Physiology, Chicago, IL, United States; Robert R. McCormick School of Engineering and Applied Science, Evanston, IL, United States; Robert R. McCormick School of Engineering and Applied Science, Evanston, IL, United States; School of Engineering and Applied Science, School of Engineering and Applied Science, Philadelphia, PA, United States; University of Pennsylvania Perelman School of Medicine, Department of Neuroscience, Philadelphia, PA, United States; Columbia University, Department of Statistics, New York, NY, United States; Columbia University, New York, NY, United States},
abstract = {Despite rapid advances in machine learning tools, the majority of neural decoding approaches still use traditional methods. Modern machine learning tools, which are versatile and easy to use, have the potential to significantly improve decoding performance. This tutorial describes how to effectively apply these algorithms for typical decoding problems. We provide descriptions, best practices, and code for applying common machine learning methods, including neural networks and gradient boosting. We also provide detailed comparisons of the performance of various methods at the task of decoding spiking activity in motor cortex, somatosensory cortex, and hippocampus. Modern methods, particularly neural networks and ensembles, significantly outper-form traditional approaches, such as Wiener and Kalman filters. Improving the performance of neural decoding algorithms allows neuroscientists to better understand the information contained in a neural population and can help to advance engineering applications such as brain–machine interfaces. Our code package is available at github.com/kordinglab/neural_decoding. © 2020 Glaser et al.}, keywords = {algorithm; Article; artificial neural network; benchmarking; BOLD signal; cross validation; electroencephalography; functional magnetic resonance imaging; hippocampus; human; kinematics; learning algorithm; local field potential; machine learning; maximum likelihood method; motor cortex; nerve cell network; neural decoding; predictive value; process optimization; recurrent neural network; sensitivity analysis; somatosensory cortex; support vector machine; task performance; brain computer interface; Algorithms; Brain-Computer Interfaces; Machine Learning; Motor Cortex; Neural Networks, Computer},
correspondence_address = {J.I. Glaser; Interdepartmental Neuroscience Program, Northwestern University, Chicago, 60611, United States; email: joshglaser88@gmail.com; J.I. Glaser; Interdepartmental Neuroscience Program, Northwestern University, Chicago, 60611, United States; email: joshglaser88@gmail.com; J.I. Glaser; Interdepartmental Neuroscience Program, Northwestern University, Chicago, 60611, United States; email: joshglaser88@gmail.com; J.I. Glaser; Interdepartmental Neuroscience Program, Northwestern University, Chicago, 60611, United States; email: joshglaser88@gmail.com; J.I. Glaser; Interdepartmental Neuroscience Program, Northwestern University, Chicago, 60611, United States; email: joshglaser88@gmail.com; J.I. Glaser; Interdepartmental Neuroscience Program, Northwestern University, Chicago, 60611, United States; email: joshglaser88@gmail.com},
publisher = {Society for Neuroscience},
issn = {23732822},
pmid = {32737181},
language = {English},
abbrev_source_title = {eNeuro},
type = {Article}}
@article{Kalaganis2020Data,
author = {Kalaganis, Fotis P. and Laskaris, Nikolaos A. and Chatzilari, Elisavet and Nikolopoulos, Spiros and Kompatsiaris, Ioannis (Yiannis)},
title = {A data augmentation scheme for geometric deep learning in personalized brainficomputer interfaces},
year = {2020},
journal = {IEEE Access},
volume = {8},
pages = {162218 - 162229},
doi = {10.1109/ACCESS.2020.3021580},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102860472&doi=10.1109%2FACCESS.2020.3021580&partnerID=40&md5=8fdc35ea82de2c0eea037bd8e0121ae4},
affiliations = {Aristotle University of Thessaloniki, Department of Informatics, Thessaloniki, Central Macedonia, Greece; Centre for Research and Technology-Hellas, MKLab, Thessaloniki, Macedonia, Greece; Aristotle University of Thessaloniki, NeuroInformatics.GRoup, Thessaloniki, Central Macedonia, Greece},
abstract = {Electroencephalography signals inherently deviate from the notion of regular spatial sampling, as they refiect the coordinated action from multiple distributed overlapping cortical networks. Hence, the observed brain dynamics are inluenced both by the topology of the sensor array and the underlying functional connectivity. Neural engineers are currently exploiting the advances in the domain of graph signal processing in an attempt to create robust and reliable brain decoding systems. In this direction, Geometric Deep Learning is a highly promising concept for combining the benefits of graph signal processing and deep learning towards revolutionising Brain-Computer Interfaces (BCIs). However, its exploitation has been hindered by its data-demanding character. As a remedy, we propose here a novel data augmentation approach that combines the multiplex network modelling of multichannel signal with a graph variant of the classical Empirical Mode Decomposition (EMD), and which proves to be a strong asset when combined with Graph Convolutional Neural Networks (GCNNs). As our graph-EMD algorithm makes no assumptions with respect to linearity and stationarity, it appears as an appealing solution towards analysing brain signals without artificially imposing regularities in either temporal or spatial domain. Our experimental results indicate that the proposed scheme for data augmentation leads to substantial improvement when it is combined with GCNNs. Using recordings from two distinct BCI applications and comparing against a state-of-theart augmentation method, we illustrate the benefits from its use. By making it available to BCI community, we hope to further foster the application of geometric deep learning in the field. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.}, keywords = {Biomedical signal processing; Brain computer interface; Convolutional neural networks; Electroencephalography; Electrophysiology; Geometry; Graph algorithms; Augmentation methods; Brain computer interfaces (BCIs); Coordinated actions; Empirical Mode Decomposition; Functional connectivity; Multichannel signals; Multiplex networks; Regular spatial samplings; Deep learning},
correspondence_address = {F.P. Kalaganis; AIIA Laboratory, Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, 54124, Greece; email: kalaganis@csd.auth.gr},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21693536},
language = {English},
abbrev_source_title = {IEEE Access},
type = {Article}}
@article{Kaneshiro2020Natural,
author = {Kaneshiro, Blair and Nguyen, Duc T. and Norcia, Anthony Matthew and Dmochowski, Jacek P. and Berger, Jonathan J.},
title = {Natural music evokes correlated EEG responses reflecting temporal structure and beat},
year = {2020},
journal = {NeuroImage},
volume = {214},
pages = {},
doi = {10.1016/j.neuroimage.2020.116559},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082744333&doi=10.1016%2Fj.neuroimage.2020.116559&partnerID=40&md5=cfa3feb9c2c24bb6d1b6461aed75d178},
affiliations = {Stanford University, Stanford, CA, United States; Stanford University, Stanford, CA, United States; Stanford University School of Medicine, Department of Otolaryngology-Head and Neck Surgery, Stanford, CA, United States; The Grove School of Engineering, New York, NY, United States; Stanford University, Stanford, CA, United States},
abstract = {The brain activity of multiple subjects has been shown to synchronize during salient moments of natural stimuli, suggesting that correlation of neural responses indexes a brain state operationally termed ‘engagement’. While past electroencephalography (EEG) studies have considered both auditory and visual stimuli, the extent to which these results generalize to music—a temporally structured stimulus for which the brain has evolved specialized circuitry—is less understood. Here we investigated neural correlation during natural music listening by recording EEG responses from N=48 adult listeners as they heard real-world musical works, some of which were temporally disrupted through shuffling of short-term segments (measures), reversal, or randomization of phase spectra. We measured correlation between multiple neural responses (inter-subject correlation) and between neural responses and stimulus envelope fluctuations (stimulus-response correlation) in the time and frequency domains. Stimuli retaining basic musical features, such as rhythm and melody, elicited significantly higher behavioral ratings and neural correlation than did phase-scrambled controls. However, while unedited songs were self-reported as most pleasant, time-domain correlations were highest during measure-shuffled versions. Frequency-domain measures of correlation (coherence) peaked at frequencies related to the musical beat, although the magnitudes of these spectral peaks did not explain the observed temporal correlations. Our findings show that natural music evokes significant inter-subject and stimulus-response correlations, and suggest that the neural correlates of musical ‘engagement’ may be distinct from those of enjoyment. © 2020 The Authors}, keywords = {adult; article; controlled study; electroencephalogram; electroencephalography; female; human; human experiment; male; music; perception; randomization; randomized controlled trial; rhythm; stimulus response; adolescent; auditory stimulation; brain; brain mapping; hearing; physiology; procedures; young adult; Acoustic Stimulation; Adolescent; Adult; Auditory Perception; Brain; Brain Mapping; Electroencephalography; Female; Humans; Male; Music; Young Adult},
correspondence_address = {B. Kaneshiro; Center for Computer Research in Music and Acoustics, Stanford University, Stanford, United States; email: blairbo@ccrma.stanford.edu},
publisher = {Academic Press Inc. apjcs@harcourt.com},
issn = {10959572; 10538119},
coden = {NEIME},
pmid = {31978543},
language = {English},
abbrev_source_title = {NeuroImage},
type = {Article}}
@article{Kozhemiako2020Neural,
author = {Kozhemiako, Nataliia and Nunes, Adonay S. and Samal, Ashok K. and Rana, Kunjan D. and Calabro, Finnegan J. and Hämäläinen, Matti S. and Khan, Sheraz and Vaina, Lucia Maria},
title = {Neural activity underlying the detection of an object movement by an observer during forward self-motion: Dynamic decoding and temporal evolution of directional cortical connectivity},
year = {2020},
journal = {Progress in Neurobiology},
volume = {195},
pages = {},
doi = {10.1016/j.pneurobio.2020.101824},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086159410&doi=10.1016%2Fj.pneurobio.2020.101824&partnerID=40&md5=8dd1b51f11b9211287ffd534b46599dc},
affiliations = {Simon Fraser University, Department of Biomedical Physiology and Kinesiology, Burnaby, BC, Canada; Massachusetts General Hospital, Department of Radiology, Boston, MA, United States; Boston University Chobanian & Avedisian School of Medicine, Department of Biomedical Engineering, Boston, MA, United States; National Institute of Mental Health, Bethesda, MD, United States; University of Pittsburgh, Department of Biomedical Engineering, Pittsburgh, PA, United States; Harvard Medical School, Boston, MA, United States},
abstract = {Relatively little is known about how the human brain identifies movement of objects while the observer is also moving in the environment. This is, ecologically, one of the most fundamental motion processing problems, critical for survival. To study this problem, we used a task which involved nine textured spheres moving in depth, eight simulating the observer's forward motion while the ninth, the target, moved independently with a different speed towards or away from the observer. Capitalizing on the high temporal resolution of magnetoencephalography (MEG) we trained a Support Vector Classifier (SVC) using the sensor-level data to identify correct and incorrect responses. Using the same MEG data, we addressed the dynamics of cortical processes involved in the detection of the independently moving object and investigated whether we could obtain confirmatory evidence for the brain activity patterns used by the classifier. Our findings indicate that response correctness could be reliably predicted by the SVC, with the highest accuracy during the blank period after motion and preceding the response. The spatial distribution of the areas critical for the correct prediction was similar but not exclusive to areas underlying the evoked activity. Importantly, SVC identified frontal areas otherwise not detected with evoked activity that seem to be important for the successful performance in the task. Dynamic connectivity further supported the involvement of frontal and occipital-temporal areas during the task periods. This is the first study to dynamically map cortical areas using a fully data-driven approach in order to investigate the neural mechanisms involved in the detection of moving objects during observer's self-motion. © 2020 Elsevier Ltd}, keywords = {adult; Article; brain function; electroencephalogram; evoked cortical response; female; functional connectivity; human; human experiment; magnetoencephalography; male; motion; neuroimaging; normal human; nuclear magnetic resonance imaging; optic flow; priority journal; support vector machine; task performance; three alternative forced choice; velocity; vision; brain cortex; connectome; depth perception; movement perception; physiology; procedures; young adult; Adult; Cerebral Cortex; Connectome; Female; Humans; Magnetoencephalography; Male; Motion Perception; Optic Flow; Space Perception; Support Vector Machine; Young Adult},
correspondence_address = {L.M. Vaina; Athinoula A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Charlestown, United States; email: vaina@bu.edu},
publisher = {Elsevier Ltd},
issn = {03010082; 18735118},
coden = {PGNBA},
pmid = {32446882},
language = {English},
abbrev_source_title = {Prog. Neurobiol.},
type = {Article}}
@article{Lee2020Neural,
author = {Lee, Seohyun and Lee, Minji and Lee, Seongwhan},
title = {Neural Decoding of Imagined Speech and Visual Imagery as Intuitive Paradigms for BCI Communication},
year = {2020},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
volume = {28},
number = {12},
pages = {2647 - 2659},
doi = {10.1109/TNSRE.2020.3040289},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097125940&doi=10.1109%2FTNSRE.2020.3040289&partnerID=40&md5=d923719bbcddad2d9eed545f256674b2},
affiliations = {Korea University, Department of Brain and Cognitive Engineering, Seoul, South Korea},
abstract = {Brain-computer interface (BCI) is oriented toward intuitive systems that users can easily operate. Imagined speech and visual imagery are emerging paradigms that can directly convey a user's intention. We investigated the underlying characteristics that affect the decoding performance of these two paradigms. Twenty-two subjects performed imagined speech and visual imagery of twelve words/phrases frequently used for patients' communication. Spectral features were analyzed with thirteen-class classification (including rest class) using EEG filtered in six frequency ranges. In addition, cortical regions relevant to the two paradigms were analyzed by classification using single-channel and pre-defined cortical groups. Furthermore, we analyzed the word properties that affect the decoding performance based on the number of syllables, concrete and abstract concepts, and the correlation between the two paradigms. Finally, we investigated multiclass scalability in both paradigms. The high-frequency band displayed a significantly superior performance to that in the case of any other spectral features in the thirteen-class classification (imagined speech: 39.73 ± 5.64%; visual imagery: 40.14 ± 4.17%). Furthermore, the performance of Broca's and Wernicke's areas and auditory cortex was found to have improved among the cortical regions in both paradigms. As the number of classes increased, the decoding performance decreased moderately. Moreover, every subject exceeded the confidence level performance, implying the strength of the two paradigms in BCI inefficiency. These two intuitive paradigms were found to be highly effective for multiclass communication systems, having considerable similarities between each other. The results could provide crucial information for improving the decoding performance for practical BCI applications. © 2001-2011 IEEE.}, keywords = {Classification (of information); Decoding; Speech communication; Abstract concept; Confidence levels; Cortical regions; Decoding performance; Frequency ranges; High frequency bands; Neural decoding; Spectral feature; Brain computer interface; adult; aphasia; Article; artificial neural network; auditory cortex; auditory stimulation; brain region; clinical article; data analysis; electroencephalography; Fourier transform; human; human experiment; machine learning; male; motor cortex; neural decoding; neuroscience; normal human; performance; receiver operating characteristic; sensory cortex; signal noise ratio; signal processing; spatial analysis; speech; support vector machine; topography; validation study; verbal communication; working memory; young adult; behavior; brain computer interface; Auditory Cortex; Brain-Computer Interfaces; Electroencephalography; Humans; Intention; Speech},
correspondence_address = {S.-W. Lee; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; email: sw.lee@korea.ac.kr},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {15580210; 15344320},
coden = {ITNSB},
pmid = {33232243},
language = {English},
abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
type = {Article}}
@article{Losorelli2020Factors,
author = {Losorelli, Steven D. and Kaneshiro, Blair and Musacchia, Gabriella A. and Blevins, Nikolas H. and Fitzgerald, Matthew B.},
title = {Factors influencing classification of frequency following responses to speech and music stimuli},
year = {2020},
journal = {Hearing Research},
volume = {398},
pages = {},
doi = {10.1016/j.heares.2020.108101},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094821273&doi=10.1016%2Fj.heares.2020.108101&partnerID=40&md5=c3aa78a5d3b5715a4d46884029879d28},
affiliations = {Stanford University School of Medicine, Department of Otolaryngology-Head and Neck Surgery, Stanford, CA, United States; University of the Pacific Arthur A. Dugoni School of Dentistry, Department of Audiology, San Francisco, CA, United States},
abstract = {Successful mapping of meaningful labels to sound input requires accurate representation of that sound's acoustic variances in time and spectrum. For some individuals, such as children or those with hearing loss, having an objective measure of the integrity of this representation could be useful. Classification is a promising machine learning approach which can be used to objectively predict a stimulus label from the brain response. This approach has been previously used with auditory evoked potentials (AEP) such as the frequency following response (FFR), but a number of key issues remain unresolved before classification can be translated into clinical practice. Specifically, past efforts at FFR classification have used data from a given subject for both training and testing the classifier. It is also unclear which components of the FFR elicit optimal classification accuracy. To address these issues, we recorded FFRs from 13 adults with normal hearing in response to speech and music stimuli. We compared labeling accuracy of two cross-validation classification approaches using FFR data: (1) a more traditional method combining subject data in both the training and testing set, and (2) a “leave-one-out” approach, in which subject data is classified based on a model built exclusively from the data of other individuals. We also examined classification accuracy on decomposed and time-segmented FFRs. Our results indicate that the accuracy of leave-one-subject-out cross validation approaches that obtained in the more conventional cross-validation classifications while allowing a subject's results to be analysed with respect to normative data pooled from a separate population. In addition, we demonstrate that classification accuracy is highest when the entire FFR is used to train the classifier. Taken together, these efforts contribute key steps toward translation of classification-based machine learning approaches into clinical practice. © 2020}, keywords = {adult; article; auditory evoked potential; classifier; clinical article; clinical practice; controlled study; cross validation; decomposition; electroencephalogram; female; hearing; human; human experiment; male; music; speech; auditory stimulation; electroencephalography; hearing impairment; speech perception; Acoustic Stimulation; Electroencephalography; Evoked Potentials, Auditory; Hearing Loss; Humans; Music; Speech; Speech Perception},
correspondence_address = {S. Losorelli; Department of Otolaryngology Head and Neck Surgery, Stanford University School of Medicine, Palo Alto, United States; email: slosorelli@stanford.edu},
publisher = {Elsevier B.V.},
issn = {03785955; 18785891},
coden = {HERED},
pmid = {33142106},
language = {English},
abbrev_source_title = {Hear. Res.},
type = {Article}}
@article{Loutit2020Dorsal,
author = {Loutit, Alastair J. and Potas, Jason R.},
title = {Dorsal Column Nuclei Neural Signal Features Permit Robust Machine-Learning of Natural Tactile- and Proprioception-Dominated Stimuli},
year = {2020},
journal = {Frontiers in Systems Neuroscience},
volume = {14},
pages = {},
doi = {10.3389/fnsys.2020.00046},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089413368&doi=10.3389%2Ffnsys.2020.00046&partnerID=40&md5=a49cc688843ba1efe583624bcd459bfc},
affiliations = {UNSW Sydney, School of Medical Sciences, Sydney, NSW, Australia; The John Curtin School of Medical Research, Canberra, ACT, Australia},
abstract = {Neural prostheses enable users to effect movement through a variety of actuators by translating brain signals into movement control signals. However, to achieve more natural limb movements from these devices, the restoration of somatosensory feedback is required. We used feature-learnability, a machine-learning approach, to assess signal features for their capacity to enhance decoding performance of neural signals evoked by natural tactile and proprioceptive somatosensory stimuli, recorded from the surface of the dorsal column nuclei (DCN) in urethane-anesthetized rats. The highest performing individual feature, spike amplitude, classified somatosensory DCN signals with 70% accuracy. The highest accuracy achieved was 87% using 13 features that were extracted from both high and low-frequency (LF) bands of DCN signals. In general, high-frequency (HF) features contained the most information about peripheral somatosensory events, but when features were acquired from short time-windows, classification accuracy was significantly improved by adding LF features to the feature set. We found that proprioception-dominated stimuli generalize across animals better than tactile-dominated stimuli, and we demonstrate how information that signal features contribute to neural decoding changes over the time-course of dynamic somatosensory events. These findings may inform the biomimetic design of artificial stimuli that can activate the DCN to substitute somatosensory feedback. Although, we investigated somatosensory structures, the feature set we investigated may also prove useful for decoding other (e.g., motor) neural signals. © Copyright © 2020 Loutit and Potas.}, keywords = {poly(ethyl 2 cyanoacrylate); urethan; algorithm; animal experiment; Article; artificial neural network; auditory stimulation; controlled study; electrocorticography; electroencephalography; machine learning; male; mechanical stimulation; mechanical stimulus test; nonhuman; proprioception; rat; signal processing; somatosensory cortex; somatosensory evoked potential; spectroscopy; tactile stimulation},
correspondence_address = {J.R. Potas; School of Medical Sciences, University of New South Wales Sydney, Kensington, Australia; email: j.potas@unsw.edu.au; J.R. Potas; School of Medical Sciences, University of New South Wales Sydney, Kensington, Australia; email: j.potas@unsw.edu.au},
publisher = {Frontiers Media S.A.},
issn = {16625137},
language = {English},
abbrev_source_title = {Front. Syst. Neurosci.},
type = {Article}}
@article{Mishra2020Analyzing,
author = {Mishra, Rahul and Bhavsar, Arnav V.},
title = {Analyzing Image Classification via EEG},
year = {2020},
journal = {Communications in Computer and Information Science},
volume = {1249},
pages = {537 - 547},
doi = {10.1007/978-981-15-8697-2_50},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097270324&doi=10.1007%2F978-981-15-8697-2_50&partnerID=40&md5=dd8181a3f0729d1016e420e2d306d685},
affiliations = {Indian Institute of Technology Mandi, Mandi, HP, India},
abstract = {Electroencephalogram (EEG) has been a popular technique for brain-computer interface (BCI) and brain decoding studies. However, decoding perceptual information (e.g. images and sound) is only recently being considered. In this work, we make an attempt to experimentally analyze and address the task of classification of images that a person sees, based on the captured EEG during the visual task. For this work, we used a publicly available dataset and, as a part of our analysis, find some important concerns associated with that dataset, which also highlights challenges in EEG based image classification. After we process the data to address these concerns, we show that there may still be some discriminative traits in the EEG data for classifying a small number of classes. © 2020, Springer Nature Singapore Pte Ltd.}, keywords = {Brain computer interface; Classification (of information); Computer vision; Decoding; Electroencephalography; Image analysis; Brain decoding; Eeg datum; Electro-encephalogram (EEG); Number of class; Perceptual information; Visual tasks; Image classification},
correspondence_address = {R. Mishra; Multimedia Analytics, Networks and Systems Lab, School of Computing and Electrical Engineering, IIT Mandi, Mandi, India; email: d16043@students.iitmandi.ac.in},
editor = {Babu, R.V. and Prasanna, M. and Namboodiri, V.P.},
publisher = {Springer Science and Business Media Deutschland GmbH},
issn = {18650937; 18650929},
isbn = {9789819671748; 9789819664610; 9783032026743; 9783032008831; 9783032026712; 9789819671779; 9783031949425; 9789819666874; 9783031936968; 9783031941207},
language = {English},
abbrev_source_title = {Commun. Comput. Info. Sci.},
type = {Conference paper}}
@article{Sharon2020Neural,
author = {Sharon, Rini A. and Narayanan, Shrikanth Shri S. and Sur, Mriganka and Murthy, Hema A.},
title = {Neural Speech Decoding during Audition, Imagination and Production},
year = {2020},
journal = {IEEE Access},
volume = {8},
pages = {149714 - 149729},
doi = {10.1109/ACCESS.2020.3016756},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090275967&doi=10.1109%2FACCESS.2020.3016756&partnerID=40&md5=84b24fb2f61ec4014b83e1111dae1906},
affiliations = {Indian Institute of Technology Madras, Department of Computer Science and Engineering, Chennai, TN, India; USC Viterbi School of Engineering, Los Angeles, CA, United States; Massachusetts Institute of Technology, Cambridge, MA, United States},
abstract = {Interpretation of neural signals to a form that is as intelligible as speech facilitates the development of communication mediums for the otherwise speech/motor-impaired individuals. Speech perception, production, and imagination often constitute phases of human communication. The primary goal of this article is to analyze the similarity between these three phases by studying electroencephalogram(EEG) patterns across these modalities, in order to establish their usefulness for brain computer interfaces. Neural decoding of speech using such non-invasive techniques necessitates the optimal choice of signal analysis and translation protocols. By employing selection-by-exclusion based temporal modeling algorithms, we discover fundamental syllable-like units that reveal similar set of signal signatures across all the three phases. Significantly higher than chance accuracies are recorded for single trial multi-unit EEG classification using machine learning approaches over three datasets across 30 subjects. Repeatability and subject independence tests performed at every step of the analysis further strengthens the findings and holds promise for translating brain signals to speech non-invasively. © 2013 IEEE.}, keywords = {Brain computer interface; Classification (of information); Decoding; Electroencephalography; Speech; Communication medium; EEG classification; Electro-encephalogram (EEG); Human communications; Independence tests; Machine learning approaches; Noninvasive technique; Temporal modeling; Speech communication},
correspondence_address = {S.S. Narayanan; Viterbi School of Engineering, University of Southern California, Los Angeles, 90007, United States; email: ee15d210@smail.iitm.ac.in},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
issn = {21693536},
language = {English},
abbrev_source_title = {IEEE Access},
type = {Article}}
@article{Shiraishi2020Neural,
author = {Shiraishi, Yoshiyuki and Kawahara, Yoshinobu and Yamashita, Okito and Fukuma, Ryohei and Yamamoto, Shota and Saitoh, Youichi and Kishima, Haruhiko and Yanagisawa, Takufumi},
title = {Neural decoding of electrocorticographic signals using dynamic mode decomposition},
year = {2020},
journal = {Journal of Neural Engineering},
volume = {17},
number = {3},
pages = {},
doi = {10.1088/1741-2552/ab8910},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085715328&doi=10.1088%2F1741-2552%2Fab8910&partnerID=40&md5=0ab9861860bd279a28bdb22f8ad02a15},
affiliations = {The University of Osaka, Institute for Advanced Co-Creation Studies, Suita, Osaka, Japan; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; Kyushu University, Institute of Mathematics for Industry, Fukuoka, Fukuoka, Japan; Advanced Telecommunications Research Institute International (ATR), Department of Computational Brain Imaging, Kyoto, Kyoto, Japan; Graduate School of Medicine, Department of Neurosurgery, Suita, Osaka, Japan; Graduate School of Medicine, Department of Neuromodulation and Neurosurgery, Suita, Osaka, Japan; Advanced Telecommunications Research Institute International (ATR), Department of Neuroinformatics, Kyoto, Kyoto, Japan},
abstract = {Objective. Brain-computer interfaces (BCIs) using electrocorticographic (ECoG) signals have been developed to restore the communication function of severely paralyzed patients. However, the limited amount of information derived from ECoG signals hinders their clinical applications. We aimed to develop a method to decode ECoG signals using spatiotemporal patterns characterizing movement types to increase the amount of information gained from these signals. Approach. Previous studies have demonstrated that motor information could be decoded using powers of specific frequency bands of the ECoG signals estimated by fast Fourier transform (FFT) or wavelet analysis. However, because FFT is evaluated for each channel, the temporal and spatial patterns among channels are difficult to evaluate. Here, we used dynamic mode decomposition (DMD) to evaluate the spatiotemporal pattern of ECoG signals and evaluated the accuracy of motor decoding with the DMD modes. We used ECoG signals during three types of hand movements, which were recorded from 11 patients implanted with subdural electrodes. From the signals at the time of the movements, the modes and powers were evaluated by DMD and FFT and were decoded using support vector machine. We used the Grassmann kernel to evaluate the distance between modes estimated by DMD (DMD mode). In addition, we decoded the DMD modes, in which the phase components were shuffled, to compare the classification accuracy. Main results. The decoding accuracy using DMD modes was significantly better than that using FFT powers. The accuracy significantly decreased when the phases of the DMD mode were shuffled. Among the frequency bands, the DMD mode at approximately 100 Hz demonstrated the highest classification accuracy. Significance. DMD successfully captured the spatiotemporal patterns characterizing the movement types and contributed to improving the decoding accuracy. This method can be applied to improve BCIs to help severely paralyzed patients communicate. © 2020 The Author(s). Published by IOP Publishing Ltd.},
keywords = {Biomedical signal processing; Brain computer interface; Decoding; Electroencephalography; Electrophysiology; Fast Fourier transforms; Support vector machines; Amount of information; Classification accuracy; Communication functions; Decomposition modes; Dynamic mode decompositions; Electrocorticographic signals; Neural decoding; Paralyzed patients; Power; Spatiotemporal patterns; Dynamic mode decomposition; adolescent; adult; aged; Article; clinical article; controlled study; decomposition; dural arteriovenous fistula; dynamic mode decomposition; electrocorticography; female; Fourier transform; hand movement; human; intractable epilepsy; male; middle aged; motor dysfunction; motor performance; priority journal; sensorimotor cortex; spatiotemporal analysis; support vector machine; task performance; wavelet analysis; brain computer interface; electroencephalography; hand; motor cortex; movement (physiology); Brain-Computer Interfaces; Electrocorticography; Hand; Humans; Motor Cortex; Movement},
publisher = {Institute of Physics},
issn = {17412560},
coden = {JNEOB},
pmid = {32289756},
language = {English},
abbrev_source_title = {J. Neural Eng.},
type = {Article}}
@article{TaschereauDumouchel2020Could,
author = {Taschereau-Dumouchel, Vincent and Roy, Mathieu L.},
title = {Could Brain Decoding Machines Change Our Minds?},
year = {2020},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {856 - 858},
doi = {10.1016/j.tics.2020.09.006},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091680224&doi=10.1016%2Fj.tics.2020.09.006&partnerID=40&md5=43426363803a6272c5acfb1e83e77739},
affiliations = {University of California, Los Angeles, Los Angeles, CA, United States; Université McGill, Department of Psychology, Montreal, QC, Canada},
abstract = {In a recent experiment, Zhang and colleagues designed a closed-loop brain–machine interface that learned to reduce participants’ pain by decoding pain-related brain activity. In doing so, they also highlighted some of the challenges associated with coadaptive processes in brain–machine communication. © 2020 Elsevier Ltd}, keywords = {Decoding; Brain activity; Brain decoding; Closed loops; Machine communications; Machine interfaces; Brain; brain depth stimulation; brain function; electroencephalogram; electroencephalography; human; neurofeedback; pain; reinforcement learning (machine learning); sensory feedback; Short Survey; brain; brain computer interface; brain mapping; learning; Brain Mapping; Brain-Computer Interfaces; Humans; Learning; Pain},
correspondence_address = {M. Roy; Department of Psychology, McGill University, Montreal, Canada; email: mathieu.roy3@mcgill.ca},
publisher = {Elsevier Ltd},
issn = {13646613; 1879307X},
isbn = {9781613244616},
coden = {TCSCF},
pmid = {32994059},
language = {English},
abbrev_source_title = {Trends Cogn. Sci.},
type = {Short survey}}
@article{Tovar2020Selective,
author = {Tovar, David A. and Murray, Micah M. and Wallace, Mark T.},
title = {Selective enhancement of object representations through multisensory integration},
year = {2020},
journal = {Journal of Neuroscience},
volume = {40},
number = {29},
pages = {5604 - 5615},
doi = {10.1523/JNEUROSCI.2139-19.2020},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088234387&doi=10.1523%2FJNEUROSCI.2139-19.2020&partnerID=40&md5=00acda98b8a8b33bf0b8b37f649a7919},
affiliations = {Vanderbilt University School of Medicine, Nashville, TN, United States; Vanderbilt University School of Medicine, Nashville, TN, United States; Centre Hospitalier Universitaire Vaudois, Department of Radiology, Lausanne, VD, Switzerland; Center for Biomedical Imaging, Department of Sensory, Lausanne, VD, Switzerland; Université de Lausanne (UNIL), Lausanne, VD, Switzerland; Vanderbilt University Medical Center, Department of Hearing and Speech Sciences, Nashville, TN, United States; Vanderbilt University, Department of Psychology, Nashville, TN, United States; Vanderbilt University Medical Center, Department of Psychiatry and Behavioral Sciences, Nashville, TN, United States; Vanderbilt University School of Medicine, Nashville, TN, United States},
abstract = {Objects are the fundamental building blocks of how we create a representation of the external world. One major distinction among objects is between those that are animate versus those that are inanimate. In addition, many objects are specified by more than a single sense, yet the nature by which multisensory objects are represented by the brain remains poorly understood. Using representational similarity analysis of male and female human EEG signals, we show enhanced encoding of audiovisual objects when compared with their corresponding visual and auditory objects. Surprisingly, we discovered that the often-found processing advantages for animate objects were not evident under multisensory conditions. This was due to a greater neural enhancement of inanimate objects-which are more weakly encoded under unisensory conditions. Further analysis showed that the selective enhancement of inanimate audiovisual objects corresponded with an increase in shared representations across brain areas, suggesting that the enhancement was mediated by multisensory integration. Moreover, a distance-to-bound analysis provided critical links between neural findings and behavior. Improvements in neural decoding at the individual exemplar level for audiovisual inanimate objects predicted reaction time differences between multisensory and unisensory presentations during a Go/No-Go animate categorization task. Links between neural activity and behavioral measures were most evident at intervals of 100-200 ms and 350-500 ms after stimulus presentation, corresponding to time periods associated with sensory evidence accumulation and decision-making, respectively. Collectively, these findings provide key insights into a fundamental process the brain uses to maximize the information it captures across sensory systems to perform object recognition. © 2020 Society for Neuroscience. All rights reserved.}, keywords = {adult; article; decision making; electroencephalogram; female; human; male; reaction time; recognition; sensory system; auditory stimulation; brain; electroencephalography; hearing; photostimulation; physiology; vision; young adult; Acoustic Stimulation; Adult; Auditory Perception; Brain; Electroencephalography; Female; Humans; Male; Photic Stimulation; Recognition, Psychology; Visual Perception; Young Adult},
correspondence_address = {D.A. Tovar; School of Medicine, Vanderbilt University, Nashville, 37240, United States; email: david.tovar@vanderbilt.edu.; D.A. Tovar; School of Medicine, Vanderbilt University, Nashville, 37240, United States; email: david.tovar@vanderbilt.edu.},
publisher = {Society for Neuroscience},
issn = {02706474; 15292401},
coden = {JNRSD},
pmid = {32499378},
language = {English},
abbrev_source_title = {J. Neurosci.},
type = {Article}}
@article{Valentin2020Interpreting,
author = {Valentin, Simon and Harkotte, Maximilian and Popov, Tzvetan G.},
title = {Interpreting neural decoding models using grouped model reliance},
year = {2020},
journal = {PLOS Computational Biology},
volume = {16},
number = {1},
pages = {},
doi = {10.1371/journal.pcbi.1007148},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078511005&doi=10.1371%2Fjournal.pcbi.1007148&partnerID=40&md5=2da0645e80f81d40924bc66cf38d2457},
affiliations = {The University of Edinburgh, Edinburgh, Scotland, United Kingdom; Universität Konstanz, Department of Psychology, Konstanz, Baden-Wurttemberg, Germany; Eberhard Karls Universität Tübingen, Department of Psychology, Tubingen, Baden-Wurttemberg, Germany; Medizinische Fakultät Mannheim, Mannheim, Baden-Wurttemberg, Germany},
abstract = {Machine learning algorithms are becoming increasingly popular for decoding psychological constructs based on neural data. However, as a step towards bridging the gap between theory-driven cognitive neuroscience and data-driven decoding approaches, there is a need for methods that allow to interpret trained decoding models. The present study demonstrates grouped model reliance as a model-agnostic permutation-based approach to this problem. Grouped model reliance indicates the extent to which a trained model relies on conceptually related groups of variables, such as frequency bands or regions of interest in electroencephalographic (EEG) data. As a case study to demonstrate the method, random forest and support vector machine models were trained on within-participant single-trial EEG data from a Sternberg working memory task. Participants were asked to memorize a sequence of digits (0–9), varying randomly in length between one, four and seven digits, where EEG recordings for working memory load estimation were taken from a 3-second retention interval. The present results confirm previous findings insofar as both random forest and support vector machine models relied on alpha-band activity in most subjects. However, as revealed by further analyses, patterns in frequency and topography varied considerably between individuals, pointing to more pronounced inter-individual differences than previously reported. © 2020 Valentin et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords = {Decision trees; Decoding; Electroencephalography; Random forests; Support vector machines; Cognitive neurosciences; Data driven; Machine learning algorithms; Neural data; Neural decoding; Region-of-interest; Regions of interest; Support vector machine models; Working memory; Topography; adult; Agnostic; article; female; human; human experiment; male; random forest; support vector machine; topography; working memory; biological model; biology; brain; classification; cognitive neuroscience; decision tree; electroencephalography; machine learning; physiology; procedures; short term memory; task performance; young adult; Adult; Brain; Cognitive Neuroscience; Computational Biology; Decision Trees; Female; Humans; Machine Learning; Male; Memory, Short-Term; Models, Neurological; Support Vector Machine; Task Performance and Analysis; Young Adult},
correspondence_address = {S. Valentin; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom; email: simonvalentin@me.com},
publisher = {Public Library of Science},
issn = {1553734X; 15537358},
pmid = {31905373},
language = {English},
abbrev_source_title = {PLoS Comput. Biol.},
type = {Article}}
@article{Verschueren2020Effect,
author = {Verschueren, Eline and Vanthornhout, Jonas and Francart, Tom},
title = {The Effect of Stimulus Choice on an EEG-Based Objective Measure of Speech Intelligibility},
year = {2020},
journal = {Ear and Hearing},
volume = {41},
number = {6},
pages = {1586 - 1597},
doi = {10.1097/AUD.0000000000000875},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095394261&doi=10.1097%2FAUD.0000000000000875&partnerID=40&md5=afb184724e6271e8e7c03a647eef6692},
affiliations = {Departement Neurowetenschappen, Leuven, Vlaams-Brabant, Belgium},
abstract = {Objectives: Recently, an objective measure of speech intelligibility (SI), based on brain responses derived from the electroencephalogram (EEG), has been developed using isolated Matrix sentences as a stimulus. We investigated whether this objective measure of SI can also be used with natural speech as a stimulus, as this would be beneficial for clinical applications. Design: We recorded the EEG in 19 normal-hearing participants while they listened to two types of stimuli: Matrix sentences and a natural story. Each stimulus was presented at different levels of SI by adding speech weighted noise. SI was assessed in two ways for both stimuli: (1) behaviorally and (2) objectively by reconstructing the speech envelope from the EEG using a linear decoder and correlating it with the acoustic envelope. We also calculated temporal response functions (TRFs) to investigate the temporal characteristics of the brain responses in the EEG channels covering different brain areas. Results: For both stimulus types, the correlation between the speech envelope and the reconstructed envelope increased with increasing SI. In addition, correlations were higher for the natural story than for the Matrix sentences. Similar to the linear decoder analysis, TRF amplitudes increased with increasing SI for both stimuli. Remarkable is that although SI remained unchanged under the no-noise and +2.5 dB SNR conditions, neural speech processing was affected by the addition of this small amount of noise: TRF amplitudes across the entire scalp decreased between 0 and 150 ms, while amplitudes between 150 and 200 ms increased in the presence of noise. TRF latency changes in function of SI appeared to be stimulus specific: the latency of the prominent negative peak in the early responses (50 to 300 ms) increased with increasing SI for the Matrix sentences, but remained unchanged for the natural story. Conclusions: These results show (1) the feasibility of natural speech as a stimulus for the objective measure of SI; (2) that neural tracking of speech is enhanced using a natural story compared to Matrix sentences; and (3) that noise and the stimulus type can change the temporal characteristics of the brain responses. These results might reflect the integration of incoming acoustic features and top-down information, suggesting that the choice of the stimulus has to be considered based on the intended purpose of the measurement. © 2020 Lippincott Williams and Wilkins. All rights reserved.}, keywords = {adult; article; clinical article; controlled study; electroencephalogram; feasibility study; female; hearing; human; human experiment; male; scalp; speech intelligibility; auditory stimulation; electroencephalography; noise; speech perception; Acoustic Stimulation; Auditory Perception; Electroencephalography; Humans; Noise; Speech Intelligibility; Speech Perception},
correspondence_address = {E. Verschueren; Research Group Experimental Oto-rhino-laryngology (ExpORL), Department of Neurosciences, KU Leuven, University of Leuven, Leuven, Belgium; email: eline.verschueren@kuleuven.be},
publisher = {Wolters Kluwer Health},
issn = {15384667; 01960202},
coden = {EAHED},
pmid = {33136634},
language = {English},
abbrev_source_title = {Ear Hear.},
type = {Article}}
@article{Zhang2020Pain,
author = {Zhang, Suyi and Yoshida, Wako and Mano, Hiroaki and Yanagisawa, Takufumi and Mancini, Flavia and Shibata, Kazuhisa and Kawato, Mitsuo and Seymour, Ben},
title = {Pain Control by Co-adaptive Learning in a Brain-Machine Interface},
year = {2020},
journal = {Current Biology},
volume = {30},
number = {20},
pages = {3935 - 3944.e7},
doi = {10.1016/j.cub.2020.07.066},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090060871&doi=10.1016%2Fj.cub.2020.07.066&partnerID=40&md5=a3fa5e48664d789e79115e429e0399fb},
affiliations = {University of Cambridge, Department of Engineering, Cambridge, Cambridgeshire, United Kingdom; Advanced Telecommunications Research Institute International (ATR), Brain Information Communication Research Laboratory Group, Kyoto, Kyoto, Japan; National Institute of Information and Communications Technology, Koganei, Tokyo, Japan; The University of Osaka, Endowed Research Department of Clinical Neuroengineering, Suita, Osaka, Japan; Riken, Laboratory for Human Cognition and Learning, Wako, Saitama, Japan; University of Oxford Medical Sciences Division, Oxford, Oxfordshire, United Kingdom},
abstract = {Innovation in the field of brain-machine interfacing offers a new approach to managing human pain. In principle, it should be possible to use brain activity to directly control a therapeutic intervention in an interactive, closed-loop manner. But this raises the question as to whether the brain activity changes as a function of this interaction. Here, we used real-time decoded functional MRI responses from the insula cortex as input into a closed-loop control system aimed at reducing pain and looked for co-adaptive neural and behavioral changes. As subjects engaged in active cognitive strategies orientated toward the control system, such as trying to enhance their brain activity, pain encoding in the insula was paradoxically degraded. From a mechanistic perspective, we found that cognitive engagement was accompanied by activation of the endogenous pain modulation system, manifested by the attentional modulation of pain ratings and enhanced pain responses in pregenual anterior cingulate cortex and periaqueductal gray. Further behavioral evidence of endogenous modulation was confirmed in a second experiment using an EEG-based closed-loop system. Overall, the results show that implementing brain-machine control systems for pain induces a parallel set of co-adaptive changes in the brain, and this can interfere with the brain signals and behavior under control. More generally, this illustrates a fundamental challenge of brain decoding applications—that the brain inherently adapts to being decoded, especially as a result of cognitive processes related to learning and cooperation. Understanding the nature of these co-adaptive processes informs strategies to mitigate or exploit them.; Zhang et al. use real-time decoded fMRI responses from the insula cortex as input into a closed-loop control system aimed at reducing pain. They find that brain-machine interface control systems for pain induces a parallel set of co-adaptive changes in brain and behavior, illustrating the brain's ability to adapt to being decoded through learning. © 2020 The Authors; © 2020 The Authors}, keywords = {analgesia; brain computer interface; brain cortex; brain mapping; cingulate gyrus; electroencephalography; learning; nerve tract; neurofeedback; nuclear magnetic resonance imaging; pain; pathology; periaqueductal gray matter; physiology; procedures; Brain Mapping; Brain-Computer Interfaces; Cerebral Cortex; Electroencephalography; Gyrus Cinguli; Learning; Magnetic Resonance Imaging; Neural Pathways; Neurofeedback; Pain; Pain Management; Periaqueductal Gray},
correspondence_address = {S. Zhang; Computational and Biological Learning Laboratory, Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom; email: suyi.zhang@ndcn.ox.ac.uk; M. Kawato; Brain Information Communication Research Laboratory Group, Advanced Telecommunications Research Institute International, Kyoto, 619-0237, Japan; email: kawato@atr.jp; B. Seymour; Computational and Biological Learning Laboratory, Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom; email: ben.seymour@ndcn.ox.ac.uk},
publisher = {Cell Press subs@cell.com},
issn = {18790445; 09609822},
coden = {CUBLE},
pmid = {32795441},
language = {English},
abbrev_source_title = {Curr. Biol.},
type = {Article}}
@article{Zheng2020Decoding,
author = {Zheng, Xiao and Chen, Wanzhong and Li, Mingyang and Zhang, Tao and You, Yang and Jiang, Yun},
title = {Decoding human brain activity with deep learning},
year = {2020},
journal = {Biomedical Signal Processing and Control},
volume = {56},
pages = {},
doi = {10.1016/j.bspc.2019.101730},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073962696&doi=10.1016%2Fj.bspc.2019.101730&partnerID=40&md5=468973de59ac2b2e5b4b84f8968da6dd},
affiliations = {Jilin University, College of Communication Engineering, Changchun, Jilin, China},
abstract = {Building a brain-computer fusion system that would integrate biological intelligence and machine intelligence became a research topic of great concern. Recent research has proved that human brain activity can be decoded from neurological data. Meanwhile, deep learning has become an effective way to solve practical problems. Taking advantage of these trends, in this paper, we propose a novel method of decoding brain activity evoked by visual stimuli. To achieve this goal, we first introduce a combined long short-term memory—convolutional neural network (LSTM-CNN) architecture to extract the compact category-dependent representations of electroencephalograms (EEG). Our approach combines the ability of LSTM to extract sequential features and the capability of CNN to distil local features. Next, we employ an improved spectral normalization generative adversarial network (SNGAN) to conditionally generate images using the learned EEG features. We evaluate our approach in terms of the classification accuracy of EEG and the quality of the generated images. The results show that the proposed LSTM-CNN algorithm that discriminates the object classes by using EEG can be more accurate than the existing methods. In qualitative and quantitative tests, the improved SNGAN performs better in the task of generating conditional images from the learned EEG representations; the produced images are realistic and highly resemble the original images. Our method can reconstruct the content of visual stimuli according to the brain's response. Therefore, it helps to decode the human brain activity by using an image-EEG-image transformation. © 2019 Elsevier Ltd}, keywords = {Bioelectric phenomena; Convolutional neural networks; Decoding; Deep learning; Electroencephalography; Image enhancement; Long short-term memory; Neurophysiology; Adversarial networks; Brain decoding; Classification accuracy; Electro-encephalogram (EEG); Image transformations; Machine intelligence; Quantitative tests; Spectral normalization; Brain; article; brain function; convolutional neural network; deep learning; electroencephalogram; human; human experiment; quantitative analysis; short term memory},
correspondence_address = {W. Chen; College of Communication Engineering, Jilin University, Changchun, 130012, China; email: chenwz@jlu.edu.cn},
publisher = {Elsevier Ltd},
issn = {17468108; 17468094},
language = {English},
abbrev_source_title = {Biomed. Signal Process. Control},
type = {Article}}
@inproceedings{Luu2019Ieee,
author = {Luu, Trieu Phat and Eguren, David and Cestari, Manuel and Contreras-Vidal, Jose L.},
booktitle = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
title = {EEG-based Neural Decoding of Gait in Developing Children},
year = {2019},
volume = {},
number = {},
pages = {3608--3612},
abstract = {Neural decoding of human locomotion, including automated gait intention detection and continuous decoding of lower limb joint angles, has been of great interest in the field of Brain Machine Interface (BMI). However, neural decoding of gait in developing children has yet to be demonstrated. In this study, we collected physiological data (electroencephalography (EEG), electromyography (EMG)), and kinematic data from children performing different locomotion tasks. We also developed a state space estimation model to decode lower limb joint angles from scalp EEG. Fluctuations in the amplitude of slow cortical potentials of EEG in the delta band (0.1 - 3 Hz) were used for prediction. The decoding accuracies (Pearson's r values) were promising (Hip: 0.71; Knee: 0.59; Ankle: 0.51). Our results demonstrate the feasibility of neural decoding of children walking and have implications for the development of a real-time closed-loop BMI system for the control of a pediatric exoskeleton.},
keywords = {Electroencephalography;Decoding;Legged locomotion;Integrated circuits;Pediatrics;Brain modeling;Scalp;Brain-computer interface;neural decoding;children walking;EEG;EMG;gait},
doi = {10.1109/SMC.2019.8914380},
issn = {2577-1655},
month = {Oct}}
@article{Pei2019Neural,
author = {Pei, Dingyi and Patel, Vrajeshri and Burns, Martin and Chandramouli, Rajarathnam and Vinjamuri, Ramana},
journal = {IEEE Access},
title = {Neural Decoding of Synergy-Based Hand Movements Using Electroencephalography},
year = {2019},
volume = {7},
number = {},
pages = {18155--18163},
abstract = {The human central nervous system (CNS) effortlessly performs complex hand movements with the control and coordination of multiple degrees of freedom. It is hypothesized that the CNS might use kinematic synergies to reduce the complexity of movements, but how these kinematic synergies are encoded in the CNS remains unclear. In order to investigate the neural representations of kinematic synergies, scalp electroencephalographic (EEG) signals and hand kinematics were recorded from ten subjects during six representative types of hand grasping. Kinematic synergies were obtained from recorded hand kinematics using singular value decomposition. The recorded kinematics were then reconstructed using weighted linear combinations of synergies, and the optimal weights were computed using optimal linear estimation. Using EEG spectral powers as neural features, a multivariate linear regression model was trained on the weights of the kinematic synergies. Using this model, kinematics from the testing subset of data was decoded from the EEG features with threefold cross-validation. The results show that the weights of kinematic synergies used in a particular movement reconstruction were strongly correlated to EEG features obtained from that movement. EEG features were able to successfully decode synergy-based movements with an average decoding accuracy of 80.1 ± 6.1 % (best up to 93.4 ± 2.3%). These results have promising applications in noninvasive neural control of synergy-based prostheses and exoskeletons.},
keywords = {Electroencephalography;Kinematics;Task analysis;Grasping;Thumb;Electrodes;Brain modeling;Electroencephalography;principal component analysis;kinematic synergies;movement primitives;multivariate linear regression},
doi = {10.1109/ACCESS.2019.2895566},
issn = {2169-3536},
month = {}}